Bug text,Story point
"Job definitions request limits 20 results by default As a user, I'm trying to get all job definitions, but the first 20 alone are returned.

Job samples:
{code}
job create aaa --definition ""hello"" --deploy
job create bbb --definition ""hello"" --deploy
job create ccc --definition ""hello"" --deploy
job create ddd --definition ""hello"" --deploy
job create eee --definition ""hello"" --deploy
job create fff --definition ""hello"" --deploy
job create ggg --definition ""hello"" --deploy
job create hhh --definition ""hello"" --deploy
job create iii --definition ""hello"" --deploy
job create jjj --definition ""hello"" --deploy
job create kkk --definition ""hello"" --deploy
job create lll --definition ""hello"" --deploy
job create mmm --definition ""hello"" --deploy
job create nnn --definition ""hello"" --deploy
job create ooo --definition ""hello"" --deploy
job create ppp --definition ""hello"" --deploy
job create qqq --definition ""hello"" --deploy
job create rrr --definition ""hello"" --deploy
job create sss --definition ""hello"" --deploy
job create ttt --definition ""hello"" --deploy
job create uuu --definition ""hello"" --deploy
job create vvv --definition ""hello"" --deploy
job create www --definition ""hello"" --deploy
job create xxx --definition ""hello"" --deploy
job create yyy --definition ""hello"" --deploy
job create zzz --definition ""hello"" --deploy
job create aaa1 --definition ""hello"" --deploy
job create bbb1 --definition ""hello"" --deploy
job create ccc1 --definition ""hello"" --deploy
job create ddd1 --definition ""hello"" --deploy
job create eee1 --definition ""hello"" --deploy
{code}

Request:
{{http://localhost:9393/jobs/definitions.json}} - returns top 20; the other experiments with page size of either 0 or -1 still brings the top 20.",1
"Counter sink does not accept SpEL expressions As a user, I'm trying to use {{counter}} sink with {SpEL}} expression, but I'm not able to use them in combination. It [throws|https://github.com/spring-cloud/spring-cloud-stream-modules/blob/master/counter-sink/src/main/java/org/springframework/cloud/stream/module/metrics/CounterSinkProperties.java#L77] {{exactly one of 'name' and 'nameExpression' must be set}} as error message.

",1
"Execution list page includes child jobs in pagination scope As a user, I created a composed job with over 10 child jobs in the workflow; I expected to see 'a' job in the execution list page without any pagination, but instead I noticed empty pagination to skip to next page.",1
"Ensure Job definitions are escaped in UI If using the definition <aaa || bbb> where the definition starts with a ""<"" and ends with a "">"" the definition for the composed job does not appear on the definition page.",2
"log4j/log4j-over-slf4j logging issue I got below error when executing modules on yarn and it was written in appmaster stderr output.
{code}
Exception in thread ""Thread-2"" java.lang.NoClassDefFoundError: org/apache/log4j/spi/ThrowableInformation
        at org.apache.log4j.spi.LoggingEvent.<init>(LoggingEvent.java:165)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.log(Category.java:856)
        at org.slf4j.impl.Log4jLoggerAdapter.log(Log4jLoggerAdapter.java:595)
        at org.apache.commons.logging.impl.SLF4JLocationAwareLog.warn(SLF4JLocationAwareLog.java:192)
        at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:969)
        at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.doClose(EmbeddedWebApplicationContext.java:150)
        at org.springframework.context.support.AbstractApplicationContext$1.run(AbstractApplicationContext.java:893)
{code}

`LoggingEvent` is found from both `log4j-over-slf4j-1.7.12.jar` and `log4j-1.2.17.jar`. I suppose it depends on which one is used first to load this class.

Here's what we have in admin and appmaster jar files(spring-cloud-dataflow-yarn-build-tests is my local new sub-project to run tests on a hadoop minicluster):
{code}
unzip -l target/spring-cloud-dataflow-yarn-build-tests/spring-cloud-dataflow-yarn-appmaster-1.0.0.BUILD-SNAPSHOT.jar|grep jar|grep -i log
    62050  2013-05-16 22:04   lib/commons-logging-1.1.3.jar
   489884  2012-05-06 13:24   lib/log4j-1.2.17.jar
     8860  2015-03-26 21:56   lib/slf4j-log4j12-1.7.12.jar
     2234  2015-09-03 16:30   lib/spring-boot-starter-logging-1.3.0.M5.jar
    24567  2015-03-26 21:57   lib/log4j-over-slf4j-1.7.12.jar
    40824  2015-08-18 12:39   lib/tomcat-embed-logging-juli-8.0.26.jar
    66802  2015-05-28 09:49   lib/jboss-logging-3.3.0.Final.jar
{code}

{code}
unzip -l spring-cloud-dataflow-admin/target/spring-cloud-dataflow-admin-1.0.0.BUILD-SNAPSHOT.jar |grep jar|grep -i log
    62050  2013-05-16 22:04   lib/commons-logging-1.1.3.jar
   489884  2012-05-06 13:24   lib/log4j-1.2.17.jar
    40824  2015-08-18 12:39   lib/tomcat-embed-logging-juli-8.0.26.jar
    66802  2015-05-28 09:49   lib/jboss-logging-3.3.0.Final.jar
     2234  2015-09-03 16:30   lib/spring-boot-starter-logging-1.3.0.M5.jar
   280928  2015-03-24 12:06   lib/logback-classic-1.1.3.jar
   455041  2015-03-24 12:05   lib/logback-core-1.1.3.jar
    24567  2015-03-26 21:57   lib/log4j-over-slf4j-1.7.12.jar
{code}

Error went away when I removed `log4j-over-slf4j-1.7.12.jar` from maven deps for yarn appmaster jar. I suppose we have same issue with admin server.
",1
"Job Definitions page fails to display definitions if page  In this scenario we created 30 jobs that can be used for a composed job.  
if the composed job uses jobs in its composition that are not present on the first page of the of the result set the following exception is thrown.  

{noformat}
2015-11-02T14:47:17-0500 1.3.0.SNAP ERROR qtp1587928736-26 rest.RestControllerAdvice - Caught exception while handling a request
java.lang.IllegalStateException: Not all instances were looked at: fff
	at org.springframework.xd.dirt.rest.XDController.enhanceWithDeployments(XDController.java:244) ~[spring-xd-dirt-1.3.0.BUILD-SNAPSHOT.jar:1.3.0.BUILD-SNAPSHOT]
	at org.springframework.xd.dirt.rest.XDController.listValues(XDController.java:209) ~[spring-xd-dirt-1.3.0.BUILD-SNAPSHOT.jar:1.3.0.BUILD-SNAPSHOT]
	at org.springframework.xd.dirt.rest.JobsController.list(JobsController.java:128) ~[spring-xd-dirt-1.3.0.BUILD-SNAPSHOT.jar:1.3.0.BUILD-SNAPSHOT]
	at sun.reflect.GeneratedMethodAccessor133.invoke(Unknown Source) ~[na:na]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_67]
	at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_67]
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221) ~[spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137) ~[spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:806) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:729) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) [spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:735) [javax.servlet-3.0.0.v201112011016.jar:na]
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848) [javax.servlet-3.0.0.v201112011016.jar:na]
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:291) [spring-boot-actuator-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:102) [spring-boot-actuator-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:207) [spring-security-web-4.0.2.RELEASE.jar:4.0.2.RELEASE]
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:176) [spring-security-web-4.0.2.RELEASE.jar:4.0.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:90) [spring-boot-actuator-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557) [jetty-security-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.Server.handle(Server.java:370) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644) [jetty-http-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) [jetty-http-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667) [jetty-io-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52) [jetty-io-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608) [jetty-util-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543) [jetty-util-8.1.14.v20131031.jar:8.1.14.v20131031]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]
{noformat}",3
"Job composition fails for large transitions As a user, I'm trying to create a composed job with >20 steps/transitions using Rabbit as the message bus and it doesn't complete successfully.",3
"Fix composed job error message As a user, I'm trying to compose a job just with one definition; however, I'm getting the following error message, which could be misinterpreted.

{code}
xd:>job create salsa --definition timestampfile
Successfully created job 'salsa'
xd:>job create foo --definition ""salsa || salsa""
Successfully created job 'foo'
xd:>job create foo222 --definition ""salsa""
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Could not find module with name 'salsa' and type 'job'
{code}",1
"Multiple module instances produces duplicate messages  As a follow-up from [XD-3613|https://jira.spring.io/browse/XD-3629], we would want to fix this experience for Kafka message bus.",5
"CF SPI REST calls are not working  As a developer, I'd like to troubleshoot and fix {{root}} level access over CF SPI REST calls; they're broke at the moment. 

Access for following calls fail:

{code}

href: ""https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/streams""
href: ""https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/tasks""
href: ""https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/metrics/counters""
href: ""https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/metrics/counters/{name}"",
href: ""https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/modules""
href: ""https://s-c-dataflow-admin.cfapps.pez.pivotal.io:80/completions/stream{?start,detailLevel}"",
{code}",1
"UI: Task deployment page is not loading As a user, I'm trying to load Task, Task Deployment, and Task Executions page, but I'm seeing an error {{(Error fetching data. Is the XD server running?)}} instead. ",1
"UI: Job modules page wouldn't load As a user, I'm trying to load Job - Modules page in admin-ui, but I'm seeing exceptions in console and the page wouldn't load. 

{code}
Failed to convert value of type 'java.lang.String' to required type 'org.springframework.cloud.dataflow.core.ArtifactType'; nested exception is org.springframework.core.convert.ConversionFailedException: Failed to convert from type java.lang.String to type @org.springframework.web.bind.annotation.RequestParam org.springframework.cloud.dataflow.core.ArtifactType for value 'job'; nested exception is java.lang.IllegalArgumentException: No enum constant org.springframework.cloud.dataflow.core.ArtifactType.job
{code}",2
"Admin UI does not load on master build As a user, I cannot use {{admin-ui}} on the master build. It won't come up. ",2
Job Executions without Deployed Job (deleted) shall not be restartable ,1
"Turning on HA via Ambari plugin requires custom configuration As a user, I'd like to enable HA on {{namenode}} without having to enable custom configuration. 

More details [here|https://github.com/spring-projects/spring-xd-ambari/issues/6].",3
"Ambari plugin doesn't work with security_enabled It seems like springxd_shell will pull jhs principal and keytab from mapred-site.xml. When springxd_shell is installed in edge node, Amabri returns ""can't find jhs keytab"" and failed.

Details [here|https://github.com/spring-projects/spring-xd-ambari/issues/8].",1
"Accessing Admin REST APIs on CF returns unexpected results As an s-c-d user, I'm trying to access {{admin}} REST endpoints running on CF but I'm getting SSL authentication errors.
",1
"AdminServer fails on HDP 2.3 Submitting XD on YARN for HDP 2.3 fails due to some Solr issue in Boot - https://github.com/spring-projects/spring-boot/issues/2795

The xd-admin sysout is:

{code}
Started : AdminServerApplication
Documentation: https://github.com/spring-projects/spring-xd/wiki

02:51:36,624  ERROR main boot.SpringApplication - Application startup failed
java.lang.IllegalStateException: Error processing condition on org.springframework.boot.autoconfigure.solr.SolrAutoConfiguration.solrServer
	at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:58)
	at org.springframework.context.annotation.ConditionEvaluator.shouldSkip(ConditionEvaluator.java:102)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForBeanMethod(ConfigurationClassBeanDefinitionReader.java:178)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForConfigurationClass(ConfigurationClassBeanDefinitionReader.java:140)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitions(ConfigurationClassBeanDefinitionReader.java:116)
	at org.springframework.context.annotation.ConfigurationClassPostProcessor.processConfigBeanDefinitions(ConfigurationClassPostProcessor.java:333)
	at org.springframework.context.annotation.ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry(ConfigurationClassPostProcessor.java:243)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanDefinitionRegistryPostProcessors(PostProcessorRegistrationDelegate.java:273)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:98)
	at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:673)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:519)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:686)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:320)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.xd.dirt.server.admin.AdminServerApplication.run(AdminServerApplication.java:95)
	at org.springframework.xd.dirt.server.admin.AdminServerApplication.main(AdminServerApplication.java:79)
Caused by: java.lang.IllegalArgumentException: @ConditionalOnMissingBean annotations must specify at least one bean (type, name or annotation)
	at org.springframework.util.Assert.isTrue(Assert.java:68)
	at org.springframework.boot.autoconfigure.condition.OnBeanCondition$BeanSearchSpec.<init>(OnBeanCondition.java:223)
	at org.springframework.boot.autoconfigure.condition.OnBeanCondition.getMatchOutcome(OnBeanCondition.java:92)
	at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:45)
	... 17 more
02:51:36,628   WARN main annotation.AnnotationConfigApplicationContext - Exception thrown from LifecycleProcessor on context close
java.lang.IllegalStateException: LifecycleProcessor not initialized - call 'refresh' before invoking lifecycle methods via the context: org.springframework.context.annotation.AnnotationConfigApplicationContext@1cf1df22: startup date [Fri Oct 02 02:51:31 UTC 2015]; root of context hierarchy
	at org.springframework.context.support.AbstractApplicationContext.getLifecycleProcessor(AbstractApplicationContext.java:414)
	at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:966)
	at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:925)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:342)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.xd.dirt.server.admin.AdminServerApplication.run(AdminServerApplication.java:95)
	at org.springframework.xd.dirt.server.admin.AdminServerApplication.main(AdminServerApplication.java:79)
02:51:36,642  ERROR main admin.AdminServerApplication - Error processing condition on org.springframework.boot.autoconfigure.solr.SolrAutoConfiguration.solrServer
{code}
",3
"Fix classpath and servlet container issues Several issues with 1.3.0.M1 staged version

- we now use Tomcat instead of Jetty which prevent s xd-admin from starting on YARN

- we now have Guava 18.0 on classpath instead of 16.0.1

- xd-yarn push doesn't work, hadoop client for 2.7.1 needs Servlet API 

- updating Hadoop to 2.7.1 instead of 2.6.0
  -- this causes Curator to also update to 2.7.1 which throws exception on startup
",3
"Admin app crashes with SSL certification errors As a s-c-d user, I'm unable to push admin app to CF due to SSL certification errors while bootstrapping. 

Consider adding [CF trusted certificate|https://github.com/pivotal-cf/cloudfoundry-certificate-truster] as a CF SPI dependency.

Adding CF trusted certificate as dependency doesn't help either:

{code}
> Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
> Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:387)
> Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:292)
> Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.validator.Validator.validate(Validator.java:260)
> Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:324)
> Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:229)
> Fri Sep 25 2015 12:55:32 GMT-...
{code}",3
"Unable to set --closeTimeout on SCSM hdfs sink module Creating a stream like this:

  stream create --name myhdfsstream1 --definition ""time | hdfs --closeTimeout=5000"" --deploy

causes:

java.lang.IllegalArgumentException: Task executor must be set
        at org.springframework.util.Assert.notNull(Assert.java:115) ~[spring-core-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.data.hadoop.store.support.PollingTaskSupport.init(PollingTaskSupport.java:105) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]
        at org.springframework.data.hadoop.store.support.StoreObjectSupport.onInit(StoreObjectSupport.java:97) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]
        at org.springframework.data.hadoop.store.support.OutputStoreObjectSupport.onInit(OutputStoreObjectSupport.java:81) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]
        at org.springframework.data.hadoop.store.support.LifecycleObjectSupport.afterPropertiesSet(LifecycleObjectSupport.java:67) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]
        at org.springframework.cloud.stream.module.hdfs.sink.DataStoreWriterFactoryBean.afterPropertiesSet(DataStoreWriterFactoryBean.java:175) ~[hdfs-sink-1.0.0.BUILD-SNAPSHOT-exec.jar!/:1.0.0.BUILD-SNAPSHOT]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1637) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1574) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]        ... 35 common frames omittedWrapped by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataStoreWriter' defined in class path resource [org/springframework/cloud/stream/module/hdfs/sink/HdfsSinkConfiguration.class]: Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Task executor must be set
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1578) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:545) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:482) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:305) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:301) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBea",2
"Stream Destroy fails if stream deploy failed (From Eric)

Deploying using the following stream fails (probably because of issues around quoting):

`stream create foo --definition ""time | filter --expression=payload.contains('0') | log"" --deploy`

When you try to destroy the stream the destroy fails, which shouldn't happen whether the stream was valid or not.",2
"Support underscore delimited module args for module launcher If the module launcher's module arg is delimited by underscore (--args_0_fixedDelay=1), then boot ignores that property. It is important to support the underscore delimited property arguments as we set environment properties of these in CF and lattice environment.

The spring boot fix (https://github.com/spring-projects/spring-boot/commit/5a287455273270a20742f03e4546acde9e857bee) doesn't resolve the property if the value type of the Map is Map itself.",2
"Fix Cloud connector dependencies and service resolution This JIRA addresses couple of issues:
1) When the modules are deployed into cloud environment there is an issue where local configuration beans collide with cloud service beans. We witnessed an issue where there are two `RedisConnectionFactory` beans registered in the same application context.
We need to have a control the way in which the auto configuration gets invoked and service beans are created.
2) We need to align the cloud connector dependencies into a common place so that we don't have to specify them at various places like (SCS, SCS-Binder, SCS-modules) etc.,
It is a good idea to have these dependencies specified in SCS-modules so that it get used subsequently by SCS when the module is assembled at runtime.",3
"Can't build and run singlenode spring-cloud-data-rest app on Ubuntu Building and then running spring-cloud-data-rest app on Ubuntu fails when trying to create the first stream. The configuration ends up with a CloudFoundryConfig instead of LocalConfig for the moduleDeployer.

Env:
Ubuntu 15.04
java version ""1.8.0_51""
Java(TM) SE Runtime Environment (build 1.8.0_51-b16)
Java HotSpot(TM) 64-Bit Server VM (build 25.51-b03, mixed mode)

Error:
{code}
2015-08-10 11:43:47.199 ERROR 11062 --- [nio-9393-exec-1] o.s.c.d.r.c.RestControllerAdvice         : Caught exception while handling a request
java.lang.UnsupportedOperationException: null
	at org.springframework.cloud.data.module.deployer.cloudfoundry.CloudFoundryModuleDeployer.deploy(CloudFoundryModuleDeployer.java:30) ~[spring-cloud-data-module-deployer-cloudfoundry-1.0.0.BUILD-SNAPSHOT.jar!/:1.0.0.BUILD-SNAPSHOT]
	at org.springframework.cloud.data.rest.controller.StreamController.deployStream(StreamController.java:213) ~[spring-cloud-data-rest-1.0.0.BUILD-SNAPSHOT.jar!/:1.0.0.BUILD-SNAPSHOT]
	at org.springframework.cloud.data.rest.controller.StreamController.save(StreamController.java:140) ~[spring-cloud-data-rest-1.0.0.BUILD-SNAPSHOT.jar!/:1.0.0.BUILD-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_51]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_51]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_51]
	at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_51]
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221) ~[spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137) ~[spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:111) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:806) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:729) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872) [spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:648) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:729) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:291) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) [tomcat-embed-websocket-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:235) [spring-boot-actuator-1.3.0.BUILD-SNAPSHOT.jar!/:1.3.0.BUILD-SNAPSHOT]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:102) [spring-boot-actuator-1.3.0.BUILD-SNAPSHOT.jar!/:1.3.0.BUILD-SNAPSHOT]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:85) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:69) [spring-boot-actuator-1.3.0.BUILD-SNAPSHOT.jar!/:1.3.0.BUILD-SNAPSHOT]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:219) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:106) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:502) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:142) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:88) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:518) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1091) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:668) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1521) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1478) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
2015-08-10 11:43:47.284  WARN 11062 --- [nio-9393-exec-1] .m.m.a.ExceptionHandlerExceptionResolver : Handler execution resulted in exception: null
{code}",3
"Accessing step progress via REST fails with 403 As a XD user, I'm trying to access URI (- GET /jobs/executions//steps//progress => hasRole('ROLE_VIEW')), but it fails with 403 forbidden error for the role with view access. More details [here|https://issuetracker.springsource.com/browse/VESC-475].

Another URL with the same error: http://<HOST>:9393/streams/definitions.json?page=0&size=10",1
"Do not include optional dependencies automatically via 'includes' As a s-c-d developer, I'd like the 'includes' feature of the module launcher not to include optional dependencies, so that I can have better control over what gets added to the class path.",2
"Kafka Source must set autoStartup=false on KafkaMessageDrivenChannelAdapter If the value is not set, the source may start before being bound to the bus, throwing a ""Dispatcher has no subscribers"" error",3
"Make requirement for MD5 hash files configurable for the custom module registry  Post 1.2 upgrade, the custom modules no longer show up by just copying the jars to the {{xd.customModule.home}} directory. Instead I have to use the 'module upload' command to install the modules. This is because an MD5 file is required. More details in [SO thread|http://stackoverflow.com/questions/31792220/spring-xd-1-2-0-custom-module-deployment]. ",3
"With Security - Unable to upload module Once security is enabled, one cannot upload modules using the shell any longer.",2
"[Flo] Some streams can't be created using FLO Trying to create streams from the flo UI may end up in weird exceptions, whereas doing the same thing (copying/pasting the stream) directly from XD shell works smoothly.

This simple stream is an example, but this situation happens in multiple scenarios (for example using the same module several times with labels).


{code:java}
trigger --cron='0 05 14 ? * MON-FRI' | mail --from='''xd@mycompany.com''' --to='''a-wise-guy@mycompany.com''' --bcc='''me@mycompany.com'''
{code}
",0
"Add IPython notebook integration through Flo As a Spring XD user, I'd like to have [IPython Notebook|http://ipython.org/notebook.html] integration, so I can perform interactive data computations in real-time.",8
"Add HA support for NameNode when installed using Ambari As a user, I'm trying to setup HA cluster using Ambari installed Spring XD; however, I'm running into issues with the overrides. More details [here|https://github.com/spring-projects/spring-xd-ambari/issues/6].",5
"Strange language prefix shown on some code listings when hovering the mouse over them Looking at the online docs the code listings have inconsistent  syntax highlighting and when you hover the mouse over some of them there is a language  prefix like ruby or javascript inserted at the beginning of the first line. Very strange.

To see this go to http://docs.spring.io/spring-xd/docs/1.2.0.RELEASE/reference/html/#_server_configuration and scroll down to the HSQLDB section. The listing for HSQLDB looks fine, but scroll down further and put the mouse pointer on the MySQL or PostgreSQL listings and you should see 'javascript' being inserted on the first line.
",3
"Additional REST endpoint not working with security enabled I see the following error from the Admin UI:

GET http://localhost:9393/jobs/executions/4/steps/4/progress.json 403 (Forbidden)",1
"Enabling security breaks job launching from Admin UI After enabling security (see XD-3214) and granting user {{ROLE_VIEW, ROLE_CREATE, ROLE_ADMIN}} privileges it's not possible to launch jobs from Admin UI. 

For {{bdl-sqoop-combo-lukasz-MONGO-DEV}} job, 403 error is returned when Admin UI attempts to access following URL after ""Launch Job"" button is pressed:
{code}
http://ilabphd12.isus.emc.com:9393/jobs/executions?jobParameters=%7B%7D&jobname=bdl-sqoop-combo-lukasz-MONGO-DEV
{code}

Please see attached screenshot.",2
"Cannot connect to admin server with basic security enabled As a user, I'm trying to connect to {{xd-admin}} server with basic security enabled; however, I'm unable to successfully connect to the server and I get the following error message.


{code:java}
server-unknown:>admin config server --uri http://localhost:9393 --username bob --password bobspwd
Unable to contact XD Admin Server at 'http://localhost:9393'.
{code}",5
"On specific shutdown scenarios, the stream resumes from the start of the bus topic https://github.com/spring-projects/spring-xd/issues/1727",2
"Enabling security breaks Jobs page in Admin UI After enabling Spring XD security in {{XD_HOME/config/servers.yml}}:

{code}
spring:
  profiles: admin
security:
  basic:
    enabled: true
    realm: SpringXD
xd:
  security:
    authentication:
      file:
        enabled: true
        users:
          user: password, ROLE_VIEW
          admin: password, ROLE_VIEW, ROLE_CREATE, ROLE_ADMIN
{code}

after logging in as {{user}} with only {{ROLE_VIEW}} privilege, Jobs admin page is broken and is not displaying data. 403 error code is returned for following URLs:

{code}
http://localhost:9393/jobs/configurations.json?page=0&size=10
http://localhost:9393/jobs/definitions.json?page=0&size=10
{code}

Looks like {{/jobs/configurations.\*}} and {{/jobs/definitions.\*}} URLs are not covered in security section of applications.yml file.",2
"Module delete command on windows does not delete the module entirely As a user, I'm trying to delete the custom module using the {{module delete}} command via shell; though the command is successfully, I'm still seeing the associated artifact (_.jar file_) present in the custom_modules folder. Refer to [SO thread|http://stackoverflow.com/questions/30984922/springxd-module-delete-command-does-not-delete-the-uploaded-jar-file] for more details.

",5
"Hadoop Distro log message shows wrong version when set via env var If we export HADOOP_DISTRO env var instead of using --hadoopDistro parameter then the logging message is wrong, it always says

Hadoop Distro: hadoop26

even if we set HADOOP_DISTRO to something else

The classpath is built correctly. Maybe we should just remove this logging message since we log the actual version used in the next log message.
",1
"the 'filepollhdfs' job fails on second submission Definitions:

>job create pollHdfs --definition ""filepollhdfs --names=name,age"" --deploy true

>stream create csvStream --definition ""file --mode=ref --dir=/Users/trisberg/Test/files --pattern=*.csv > queue:job:pollHdfs"" --deploy

Here is the exception:

{code}
org.springframework.data.hadoop.store.StoreException: Error while flushing stream; nested exception is java.nio.channels.ClosedChannelException
	at org.springframework.xd.batch.item.hadoop.HdfsTextItemWriter.update(HdfsTextItemWriter.java:135)
	at org.springframework.batch.item.support.CompositeItemStream.update(CompositeItemStream.java:74)
	at org.springframework.batch.core.step.tasklet.TaskletStep.doExecute(TaskletStep.java:250)
	at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)
	at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)
	at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)
	at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:165)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:144)
	at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)
	at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:127)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy54.run(Unknown Source)
	at org.springframework.batch.integration.launch.JobLaunching
{code}",3
"Example hashtag-count MR job fails when running XD on YARN with PHD 3.0 Running XD on YARN on PHD 3.0 Ambari install.

Uploading and submitting a custom job fails with the following:

{code}
2015-06-02 16:54:15,580 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1433273561345_0009_m_000000_0: Error: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.springframework.xd.examples.hadoop.HashtagCount$TokenizerMapper not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2076)
	at org.apache.hadoop.mapreduce.task.JobContextImpl.getMapperClass(JobContextImpl.java:186)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:742)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.ClassNotFoundException: Class org.springframework.xd.examples.hadoop.HashtagCount$TokenizerMapper not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1982)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2074)
	... 8 more
{code}

Same example jar works fine when submitted from XD cluster.",5
"Spark streaming module includes logback jar when using dist zip When running spark streaming module on spark standalone cluster from XD distribution, I see the following error:

[Stage 3:=============================>                             (1 + 1) / 2]2015-06-02T10:05:53-0700 1.2.0.SNAP WARN task-result-getter-3 scheduler.TaskSetManager - Lost task 0.0 in stage 3.0 (TID 50, 192.168.2.8): java.lang.IllegalArgumentException: LoggerFactory is not a Logback LoggerContext but Logback is on the classpath. Either remove Logback or the competing implementation (class org.slf4j.impl.Log4jLoggerFactory loaded from file:/Users/igopinatha/workspace/git/ilayaperumalg/spark/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop2.2.0.jar). If you are using Weblogic you will need to add 'org.slf4j' to prefer-application-packages in WEB-INF/weblogic.xml Object of class [org.slf4j.impl.Log4jLoggerFactory] must be an instance of class ch.qos.logback.classic.LoggerContext
     at org.springframework.util.Assert.isInstanceOf(Assert.java:339)
     at org.springframework.boot.logging.logback.LogbackLoggingSystem.getLoggerContext(LogbackLoggingSystem.java:151)
     at org.springframework.boot.logging.logback.LogbackLoggingSystem.getLogger(LogbackLoggingSystem.java:143)
     at org.springframework.boot.logging.logback.LogbackLoggingSystem.beforeInitialize(LogbackLoggingSystem.java:89)
     at org.springframework.boot.logging.LoggingApplicationListener.onApplicationStartedEvent(LoggingApplicationListener.java:152)
     at org.springframework.boot.logging.LoggingApplicationListener.onApplicationEvent(LoggingApplicationListener.java:139)
     at org.springframework.context.event.SimpleApplicationEventMulticaster.invokeListener(SimpleApplicationEventMulticaster.java:151)
     at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:128)
     at org.springframework.boot.context.event.EventPublishingRunListener.publishEvent(EventPublishingRunListener.java:100)
     at org.springframework.boot.context.event.EventPublishingRunListener.started(EventPublishingRunListener.java:54)
     at org.springframework.boot.SpringApplication.run(SpringApplication.java:277)
     at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
     at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusConfiguration.createApplicationContext(MessageBusConfiguration.java:82)
     at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusSender.start(MessageBusSender.java:105)
     at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:58)
     at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:53)
",2
Update YARN deployment classpath settings for HDP 2.2 and PHD 3.0 Need to update classpath settings for PHD 3.0 and HDP 2.2 ,1
"`minPartitionCount` is ignored by the consumer `minPartitionCount` is ignored by the consumer, so downstream modules end up listening to fewer partitions",3
"JdbcHdfsTests sporadically fail Acceptance tests sporadically fail after https://github.com/spring-projects/spring-xd/pull/1623 was merged XD-2309.

Additional tests were added but used fixed timeouts.  Will replace them with waitForJob.

 ",2
"Unable to fully destroy hung batch job I created and launched a 'jdbchdfs' job. Due to network issues the job hung. Wasn't able to cancel it so I ended up destroying it.

After that I couldn't recreate the job since XD insisted hat there already was a job with that same name. 

{code}
xd:>job create jdbc1 --definition ""jdbchdfs --columns=ID,FILE_PATH,FILE_NAME,YEAR,FILE_LENGHT,LAST_MODIFIED --tableName=SOMEFILES --checkColumn=ID --partitionColumn=ID --partitions=3 --partitionResultsTimeout=3000000 --rollover=128000000 --commitInterval=1000 --url=jdbc:oracle:thin:@//redwood:1521/XE --driverClassName=oracle.jdbc.OracleDriver --username=spring --password=spring --testOnBorrow=false""
Successfully created job 'jdbc1'
xd:>job deploy jdbc1 --properties ""module.jdbchdfs.count=3""
Deployed job 'jdbc1'
xd:>job launch jdbc1
Successfully submitted launch request for job 'jdbc1'

(Here the job was hung)

xd:>job destroy jdbc1
Destroyed job 'jdbc1'
xd:>job create jdbc1 --definition ""jdbchdfs --columns=ID,FILE_PATH,FILE_NAME,YEAR,FILE_LENGHT,LAST_MODIFIED --tableName=SOMEFILES --checkColumn=ID --partitionColumn=ID --partitions=3 --partitionResultsTimeout=3000000 --rollover=128000000 --commitInterval=1000 --url=jdbc:oracle:thin:@//redwood:1521/XE --driverClassName=oracle.jdbc.OracleDriver --username=spring --password=spring --testOnBorrow=false""
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Batch Job with the name jdbc1 already exists

xd:>job create jdbc1 --definition ""jdbchdfs --columns=ID,FILE_PATH,FILE_NAME,YEAR,FILE_LENGHT,LAST_MODIFIED --tableName=SOMEFILES --checkColumn=ID --partitionColumn=ID --partitions=3 --partitionResultsTimeout=3000000 --rollover=128000000 --commitInterval=1000 --url=jdbc:oracle:thin:@//redwood:1521/XE --driverClassName=oracle.jdbc.OracleDriver --username=spring --password=spring --testOnBorrow=false""
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Batch Job with the name jdbc1 already exists

xd:>job destroy jdbc1
Command failed org.springframework.xd.rest.client.impl.SpringXDException: There is no job definition named 'jdbc1'

xd:>job create jdbc1 --definition ""jdbchdfs --columns=ID,FILE_PATH,FILE_NAME,YEAR,FILE_LENGHT,LAST_MODIFIED --tableName=SOMEFILES --checkColumn=ID --partitionColumn=ID --partitions=3 --partitionResultsTimeout=3000000 --rollover=128000000 --commitInterval=1000 --url=jdbc:oracle:thin:@//redwood:1521/XE --driverClassName=oracle.jdbc.OracleDriver --username=spring --password=spring --testOnBorrow=false""
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Batch Job with the name jdbc1 already exists
{code}",3
Fix random Spark streaming test failures The {{testTapSparkProcessor}} has the test that checks the contents at the output of spark streaming word count processor. It turns out that the order in which these messages are processed are not always in order.,1
"Backport Kafka Sink input fix As part of XD-2958, we've changed the input type of the Kafka sink from String to byte[]. The main reason for the change was that it required an arbitrary and often unneeded (but expensive) conversion to String for the bus payloads. 

Apply the same change to 1.1 branch.",1
Backport metadata retrieval stability improvements Backport stability improvements added as part of XD-2958 to the 1.1.x branch.,1
"Spark streaming integration module fails to initialize codec XD Spark streaming module fails to load:

Caused by: org.springframework.beans.factory.CannotLoadBeanClassException: Error loading class [org.springframework.xd.tuple.serializer.kryo.TupleCodec] for bean with name 'org.springframework.xd.tuple.serializer.kryo.TupleCodec#2e8f5f36' defined in class path resource [META-INF/spring-xd/bus/codec.xml]: problem with class file or dependent class; nested exception is java.lang.IllegalAccessError: class org.springframework.xd.tuple.serializer.kryo.TupleCodec cannot access its superclass org.springframework.xd.dirt.integration.bus.serializer.kryo.AbstractKryoCodec
	at org.springframework.beans.factory.support.AbstractBeanFactory.resolveBeanClass(AbstractBeanFactory.java:1331)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:453)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:299)
	... 67 more
Caused by: java.lang.IllegalAccessError: class org.springframework.xd.tuple.serializer.kryo.TupleCodec cannot access its superclass org.springframework.xd.dirt.integration.bus.serializer.kryo.AbstractKryoCodec
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:760)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:455)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:367)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:360)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at org.springframework.util.ClassUtils.forName(ClassUtils.java:249)
	at org.springframework.beans.factory.support.AbstractBeanDefinition.resolveBeanClass(AbstractBeanDefinition.java:395)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doResolveBeanClass(AbstractBeanFactory.java:1349)
	at org.springframework.beans.factory.support.AbstractBeanFactory.resolveBeanClass(AbstractBeanFactory.java:1320)",3
"Windows CI FTP based tests fail.   Currently Windows EC2 (master, JDK8) test is failing 
I've attempted to replicate on my EC2 environment.  The best bet is to try and reproduce using the AMI and machine size that CI uses.  We need to check with Trevor to get this info. 
The error is:
{noformat}
java.lang.AssertionError: java.lang.AssertionError
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.springframework.xd.shell.command.StreamCommandTemplate.verifyExists(StreamCommandTemplate.java:162)
	at org.springframework.xd.shell.command.StreamCommandTemplate.doCreate(StreamCommandTemplate.java:99)
	at org.springframework.xd.shell.command.StreamCommandTemplate.create(StreamCommandTemplate.java:65)
	at org.springframework.xd.shell.command.FtpModulesTests.testRefOptionEqualsFalse(FtpModulesTests.java:70)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.springframework.xd.test.AbstractExternalResourceTestSupport$1.evaluate(AbstractExternalResourceTestSupport.java:71)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:27)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.springframework.xd.test.AbstractExternalResourceTestSupport$1.evaluate(AbstractExternalResourceTestSupport.java:71)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.runTestClass(JUnitTestClassExecuter.java:86)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.execute(JUnitTestClassExecuter.java:49)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassProcessor.processTestClass(JUnitTestClassProcessor.java:69)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:48)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
	at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32)
	at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:105)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
	at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:360)
	at org.gradle.internal.concurrent.DefaultExecutorFactory$StoppableExecutorImpl$1.run(DefaultExecutorFactory.java:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat}",3
Fix compilation errors after moving SingleNodeApplication package Samples including Singlenode tests need to update for package changes,2
"SqoopRunner class not found errror  We have installed the SpringXD 1.2 M1 release via the rpm and it seems that the sqoop-1.4.5-hadoop200.jar file are not part of the rpm. The sqoop jar file are not in the xd/lib directory.

This is causing a problem during customer module development if we include the sqoop-1.4.5-hadoop200 dependency as part of the pom file and forces us to redeploy the our jar as separate deployment.

Should we be referencing different dependencies or have or should the sqoop-1.4.5-hadoop200.jar be part of the rpm definition so it part of the xd/lib?

I have currently the following dependency in the pom file:

{code}
		<!-- Sqoop -->
		<dependency>
			<groupId>org.apache.sqoop</groupId>
			<artifactId>sqoop</artifactId>
			<version>1.4.5</version>
			<classifier>hadoop200</classifier>
		</dependency>
{code}

It would be great be great if the sqoop jar are part of rpm so we don't have to do any additional jar deployment.

Thanks, ",2
"SpringXD sqoop module is hanging The SpringXD Sqoop module is in execution status until it times out, it is hanging. 

The container logs show:

2015-05-04 15:15:45,365 1.2.0.M1  INFO DeploymentsPathChildrenCache-0 container.DeploymentListener - Deploying job 'sqoop_lookup'
2015-05-04 15:15:45,536 1.2.0.M1  INFO DeploymentsPathChildrenCache-0 container.DeploymentListener - Deploying module [ModuleDescriptor@239b037a moduleName = 'sqoop', moduleLabel = 'sqoop', group = 'sqoop_lookup', sourceChannelName = [null], sinkChannelName = [null], index = 0, type = job, parameters = map['args' -> '--connect=jdbc:oracle:thin:@************:****/******* username=******** --password-file=/user/zeybeb/workspace/secure-files/gdw.password --table=MASTERDATA.W_LOOKUP_D --target-dir=/user/zeybeb/workspace/ent/masterdata_src/lookup_d -m 1', 'command' -> 'import'], children = list[[empty]]]
2015-05-04 15:16:17,061 1.2.0.M1  INFO inbound.job:sqoop_lookup-redis:queue-inbound-channel-adapter1 sqoop.SqoopTasklet - Sqoop system.out: /tmp/Sqoop-948322291323951735.out

The /tmp/Sqoop-948322291323951735.out file content is:

15:16:17,612  INFO main sqoop.SqoopRunner - Sqoop command: import
15:16:17,613  INFO main sqoop.SqoopRunner - Using args: [--connect=jdbc:oracle:thin:@************:****/*******, username=*********, --password-file=/user/zeybeb/workspace/secure-files/gdw.password, --table=MASTERDATA.W_LOOKUP_D, --target-dir=/user/zeybeb/workspace/ent/masterdata_src/lookup_d, -m, 1]
15:16:17,613  INFO main sqoop.SqoopRunner - Mapreduce home: /opt/pivotal/spring-xd-1.2.0.M1/xd/lib/phd21
15:16:17,631  INFO main sqoop.SqoopRunner - Setting configuration property: fs.defaultFS=hdfs://ilabphd07.isus.emc.com:8020
15:16:17,753  INFO main sqoop.SqoopRunner - Setting configuration property: yarn.resourcemanager.address=ilabphd08.isus.emc.com:8050
15:16:17,753  INFO main sqoop.SqoopRunner - Setting configuration property: yarn.application.classpath=/usr/hdp/2.2.0.0-2041/etc/hadoop/conf.empty,/usr/hdp/2.2.0.0-2041/hadoop/*,/usr/hdp/2.2.0.0-2041/hadoop/lib/*,/usr/hdp/2.2.0.0-2041/hadoop-hdfs/*,/usr/hdp/2.2.0.0-2041/hadoop-hdfs/lib/*,/usr/hdp/2.2.0.0-2041/hadoop-yarn/*,/usr/hdp/2.2.0.0-2041/hadoop-yarn/lib/*,/usr/hdp/2.2.0.0-2041/hadoop-mapreduce/*,/usr/hdp/2.2.0.0-2041/hadoop-mapreduce/lib/*,/usr/hdp/2.2.0.0-2041/sqoop/*,/usr/hdp/2.2.0.0-2041/sqoop/lib/*,/usr/hdp/2.2.0.0-2041/flume/*,/usr/hdp/2.2.0.0-2041/flume/lib/*,/usr/hdp/2.2.0.0-2041/storm/*,/usr/hdp/2.2.0.0-2041/storm/lib/*
15:16:17,754  INFO main sqoop.SqoopRunner - Setting configuration property: mapreduce.framework.name=yarn
15:16:17,837  WARN main tool.SqoopTool - $SQOOP_CONF_DIR has not been set in the environment. Cannot check for additional configuration.
15:16:17,907  INFO main sqoop.Sqoop - Running Sqoop version: 1.4.5
15:16:18,282  WARN main util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15:16:19,552  WARN main sqoop.ConnFactory - $SQOOP_CONF_DIR has not been set in the environment. Cannot check for additional configuration.
15:16:19,657  INFO main oracle.OraOopManagerFactory - Data Connector for Oracle and Hadoop is disabled.
15:16:19,673  INFO main manager.SqlManager - Using default fetchSize of 1000
15:16:19,673  INFO main tool.CodeGenTool - Beginning code generation
15:16:20,639  INFO main manager.OracleManager - Time zone has been set to GMT
15:16:20,853  INFO main manager.SqlManager - Executing SQL statement: SELECT t.* FROM MASTERDATA.W_LOOKUP_D t WHERE 1=0
15:16:21,018  INFO main orm.CompilationManager - HADOOP_MAPRED_HOME is /opt/pivotal/spring-xd-1.2.0.M1/xd/lib/phd21
15:16:23,171  INFO main orm.CompilationManager - Writing jar file: /tmp/sqoop-spring-xd/compile/4e11123a52fa36d6677efdb47bcdc43b/MASTERDATA.W_LOOKUP_D.jar
15:16:23,191  INFO main manager.OracleManager - Time zone has been set to GMT
15:16:24,109  INFO main manager.OracleManager - Time zone has been set to GMT
15:16:24,825  INFO main mapreduce.ImportJobBase - Beginning import of MASTERDATA.W_LOOKUP_D
15:16:24,848  INFO main manager.OracleManager - Time zone has been set to GMT
15:16:24,876  WARN main mapreduce.JobBase - SQOOP_HOME is unset. May not be able to find all job dependencies.
15:16:25,083  INFO main client.RMProxy - Connecting to ResourceManager at ilabphd08.isus.emc.com/10.15.232.191:8050
15:16:25,977  INFO main db.DBInputFormat - Using read commited transaction isolation
15:16:26,117  INFO main mapreduce.JobSubmitter - number of splits:1
15:16:26,361  INFO main mapreduce.JobSubmitter - Submitting tokens for job: job_1429280992648_0019
15:16:26,717  INFO main impl.YarnClientImpl - Submitted application application_1429280992648_0019 to ResourceManager at ilabphd08.isus.emc.com/10.15.232.191:8050
15:16:26,782  INFO main mapreduce.Job - The url to track the job: http://http://ilabphd08.isus.emc.com:8088/proxy/application_1429280992648_0019/
15:16:26,783  INFO main mapreduce.Job - Running job: job_1429280992648_0019

The logs on the Hadoop side are:

Showing 4096 bytes. Click here for full log
mumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:04,026 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:05,033 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:06,042 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:07,049 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:08,056 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:09,062 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:10,069 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:41,093 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:42,100 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:43,107 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:44,113 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:45,120 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:46,129 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:47,136 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:48,143 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:49,150 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:50,156 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)

Even though  the maxRetries is set to 10 the process is going into several sets of retrials. 

",5
"Kafka Message Bus ignores consumer concurrency when computing partition count This is a combination of two issues:
- the internal property `next.module.concurrency` is computed from `concurrency` when it should be computed from `consumer.concurrency`
- even if `next.module.concurrency` is set, the KafkaMessageBus rejects it, since it's not set in SUPPORTED_CONSUMER_PROPERTIES

As a result, the value used in partition calculation is always 1.

A workaround exists, by setting the `module.[moduleName].producer.minPartitionCount` property to the expected total value. ",3
"RemoteFileToHadoopTests fails on 1.1.x This error surfaced recently as a result of a fix to a bug in HostNotWindowsRule which disabled this test in all environments. Now the test has been reactivated it is failing on the 1.1.x branch.  The test runs OK on master.
{noformat}
Encountered an error executing step step1-master in job job
org.springframework.messaging.MessageDeliveryException: failed to send Message to channel 'null'; nested exception is java.lang.IllegalStateException: ThreadPoolTaskExecutor not initialized
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:292)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.xd.dirt.integration.bus.local.LocalMessageBus$3.handleMessage(LocalMessageBus.java:262)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutput(AbstractMessageProducingHandler.java:248)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.produceOutput(AbstractMessageProducingHandler.java:171)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutputs(AbstractMessageProducingHandler.java:119)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:105)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:85)
	at org.springframework.batch.integration.partition.MessageChannelPartitionHandler.handle(MessageChannelPartitionHandler.java:224)
	at org.springframework.batch.core.partition.support.PartitionStep.doExecute(PartitionStep.java:106)
	at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)
	at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)
	at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)
	at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:165)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:144)
	at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)
	at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)
	at org.springframework.batch.integration.x.RemoteFileToHadoopTests.testSimple(RemoteFileToHadoopTests.java:161)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:73)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:82)
	at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:73)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:217)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:83)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:68)
	at org.springframework.xd.test.HostNotWindowsRule$1.evaluate(HostNotWindowsRule.java:38)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:163)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:211)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:67)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
Caused by: java.lang.IllegalStateException: ThreadPoolTaskExecutor not initialized
	at org.springframework.util.Assert.state(Assert.java:385)
	at org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor.getThreadPoolExecutor(ThreadPoolTaskExecutor.java:221)
	at org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor.execute(ThreadPoolTaskExecutor.java:252)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:89)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	... 76 more

java.lang.AssertionError: 
Expected :exitCode=COMPLETED;exitDescription=
Actual   :exitCode=FAILED;exitDescription=
   <Click to see difference>


	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.springframework.batch.integration.x.RemoteFileToHadoopTests.testSimple(RemoteFileToHadoopTests.java:162)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:73)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:82)
	at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:73)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:217)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:83)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:68)
	at org.springframework.xd.test.HostNotWindowsRule$1.evaluate(HostNotWindowsRule.java:38)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:163)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:211)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:67)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
{noformat}",2
"Align Spring XD partitioning with Kafka partitioning for the Kafka message bus As a developer, I want that the Spring XD partitioning process targets Kafka bus partitions directly, so that the design of my stream processing application is easier to understand and the order of messages is not altered

Current situation
- Spring XD partitioning logic that builds on top of Kafka partitioning;
- The number of Spring XD partitions is not explicitly configured (it's inferred from the number of consumer modules)
- If the concurrency of the consumer modules is 1, then Spring XD partitions are matched 1:1 with Kafka partitions;
-  If the concurrency of the consumer modules is n, then a Spring XD partition uses n Kafka partitions, and the message bus distributes messages across the Kafka partitions that correspond to the Spring XD partitions;
- this could be confusing to the end user, especially if they are used to the Kafka partitioning process;
- this can also lead to changes of ordering between messages, as messages within the same Spring XD partitions will be sent to different Kafka partitions (this only happens if the concurrency of the receiving module is higher than 1)

Improvement:
- *For the Kafka message bus* the number of Spring XD partitions does not need to be equal to the number of modules (must be higher or equal, though, so that consumers can be created), and should be configured explicitly - using the `partitionCount` property - (as an option, the module count * concurrency can be used as a default)
 - as a result, in the case of Kafka there will always be a 1:1 match between Kafka partitions and Spring XD partitions, optionally processed by fewer modules than the partition count;",5
"Add polling twitter source  i've created a source module for polling a twitter timeline using the spring integration twitter inbound adapter. I've placed my api keys in the modules.yml file and verified that they are picked up by the twitterstream and twitterseach modules that come with SpringXD. However, they are not picked up by my custom module. I viewed the source for both the stream and and the search and i feel my project is near identical in configuration. Am i missing something or are these properties somehow special for just the two twitter sources that come with SpringXD?

I'd like to get this working and commit it to the project.",1
"Message Bus: Shut down Kafka Consumers completely before unbinding This causes the following exception to be thrown in the log (without functional adverse effects)

org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'unknown.channel.name'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:101)
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.access$300(KafkaMessageDrivenChannelAdapter.java:43)
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$AutoAcknowledgingChannelForwardingMessageListener.doOnMessage(KafkaMessageDrivenChannelAdapter.java:172)
	at org.springframework.integration.kafka.listener.AbstractDecodingMessageListener.onMessage(AbstractDecodingMessageListener.java:50)
	at org.springframework.integration.kafka.listener.QueueingMessageListenerInvoker.run(QueueingMessageListenerInvoker.java:121)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:107)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	... 13 more

",2
"Kafka source should not try to decode payloads as Strings Currently, the Kafka source uses a StringDecoder by default - which is an invalid assumption if the payload is not the result of String conversion.

 ",2
"JMS Source Does Not Expose `acknowledge` Since the message-driven adapter uses a {{DMLC}}, the default behavior is to lose messages on exceptions (with the DMLC, the message is ack'd before the listener is invoked).

In order to provide recovery of such situations, the source needs to expose {{acknowledge}} so it can be set to {{transacted}}.

Or, perhaps, given that we don't expose complex configuration, the source should use a {{SimpleMessageListenerContainer}} instead (where the ack is sent after the listener is successfully invoked).
",1
Fix failing KafkaSingleNodeStreamDeploymentIntegrationTests.verifyOnDemandQueues() Test fails in CI when the topic used by the test had its initial segment removed during cleanup.,1
"Scala processor module executor trims messages How to reproduce:

1. Run xd-singlenode (for which setting the Spark master URL to 'local' is a requirement). Use more than 1 worker thread. e.g. {{local[4]}}

2. Deploy the word-count example

3. Create a stream
{{stream create spark-streaming-word-count --definition ""http | word-count | log"" --deploy}}

4. Send data
{{xd:>http post --data ""a b c d e f g""}}

{{xd:>http post --data ""a b c""}}

5.Observe the result

2015-02-24 15:12:46,018 1.2.0.SNAP  INFO Executor task launch worker-3 sink.spark-streaming-word-count - (e,1)
2015-02-24 15:12:46,018 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (d,1)
2015-02-24 15:12:46,019 1.2.0.SNAP  INFO Executor task launch worker-2 sink.spark-streaming-word-count - (b,1)
2015-02-24 15:12:46,020 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (g,1)
2015-02-24 15:13:40,020 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (a,1)
2015-02-24 15:13:40,020 1.2.0.SNAP  INFO Executor task launch worker-2 sink.spark-streaming-word-count - (b,1)
2015-02-24 15:13:40,021 1.2.0.SNAP  INFO Executor task launch worker-3 sink.spark-streaming-word-count - (c,1)

(the last three results are coming from the second invocation))

Note: there seems to be a correlation between the number of values emitted and the number of workers, as, in all the attempts, there aren't more values emitted than the number of workers.",5
"SqoopTasklet not using hadoop configuration Hey Guys,

I'm trying to use a SqoopTasklet but for some reason it is not getting the hadoop configuration. In the attached sqoop job configuration using the sqooprunner class directly works without problems but the SqoopTasklet is not getting the correct configuration throwing kerberos authentication problems (see singlenode.log).  Could please you guys help me to solve this problem?

Thanks in advance.
Regards,",8
"Partitioned job throws: java.lang.RuntimeException: Could not serialize lambda Running a partitioned jdbchdfs job with 12 partitions and 3 xd-containers. Some steps fail with the jdbc connection pool exception XD-2720. I also sometimes see a serialization exception. This results in the partitioner never getting the status for some of the steps, so it keeps running until it times out even though all steps are either complete of failed.

{code}
2015-02-13 13:18:36,294 1.1.0.RELEASE ERROR inbound.files4.0-redis:queue-inbound-channel-adapter1 redis.RedisMessageBus$1 - Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:name'
org.springframework.messaging.MessageHandlingException: error occurred in message handler [files4.0.bridge.handler]; nested exception is com.esotericsoftware.kryo.KryoException: java.lang.RuntimeException: Could not serialize lambda
Serialization trace:
stepExecutions (org.springframework.batch.core.JobExecution)
jobExecution (org.springframework.batch.core.StepExecution)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:84)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:267)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:263)
	at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:263)
	at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:168)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1.doSend(RedisMessageBus.java:263)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:101)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:220)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:314)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at java.lang.Thread.run(Thread.java:745)
Caused by: com.esotericsoftware.kryo.KryoException: java.lang.RuntimeException: Could not serialize lambda
Serialization trace:
stepExecutions (org.springframework.batch.core.JobExecution)
jobExecution (org.springframework.batch.core.StepExecution)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:704)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:682)
	at org.springframework.xd.dirt.integration.bus.serializer.kryo.PojoCodec.doDeserialize(PojoCodec.java:41)
	at org.springframework.xd.dirt.integration.bus.serializer.kryo.AbstractKryoMultiTypeCodec$1.execute(AbstractKryoMultiTypeCodec.java:63)
	at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.run(KryoPoolQueueImpl.java:43)
	at org.springframework.xd.dirt.integration.bus.serializer.kryo.AbstractKryoMultiTypeCodec.deserialize(AbstractKryoMultiTypeCodec.java:60)
	at org.springframework.xd.dirt.integration.bus.serializer.kryo.PojoCodec.deserialize(PojoCodec.java:30)
	at org.springframework.xd.dirt.integration.bus.serializer.CompositeCodec.deserialize(CompositeCodec.java:72)
	at org.springframework.xd.dirt.integration.bus.serializer.CompositeCodec.deserialize(CompositeCodec.java:78)
	at org.springframework.xd.dirt.integration.bus.MessageBusSupport.deserializePayload(MessageBusSupport.java:588)
	at org.springframework.xd.dirt.integration.bus.MessageBusSupport.deserializePayload(MessageBusSupport.java:573)
	at org.springframework.xd.dirt.integration.bus.MessageBusSupport.deserializePayloadIfNecessary(MessageBusSupport.java:556)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus.access$1000(RedisMessageBus.java:68)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$ReceivingHandler.handleRequestMessage(RedisMessageBus.java:465)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:99)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	... 21 more
Caused by: java.lang.RuntimeException: Could not serialize lambda
	at com.esotericsoftware.kryo.serializers.ClosureSerializer.read(ClosureSerializer.java:52)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:786)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:116)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:22)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:704)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)
	... 40 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: -2
	at java.util.ArrayList.elementData(ArrayList.java:418)
	at java.util.ArrayList.get(ArrayList.java:431)
	at com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:42)
	at com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:830)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:680)
	at com.esotericsoftware.kryo.serializers.ClosureSerializer.read(ClosureSerializer.java:49)
	... 45 more
{code} ",5
"Kafka Tests shouldn't assume offset 0  In `org.springframework.xd.dirt.stream.KafkaSingleNodeStreamDeploymentIntegrationTests#verifyOnDemandQueues`, when testing the queue partitions for content, the read is assumed to start at offset 0.

This is incorrect, because the topics may exist already, especially in a CI environment",1
"Module delete command sporadically fails on windows When a user executes a module delete on a custom module it sporadically fails with the following exception below at the bottom of the description.
Deployment:
OS: Windows 8 or Windows Server 2012 R2
Java version Java 8 (build 25.31-b07, mixed mode)
XD Deployment type. XD-Singlenode (embedded zookeeper)

Steps to reproduce:
1) build either the rss-feed-source or the payload-conversion samples from the spring-xd-samples
2) start xd-singlenode
3) start shell
4) from the shell execute module upload for the custom module i.e. module upload --file C:\project\spring-xd-samples\payloadconversion\build\libs\payload-conversion-1.0.0.BUILD-SNAPSHOT.jar --name myTupleProcessor --type processor
5) Verify that the module was uploaded by executing module info processor:myTupleProcessor
6) Execute module delete processor:myTupleProcessor

{noformat}
2015-02-18 14:48:43,908 1.1.0.RELEASE ERROR qtp752571350-38 rest.RestControllerA
dvice - Caught exception while handling a request
org.springframework.xd.dirt.module.DependencyException: Cannot delete module pro
cessor:myTupleProcessor because it is used by [stream:test]
        at org.springframework.xd.dirt.module.ModuleDefinitionService.delete(Mod
uleDefinitionService.java:116)
        at org.springframework.xd.dirt.rest.ModulesController.delete(ModulesCont
roller.java:155)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.
java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces
sorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at org.springframework.web.method.support.InvocableHandlerMethod.doInvok
e(InvocableHandlerMethod.java:221)
        at org.springframework.web.method.support.InvocableHandlerMethod.invokeF
orRequest(InvocableHandlerMethod.java:137)
        at org.springframework.web.servlet.mvc.method.annotation.ServletInvocabl
eHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110)
        at org.springframework.web.servlet.mvc.method.annotation.RequestMappingH
andlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:777)
        at org.springframework.web.servlet.mvc.method.annotation.RequestMappingH
andlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:706)
        at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapt
er.handle(AbstractHandlerMethodAdapter.java:85)
        at org.springframework.web.servlet.DispatcherServlet.doDispatch(Dispatch
erServlet.java:943)
        at org.springframework.web.servlet.DispatcherServlet.doService(Dispatche
rServlet.java:877)
        at org.springframework.web.servlet.FrameworkServlet.processRequest(Frame
workServlet.java:966)
        at org.springframework.web.servlet.FrameworkServlet.doDelete(FrameworkSe
rvlet.java:890)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:761)
        at org.springframework.web.servlet.FrameworkServlet.service(FrameworkSer
vlet.java:842)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684
)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet
Handler.java:1496)
        at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConf
iguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConf
iguration.java:291)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerR
equestFilter.java:107)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet
Handler.java:1467)
        at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInterna
l(HiddenHttpMethodFilter.java:77)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerR
equestFilter.java:107)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet
Handler.java:1467)
        at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInter
nal(HttpPutFormContentFilter.java:87)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerR
equestFilter.java:107)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet
Handler.java:1467)
        at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilter
Internal(WebRequestTraceFilter.java:100)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerR
equestFilter.java:107)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet
Handler.java:1467)
        at org.springframework.security.web.FilterChainProxy.doFilterInternal(Fi
lterChainProxy.java:186)
        at org.springframework.security.web.FilterChainProxy.doFilter(FilterChai
nProxy.java:160)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet
Handler.java:1467)
        at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfig
uration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:90)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerR
equestFilter.java:107)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(Servlet
Handler.java:1467)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java
:499)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.j
ava:137)
        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.jav
a:557)
        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandl
er.java:231)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandl
er.java:1086)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:
428)
        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandle
r.java:193)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandle
r.java:1020)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.j
ava:135)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper
.java:116)
        at org.eclipse.jetty.server.Server.handle(Server.java:370)
        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(Abstrac
tHttpConnection.java:494)
        at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(Abstra
ctHttpConnection.java:971)
        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.header
Complete(AbstractHttpConnection.java:1033)
        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)

        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnecti
on.java:82)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEn
dPoint.java:667)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEnd
Point.java:52)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPoo
l.java:608)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool
.java:543)
        at java.lang.Thread.run(Thread.java:745)
2015-02-18 14:50:38,191 1.1.0.RELEASE  INFO main-EventThread server.DeploymentLi
stener - Undeploying module [ModuleDescriptor@14b69ddf moduleName = 'http', modu
leLabel = 'http', group = 'test', sourceChannelName = [null], sinkChannelName =
[null], index = 0, type = source, parameters = map[[empty]], children = list[[em
pty]]]
2015-02-18 14:50:38,380 1.1.0.RELEASE  INFO main-EventThread server.DeploymentLi
stener - Undeploying module [ModuleDescriptor@7c2cd32 moduleName = 'myTupleProce
ssor', moduleLabel = 'myTupleProcessor', group = 'test', sourceChannelName = [nu
ll], sinkChannelName = [null], index = 1, type = processor, parameters = map['in
putType' -> 'application/x-xd-tuple'], children = list[[empty]]]
2015-02-18 14:50:38,470 1.1.0.RELEASE  INFO main-EventThread server.DeploymentLi
stener - Undeploying module [ModuleDescriptor@30ba1084 moduleName = 'log', modul
eLabel = 'log', group = 'test', sourceChannelName = [null], sinkChannelName = [n
ull], index = 2, type = sink, parameters = map[[empty]], children = list[[empty]
]]
2015-02-18 14:50:38,527 1.1.0.RELEASE  INFO DeploymentSupervisor-0 server.Initia
lDeploymentListener - Path cache event: path=/deployments/streams/test, type=CHI
LD_REMOVED
2015-02-18 14:50:38,528 1.1.0.RELEASE  INFO DeploymentsPathChildrenCache-0 serve
r.DeploymentListener - Path cache event: path=/deployments/modules/allocated/f66
35afd-7351-4ac3-baa3-3b98e74a38ca/test.source.http.1, type=CHILD_REMOVED
2015-02-18 14:50:38,530 1.1.0.RELEASE  INFO DeploymentsPathChildrenCache-0 serve
r.DeploymentListener - Path cache event: path=/deployments/modules/allocated/f66
35afd-7351-4ac3-baa3-3b98e74a38ca/test.processor.myTupleProcessor.1, type=CHILD_
REMOVED
2015-02-18 14:50:38,532 1.1.0.RELEASE  INFO DeploymentsPathChildrenCache-0 serve
r.DeploymentListener - Path cache event: path=/deployments/modules/allocated/f66
35afd-7351-4ac3-baa3-3b98e74a38ca/test.sink.log.1, type=CHILD_REMOVED
2015-02-18 14:50:41,486 1.1.0.RELEASE  INFO LeaderSelector-0 server.DeploymentSu
pervisor - Leadership canceled due to thread interrupt
2015-02-18 14:50:41,592 1.1.0.RELEASE  WARN NIOServerCxn.Factory:0.0.0.0/0.0.0.0
:5156 server.NIOServerCnxn - caught end of stream exception
EndOfStreamException: Unable to read additional data from client sessionid 0x14b
9d2656c30000, likely client has closed socket
        at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228
)
        at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFac
tory.java:208)
        at java.lang.Thread.run(Thread.java:745)
{noformat}
",3
"Ensure that metadata for Kafka message bus is propagated before producing/consuming Currently, `ensureTopicCreated` will invoke the creation of the topic on the brokers, however, the calls is not blocking. So, before proceeding, we should make sure that the metadata is readable (therefore propagated)",3
"Error when listing Streams in admin-ui As a user, I'm trying to list streams (>20) in admin-ui to use the pagination; however, I ended up with blank page and the server-side errored with _java.lang.IllegalStateException_.

Version: 1.1.0 SNAPSHOT (master build)
Distributed: 1 admin and 2 containers

*Steps to reproduce:*
1) Deploy the following streams.
stream create foo1 --definition ""time | log"" --deploy
stream create foo2 --definition ""time | log"" --deploy
stream create foo3 --definition ""time | log"" --deploy
stream create foo4 --definition ""time | log"" --deploy
stream create foo5 --definition ""time | log"" --deploy
stream create foo6 --definition ""time | log"" --deploy
stream create foo7 --definition ""time | log"" --deploy
stream create foo8 --definition ""time | log"" --deploy
stream create foo9 --definition ""time | log"" --deploy
stream create foo10 --definition ""time | log"" --deploy
stream create foo11 --definition ""time | log"" --deploy
stream create foo12 --definition ""time | log"" --deploy
stream create foo13 --definition ""time | log"" --deploy
stream create foo14 --definition ""time | log"" --deploy
stream create foo15 --definition ""time | log"" --deploy
stream create foo16 --definition ""time | log"" --deploy
stream create foo17 --definition ""time | log"" --deploy
stream create foo18 --definition ""time | log"" --deploy
stream create foo19 --definition ""time | log"" --deploy
stream create foo20 --definition ""time | log"" --deploy
stream create foo21 --definition ""time | log"" --deploy
stream create foo22 --definition ""time | log"" --deploy

2) Go to Streams tab in admin-ui to get a blank page and the following exception in admin logs.

*Error:*
16:55:19,107 1.1.0.SNAP ERROR http-nio-9393-exec-2 rest.RestControllerAdvice - Caught exception while handling a request
java.lang.IllegalStateException: Not all instances were looked at
	at org.springframework.util.Assert.state(Assert.java:385)
	at org.springframework.xd.dirt.rest.XDController.enhanceWithDeployments(XDController.java:207)
	at org.springframework.xd.dirt.rest.XDController.listValues(XDController.java:178)
	at org.springframework.xd.dirt.rest.StreamsController.list(StreamsController.java:63)
",1
"Windows build fails This Hadoop scenario will not work in Windows. The scope is to *disable* the test for windows build.

org.springframework.batch.integration.x.RemoteFileToHadoopTaskletTests > testWrite FAILED
    java.lang.IllegalStateException
        Caused by: org.springframework.beans.factory.BeanCreationException
            Caused by: java.lang.UnsatisfiedLinkError

org.springframework.batch.integration.x.RemoteFileToHadoopTests > testSimple FAILED
    java.lang.IllegalStateException
        Caused by: org.springframework.beans.factory.BeanCreationException
            Caused by: java.lang.UnsatisfiedLinkError
Java HotSpot(TM) Client VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0

3 tests completed, 2 failed
:spring-xd-extension-batch:test FAILED

FAILURE: Build failed with an exception.",1
"Full build with tests fail on Ubuntu On Ubuntu 14.04 LTS using OpenJDK version  ""1.7.0_65""

OpenJDK Runtime Environment (IcedTea 2.5.3) (7u71-2.5.3-0ubuntu0.14.04.1)
OpenJDK 64-Bit Server VM (build 24.65-b04, mixed mode)

I see the following failures:

:spring-xd-dirt:test

org.springframework.xd.dirt.security.SingleNodeApplicationWithUserBasedSecurityTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

org.springframework.xd.dirt.security.SingleNodeApplicationWithLdapSimpleBindTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

org.springframework.xd.dirt.security.SingleNodeApplicationWithDefaultSecurityTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

org.springframework.xd.dirt.security.SingleNodeApplicationWithSslTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

org.springframework.xd.dirt.security.SingleNodeApplicationWithUsersFileTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

org.springframework.xd.dirt.security.SingleNodeApplicationWithLdapSearchAndBindTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

595 tests completed, 6 failed, 2 skipped
:spring-xd-dirt:test FAILED

The test reports has this:

Caused by: java.lang.IllegalStateException: HSQLDB could not be started on 0.0.0.0:7714, state: SHUTDOWN
	at org.springframework.xd.batch.hsqldb.server.HSQLServerBean.startServer(HSQLServerBean.java:162)
	at org.springframework.xd.batch.hsqldb.server.HSQLServerBean.afterPropertiesSet(HSQLServerBean.java:82)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1625)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1562)
	... 42 more
Caused by: java.net.BindException: Address already in use
	at java.net.PlainSocketImpl.socketBind(Native Method)
	at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:376)
	at java.net.ServerSocket.bind(ServerSocket.java:376)
	at java.net.ServerSocket.<init>(ServerSocket.java:237)
	at java.net.ServerSocket.<init>(ServerSocket.java:128)
	at org.hsqldb.server.HsqlSocketFactory.createServerSocket(Unknown Source)
	at org.hsqldb.server.Server.openServerSocket(Unknown Source)
	at org.hsqldb.server.Server.run(Unknown Source)
	at org.hsqldb.server.Server.access$000(Unknown Source)
	at org.hsqldb.server.Server$ServerThread.run(Unknown Source)

So I assume I see this due HSQL running from another test.",5
"Strip MessageBus DeliveryMode Header Since the messagebus refactoring, we now see 

{noformat}
15:43:06,379 1.1.0.M2  WARN xdbus.foo.0-1 support.DefaultAmqpHeaderMapper - skipping header 'amqp_deliveryMode' since it is not of expected type [class org.springframework.amqp.core.MessageDeliveryMode], it is [class org.springframework.amqp.core.MessageDeliveryMode]
{noformat}

When using a rabbit transport and a rabbit sink (the sink Spring AMQP is in its own classloader).",1
"Boot upgrade caused test failures spring.groovy.template.check-template-location=false must now be set in the properties file.
",3
"UI should quote parameters containing a space Trying to deploy the `timestampfile` job using the UI.

Seems the UI doesn't quote string parameters that contains a space so the job creation fails.

Keeping all the defaults I get the following ""Resulting Definition"" in the UI:

timestampfile --restartable=false --directory=/tmp/xd/output/ --fileExtension=txt --fileName=${xd.job.name} --format=yyyy-MM-dd HH:mm:ss --dateFormat=yyyy-MM-dd --makeUnique=true

(note: the --format parameter has a space)

which causes:

XD100E:(pos 128): Found unexpected data after stream definition: 'HH' timestampfile --restartable=false --directory=/tmp/xd/output/ --fileExtension=txt --fileName=${xd.job.name} --format=yyyy-MM-dd HH:mm:ss --dateFormat=yyyy-MM-dd --makeUnique=true *^
",3
"Enable configuration of replication factor on the Kafka message bus The field exists and it is referred to in application.yml, but it does not have a setter and the bus will always use the configured default, which is 1.",3
"Parsing issues with kafka-bus.xml Using Kafka as a transport option yields:

[2014-11-04 12:18:30.528] boot - 24061 ERROR [main] --- SpringApplication: Application startup failed
org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Failed to import bean definitions from URL location [classpath*:/META-INF/spring-xd/transports/kafka-bus.xml]
Offending resource: class path resource [META-INF/spring-xd/bus/message-bus.xml]; nested exception is org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 9 in XML document from URL [jar:file:/Users/mbogoevici/.gradle/caches/modules-2/files-2.1/org.springframework.xd/spring-xd-dirt/1.1.0.BUILD-SNAPSHOT/cf6a9a013dbde49d2925e2b5177d01a028379758/spring-xd-dirt-1.1.0.BUILD-SNAPSHOT.jar!/META-INF/spring-xd/transports/kafka-bus.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 9; columnNumber: 26; Open quote is expected for attribute ""{1}"" associated with an  element type  ""value"".
	at org.springframework.beans.factory.parsing.FailFastProblemReporter.error(FailFastProblemReporter.java:70)
	at org.springframework.beans.factory.parsing.ReaderContext.error(ReaderContext.java:85)
	at org.springframework.beans.factory.parsing.ReaderContext.error(ReaderContext.java:76)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.importBeanDefinitionResource(DefaultBeanDefinitionDocumentReader.java:248)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.parseDefaultElement(DefaultBeanDefinitionDocumentReader.java:199)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.parseBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:184)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.doRegisterBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:141)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.registerBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:110)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.registerBeanDefinitions(XmlBeanDefinitionReader.java:508)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:391)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:335)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:303)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:180)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:216)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:187)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsFromImportedResources(ConfigurationClassBeanDefinitionReader.java:313)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForConfigurationClass(ConfigurationClassBeanDefinitionReader.java:138)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitions(ConfigurationClassBeanDefinitionReader.java:116)
	at org.springframework.context.annotation.ConfigurationClassPostProcessor.processConfigBeanDefinitions(ConfigurationClassPostProcessor.java:330)
	at org.springframework.context.annotation.ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry(ConfigurationClassPostProcessor.java:243)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanDefinitionRegistryPostProcessors(PostProcessorRegistrationDelegate.java:254)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:94)
	at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:609)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:464)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:320)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:142)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.xd.dirt.server.SingleNodeApplication.run(SingleNodeApplication.java:63)
	at org.springframework.xd.demo.kafka.KafkaDemo.main(KafkaDemo.java:28)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
Caused by: org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 9 in XML document from URL [jar:file:/Users/mbogoevici/.gradle/caches/modules-2/files-2.1/org.springframework.xd/spring-xd-dirt/1.1.0.BUILD-SNAPSHOT/cf6a9a013dbde49d2925e2b5177d01a028379758/spring-xd-dirt-1.1.0.BUILD-SNAPSHOT.jar!/META-INF/spring-xd/transports/kafka-bus.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 9; columnNumber: 26; Open quote is expected for attribute ""{1}"" associated with an  element type  ""value"".
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:398)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:335)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:303)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:180)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:216)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.importBeanDefinitionResource(DefaultBeanDefinitionDocumentReader.java:242)
	... 31 more
Caused by: org.xml.sax.SAXParseException; lineNumber: 9; columnNumber: 26; Open quote is expected for attribute ""{1}"" associated with an  element type  ""value"".
	at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)
	at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.fatalError(ErrorHandlerWrapper.java:177)
	at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:441)
	at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)
	at com.sun.org.apache.xerces.internal.impl.XMLScanner.reportFatalError(XMLScanner.java:1436)
	at com.sun.org.apache.xerces.internal.impl.XMLScanner.scanAttributeValue(XMLScanner.java:829)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanAttribute(XMLNSDocumentScannerImpl.java:439)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:255)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2786)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:117)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)
	at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)
	at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:243)
	at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)
	at org.springframework.beans.factory.xml.DefaultDocumentLoader.loadDocument(DefaultDocumentLoader.java:76)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadDocument(XmlBeanDefinitionReader.java:428)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:390)
	... 36 more",1
"Fix incorrect IP Address associated with containers The XD Container IP address displayed on both 'singlenode' and Distributed modes are incorrect both on _Shell_ as well as _Admin-UI_. 

*Example:*
Noticed IP address as 10.10.10.*

Following function in [RuntimeUtils|http://docs.spring.io/autorepo/docs/spring-xd/1.0.1.RELEASE/api/org/springframework/xd/dirt/util/RuntimeUtils.html] could be flawed:
{code}
public static String getIpAddress() {
	try {
		for(Enumeration<NetworkInterface> enumNic = NetworkInterface.getNetworkInterfaces();
				enumNic.hasMoreElements();) {
			NetworkInterface ifc = enumNic.nextElement();
			if (ifc.isUp()) {
				for (Enumeration<InetAddress> enumAddr = ifc.getInetAddresses();
						enumAddr.hasMoreElements(); ) {
					InetAddress address = enumAddr.nextElement();
					if (address instanceof Inet4Address && !address.isLoopbackAddress()) {
						return address.getHostAddress();
					}
				}
			}
		}
	}
	catch (IOException e) {
		// ignore
	}

	return ""unknown"";
}
{code}
",5
"Basic authentication realm is always 'null' Besides the Basic authentication realm being always {{null}}, {{security.basic.realm}} is always ignored.",1
"Fix the configuration problem with Filter and Transform modules As a user, I'd like to leverage _propertieis-location_ parameter while using *Filter* or *Transform* modules so that I can load the user-defined properties included in the external properties file. 

Attempting to include the _propertieis-location_ attribute errors out - refer to the attachment.

It could also be beneficial to load user-defined properties through stream definition similar to deployment properties.

Example:
--script=myscript.groovy --variables=foo=bar,goo=gaz
",5
"Need to set small commit level for Acceptance tests.   XD-2180 introduced a default commit level for jobs to be 1000, vs the original 100.  Now tests sporadically fail.  Need to set the --commitInterval for the tests to a small value.",3
"xd-shell from 1.0.1 doesn't work with 1.0.0 GA admin Targeting xd-shell from 1.0.1 to 1.0.0 GA admin server fails

server-unknown:>admin config info
  -------------  -------------------------------------------------------------
  Result         Unable to contact XD Admin Server at 'http://localhost:9393'.
  Target         http://localhost:9393
  Timezone used  Pacific Standard Time (UTC -8:00)
  -------------  -------------------------------------------------------------
-------------------------------------------------------------------------------
An exception ocurred during targeting:
java.lang.NullPointerException
    at org.springframework.xd.rest.client.impl.SpringXDTemplate.<init>(SpringXDTemplate.java:110)
    at org.springframework.xd.rest.client.impl.SpringXDTemplate.<init>(SpringXDTemplate.java:137)
    at org.springframework.xd.shell.command.ConfigCommands.target(ConfigCommands.java:106)
    at org.springframework.xd.shell.command.ConfigCommands.afterPropertiesSet(ConfigCommands.java:191)
",1
"FilePollHdfs sporadically fails  Need to add a retry to the mkdir command, in the case that it fails.",3
"testHdfsSink  sporadically fails because twitter data is written to file. The hdfs sink needs to have a unique stream name.  Because the twitterSearch test uses the ""defaultName"" and it may broadcast more than one message to the sink for the search.  So when the stream is destroyed the message is abandoned until hdfsTest starts up using the same 
""defaultName"" and the message is delivered to hdfs sink and thus the twitter data is written to the file.  ",3
"Throughput in a stream with any processor One of the goal for a micro benchmark is to compare throughput difference  between two types of streams:
1. source | sink
2. source | processor | sink

For this test I used reactor-tcp source, throughput-sampler as sink and created a NoOp processor. Tests were performed on a single node container with direct binding turned on for all streams.

1. Throughput for ""source|sink""

{noformat}
stream create reactortcp --definition ""reactor-ip --transport=tcp --port=4000 | throughput-sampler""
stream deploy reactortcp --properties module.*.count=0
{noformat}

On my system I get following numbers:
Throughput sampled for 5000000 items: 345423/s in 14475ms elapsed time

2. Throughput for ""source|processor|sink""

Code for NoOpProcessor is available here:
https://github.com/parikhkc/xd-noop-processor

{noformat}
stream create reactornoop --definition ""reactor-ip --transport=tcp --port=5000 | noopprocessor | throughput-sampler""
stream deploy reactornoop --properties module.*.count=0
{noformat}

On the same system the throughput reduces to less then 70K/sec.
Throughput sampled for 5000000 items: 67250/s in 74349ms elapsed time

Yourkit shows 50% of CPU time on following thread:

{noformat}
* ringBuffer-17 [RUNNABLE] [DAEMON]
java.lang.reflect.Method.getParameterAnnotations() Method.java:770
org.springframework.xd.integration.reactor.net.NetServerInboundChannelAdapter$1.accept(Object) NetServerInboundChannelAdapter.java:53
reactor.net.AbstractNetChannel$3.accept(Event) AbstractNetChannel.java:131
reactor.net.AbstractNetChannel$3.accept(Object) AbstractNetChannel.java:128
reactor.event.routing.ArgumentConvertingConsumerInvoker.invoke(Consumer, Class, Object) ArgumentConvertingConsumerInvoker.java:73
reactor.event.routing.ConsumerFilteringEventRouter.route(Object, Event, List, Consumer, Consumer) ConsumerFilteringEventRouter.java:78
reactor.event.dispatch.AbstractLifecycleDispatcher.route(AbstractLifecycleDispatcher$Task) AbstractLifecycleDispatcher.java:64
reactor.event.dispatch.AbstractSingleThreadDispatcher$SingleThreadTask.run() AbstractSingleThreadDispatcher.java:50
reactor.event.dispatch.RingBufferDispatcher$3.onEvent(RingBufferDispatcher$RingBufferTask, long, boolean) RingBufferDispatcher.java:115
reactor.event.dispatch.RingBufferDispatcher$3.onEvent(Object, long, boolean) RingBufferDispatcher.java:112
com.lmax.disruptor.BatchEventProcessor.run() BatchEventProcessor.java:128
java.lang.Thread.run() Thread.java:745

{noformat}
",1
"HDFS sink should honor --fileExtension parameter for bzip2 compressed files Looks like the --fileExtension isn't used when compressing files with bzip2, some use cases requirer bz2 instead of bzip2 as the extension. Also, '.bz2' should be the default extension. At the same time we should change the default gzip extension to '.gz'.",3
"FilePollHdfsTest fails intermittently  From time to time FilePollHdfsTest fails in the CI Acceptance Tests. The exception that is fired is as follows: 
java.lang.AssertionError: java.lang.AssertionError: The data returned from hadoop was different than was sent.   expected:<942b9f47-8169-4dc3-a2ba-3d8fab04a4dc
> but was:<null>

Need to investigate if this is a timing issue with the test (more than likely) or with the actual module.",3
"Preserve partition state on container restarts As a user, I'd like to retain the data partitioning state so that when I restart the containers, I continue to write based on the original partitioning strategy. 

Currently, the state is not preserved; hence, on restarts the definition of partitioning strategy is lost due to different _hashCode()_.

*Design consideration:*
Mine through the container info to derive the ""partition index"" instead of relying on _hashCode()_.",5
"Restrict the use of reserved keywords As a user, I should not be allowed to create a custom module with a _reserved_ keywords so that I it will avoid confusions from seeing duplicate strings in deployment manifest.

*Example:*
We would like to avoid a _custom_ module name of *producer* to eliminate the confusion below:
{code}

xd:>stream deploy --name test1 --properties ""module.producer.producer.deliveryMode= PERSISTENT,module.log.criteria=groups.contains('group1')""

{code}

[List of available reserved keywords|https://github.com/spring-projects/spring-xd/wiki/Deployment#deployment-properties]",5
"Spring XD very poor performance when using redis as transport When using redis as transport bus there is a problem when using many streams and taps. Basically the maxTotal parameter of org.apache.commons.pool2.GenericObjectPool default is 8. After some streams are deployed it starts to occur concurrency problems hence the number of inbound redis channel adapters is larger than that number.

A more detailed explanation is in stackoverflow:

http://stackoverflow.com/questions/25851660/spring-xd-very-poor-performance-when-using-redis-as-transport",8
"FilePollHdfs sporadically fails to create files on remote machines Need to add a retry to the createDataFileOnRemote machine, because the creation of the test file on the remote machine fails from time to time.
Usually related to network issues.
",3
"Spring XD UI: end-to-end tests do not work Currently, end-to-end tests of Spring XD UI will not run, as protractor relies on a non-existing chromedriver.exe file.

Either the configuration has to be removed from the Gruntfile or the necessary dependencies should be there.",2
"Acceptance Tests fail to map some EC2 internal IPs to External IPs The acceptance tests interrogate the XD-Admin for the containers that are available.  When on EC2 the admin only returns the internal EC2 addresses without the associated suffix of .ec2.internal or .compute-1.internal.   

[Defect]
The acceptance tests only handled the most common suffix of .ec2.internal.  Thus some CI Acceptance tests will fail because, because the container's IPs were not properly mapped.  Thus the acceptance tests should map internal to external IP without regard to the suffixes EC2 issues.

FYI
EC2 issues addresses in 2 different formats: ip-XXX-XXX-XXX-XXX.ec2.internal or domU-XX-XX-XX-XX-XX-XX.compute-1.internal.  The code only able to handle ip-XXX-XXX-XXX-XXX.ec2.internal.  ",3
"No main manifest attribute in xd-yarn-client jar Error deploying to YARN - 

$ ./spring-xd-1.0.0.BUILD-SNAPSHOT-yarn/bin/xd-yarn push -p spring-xd-1.0.0.BUILD-SNAPSHOT-yarn
no main manifest attribute, in spring-xd-1.0.0.BUILD-SNAPSHOT-yarn/lib/spring-xd-yarn-client-1.0.0.BUILD-SNAPSHOT.jar

probably related to boot changes",3
"Batch jobs executions by jobname causes stackoverflow Whether it's after applying https://github.com/spring-projects/spring-xd/pull/1034/ or not, this causes the following problem:

{noformat}
....
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:644)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:152)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:541)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:644)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:152)
	at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:100)
	at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:21)
	at com.fasterxml.jackson.databind.ser.std.AsArraySerializerBase.serialize(AsArraySerializerBase.java:183)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:541)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:644)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:152)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:541)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:644)
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:152)
	at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:100)
	at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:21)
Caused by: java.lang.StackOverflowError
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:660)
	... 1011 more
{noformat}",4
"Can't use webhdfs with hdfs sink When using spring.hadoop.fsUri set to webhdfs://localhost/ I'm getting an error:

java.lang.NoClassDefFoundError: javax/ws/rs/core/MediaType

including the following in xd/lib seems to fix this:
- jersey-core-1.9.jar
- jersey-server-1.9.jar
",3
"FileSourceTest needs to apply label to source and sink * Currently Acceptance FileSource Acceptance Tests are failing
** This is because the sink that tests the result for the file source test is a filesink.  Both use the ""file"" token.  Thus causing a failure
* SimpleFileSource and SimpleFileSink needs to support a label method.
* Update testFileSource to use the labels.",3
"Module count value at module deployments path In case of module count > 1, the module deployments path for each deployed module always has: {""count"":""1""}

For a scenario:
The stream test1: ""http | log""
with the deployment manifest:
module.log.count=3,module.log.criteria=groups.contains('test')

get /xd/deployments/streams/test1module.log.count=3,module.log.criteria=groups.contains('test')
get /xd/deployments/modules/9ecaf59a-a1f5-4ed9-984d-f5dff8cc9b57/test1.sink.log-1{""count"":""1""}
get /xd/deployments/modules/1bbdb2dd-97ed-48a2-a3cd-3633c3e82f52/test1.sink.log-1{""count"":""1""}",5
"Handle NPE while deploying stream module at the Container When trying to deploy a stream module, the ContainerRegistrar throws NPE if the deployment loader couldn't load a non-null stream based on the stream name.

07:10:29,902 ERROR DeploymentsPathChildrenCache-0 server.ContainerRegistrar:450 - Exception deploying module
java.lang.NullPointerException
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployStreamModule(ContainerRegistrar.java:549)
	at org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:436)
	at org.springframework.xd.dirt.server.ContainerRegistrar.access$800(ContainerRegistrar.java:96)
	at org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:803)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)",1
"Status on Shell command prompt is inconsistent Deployment: xd-shell local, xd-singlenode (ec2)
SHA: 942c7868e3e0d0cf7730b536170438a0291f5cab

[Description]
<Case 1>
Once successfully connected to a server, if you connect to a server that is not present.  The prompt still shows XD when it should show server-unknown.  Can be reproduced consistently.
Conversely:
<Case 2>
Attempted to connect to a xd-singlenode on ec2 using a local xd-shell.  
The xd-singlenode was not running.  After bringing up the xd-singlenode, I was able to connect however the status did not change from ""server-unknown""  *This behavior, can not be consistently reproduced, but have seen it happen on multiple accounts.* 

[Steps to reproduce]
<Case 1>
1. Bring up shell while xd-singlenode is not running.
2. Bring up xd-singlenode
3. Connect to xd-singlenode
* xd:>admin config server http://localhost:9393

4. Connect to a fake address
* xd:>admin config server http://foo.bar:9393

<Case 2>
1.  Attempt to connect to remote server that is not available
* xd:>admin config server http://ec2-54-237-186-186.compute-1.amazonaws.com:9393
* Unable to contact XD Admin Server at 'http://ec2-54-237-186-186.compute-1.amazonaws.com:9393'.

2. Bring up xd-singlenode on remote
* server-unknown:>admin config server http://ec2-54-237-186-186.compute-1.amazonaws.com:9393
* Successfully targeted http://ec2-54-237-186-186.compute-1.amazonaws.com:9393

3. Still see the incorrect prompt.
server-unknown:>
server-unknown:>",5
"Container reconnection to ZK fails intermittently As reported by Matt Stine:

After closing and reopening a laptop, the following stack trace appears in the container log:

{noformat}
00:47:28,226  INFO main-EventThread state.ConnectionStateManager:194 - State change: RECONNECTED
00:47:28,226  INFO ConnectionStateManager-0 zookeeper.ZooKeeperConnection:255 - >>> Curator connected event: RECONNECTED
00:47:28,322 ERROR ConnectionStateManager-0 listen.ListenerContainer:96 - Listener (org.springframework.xd.dirt.zookeeper.ZooKeeperConnection$DelegatingConnectionStateListener@6abf4158) threw an exception
java.lang.RuntimeException: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/containers/5a8deb7b-fd93-42a7-a393-2f15023e007a
        at org.springframework.xd.dirt.server.ContainerRegistrar.registerWithZooKeeper(ContainerRegistrar.java:301)
        at org.springframework.xd.dirt.server.ContainerRegistrar.access$100(ContainerRegistrar.java:93)
        at org.springframework.xd.dirt.server.ContainerRegistrar$ContainerAttributesRegisteringZooKeeperConnectionListener.onConnect(ContainerRegistrar.java:316)
        at org.springframework.xd.dirt.zookeeper.ZooKeeperConnection$DelegatingConnectionStateListener.stateChanged(ZooKeeperConnection.java:257)
        at org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:222)
        at org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:218)
        at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
        at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
        at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
        at org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:215)
        at org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:42)
        at org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:110)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/containers/5a8deb7b-fd93-42a7-a393-2f15023e007a
        at org.springframework.xd.dirt.container.store.ZooKeeperContainerAttributesRepository.save(ZooKeeperContainerAttributesRepository.java:75)
        at org.springframework.xd.dirt.container.store.ZooKeeperContainerAttributesRepository.save(ZooKeeperContainerAttributesRepository.java:42)
        at org.springframework.xd.dirt.server.ContainerRegistrar.registerWithZooKeeper(ContainerRegistrar.java:295)
        ... 15 more
Caused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/containers/5a8deb7b-fd93-42a7-a393-2f15023e007a
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:119)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
        at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676)
        at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660)
        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
        at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656)
        at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441)
        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431)
        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)
        at org.springframework.xd.dirt.container.store.ZooKeeperContainerAttributesRepository.save(ZooKeeperContainerAttributesRepository.java:69)
        ... 17 more
{noformat}

This can occur if ZK does not remove the ephemeral node before the container creates a new one. This can be fixed in the following ways:

* Remove the existing ephemeral node if it already exists
* Register containers with a new UUID upon every new connection

For now I'll implement the first solution.",2
"FileJdbcTest & JdbcHdfsTest failing JdbcHdfsTest, FileJdbcTest works for singlenode but not for admin & Container on the same machine.",5
"Zookeeper NoNode exception when deploying stream Same problem on M6 and using BUILD-SNAPSHOT.

When deploying a stream that has a slow-starting component (that connects to Gemfire), the deployment fails with a ZK NoNode exception.

No log from the component seen, but in all honesty, the component could be waiting on a timeout.

org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/2af8624b-777c-4084-aa1a-9d675b53afe3/test1.sink.reactor-batching-client-1/metadata
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676)
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656)
	at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441)
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431)
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)
	at org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:370)
	at org.springframework.xd.dirt.server.ContainerRegistrar.access$800(ContainerRegistrar.java:93)
	at org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:706)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)",2
"ZooKeeper Job deployments path state is not updated after successful deployment After successful job deployment, the Job deployments path in ZK doesn't get updated with the data {""state"": ""deployed""}

Though this data is not used for deployed instance repository (org.springframework.xd.dirt.stream.zookeeper.ZooKeeperJobRepository) to check for the deployment status, it may be better to have this state updated like stream deployment path.",1
"Re-deployment of stream/job modules upon container departure doesn't choose appropriate container candidates Upon container departure, the ContainerListener's onChildLeft() event triggers redeployment of stream/job modules that were deployed into the leaving container. During the redeployment, it happens that the container candidates from the DefaultContainerMatcher *sometimes* (based on the subset from distributeForRequestedCount(List<Container> candidates, int count))  includes the container which already have the module of the *same* stream/job definition deployed. This causes the re-deployment silently swallowing the NodeExistsException and the module being re-deployed doesn't actually get deployed.
",4
"hdfs sink loads Codecs class during 'module info --name sink:hdfs' command The hdfs sink metadata causes loading of  org.springframework.data.hadoop.store.codec.Codecs class during 'module info --name sink:hdfs' command since the type is a specific Spring Hadoop class

options.codec.description = compression codec alias name
options.codec.type = org.springframework.data.hadoop.store.codec.Codecs
options.codec.default =

Don't think we want to tie the sink module to specific Spring Hadoop classes during runtime of the admin, we can't be sure that admin has hadoop classes on classpath in all environments and there is no way of specifying the hadoop distro for admin.

Wouldn't it be better to have this option as a String to be passed in to the module's context that could then load the class",3
"Accessing non-existing module causes NullPointerException This source exists:
{code}
http://localhost:9393/modules/source/time
{code}
But trying to access a non-existing source such as:
{code}
http://localhost:9393/modules/source/time2
{code}
Triggers in the UI: 
{code}
[{""links"":[],""logref"":""NullPointerException"",""message"":""NullPointerException""}]
{code}
On the server-side:
{code}
6:03:45,387 ERROR http-nio-9393-exec-3 rest.RestControllerAdvice:199 - Caught exception while handling a request
java.lang.NullPointerException
	at org.springframework.xd.dirt.rest.DetailedModuleDefinitionResourceAssembler.toResource(DetailedModuleDefinitionResourceAssembler.java:49)
	at org.springframework.xd.dirt.rest.ModulesController.info(ModulesController.java:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:689)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:852)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:621)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:728)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilter(WebRequestTraceFilter.java:115)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextFilterConfiguration$1.doFilterInternal(EndpointWebMvcAutoConfiguration.java:137)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:85)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:222)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472)
	at org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:680)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:407)
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1680)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:744)
{code}

Accessing a non existing resource should probably result in a 404 status code.",3
"NPE when a container departs When a container departs the cluster the admin will try to redeploy any modules that container was running. If the stream was *destroyed* and the container exited before it had the chance to clean up its deployments under {{/xd/deployments/modules}} (for example, with {{kill -9}}) the following NPE occurs:

{noformat}
java.lang.NullPointerException
	at org.springframework.xd.dirt.server.ContainerListener.loadStream(ContainerListener.java:347)
	at org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:403)
	at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:158)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
{noformat}

If the stream was *undeployed* the following stack appears:
{noformat}
15:13:06,002 ERROR ContainersPathChildrenCache-0 cache.PathChildrenCache:550 - 
java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/streams/t0
	at org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:468)
	at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:159)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:744)
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/streams/t0
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:302)
	at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:291)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:287)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:279)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:41)
	at org.springframework.xd.dirt.server.ContainerListener.loadStream(ContainerListener.java:358)
	at org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:417)
	... 16 more
{noformat}

In short, this logic makes the assumption that the stream is still present and deployed. It needs to take into account the fact that neither assumption can be made.",2
Spring XD using Redis as data transport is failing to start in CI Acceptance Test. ,5
"Rest: Improve the determination whether a Job Execution is Restartable In *BatchJobExecutionsController$restartJobExecution()* we need to do a better check whether a Batch Job Execution is restartable. 

This is also true when executing *BatchJobExecutionsController$list()*. The check performed under *new JobExecutionInfo(jobExecution, timeZone)* is not sufficient.

*Reason*:

Currently in the UI when I have failed Job Executions, I can restart those (good). However, if the next execution succeeds, the previously restartable jobs should NOT be marked as restartable anymore. 

Right now you can restart those jobs, resulting in a:

{code}
Caused by: org.springframework.batch.core.repository.JobInstanceAlreadyCompleteException: A job instance already exists and is complete for parameters={random=0.5735953106895085, throwError=true}.  If you want to run this job again, change the parameters.
	at org.springframework.batch.core.repository.support.SimpleJobRepository.createJobExecution(SimpleJobRepository.java:126)
	at sun.reflect.GeneratedMethodAccessor211.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.transaction.interceptor.TransactionInterceptor$1.proceedWithInvocation(TransactionInterceptor.java:98)
	at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:262)
	at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:95)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.batch.core.repository.support.AbstractJobRepositoryFactoryBean$1.invoke(AbstractJobRepositoryFactoryBean.java:172)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy40.createJobExecution(Unknown Source)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:125)
	at sun.reflect.GeneratedMethodAccessor209.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:117)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy42.run(Unknown Source)
	at org.springframework.batch.integration.launch.JobLaunchingMessageHandler.launch(JobLaunchingMessageHandler.java:50)
	at sun.reflect.GeneratedMethodAccessor208.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:63)
	at org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:95)
	at org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:44)
	at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:258)
	at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:84)
	at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:114)
	at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)
	at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:159)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:268)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)
	... 98 more
{code}



",4
"Upon a container departure, redeployment of batch job fails on an existing container When there are multiple containers (A, B and C) and a batch job is deployed into one of the containers A. When the container A goes down, the admin server tries re-deploy the job module that was deployed in container A into other matching container. But, when the re-deployment happens, it tries to update the distributed job locator as if a new job is being deployed and following exception is thrown:

17:13:38,811 ERROR DeploymentsPathChildrenCache-0 cache.PathChildrenCache:550 - 
java.lang.RuntimeException: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'job': Post-processing of the FactoryBean's object failed; nested exception is org.springframework.xd.dirt.job.BatchJobAlreadyExistsException: Batch Job with the name myjob3 already exists
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployJob(ContainerRegistrar.java:411)
	at org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:355)
	at org.springframework.xd.dirt.server.ContainerRegistrar.access$8(ContainerRegistrar.java:349)
	at org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:695)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:253)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'job': Post-processing of the FactoryBean's object failed; nested exception is org.springframework.xd.dirt.job.BatchJobAlreadyExistsException: Batch Job with the name myjob3 already exists
	at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:167)
	at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.getObjectFromFactoryBean(FactoryBeanRegistrySupport.java:103)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getObjectForBeanInstance(AbstractBeanFactory.java:1514)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:252)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:699)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:648)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:311)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:130)
	at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:241)
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:186)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:176)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:166)
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployModule(ContainerRegistrar.java:230)
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployJob(ContainerRegistrar.java:399)
	... 20 more
Caused by: org.springframework.xd.dirt.job.BatchJobAlreadyExistsException: Batch Job with the name myjob3 already exists
	at org.springframework.xd.dirt.plugins.job.DistributedJobLocator.addJob(DistributedJobLocator.java:114)
	at org.springframework.xd.dirt.plugins.job.BatchJobRegistryBeanPostProcessor.postProcessAfterInitialization(BatchJobRegistryBeanPostProcessor.java:106)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:421)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.postProcessObjectFromFactoryBean(AbstractAutowireCapableBeanFactory.java:1698)
	at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:164)
	... 36 more",2
"Stream should not be in deployed state following module failure.  Run singlenode. Ensure twitterstream credentials are not valid. e.g.,  no consumerKey property. This is the default state.

>stream create tweets --definition ""twitterstream | log"" --deploy
Created and deployed stream 'tweets'

Meanwhile, Singlenode throws an exception, the stacktrace below 

xd:>stream list
  Stream Name  Stream Definition    Status
  -----------  -------------------  --------
  tweets       twitterstream | log  deployed

{code}
15:54:07,298 ERROR DeploymentsPathChildrenCache-0 cache.PathChildrenCache:550 -
java.lang.RuntimeException: org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'twitterTemplate' defined in URL [file:/Users/dturanski/spring-xd/spring-xd-1.0.0.M6/xd/modules/source/twitterstream/config/twitterstream.xml]: Could not resolve placeholder 'consumerKey' in string value ""${consumerKey}""; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'consumerKey' in string value ""${consumerKey}""
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployStreamModule(ContainerRegistrar.java:448)
	at org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:347)
	at org.springframework.xd.dirt.server.ContainerRegistrar.access$700(ContainerRegistrar.java:93)
	at org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:678)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'twitterTemplate' defined in URL [file:/Users/dturanski/spring-xd/spring-xd-1.0.0.M6/xd/modules/source/twitterstream/config/twitterstream.xml]: Could not resolve placeholder 'consumerKey' in string value ""${consumerKey}""; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'consumerKey' in string value ""${consumerKey}""
{/code}",3
"Tab completion does not work for stream definition following >  >stream create ""tap:stream:foo > 

does not suggest modules",8
"Undeploy modules when container disconnected from ZK Consider a module running in a container when it is disconnected from ZK:

{noformat}
12:30:13,021  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:13
12:30:14,025  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:14
12:30:15,029  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:15
12:30:16,031  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:16
12:30:32,590  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:32
12:37:42,985  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:42
12:37:43,398  INFO main-SendThread(fe80:0:0:0:0:0:0:1%1:2181) zookeeper.ClientCnxn:1096 - Client session timed out, have not heard from server in 430809ms for sessionid 0x145662be03e0002, closing socket connection and attempting reconnect
12:37:43,985  INFO main-SendThread(localhost:2181) zookeeper.ClientCnxn:975 - Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
12:37:43,986  INFO main-SendThread(localhost:2181) zookeeper.ClientCnxn:852 - Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session
12:37:43,988  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:43
12:37:43,989  INFO main-SendThread(localhost:2181) zookeeper.ClientCnxn:1094 - Unable to reconnect to ZooKeeper service, session 0x145662be03e0002 has expired, closing socket connection
{noformat}

Currently the module for the disconnected container continues to execute:

{noformat}
12:37:45,994  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:45
12:37:46,997  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:46
12:37:48,000  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:47
12:37:48,094 ERROR main-EventThread imps.CuratorFrameworkImpl:512 - Watcher exception
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /xd
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:302)
	at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:291)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:287)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:279)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:41)
	at org.springframework.xd.dirt.server.ContainerRegistrar$StreamModuleWatcher.process(ContainerRegistrar.java:609)
	at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:56)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
12:37:48,097  INFO main-EventThread state.ConnectionStateManager:194 - State change: SUSPENDED
12:37:48,097  INFO ConnectionStateManager-0 zookeeper.ZooKeeperConnection:262 - >>> Curator disconnected event: SUSPENDED
12:37:48,097  WARN ConnectionStateManager-0 server.ContainerRegistrar:325 - >>> disconnected container: 88ba115b-6190-497a-a67c-df1e295bf158
12:37:49,001  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:49
12:37:50,004  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:50
12:37:51,008  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:51
12:37:52,012  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:52
12:37:53,016  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:53
12:37:54,021  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:54
12:37:55,023  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:55
...
{noformat}

This container should not continue executing the module because the leader admin will likely select another container to execute this module. If and when this container reconnects to ZK, it can be (re)assigned modules for deployment.

This can be done via a simple undeployment; or we may even consider closing and reopening the application context.",4
"clean up dead entries in ZooKeeper /xd/deployments/modules When starting and stopping xd containers there are entries left in the /xd/deployments/modules directory that will cause 'runtime modules' command to fail.

xd:>runtime modules 
Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/5201ac3f-e952-48a2-a807-4f0bb2dab82b/test.sink.hdfs-1/metadata

here the ""/xd/deployments/modules/5201ac3f-e952-48a2-a807-4f0bb2dab82b"" container is no longer running, but there is some data left over.
",3
"All jobs end up on the same container node The jobs aren't spread evenly across available container nodes as they are created/deployed. I had 3 nodes but only one has the job modules.

[zk: localhost:2181(CONNECTED) 56] ls /xd/deployments/modules/621230e0-a089-4fbe-afc8-611ae527fcbc
[myjob9.job.jdbchdfs-0, myjob5.job.jdbchdfs-0, myjob8.job.jdbchdfs-0, myjob4.job.jdbchdfs-0, myjob6.job.jdbchdfs-0, myjob7.job.jdbchdfs-0]
[zk: localhost:2181(CONNECTED) 57] ls /xd/deployments/modules/6969579c-0cf4-4cc1-8e21-e01d73a70965
[]
[zk: localhost:2181(CONNECTED) 58] ls /xd/deployments/modules/d0667cd1-a57a-4279-b7fb-dd63e4dd40d4
[]

",3
"Prevent submiting jobs that are not currently deployed using Admin UI Job modules ""Launch"" and ""Schedule"" command buttons are active even if the job module isn't deployed or has been destroyed.

Get errors like:
 ""Yikes, something bad happened while launching job myjob4""
""The job named 'myjob4' is not currently deployed""",3
"Investigate failing LocalSingleNodeStreamDeploymentIntegrationTests Investigate the failing test LocalSingleNodeStreamDeploymentIntegrationTests.moduleChannelsRegisteredWithMessageBus:

{noformat}
java.lang.AssertionError: expected:<3> but was:<0>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.springframework.xd.dirt.stream.AbstractSingleNodeStreamDeploymentIntegrationTests.moduleChannelsRegisteredWithMessageBus(AbstractSingleNodeStreamDeploymentIntegrationTests.java:270)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
{noformat}

This can be most easily reproduced on Ubuntu.",1
"Stream deployment race condition When a container is started, the leader admin will scan the deployed streams to determine if any have modules that need to be deployed on the new container. 

When a stream is deployed, the leader admin will select containers to deploy modules to.

If a new container and stream are deployed at the same time, there is the window for a race condition where both attempt to deploy a module to a container. This can be solved by (at least one) of the following:

* Consider using a single thread in the admin leader to handle all ZooKeeper updates. This means that the handling of new containers and stream deployment requests will not happen concurrently.
* Trap the {{NodeExists}} exception when creating the {{/xd/deployments/modules/...}} node in ZooKeeper",5
"Exception thrown when accessing Jolokia via the management context path When trying to access Jolokia via the management/jolokia (http://localhost:9393/management/jolokia) I get the following exception.   

{""error_type"":""java.lang.IllegalArgumentException"",""error"":""java.lang.IllegalArgumentException : No type with name 'management' exists"",""status"":400,""stacktrace"":""java.lang.IllegalArgumentException: No type with name 'management' exists\n\tat org.jolokia.util.RequestType.getTypeByName(RequestType.java:69)\n\tat org.jolokia.request.JmxRequestFactory.createGetRequest(JmxRequestFactory.java:94)\n\tat org.jolokia.http.HttpRequestHandler.handleGetRequest(HttpRequestHandler.java:78)\n\tat org.jolokia.http.AgentServlet$3.handleRequest(AgentServlet.java:298)\n\tat org.jolokia.http.AgentServlet.handle(AgentServlet.java:229)\n\tat org.jolokia.http.AgentServlet.doGet(AgentServlet.java:194)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:621)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:728)\n\tat org.springframework.web.servlet.mvc.ServletWrappingController.handleRequestInternal(ServletWrappingController.java:158)\n\tat org.springframework.web.servlet.mvc.AbstractController.handleRequest(AbstractController.java:154)\n\tat org.springframework.boot.actuate.endpoint.mvc.JolokiaMvcEndpoint.handle(JolokiaMvcEndpoint.java:120)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)\n\tat org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:689)\n\tat org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)\n\tat org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938)\n\tat org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870)\n\tat org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)\n\tat org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:852)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:621)\n\tat org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:728)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilter(WebRequestTraceFilter.java:115)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextFilterConfiguration$1.doFilterInternal(EndpointWebMvcAutoConfiguration.java:137)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:85)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:222)\n\tat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123)\n\tat org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472)\n\tat org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:680)\n\tat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)\n\tat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99)\n\tat org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)\n\tat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:407)\n\tat org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)\n\tat org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)\n\tat org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1680)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:724)\n""}",5
"Hadoop distro option hdp20 is broken Starting the shell with --hadoopDistro hdp20 causes this:

Exception in thread ""main"" org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Unable to locate Spring NamespaceHandler for XML schema namespace [http://www.springframework.org/schema/hadoop]
Offending resource: URL [jar:file:/Users/trisberg/Demo/spring-xd-1.0.0.BUILD-SNAPSHOT/shell/lib/spring-xd-shell-1.0.0.BUILD-SNAPSHOT.jar!/META-INF/spring/spring-shell-plugin.xml]

Creating a stream ""time | hdfs"" in xd-singlenode started with --hadoopDistro hdp20 causes this:

java.lang.IllegalStateException: Can't find class used for type of option 'codec': org.springframework.data.hadoop.store.codec.Codecs
",3
"job launch doesn't do tab completion of --name In the shell:

job launch doesn't do completion of --name, this is different behavior compared to job destroy

typing 'job destroy' and hitting tab completes with '--name'

typing 'job launch' and hitting tab does nothing",5
Re-deployment of hdfs sink reuses filename of first deployment Need to check for existing files with the same file counter,3
"The lib directory for hadoop12 contains mix of hadoop versions This causes issues depending on which version of the core/common jar gets loaded first - like:

xd:>hadoop fs ls
-ls: Fatal internal error
java.lang.UnsupportedOperationException: Not implemented by the DistributedFileSystem FileSystem implementation
at org.apache.hadoop.fs.FileSystem.getScheme(FileSystem.java:213)
at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2401)
at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2411)
at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2428)
at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:88)
at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2467)
at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2449)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:367)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:166)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:351)
at org.apache.hadoop.fs.Path.getFileSystem(Path.java:287)
at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325)
at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:224)
at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:207)
at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:190)
at org.apache.hadoop.fs.shell.Command.run(Command.java:154)
at org.apache.hadoop.fs.FsShell.run(FsShell.java:255)
at org.springframework.xd.shell.hadoop.FsShellCommands.run(FsShellCommands.java:412)
at org.springframework.xd.shell.hadoop.FsShellCommands.runCommand(FsShellCommands.java:407)
at org.springframework.xd.shell.hadoop.FsShellCommands.ls(FsShellCommands.java:110)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.springframework.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:191)
at org.springframework.shell.core.SimpleExecutionStrategy.invoke(SimpleExecutionStrategy.java:64)
at org.springframework.shell.core.SimpleExecutionStrategy.execute(SimpleExecutionStrategy.java:48)
at org.springframework.shell.core.AbstractShell.executeCommand(AbstractShell.java:127)
at org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:483)
at org.springframework.shell.core.JLineShell.run(JLineShell.java:157)
at java.lang.Thread.run(Thread.java:724)
",3
Failure in writing to HDFS when undeploying and redeploying a stream with numbers in directory and/or file name ,3
"Close HDFS file when Batch job ends The File to HDFS batch job will not close the file being written to in HDFS when the job completes.  The ItemWriter for HDFS needs to incorporate functionality that is present in the standard FlatFileWriter, perhaps inheriting from AbstractItemStreamItemWriter",6
"File to HDFS batch job fails due to ""/data"" directory not available in HDFS The batch job for File to HDFS will try to check for the default '/data/' directory even if the target directory in HDFS is something else.  If the /data directory isn't there, the job will fail.

This should be fixed so there isn't a check on the directory that isn't the final HDFS target directory and the target directory should be created if it doesn't exist.",4
"Topic channels are not broadcasting This needs to work for all transports (local, rabbit, and redis), and we need to ensure that we have test coverage for each of those to avoid any regressions.

The incorrect behavior was observed with all three transports:

{code}
xd:>stream create a --definition ""topic:foo > transform --expression=payload+'-a' | log""
Created new stream 'a'

xd:>stream create b --definition ""topic:foo > transform --expression=payload+'-b' | log""
Created new stream 'b'

xd:>stream create s --definition ""http > topic:foo""
Created new stream 's'

xd:>http post --data hi
> POST (text/plain;Charset=UTF-8) http://localhost:9000 hi
> 200 OK

// only one line in log!
{code}
",5
"Batch Job's step execution count is always '0' The batch job's step execution count is retrieved from org.springframework.batch.admin.web.JobExecutionInfo in batch job repository.

But, the JobExecutionInfo always have the stepExecutionCount set to '0'.",2
"Spring Batch Admin UI looks to localhost when getting status updates Currently when you bring up a admin-ui on a browser that is not on the XD admin server, the page reports an error.

This is because the javascript is looking to localhost to get its updates.  ",5
"Running gradle idea creates project configured with source 1.6 The gradle idea task creates a project configured with source 1.6

This results in compile failures on Java 7 specific code",3
"Provide a way to access currently deployed modules For testing, it would be useful to access the deployed Module instances to connect sources and or sinks to a module's input and output channels, etc. This could be a simple as exposing the deployedModuleMap on ModuleDeployer or possibly something more elaborate if this level of granularity is generally useful for runtime administration.",3
"No indication of failure in shell when deploying job referencing nonexistent trigger I see the following output on the shell if I create a job and reference a non-existent trigger. There's a big stack trace in the server log, but nothing on the shell side indicating failure. A subsequent ""jobs list"" also shows the job. The same thing happens if I deploy an undeployed Job after deleting its associated Trigger.

$ job create --name helloWorldJob --definition ""myjob --trigger=nonexistenttrigger""
Successfully created and deployed job 'helloWorldJob'",2
"Problem with tapping and > (source channels) This is a follow on from XD-592. In that bug we fixed up the ability to use tap with pipe.  Tap when used as a source channel should also work (and should deploy in a more optimized fashion since source channels can be directly connected to the subsequent module, creation of a pass-through tap instance isn't necessary).  This test shows the syntax that should work and the current information about how it fails:

{code}
public void testTapSourceChannel() throws IOException {
  FileSink sink1 = newFileSink();
  FileSink sink2 = newFileSink();

  stream().create(""myhttp"",
    ""http --port=9314 | transform --expression=payload.toUpperCase() | filter --expression=true > :foo"");

  // fails with: java.lang.IllegalArgumentException: bean 'myhttp.1' is already
  // registered but does not match the required type
  tap().create(""wiretap1"", ""tap myhttp.transform > transform --expression=payload.replaceAll('a','.') | %s"", sink1);

  // fails in TapDefinition ctor with: java.lang.IllegalArgumentException:
  // streamName cannot be empty or null
  tap().create(""wiretap2"", ""tap :foo > transform --expression=payload.replaceAll('a','.') | %s"", sink2);

  httpPostData(""http://localhost:9314"", ""Dracarys!"");
}
{code}

I suspect part of the problem initially lies with the code around EnhancedStreamParser that builds the module deployment requests from the Ast parsed from the input DSL string.  Whether a source channel was originally specified with 'tap' is captured in that Ast but that knowledge doesn't appear to be getting used.

",4
"Cannot start xd-container with the --hadoopDistro option Trying to use xd-container with PHD, and therefore need to start with --hadoopDistro. I get the following error:

$ bin/xd-container --hadoopDistro phd1
17:11:20,305 ERROR main server.ContainerMain:59 - ""--hadoopDistro"" is not a valid option
",3
"StreamCommandTests - asserting sink contents sometimes failing There are some asserts in StreamCommandTests that are commented out (see the TODOs in there). These asserts are verifying the contents of the various sinks employed in the tests. I am finding that if the tests are run all together with the asserts enabled (run all StreamCommandTests), some of the assertions fail, with something like:

{code}
org.junit.ComparisonFailure: expected:<[DRACARYS!
]> but was:<[]>
	at org.junit.Assert.assertEquals(Assert.java:115)
	at org.junit.Assert.assertEquals(Assert.java:144)

{code}

When run individually the tests succeed. Not sure if it is timing (checking sinks before they've been written to) or something else...",2
"Problem with tapping on a module using a named sink channel {code}
mystream = http | transform --payload=expression.toUpperCase() > :foo
tap mystream.transform | log
{code}
This appears to fail because we can't tap into whatever was created to represent the named channel 'foo'. There is an @Ignore test in StreamCommandTests called testTappingModulesVariationsWithSinkChannel() which checks this.

(The parser is currently resolving 'tap mystream.transform' to 'tap --channel=foo'.)",2
Streams created without a '|' (substreams) are being typed by the parser as a Job ,3
"NPE on stream destroy xd:>stream create ticktock --definition ""time | log"" --deploy true
18:45:13,310  WARN Spring Shell client.RestTemplate:524 - POST request for ""http://localhost:8080/streams"" resulted in 400 (Bad Request); invoking error handler
Command failed org.springframework.xd.rest.client.impl.SpringXDException: There is already a stream named 'ticktock'

xd:>stream destroy ticktock
18:45:16,505  WARN Spring Shell client.RestTemplate:524 - DELETE request for ""http://localhost:8080/streams/ticktock"" resulted in 500 (Internal Server Error); invoking error handler
Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.NullPointerException


Caused by: java.lang.NullPointerException
	at org.springframework.xd.dirt.module.ModuleDeployer.undeploy(ModuleDeployer.java:143)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:97)


",2
"Add deploy/undeploy/destroy 'all' commands for all applicable resources (streams, tap, job & trigger,) ",5
"Problems with advanced tapping Start of a test program that can be placed in StreamCommandTests:

{code}
@Test
public void testTappingAndChannels() {
  executeStreamCreate(""myhttp"",""http --port=9314 | transform --expression=payload.toUpperCase() | log"",true);
  executeStreamCreate(""tap"",""tap @myhttp.1 | log"",true);		
  executeStreamCreate(""tap_new"",""tap myhttp.1 > log"",true);				
  executeCommand(""http post --data Dracarys! --target http://localhost:9314"");
  // TODO verify both logs output DRACARYS!
}

{code}

In the test program see two taps. One using the older style and one using the newer style and '>' so that there is no real tap module source, the log module just gets its input channel wired directly to myhttp.1 (the output of transform).  They should be doing the same thing.  However when run the output for tap_new is missing, all I see is:

{code}
11:39:36,055  WARN New I/O worker #28 logger.tap:141 - DRACARYS!
11:39:36,059  WARN New I/O worker #28 logger.myhttp:141 - DRACARYS!
{code}

No errors are reported, there is just no output for tap_new.",8
"Deploying with twittersearch source throws Jackson ClassDefNotFound exception The upgrade to Jackson 2.2 included the following change to the build script
{code}
project('spring-xd-dirt') {
	description = 'Spring XD DIRT'
	configurations {
	  [runtime,testRuntime]*.exclude group: 'org.codehaus.jackson'
	}
{code}

Spring social twitter template depends on these classes
",1
"Parsing stream definition with parameter containing single quotes not working The documented gemfire-cq example (https://github.com/springsource/spring-xd/wiki/Sources#wiki-gemfire-cq) fails:

xd:>stream create --name cqtest --definition ""gemfire-cq --query=""Select * from /Stocks where symbol=
'VMW'"" | file""
You cannot specify option 'name' when you have also specified '' in the same command
xd:>stream create --name cqtest --definition ""gemfire-cq --query=Select * from /Stocks where symbol='
VMW' | file""
10:01:46,249  WARN Spring Shell client.RestTemplate:524 - POST request for ""http://localhost:8080/str
eams"" resulted in 400 (Bad Request); invoking error handler
Command failed org.springframework.xd.rest.client.impl.SpringXDException: XD115E:(pos 26): unexpected
 data in stream definition '*'
gemfire-cq --query=Select * from /Stocks where symbol='VMW' | file
                          ^
",0
The parser should be able to handle a parameter name with a '-' hyphen embedded. Right now it treats it as a new parameter start and fails.,1
"Investigate stream lifecycle issues with redis store When running with ""./gradlew launch"" I get a 500 error when I try to re-deploy an undeployed stream. A subsequent create or destroy also fail

http://localhost:8080:>stream create --definition ""time | log"" --name ticktock
Created new stream 'ticktock'
http://localhost:8080:>stream undeploy --name ticktock
Un-deployed stream 'ticktock'
http://localhost:8080:>stream deploy --name ticktock
13:02:09,936  WARN Spring Shell client.RestTemplate:524 - PUT request for ""http://localhost:8080/streams/ticktock"" resulted in 500 (Internal Server Error); invoking error handler
Command failed org.springframework.web.client.HttpServerErrorException: 500 Internal Server Error
http://localhost:8080:>stream deploy --name ticktock
13:03:11,453  WARN Spring Shell client.RestTemplate:524 - PUT request for ""http://localhost:8080/streams/ticktock"" resulted in 500 (Internal Server Error); invoking error handler
Command failed org.springframework.web.client.HttpServerErrorException: 500 Internal Server Error
http://localhost:8080:>stream undeploy --name ticktock
13:03:54,576  WARN Spring Shell client.RestTemplate:524 - PUT request for ""http://localhost:8080/streams/ticktock"" resulted in 500 (Internal Server Error); invoking error handler
Command failed org.springframework.web.client.HttpServerErrorException: 500 Internal Server Error
http://localhost:8080:>stream undeploy --name ticktock
13:04:48,872  WARN Spring Shell client.RestTemplate:524 - PUT request for ""http://localhost:8080/streams/ticktock"" resulted in 500 (Internal Server Error); invoking error handler
Command failed org.springframework.web.client.HttpServerErrorException: 500 Internal Server Error
http://localhost:8080:>stream create --definition ""time | log"" --name ticktock
13:04:52,066  WARN Spring Shell client.RestTemplate:524 - POST request for ""http://localhost:8080/streams"" resulted in 500 (Internal Server Error); invoking error handler
Command failed org.springframework.web.client.HttpServerErrorException: 500 Internal Server Error
http://localhost:8080:>stream destroy --name ticktock
13:05:14,207  WARN Spring Shell client.RestTemplate:524 - DELETE request for ""http://localhost:8080/streams/ticktock"" resulted in 500 (Internal Server Error); invoking error handler
Command failed org.springframework.web.client.HttpServerErrorException: 500 Internal Server Error

",1
"Creating a tap with same name as existing streams results in infinite loop See http://stackoverflow.com/questions/17157068/counter-analytics-in-springxd

The underlying issue is stream creation with a name already taken though",5
