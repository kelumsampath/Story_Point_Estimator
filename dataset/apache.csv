Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Labels,Labels,Labels,Description,Environment,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Cloners),Outward issue link (Container),Outward issue link (Container),Outward issue link (Duplicate),Outward issue link (Incorporates),Outward issue link (Problem/Incident),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Regression),Outward issue link (Required),Outward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Custom field (Story Points),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Occational MIsses on Indexing into Elasticsearch,USERGRID-1027,12888688,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,,greyes,greyes,18/Sep/15 21:51,27/Oct/15 20:57,29/Oct/20 16:22,27/Oct/15 20:57,2.1.0,,,,,,,,,,,,,,Stack,,,,,0,landmine,,,,,,,,Occasionally applications still aren't being indexed into elasticsearch. Need to investigate why these might be occuring. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-810,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-09-18 21:51:30.0,,,,,,,"0|i001pr:",9223372036854775807,,,,,,,,,,,Usergrid 30,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Re-index using AmazonAsyncEventService should support updated date,USERGRID-900,12851255,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,tnine,mrusso,mrusso,04/Aug/15 00:04,02/Feb/16 21:49,29/Oct/20 16:07,11/Aug/15 16:14,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,"see implemented example for InMemoryAsyncEventService

org/apache/usergrid/corepersistence/asyncevents/EventBuilderImpl.java:166

This filters out based on modified/updated date for the events to be indexed.  Need to ensure AmazonAsyncEventService consumers implement something similar OR look to have the producer filter out the events and not even drop on the AWS queues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-08-11 16:14:07.23,,,false,USERGRID-603,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 11 16:14:07 UTC 2015,,,,,,,"0|i2i1pz:",9223372036854775807,,,,,,,,,,,Usergrid 27,,,,,,,,,,,3.0,,,,,,,,,,,"11/Aug/15 16:14;tnine;Fixed in these PRS

https://github.com/apache/incubator-usergrid/pull/337

https://github.com/apache/incubator-usergrid/pull/338

https://github.com/apache/incubator-usergrid/pull/339/files

",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix and prune Usergrid tools in 2.1,USERGRID-872,12846883,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,Fixed,djohnson,djohnson,djohnson,22/Jul/15 12:38,02/Feb/16 21:49,29/Oct/20 16:31,05/Jan/16 19:11,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,"The Usergrid tools are broken in a variety of ways in the two-dot-o and two-dot-o-dev brands. The tools do not compile. Many rely on outdated 1.0 concepts. Many are obsolete.

1. Delete any obsolete tools
2. Make all tools compile",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-05 19:11:50.904,,,false,USERGRID-837,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 05 19:11:50 UTC 2016,,,,,,,"0|hzzzwm:vr",9223372036854775807,,,,,,,,,,,Usergrid 32,,,,,,,,,,,3.0,,,,,,,,,,,"30/Oct/15 21:11;djohnson;PR is ready for merge: https://github.com/apache/usergrid/pull/420","05/Jan/16 19:11;jeffreyawest;Tests don't seem to pass for this but we can address another time.",,,,,,,,,,,,,,,,,,,,,,,,,,
Notifications POST throwing NPE,USERGRID-858,12845321,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,Fixed,mrusso,brandonshelley,brandonshelley,15/Jul/15 17:34,11/May/16 06:59,29/Oct/20 16:31,12/Jan/16 22:23,1.0.0,2.1.0,,,,,,,2.1.0,,,,,,Stack,,,,,0,,,,,,,,,"{code}curl -X POST -d '{ ""payloads"": { ""apple-dev"": ""Hello World!"" } }' 'https://api.usergrid.com/brandon.apigee/baas-integration-tests/devices/*/notifications?client_id=redacted&client_secret=redacted'{code}

throws:

{code}{
  ""error"": ""runtime"",
  ""timestamp"": 1436981233645,
  ""duration"": 0,
  ""exception"": ""java.lang.RuntimeException"",
  ""error_description"": ""java.lang.NullPointerException""
}{code}

1) According to docs, it should work: http://apigee.com/docs/app-services/content/creating-and-managing-notifications

2) If it doesn't and isn't supposed to, it shouldn't throw an NPE.

Note that a notifier with a sandbox cert has been created with the name 'apple-dev'.

---
UPDATE

It does work with {code}/devices;ql=select */notifications{code}

we should address this but also update the documentation to reflect the way in which it works.

---
UPDATE

It also appears that although the NPE is returned via the API, a notification *is* created in /notifications. In this particular test, there were no devices in the /devices collection:

{code}{
  ""uuid"": ""11c5e62a-2a84-11e5-af28-65b8f0f35f5f"",
  ""type"": ""notifications"",
  ""created"": 1436918242690,
  ""modified"": 1436918242690,
  ""payloads"": {
    ""apple-dev"": ""Hello World!""
  },
  ""debug"": false,
  ""state"": ""CREATED"",
  ""metadata"": {
    ""path"": ""/notifications/11c5e62a-2a84-11e5-af28-65b8f0f35f5f"",
    ""collections"": {
      ""receipts"": ""/notifications/11c5e62a-2a84-11e5-af28-65b8f0f35f5f/receipts""
    }
  }
}{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,USERGRID-971,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-07-15 17:40:17.535,,,false,USERGRID-814,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 12 01:44:15 UTC 2016,,,,,,,"0|i001pr:000000010coz",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,"15/Jul/15 17:40;jeffreyawest;Does it work on  
{code}/devices;ql=select */notifications{code}?","15/Jul/15 17:41;brandonshelley;Yes","12/Jan/16 01:44;mrusso;This is being fixed in 2.1 +. ",,,,,,,,,,,,,,,,,,,,,,,,,
SNS<->SQS Subscriptions Not Created cross-region,USERGRID-798,12844446,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,mrusso,jeffreyawest,jeffreyawest,12/Jul/15 16:17,15/Jul/15 20:40,29/Oct/20 16:31,12/Jul/15 23:40,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-07-12 23:40:57.562,,,false,USERGRID-731,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 12 23:40:57 UTC 2015,,,,,,,"0|i001n3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,"12/Jul/15 23:40;mrusso;https://github.com/apache/incubator-usergrid/pull/304

Fixed by removing Topics.subscribe as it has issues subscribing cross-regions.  Usergrid now adds subscribes from the SNS client and then adds specific policies to allow the SNS ARNs in all regions.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Org (and presumably App) credentials can't create connections by default,USERGRID-797,12844272,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,brandonshelley,brandonshelley,10/Jul/15 20:10,11/May/16 06:59,29/Oct/20 16:31,,1.0.0,2.1.0,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"If you try to create a new connection using org or app credentials, it will fail with:

{code}
{
    ""duration"": 1,
    ""error"": ""unauthorized"",
    ""error_description"": ""Subject does not have permission [applications:post:uuid:/col1/uuid/relationship/uid]"",
    ""exception"": ""org.apache.shiro.authz.UnauthorizedException"",
    ""timestamp"": ""....""
}
{code}

The only workaround seems to be to give {code}POST /**{code} to the Guest role.

Expected behavior: app and org credentials should (out of the box) both be able to create and delete connections, as well as retrieve all depths of them:

{code}GET /collection/uuid/verb
GET /collection/uuid/connecting/verb
GET /collection/uuid/connections/verb{code}
etc.

In short, all tests in the URAP integration_tests should pass on a net new instance without having to set Guest role permissions to {code}/**{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-07-13 23:29:03.265,,,false,USERGRID-827,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 13 23:29:03 UTC 2015,,,,,,,"0|i001pr:000000010cozzzzzx",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"13/Jul/15 23:29;greyes;Just commenting to say that app credentials also don't work. ( Confirmed )",,,,,,,,,,,,,,,,,,,,,,,,,,,
"'order by created desc' causing no results in location query (affects 1.0, not sure about 2.0)",USERGRID-787,12843076,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,,,brandonshelley,brandonshelley,07/Jul/15 02:40,11/May/16 06:59,29/Oct/20 16:32,,1.0.0,,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"Making the following API call returns no results:

{code}https://api.usergrid.com/org/app/collection?ql=location%20within%201000%20of%2051.73213%2C%20-1.20631%20order%20by%20created%20desc{code}

but this one works:

{code}https://api.usergrid.com/org/app/collection?ql=location%20within%201000%20of%2051.73213%2C%20-1.20631{code}

Definitely affects 1.0, not tested against 2.x",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-817,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-07-07 02:40:14.0,,,,,,,"0|i001pr:000000010cohzzzzzz",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit on Queries results in a highly variable number of results,USERGRID-778,12842102,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,djohnson,jeffreyawest,jeffreyawest,01/Jul/15 20:42,02/Feb/16 21:49,29/Oct/20 16:32,11/Jan/16 20:56,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,"This behavior is observed on a build from the two-dot-o branch with commit ID 2d1c8b8ac7b20b63a11d83adca56839d8b409cca.

For example, limit=2 gives you 1 sometimes, limit=750 gives anywhere from 625 to 749.  For example this script:

{code}
#!/bin/bash
for count in `seq 1 10`
do 
    curl -s ""https://example.com/appservices/testorg/sandbox/scmocks?limit=750"" > file${count}
    grep uuid file${count} | wc
    rm file${count}
done
{code}

Produces these results:

     685    2055   36305
     750    2250   39750
     749    2247   39697
     742    2226   39326
     750    2250   39750
     749    2247   39697
     747    2241   39591
     744    2232   39432
     750    2250   39750
     749    2247   39697

A different count every time.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-07-02 14:47:02.946,,,false,USERGRID-817,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 11 20:56:23 UTC 2016,,,,,,,"0|i001pr:000000010cohzzzzzr",9223372036854775807,,,,,,,,,,,Usergrid 24,Usergrid 25,Usergrid 26,,,,,,,,,3.0,,,,,,,,,,,"02/Jul/15 14:47;djohnson;I cannot reproduce this locally on a system running the exact same code. With a collection of 2000 entities, a query with limit 750 always returns 750 entities.","06/Jul/15 18:49;djohnson;Having a hard time reproducing this problem in production, e.g. 20 queries in a a row an none failed:

$ ./curl.sh
    750    2250   39750
    750    2250   39750
    750    2250   39750
    750    2250   39750
    750    2250   39750
    750    2250   39750
    750    2250   39750
    750    2250   39750
    750    2250   39750
    750    2250   39750
    750    2250   39750
    750    2250   39750
    750    2250   39750
    750    2250   39750
    750    2250   39750
    750    2250   39750
    750    2250   39750
    750    2250   39750
    750    2250   39750
    750    2250   39750","11/Jan/16 20:56;jeffreyawest;We have made significant changes to how this works.  In 1.0, the result set will be less than the limit regularly even if there are more entities than specified in the limit.  In 2.1 the result set should include the number of entities specified in the limit if the number of entities >= limit.",,,,,,,,,,,,,,,,,,,,,,,,,
Refactor App Info Migration to make it functional,USERGRID-777,12842101,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,jeffreyawest,jeffreyawest,01/Jul/15 20:39,12/Jul/15 15:39,29/Oct/20 16:32,02/Jul/15 16:36,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,USERGRID-586,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-603,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-07-01 20:39:36.0,,,,,,,"0|i001lr:",9223372036854775807,,,,,,,,,,,Usergrid 24,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Queries without 'ql' return a Null Pointer instead of a meaningful message,USERGRID-776,12842043,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,,,jeffreyawest,jeffreyawest,01/Jul/15 17:10,02/Feb/16 22:09,29/Oct/20 16:05,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,"http://api.usergrid.com/vta/sandbox/stops?select * where location within 3 of 37.415449732, -121.920367767  - this will return an 500/NPE when it should be a 4xx, and preferably have a better error message.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-817,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-07-01 17:10:45.0,,,,,,,"0|i001pr:000000010cohx",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception being masked in ClientCredentialsSecurityFilter,USERGRID-774,12840890,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,jeffreyawest,jeffreyawest,26/Jun/15 15:45,02/Nov/15 17:41,29/Oct/20 16:05,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,"An exception is being masked and causing issues in a deployment.  Here is the class and line.  We should be more specific on what exceptions we catch and at least log them.

https://github.com/apache/incubator-usergrid/blob/49ae4ac5b8d5d77e90e6e6c6e9d8b299a5423863/stack/rest/src/main/java/org/apache/usergrid/rest/security/shiro/filters/ClientCredentialsSecurityFilter.java#L65
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-827,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-06-26 15:45:44.0,,,,,,,"0|i2gjsv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column Querying Issues,USERGRID-773,12840879,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,jeffreyawest,jeffreyawest,26/Jun/15 15:15,02/Nov/15 17:40,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,"From http://community.apigee.com/questions/2432/error-analysis-of-response-when-trying-to-curl-que.html#answer-2454.
I have seen similar issues. I am using the org devnexus and the app 2015, which should be publicly visible.
In the UI, if I set Path to /sessions and Query to select name,title,room where title contains 'Java', I get no response and no error. However, if I capture the query made by the browser (https://api.usergrid.com/devnexus/2015/sessions?ql=select%20name%2Ctitle%2Croom%20where%20title%20contains%20%27Java%27%20order%20by%20created%20desc&access_token=YWMtuzeubNpKEeSm49lERp5XKQAAAUylS2HCr3MkIzCtKZHIYLT1VUiiygN9uUs) using Chrome dev tools, the same query works with curl.
You can see the same result with something as simple as select title.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-817,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-06-26 15:15:47.0,,,,,,,"0|i004hj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tokens generated with app-level credentials cannot be used successfully in Header,USERGRID-770,12840554,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Blocker,Cannot Reproduce,mrusso,jeffreyawest,jeffreyawest,25/Jun/15 14:27,08/Feb/17 17:20,29/Oct/20 16:05,15/Jul/15 00:15,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,They can be used in the access_token qparam but not a header.  They need to work in the header as well.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-07-15 00:15:13.871,,,false,USERGRID-603,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 15 00:15:13 UTC 2015,,,,,,,"0|i001mv:",9223372036854775807,,,,,,,,,,,Usergrid 24,,,,,,,,,,,0.0,,,,,,,,,,,"15/Jul/15 00:15;mrusso;Unable to reproduce this in 2.0 or 2.1. ",,,,,,,,,,,,,,,,,,,,,,,,,,,
ACL not working as expected?,USERGRID-769,12840535,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,pandacat,pandacat,25/Jun/15 13:31,02/Nov/15 17:41,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,"When setting following ACL rule /assets/* user has access to /assets/xxx but also to /assets meaning that he can list all the assets.

I expected that when allowing /assets/* it would only allow /assets/xxx",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-827,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-06-25 13:31:18.0,,,,,,,"0|i2ghp3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Bad characters in query causes 500 response; expecting 400?",USERGRID-768,12840313,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,brandonshelley,brandonshelley,24/Jun/15 20:20,02/Feb/16 22:09,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,"If I make the following call:

https://api.usergrid.com/org/app/collection?ql=where%20thirdPropertyTypeInt%20%3E%2030000%20&&%20thirdPropertyTypeInt%20%3C%2040000&limit=10

I get the response:

{code}
{
  ""error"": ""null_pointer"",
  ""timestamp"": 1435176848310,
  ""duration"": 1,
  ""exception"": ""java.lang.NullPointerException""
}
{code}

The cause is that the ampersands in the query string aren't url encoded (should be %26 instead of &). I would expect that if the query string is bad, we should be returning a 400 Bad Request, rather than an ominous NullPointerException/500.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-817,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-06-24 20:20:01.0,,,,,,,"0|i0020f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Portal does not show progress indicator for registration,USERGRID-763,12839647,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,,,jeffreyawest,jeffreyawest,22/Jun/15 19:25,02/Feb/16 22:00,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,When registering/signing up the form does not indicate any change in state when the 'Register' button is pressed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-353,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-06-22 19:25:05.0,,,,,,,"0|i001pr:000000010cohzzzzzx",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[FIXED?] NPE in CpEntityManager.validate,USERGRID-756,12838322,Bug,Resolved,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Blocker,Won't Fix,,jeffreyawest,jeffreyawest,16/Jun/15 22:48,12/May/16 08:59,29/Oct/20 16:32,21/Jul/15 22:11,,,,,,,,,2.1.0,,,,,,,,,,,0,Fixed?,,,,,,,,"Need to be investigated before 2.1 release.

2015-06-16 22:47:20,361 [http-bio-8080-exec-7] ERROR org.apache.usergrid.corepersistence.CpEntityManager- Unable to load entity saparticles-fixed:e466f39a-cb97-11e4-b30c-f7cc05dca022
java.lang.NullPointerException
	at org.apache.usergrid.corepersistence.CpEntityManager.validate(CpEntityManager.java:956)
	at org.apache.usergrid.corepersistence.CpEntityManager.validate(CpEntityManager.java:942)
	at org.apache.usergrid.corepersistence.CpEntityManager.updateProperties(CpEntityManager.java:1030)
	at org.apache.usergrid.services.AbstractService.updateEntity(AbstractService.java:456)
	at org.apache.usergrid.services.AbstractService.updateEntity(AbstractService.java:444)
	at org.apache.usergrid.services.AbstractCollectionService.putItemByName(AbstractCollectionService.java:305)
	at org.apache.usergrid.services.AbstractService.invokeItemWithName(AbstractService.java:677)
	at org.apache.usergrid.services.AbstractService.invoke(AbstractService.java:628)
	at org.apache.usergrid.services.AbstractService.invoke(AbstractService.java:544)
	at org.apache.usergrid.services.ServiceRequest.execute(ServiceRequest.java:226)
	at org.apache.usergrid.services.ServiceRequest.execute(ServiceRequest.java:193)
	at org.apache.usergrid.rest.applications.ServiceResource.executeServiceRequest(ServiceResource.java:251)
	at org.apache.usergrid.rest.applications.ServiceResource.executePutWithMap(ServiceResource.java:371)
	at org.apache.usergrid.rest.applications.ServiceResource.executePut(ServiceResource.java:420)
	at sun.reflect.GeneratedMethodAccessor167.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:540)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:909)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:857)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:811)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)
	at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
	at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
	at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
	at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:259)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.usergrid.rest.filters.ContentTypeFilter.doFilter(ContentTypeFilter.java:92)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:503)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:170)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
	at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:421)
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1070)
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)
	at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:316)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:745)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-603,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 21 22:11:06 UTC 2015,,,,,,,"0|i004fr:",9223372036854775807,,,,,,,,,,,Usergrid 26,,,,,,,,,,,3.0,,,,,,,,,,,"21/Jul/15 22:11;jeffreyawest;This environment is corrupted so the results coming out of it will not be clean.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Empty Response when using cursors after ~50 pages,USERGRID-753,12838189,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Critical,Cannot Reproduce,,jeffreyawest,jeffreyawest,16/Jun/15 15:08,03/Feb/16 00:01,29/Oct/20 16:06,02/Nov/15 19:29,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-06-16 15:34:23.699,,,false,USERGRID-817,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 02 19:29:05 UTC 2015,,,,,,,"0|i001pr:000000010co8zv",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"16/Jun/15 15:34;safeldm;received empty body in response.  may be chasing a ghost was using record size 10","22/Sep/15 21:14;jeffreyawest;This needs to be tested and confirmed with 2.1 or closed.","02/Nov/15 19:29;mdunker;We have iterated successfully over hundreds of cursor pages in 2.1. Closing.",,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: AssetResourceIT,USERGRID-745,12837207,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,djohnson,safeldm,safeldm,11/Jun/15 17:28,12/Jul/15 16:50,29/Oct/20 16:32,18/Jun/15 15:17,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-06-18 15:17:07.616,,,false,USERGRID-691,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 19 11:54:59 UTC 2015,,,,,,,"0|i004f3:",9223372036854775807,,,,,,,,,,,Usergrid 22,Usergrid 23,,,,,,,,,,1.0,,,,,,,,,,,"18/Jun/15 15:17;djohnson;PR is ready for merge https://github.com/apache/incubator-usergrid/pull/287","19/Jun/15 11:54;djohnson;merged to two-dot-o-dev",,,,,,,,,,,,,,,,,,,,,,,,,,
"Temp files from asset upload not removed, causing the file system to use all inodes",USERGRID-740,12836695,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Critical,Fixed,greyes,tnine,tnine,09/Jun/15 23:37,29/Jul/15 20:17,29/Oct/20 16:32,28/Jul/15 23:49,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,"After running Usergrid 2.0 for over 1 month on a tomcat, we have saturated the inodes of the disk on the tomcat machine.  Our temporary files for uploads are not getting removed.  Files of this structure appear in 

{code}

 ls -lrt  /var/cache/tomcat7/temp


-rw-r--r-- 1 tomcat tomcat 0 Jun  9 23:23 MIME377627531473294092.tmp
-rw-r--r-- 1 tomcat tomcat 0 Jun  9 23:23 MIME2634951521985073318.tmp
-rw-r--r-- 1 tomcat tomcat 0 Jun  9 23:23 MIME1066361445384372180.tmp
-rw-r--r-- 1 tomcat tomcat 0 Jun  9 23:23 MIME3554741621747313255.tmp
-rw-r--r-- 1 tomcat tomcat 0 Jun  9 23:23 MIME1805372544199883785.tmp
-rw-r--r-- 1 tomcat tomcat 0 Jun  9 23:23 MIME414525331725926400.tmp
{code}


We need to ensure that all temporary files are removed.  Note that all the files remaining are 0 bytes.  This seems to be caused by an edge case when a 0 byte file is uploaded, or possibly the upload is failing.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-07-28 23:08:11.917,,,false,USERGRID-603,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 29 19:26:39 UTC 2015,,,,,,,"0|i001af:",9223372036854775807,,,,,,,,,,,Usergrid 22,Usergrid 25,Usergrid 26,,,,,,,,,5.0,,,,,,,,,,,"28/Jul/15 23:08;greyes;This is being caused by https://java.net/jira/browse/JERSEY-2515 . Every time we create a jersey client then it creates the 0 byte MIME file. ","28/Jul/15 23:47;githubbot;GitHub user GERey opened a pull request:

    https://github.com/apache/incubator-usergrid/pull/326

    Added USERGRID-740 fix to two-dot-o

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-usergrid USERGRID-740-two-dot-o

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-usergrid/pull/326.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #326
    
----
commit 0c9c142dba1b09614e3abbd641a04742bb184b50
Author: GERey <greyes@apigee.com>
Date:   2015-07-28T23:46:51Z

    Added USERGRID-740 fix to two-dot-o

----
","28/Jul/15 23:49;greyes;Also added the fix to 2.0 since it is just a bump of the jersey version. ","29/Jul/15 19:26;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-usergrid/pull/326
",,,,,,,,,,,,,,,,,,,,,,,,
2.0 does not correctly handle inferred type of Date with optional time,USERGRID-738,12836216,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,jeffreyawest,jeffreyawest,08/Jun/15 17:04,02/Feb/16 21:28,29/Oct/20 16:32,15/Jul/15 16:21,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,This causes issues if the field is blank.  We need to resolve this for a production installation.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-801,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-06-08 17:04:38.0,,,,,,,"0|i004hz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SNS Queue Manager uses ARN to read instead of URL,USERGRID-729,12836054,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,jeffreyawest,jeffreyawest,jeffreyawest,08/Jun/15 03:29,12/Jul/15 15:41,29/Oct/20 16:32,08/Jun/15 16:06,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,There is a bug in org.apache.usergrid.persistence.queue.impl.SNSQueueManagerImpl which results in the ARN of the queue being used when the URL should be used.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-731,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-06-08 03:29:36.0,,,,,,,"0|i2f873:",9223372036854775807,,,,,,,,,,,Usergrid 21,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Launcher url points to Apigee,USERGRID-728,12835968,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,malaka,malaka,07/Jun/15 01:17,02/Nov/15 17:41,29/Oct/20 16:32,,,,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"The launcher contains the url to: http://apigee.github.io/usergrid-portal/. 
This should be changed to reflect the new url.
Do we still host an admin portal on github like Apigee used to? ...  which we can point out local instance to by specifying an api_url argument in the query string?   

Rgds,
Malaka",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-06-07 01:52:34.277,,,false,USERGRID-826,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 07 03:37:02 UTC 2015,,,,,,,"0|i2fq4n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"07/Jun/15 01:52;johndament;Could you clarify where you're seeing this? Docs? Source code?","07/Jun/15 03:37;malaka;Hi [~johndament], 

This is when you run the launcher project, in the launcher GUI.
It would be better it this defaults to http://localhost:3000/?api_url=http://localhost:8080 instead. Apigee used to have an portal you can point your local instance to, now sure how it is done now.

Rgds,
Malaka",,,,,,,,,,,,,,,,,,,,,,,,,,
Issue with getting started steps on github,USERGRID-727,12835965,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Critical,,,malaka,malaka,07/Jun/15 01:07,15/Dec/17 11:05,29/Oct/20 16:06,,,,,,,,,,,,,,,,Docs,,,,,0,,,,,,,,,"The getting started documents points to some Apigee links and hence don't provide the expected results. This could drive away potential users. Documentation needs to be updated.
link: https://github.com/apache/incubator-usergrid
section: Getting Started with the Admin Portal

Unable to access: 
-  https://github.com/apigee/usergrid-portal
- http://apigee.github.com/usergrid-portal/?api_url=http://localhost:8080

Rgds,
Malaka",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-06-07 01:55:21.965,,,false,USERGRID-826,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 15 11:05:36 UTC 2017,,,,,,,"0|i001pr:000000010co8zzzy",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"07/Jun/15 01:55;johndament;The section you're referring to does not exist on the link you're referring to.  Are you perhaps on a specific branch?","07/Jun/15 03:36;malaka;my bad, check https://github.com/apache/incubator-usergrid/tree/master/stack

Rgds,
Malaka","15/Dec/17 11:05;lynchlee;Budies, 

how can i assign this task to myself ??  someone should put me a special role of usergrid project??  

many thanks ~~",,,,,,,,,,,,,,,,,,,,,,,,,
SNSQueueManagerImpl needs better error handling and messaging,USERGRID-725,12835846,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,jeffreyawest,jeffreyawest,jeffreyawest,05/Jun/15 22:17,12/Jul/15 15:41,29/Oct/20 16:06,06/Jun/15 00:08,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,"It would be better if this was preceded by a message stating which queue cannot be found.

com.amazonaws.services.sqs.model.QueueDoesNotExistException: The specified queue does not exist for this wsdl version. (Service: AmazonSQS; Status Code: 400; Error Code: AWS.SimpleQueueService.NonExistentQueue; Request ID: 70c94af9-7d46-5d2b-9a7e-07155ddd1ed5)
	at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1160)
	at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:748)
	at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:467)
	at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:302)
	at com.amazonaws.services.sqs.AmazonSQSClient.invoke(AmazonSQSClient.java:2422)
	at com.amazonaws.services.sqs.AmazonSQSClient.receiveMessage(AmazonSQSClient.java:1130)
	at org.apache.usergrid.persistence.queue.impl.SNSQueueManagerImpl.getMessages(SNSQueueManagerImpl.java:234)
	at org.apache.usergrid.corepersistence.asyncevents.AmazonAsyncEventService.take(AmazonAsyncEventService.java:153)
	at org.apache.usergrid.corepersistence.asyncevents.AmazonAsyncEventService.access$300(AmazonAsyncEventService.java:66)
	at org.apache.usergrid.corepersistence.asyncevents.AmazonAsyncEventService$2.call(AmazonAsyncEventService.java:379)
	at org.apache.usergrid.corepersistence.asyncevents.AmazonAsyncEventService$2.call(AmazonAsyncEventService.java:366)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable.unsafeSubscribe(Observable.java:7495)
	at rx.internal.operators.OperatorSubscribeOn$1$1.call(OperatorSubscribeOn.java:62)
	at rx.internal.schedulers.ScheduledAction.run(ScheduledAction.java:55)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-731,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 06 00:08:39 UTC 2015,,,,,,,"0|i2f86n:",9223372036854775807,,,,,,,,,,,Usergrid 21,,,,,,,,,,,1.0,,,,,,,,,,,"06/Jun/15 00:02;jeffreyawest;PR: https://github.com/apache/incubator-usergrid/pull/269","06/Jun/15 00:08;jeffreyawest;https://github.com/apache/incubator-usergrid/pull/269",,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: DevicesresourceIT ,USERGRID-704,12834750,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,tnine,safeldm,safeldm,02/Jun/15 18:38,12/Jul/15 16:06,29/Oct/20 16:06,11/Jun/15 17:16,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,seems to be a graph compaction issue DevicesResourceIT.putWithUUIDShouldCreateAfterDelete,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-691,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-06-02 18:38:01.0,,,,,,,"0|i2di9z:",9223372036854775807,,,,,,,,,,,Usergrid 20,Usergrid 22,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: ManagementResourceIT,USERGRID-703,12834432,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,djohnson,safeldm,safeldm,01/Jun/15 23:56,12/Jul/15 16:06,29/Oct/20 16:32,18/Jun/15 00:33,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,"testSuperuserOnlyWhenValidateExternalTokensEnabled
and the other external test are broken.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-06-17 22:15:04.903,,,false,USERGRID-691,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 17:04:03 UTC 2015,,,,,,,"0|i004en:",9223372036854775807,,,,,,,,,,,Usergrid 21,Usergrid 22,Usergrid 23,,,,,,,,,2.0,,,,,,,,,,,"17/Jun/15 22:15;githubbot;GitHub user snoopdave opened a pull request:

    https://github.com/apache/incubator-usergrid/pull/286

    USERGRID-703: fixes for SSO-related ManagementResourceIT tests

    Tests relied on Codehaus Jackson and tests cannot be run concurrently.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/apache/incubator-usergrid USERGRID-703

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-usergrid/pull/286.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #286
    
----
commit fb7dc96043953cf9118017ee58680a75a625aef6
Author: Dave Johnson <dmjohnson@apigee.com>
Date:   2015-06-17T22:10:39Z

    Prevent quadruple logging during testing.

commit dd3ede07d3abd4558ceab94d82791c7ab29fb516
Author: Dave Johnson <dmjohnson@apigee.com>
Date:   2015-06-17T22:11:09Z

    We use fasterxml not codehaus.

commit c2d5e1165d8bd0b66f1233d7ef1b2997e044838b
Author: Dave Johnson <dmjohnson@apigee.com>
Date:   2015-06-17T22:11:56Z

    don't run tests concurrently, also: ensure that properties are reset to default values after tests

----
","17/Jun/15 22:15;djohnson;Fix is ready for merge: https://github.com/apache/incubator-usergrid/pull/286","18/Jun/15 00:33;djohnson;PR is ready for merge","22/Jun/15 15:45;djohnson;merged","29/Jun/15 14:26;githubbot;Github user shawnfeldman commented on the pull request:

    https://github.com/apache/incubator-usergrid/pull/286#issuecomment-116700046
  
    Tests in error:
      ManagementResourceIT.crossOrgsNotViewable » UniformInterface POST http://local...
      ManagementResourceIT.meToken » UniformInterface POST http://localhost:10004/ma...
      ManagementResourceIT.meTokenPost » UniformInterface POST http://localhost:1000...
      ManagementResourceIT.meTokenPostForm » UniformInterface POST http://localhost:...
      ManagementResourceIT.mgmtFollowsUserFeed » UniformInterface POST http://localh...
      ManagementResourceIT.setSelfAdminPasswordAsAdmin » UniformInterface POST http:...
      ManagementResourceIT.testValidateExternalToken:605 » UniformInterface POST htt...
      ManagementResourceIT.token » UniformInterface POST http://localhost:10004/mana...

","30/Jun/15 11:30;githubbot;Github user snoopdave commented on the pull request:

    https://github.com/apache/incubator-usergrid/pull/286#issuecomment-117135031
  
    I did a merge of two-dot-dev into the USERGRID-703 branch and all the REST tests passed for me. Perhaps you've got a problem in your environment?
","30/Jun/15 17:04;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-usergrid/pull/286
",,,,,,,,,,,,,,,,,,,,,
Test Not Passing: StaleIndexCleanupTests,USERGRID-700,12834263,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,safeldm,safeldm,01/Jun/15 15:15,12/Jul/15 16:06,29/Oct/20 16:32,01/Jun/15 16:00,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,"Results :

Failed tests:
  StaleIndexCleanupTest.testCleanupOnUpdate:462 Expect candidates without earlier stale entities expected:<10> but was:<60>

Tests run: 258, Failures: 1, Errors: 0, Skipped: 25",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-06-01 16:00:09.962,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 01 16:00:09 UTC 2015,,,,,,,"0|i004cn:",9223372036854775807,,,,,,,,,,,Usergrid 20,,,,,,,,,,,1.0,,,,,,,,,,,"01/Jun/15 16:00;greyes;Regression due to changing the parameters under where graph and elasticsearch are queried.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing:  NotificationsIT.testPaging:90,USERGRID-699,12834259,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,safeldm,safeldm,01/Jun/15 14:44,12/Jul/15 16:07,29/Oct/20 16:32,02/Jun/15 23:27,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-691,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-06-01 14:44:30.0,,,,,,,"0|i004d3:",9223372036854775807,,,,,,,,,,,Usergrid 20,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: ConnectionResourceTest.connectionsLoopbackTest:103 NullPointer,USERGRID-698,12834256,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,tnine,safeldm,safeldm,01/Jun/15 14:35,12/Jul/15 16:07,29/Oct/20 16:32,11/Jun/15 17:16,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-691,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-06-01 14:35:18.0,,,,,,,"0|i0036v:",9223372036854775807,,,,,,,,,,,Usergrid 20,Usergrid 21,Usergrid 22,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: AdminUsersIT,USERGRID-697,12834255,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,safeldm,safeldm,01/Jun/15 14:33,12/Jul/15 16:07,29/Oct/20 16:32,01/Jun/15 16:01,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,AdminUsersIT.mgmtUserFeed:190 » UniformInterface GET http://localhost:10006/ma...,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-06-01 16:01:53.398,,,false,USERGRID-691,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 01 16:01:53 UTC 2015,,,,,,,"0|i2fe87:",9223372036854775807,,,,,,,,,,,Usergrid 20,,,,,,,,,,,2.0,,,,,,,,,,,"01/Jun/15 14:34;safeldm;AdminUsersIT.mgmtUserFeed:190 » UniformInterface GET http://localhost:10006/ma...","01/Jun/15 16:01;greyes;This was probably fixed in a different area and now works. ",,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: RegistrationIT,USERGRID-695,12834251,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,safeldm,safeldm,01/Jun/15 14:31,12/Jul/15 16:07,29/Oct/20 16:32,01/Jun/15 19:05,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,," RegistrationIT.addExistingAdminUserToOrganization:304 » UniformInterface POST ...
  RegistrationIT.addNewAdminUserWithNoPwdToOrganization:245->postAddAdminToOrg:90 » ClientHandler
  RegistrationIT.postAddToOrganization:222->AbstractRestIT.getAdminToken:173 » UniformInterface
  RegistrationIT.postCreateOrgAndAdmin:134->AbstractRestIT.getAdminToken:173 » UniformInterface
  RegistrationIT.putAddToOrganizationFail:195->AbstractRestIT.getAdminToken:173 » UniformInterface",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-06-01 19:05:42.528,,,false,USERGRID-691,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 01 20:55:23 UTC 2015,,,,,,,"0|i2fe8f:",9223372036854775807,,,,,,,,,,,Usergrid 20,,,,,,,,,,,2.0,,,,,,,,,,,"01/Jun/15 19:05;greyes;Most of the fixes here were just to the test logic. Not sure these were working correctly and when going through a refactor these tests should be seriously looked at. ","01/Jun/15 20:26;greyes;There is one test that is still failing when run in the test suite. Investigating now","01/Jun/15 20:55;greyes;The main issue is that the tests have illogical parts that need to be carefully reviewed before we can consider registration fully tested. Currently many tests try to test additional functionality when really it should be a separate test. Ideally the tests would also be clearer as to what we should be testing in each case. If something is testing two things, modify the name to show that it is testing two things.",,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: ContentTypeResourceIT,USERGRID-693,12834249,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,safeldm,safeldm,01/Jun/15 14:29,12/Jul/15 16:07,29/Oct/20 16:32,01/Jun/15 23:31,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,"ContentTypeResourceIT.formEncodedContentType:152 expected:<200> but was:<500>
  ContentTypeResourceIT.noAcceptGet:262 expected:<200> but was:<400>",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-691,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-06-01 14:29:13.0,,,,,,,"0|i2fe8n:",9223372036854775807,,,,,,,,,,,Usergrid 20,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: PermissionsResourceIT,USERGRID-692,12834247,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,safeldm,safeldm,01/Jun/15 14:28,12/Jul/15 16:07,29/Oct/20 16:32,01/Jun/15 15:51,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,"  PermissionsResourceIT.applicationPermissions:262 expected:<[noca]> but was:<[4peaks]>
  PermissionsResourceIT.deleteUserGroup:157 null",,,,,,,,,,,,,,,,,,,,,,,,,,120,120,,0%,120,120,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-691,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-06-01 14:28:15.0,,,,,,,"0|i2di8f:",9223372036854775807,,,,,,,,,,,Usergrid 21,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: OrganizationsIT,USERGRID-690,12833912,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,safeldm,safeldm,29/May/15 23:07,12/Jul/15 16:07,29/Oct/20 16:32,01/Jun/15 15:29,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-691,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-29 23:07:26.0,,,,,,,"0|i004cf:",9223372036854775807,,,,,,,,,,,Usergrid 20,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing:  ManagementResourceIT,USERGRID-689,12833910,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,safeldm,safeldm,29/May/15 22:56,12/Jul/15 16:07,29/Oct/20 16:32,01/Jun/15 23:57,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,11 tests failing,,,,,,,,,,,,,,,,,,,,,,,,,,120,120,,0%,120,120,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-691,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-29 22:56:05.0,,,,,,,"0|i2feh3:",9223372036854775807,,,,,,,,,,,Usergrid 20,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: AndOrQueryTest,USERGRID-687,12833872,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,greyes,greyes,29/May/15 20:05,12/Jul/15 16:08,29/Oct/20 16:32,01/Jun/15 15:21,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,"Fix AndOrQueryTest:
queryReturnCheck
queryReturnCheckWithShortHand",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-691,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-29 20:05:34.0,,,,,,,"0|i2fe7z:",9223372036854775807,,,,,,,,,,,Usergrid 20,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: ConnectionsServiceIT.testEntityConnections,USERGRID-685,12833607,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,tnine,safeldm,safeldm,28/May/15 23:00,12/Jul/15 16:08,29/Oct/20 16:32,02/Jun/15 21:44,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-28 23:00:36.0,,,,,,,"0|i003m7:",9223372036854775807,,,,,,,,,,,Usergrid 20,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UG 1.0 bug for DB setup with separate locks keyspace,USERGRID-684,12833540,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,jeffreyawest,jeffreyawest,28/May/15 19:27,02/Nov/15 17:41,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,"It seems the keyspace does not get created with /database/setup.

cassandra.system.keyspace=sso_ug10
#cassandra.application.keyspace=Usergrid_Applications
cassandra.application.keyspace=sso_ug10_apps
#cassandra.lock.keyspace=Locks
cassandra.lock.keyspace=sso_ug10_Locks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-988,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-28 19:27:38.0,,,,,,,"0|i004mn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
App Credentials do not work when accessing a collection within an app (2.0),USERGRID-670,12830451,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,jeffreyawest,jeffreyawest,17/May/15 00:51,02/Feb/16 21:29,29/Oct/20 16:32,18/Jun/15 18:26,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,In order to access collections it seems the org credentials are required as app credentials do not work.  ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-06-05 19:19:40.698,,,false,USERGRID-603,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 18:45:36 UTC 2015,,,,,,,"0|i0035j:",9223372036854775807,,,,,,,,,,,Usergrid 21,Usergrid 22,Usergrid 23,,,,,,,,,5.0,,,,,,,,,,,"05/Jun/15 19:19;greyes;I wrote a test and existing tests work for two-dot-o-dev. They do not work for two-dot-o however. I will investigate further.","05/Jun/15 23:09;greyes;So the issue at hand here was that we were storing the org/app name with mixed cases. Then when we went to go search it would be in mixed case and we wouldn't be able to search the lower case values. In two-dot-o-dev I believe everything is stored in lower case values ( i could be wrong but it does work in two-dot-o-dev ) so the issue appeared fixed there. I applied a value search that would look through and lowercase the names then compare them. Then all the tests using application credentials worked. ","09/Jun/15 02:25;jeffreyawest;Can you take a look at 1.0 and see if this is the same issue?  If it is we can create another ticket for that issue.","10/Jun/15 21:58;greyes;Yes, in fact this is still an issue in two-dot-o-dev. The reason we don't see it there is because everything is lower cased anyways.","10/Jun/15 22:56;greyes;So I applied a bandage rather than a fix. It turns out we should be storing things in lowercase, so the bimap should be lowercased so we can do a O(1) comparison against it. I'll reopen this ticket then point it to 1.0 and 2.0.","11/Jun/15 23:10;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-usergrid/pull/270
","15/Jun/15 18:45;greyes;This is fixed and is also pointing to 1.0 here: https://github.com/apache/incubator-usergrid/pull/280/files",,,,,,,,,,,,,,,,,,,,,
Test Not Passing: RolesServiceIT.deleteRoles,USERGRID-664,12829627,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,safeldm,safeldm,13/May/15 17:59,12/Jul/15 16:08,29/Oct/20 16:32,29/May/15 17:08,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-05-29 17:08:38.815,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 17:08:38 UTC 2015,,,,,,,"0|i003jr:",9223372036854775807,,,,,,,,,,,Usergrid 18,Usergrid 19,Usergrid 20,,,,,,,,,1.0,,,,,,,,,,,"29/May/15 17:08;greyes;Service tests mostly work now.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: ConnectionsServiceIT,USERGRID-663,12829626,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,safeldm,safeldm,13/May/15 17:57,12/Jul/15 16:08,29/Oct/20 16:32,15/May/15 20:01,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-13 17:57:01.0,,,,,,,"0|i003qn:",9223372036854775807,,,,,,,,,,,Usergrid 18,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: AppInfoMigrationPluginTest,USERGRID-662,12829624,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,safeldm,safeldm,13/May/15 17:53,12/Jul/15 16:08,29/Oct/20 16:32,28/May/15 22:59,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-13 17:53:21.0,,,,,,,"0|i003jz:",9223372036854775807,,,,,,,,,,,Usergrid 18,Usergrid 19,Usergrid 20,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: ServiceInvocationIT,USERGRID-661,12829623,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,safeldm,safeldm,13/May/15 17:52,12/Jul/15 16:09,29/Oct/20 16:32,15/May/15 19:23,,,,,,,,,2.1.0,,,,,,Stack,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,180,180,,0%,180,180,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-13 17:52:38.0,,,,,,,"0|i003qf:",9223372036854775807,,,,,,,,,,,Usergrid 18,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: GroupServiceIT - Services,USERGRID-660,12829622,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,safeldm,safeldm,13/May/15 17:49,12/Jul/15 16:09,29/Oct/20 16:32,15/May/15 19:47,,,,,,,,,2.1.0,,,,,,Stack,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,180,180,,0%,180,180,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-13 17:49:36.0,,,,,,,"0|i003q7:",9223372036854775807,,,,,,,,,,,Usergrid 18,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: OrganizationIT Failing,USERGRID-659,12829621,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,safeldm,safeldm,13/May/15 17:47,12/Jul/15 16:09,29/Oct/20 16:32,15/May/15 14:42,,,,,,,,,2.1.0,,,,,,Stack,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,180,180,,0%,180,180,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 14:42:48 UTC 2015,,,,,,,"0|i004bz:",9223372036854775807,,,,,,,,,,,Usergrid 18,,,,,,,,,,,3.0,,,,,,,,,,,"15/May/15 14:42;safeldm;https://github.com/apache/incubator-usergrid/pull/247",,,,,,,,,,,,,,,,,,,,,,,,,,,
Issue with Unique Values - NPE,USERGRID-647,12829252,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,,jeffreyawest,jeffreyawest,12/May/15 17:08,15/Aug/16 18:07,29/Oct/20 16:32,15/Aug/16 17:11,,,,,,,,,2.2.0,,,,,,,,,,,0,,,,,,,,,"Please ensure appropriate null checks and behavior happen here:

2015-05-12 17:05:37,811 [http-bio-8080-exec-14] ERROR org.apache.usergrid.rest.exceptions.AbstractExceptionMapper- java.lang.NullPointerException Server Error (500)
java.lang.NullPointerException
	at org.apache.usergrid.services.AbstractConnectionsService.getItemByName(AbstractConnectionsService.java:238)
	at org.apache.usergrid.services.AbstractService.invokeItemWithName(AbstractService.java:671)
	at org.apache.usergrid.services.AbstractService.invoke(AbstractService.java:628)
	at org.apache.usergrid.services.AbstractService.invoke(AbstractService.java:544)
	at org.apache.usergrid.services.ServiceRequest.execute(ServiceRequest.java:226)
	at org.apache.usergrid.services.ServiceRequest.invokeMultiple(ServiceRequest.java:262)
	at org.apache.usergrid.services.ServiceRequest.execute(ServiceRequest.java:229)
	at org.apache.usergrid.services.ServiceRequest.execute(ServiceRequest.java:193)
	at org.apache.usergrid.rest.applications.ServiceResource.executeServiceRequest(ServiceResource.java:251)
	at org.apache.usergrid.rest.applications.ServiceResource.executeGet(ServiceResource.java:297)
	at sun.reflect.GeneratedMethodAccessor142.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
	at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)
	at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
	at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:540)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:909)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:857)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:811)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)
	at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
	at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
	at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
	at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:259)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.usergrid.rest.filters.ContentTypeFilter.doFilter(ContentTypeFilter.java:92)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:503)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:170)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
	at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:421)
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1070)
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)
	at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:316)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:745)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-08-12 20:44:38.197,,,false,USERGRID-838,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 12 20:44:38 UTC 2016,,,,,,,"0|i2jodw:x",9223372036854775807,,,,,,,,,,,Usergrid 36,,,,,,,,,,,3.0,,,,,,,,,,,"12/Aug/16 20:44;Ayesha12;fixed in https://github.com/apache/usergrid/pull/557",,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: IteratingQueryIt failing due to bad serializer,USERGRID-644,12828486,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,tnine,safeldm,safeldm,08/May/15 20:53,12/Jul/15 16:09,29/Oct/20 16:32,11/May/15 01:09,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,"trying to deserialize a markededge into an edge

AbstractCursorSerializer line 47


2015-05-08 14:33:25,926 DEBUG (main) IndexServiceImpl - adding edge IndexEdgeImpl{timestamp=1431117205920001} SearchEdgeImpl{nodeId=SimpleId{uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b, type='application'}, name='zzzcollzzz|tests', nodeType=TARGET} to batch for entity Entity{id=SimpleId{uuid=7a69ba0a-f5c1-11e4-ab26-47475f9887d5, type='test'}, version=7a69ba63-f5c1-11e4-9ef9-324ce75ff58b}
2015-05-08 14:33:25,942 DEBUG (main) IndexServiceImpl - adding edge IndexEdgeImpl{timestamp=1431117205934001} SearchEdgeImpl{nodeId=SimpleId{uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b, type='application'}, name='zzzcollzzz|tests', nodeType=TARGET} to batch for entity Entity{id=SimpleId{uuid=7a6bdcea-f5c1-11e4-8198-83878cd74a93, type='test'}, version=7a6c0456-f5c1-11e4-9ef9-324ce75ff58b}
2015-05-08 14:33:25,957 DEBUG (main) IndexServiceImpl - adding edge IndexEdgeImpl{timestamp=1431117205949001} SearchEdgeImpl{nodeId=SimpleId{uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b, type='application'}, name='zzzcollzzz|tests', nodeType=TARGET} to batch for entity Entity{id=SimpleId{uuid=7a6e26da-f5c1-11e4-8f8d-9b115b1298d8, type='test'}, version=7a6e2739-f5c1-11e4-9ef9-324ce75ff58b}
2015-05-08 14:33:25,970 DEBUG (main) IndexServiceImpl - adding edge IndexEdgeImpl{timestamp=1431117205964001} SearchEdgeImpl{nodeId=SimpleId{uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b, type='application'}, name='zzzcollzzz|tests', nodeType=TARGET} to batch for entity Entity{id=SimpleId{uuid=7a7070ca-f5c1-11e4-9deb-7d7bf418ca86, type='test'}, version=7a70712c-f5c1-11e4-9ef9-324ce75ff58b}
2015-05-08 14:33:25,984 DEBUG (main) IndexServiceImpl - adding edge IndexEdgeImpl{timestamp=1431117205978001} SearchEdgeImpl{nodeId=SimpleId{uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b, type='application'}, name='zzzcollzzz|tests', nodeType=TARGET} to batch for entity Entity{id=SimpleId{uuid=7a7293aa-f5c1-11e4-8cbc-6b63f058386d, type='test'}, version=7a72bb1f-f5c1-11e4-9ef9-324ce75ff58b}
2015-05-08 14:33:25,998 DEBUG (main) IndexServiceImpl - adding edge IndexEdgeImpl{timestamp=1431117205991001} SearchEdgeImpl{nodeId=SimpleId{uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b, type='application'}, name='zzzcollzzz|tests', nodeType=TARGET} to batch for entity Entity{id=SimpleId{uuid=7a748f7a-f5c1-11e4-b755-cfcc9cf90313, type='test'}, version=7a748fe2-f5c1-11e4-9ef9-324ce75ff58b}
2015-05-08 14:33:26,013 DEBUG (main) IndexServiceImpl - adding edge IndexEdgeImpl{timestamp=1431117206006001} SearchEdgeImpl{nodeId=SimpleId{uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b, type='application'}, name='zzzcollzzz|tests', nodeType=TARGET} to batch for entity Entity{id=SimpleId{uuid=7a76d96a-f5c1-11e4-b6ba-5103774b6f21, type='test'}, version=7a7700e5-f5c1-11e4-9ef9-324ce75ff58b}
2015-05-08 14:33:26,027 DEBUG (main) IndexServiceImpl - adding edge IndexEdgeImpl{timestamp=1431117206021001} SearchEdgeImpl{nodeId=SimpleId{uuid=7a1f1c82-f5c1-11e4-9ef9-324ce75ff58b, type='application'}, name='zzzcollzzz|tests', nodeType=TARGET} to batch for entity Entity{id=SimpleId{uuid=7a79235a-f5c1-11e4-a640-1f069a6b43c9, type='test'}, version=7a7923c8-f5c1-11e4-9ef9-324ce75ff58b}
2015-05-08 14:33:26,112 INFO (main) IndexRefreshCommandImpl - found record during refresh uuid: 7a7c0980-f5c1-11e4-b2bd-5994708e0639 took ms:75 
2015-05-08 14:33:26,112 INFO (main) IteratingQueryIT - Writes took 571 ms
Disconnected from the target VM, address: '127.0.0.1:49588', transport: 'socket'
2015-05-08 14:34:59,660 INFO (main) CoreApplication - Test allInConnectionNoType(org.apache.usergrid.persistence.query.IteratingQueryIT): finish with application

org.apache.usergrid.corepersistence.pipeline.cursor.CursorParseException: Unable to deserialize value
	at org.apache.usergrid.corepersistence.pipeline.cursor.AbstractCursorSerializer.fromJsonNode(AbstractCursorSerializer.java:51)
	at org.apache.usergrid.corepersistence.pipeline.cursor.RequestCursor.getCursor(RequestCursor.java:75)
	at org.apache.usergrid.corepersistence.pipeline.PipelineContext.getCursor(PipelineContext.java:68)
	at org.apache.usergrid.corepersistence.pipeline.read.AbstractPathFilter.getSeekValue(AbstractPathFilter.java:50)
	at org.apache.usergrid.corepersistence.pipeline.read.graph.AbstractReadGraphFilter.lambda$call$2(AbstractReadGraphFilter.java:73)
	at org.apache.usergrid.corepersistence.pipeline.read.graph.AbstractReadGraphFilter$$Lambda$100/1957269967.call(Unknown Source)
	at rx.internal.operators.OperatorMap$1.onNext(OperatorMap.java:55)
	at rx.internal.operators.OperatorMap$1.onNext(OperatorMap.java:55)
	at rx.internal.util.ScalarSynchronousObservable$1.call(ScalarSynchronousObservable.java:43)
	at rx.internal.util.ScalarSynchronousObservable$1.call(ScalarSynchronousObservable.java:32)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable$1.call(Observable.java:144)
	at rx.Observable$1.call(Observable.java:136)
	at rx.Observable.subscribe(Observable.java:7585)
	at rx.internal.operators.BlockingOperatorToIterator.toIterator(BlockingOperatorToIterator.java:53)
	at rx.observables.BlockingObservable.getIterator(BlockingObservable.java:156)
	at org.apache.usergrid.corepersistence.results.ObservableQueryExecutor.hasNext(ObservableQueryExecutor.java:114)
	at org.apache.usergrid.corepersistence.results.ObservableQueryExecutor.next(ObservableQueryExecutor.java:124)
	at org.apache.usergrid.corepersistence.CpRelationManager.searchConnectedEntities(CpRelationManager.java:948)
	at org.apache.usergrid.corepersistence.CpEntityManager.searchConnectedEntities(CpEntityManager.java:1546)
	at org.apache.usergrid.persistence.query.IteratingQueryIT$ConnectionNoTypeHelper.getResults(IteratingQueryIT.java:278)
	at org.apache.usergrid.persistence.query.IteratingQueryIT.allIn(IteratingQueryIT.java:1130)
	at org.apache.usergrid.persistence.query.IteratingQueryIT.allInConnectionNoType(IteratingQueryIT.java:71)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.usergrid.CoreApplication$1.evaluate(CoreApplication.java:145)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.apache.usergrid.CoreITSetupImpl$1.evaluate(CoreITSetupImpl.java:76)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)
Caused by: com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException: Unrecognized field ""deleted"" (class org.apache.usergrid.persistence.graph.impl.SimpleEdge), not marked as ignorable (4 known properties: ""type"", ""targetNode"", ""sourceNode"", ""timestamp""])
 at [Source: N/A; line: -1, column: -1] (through reference chain: org.apache.usergrid.persistence.graph.impl.SimpleEdge[""deleted""])
	at com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException.from(UnrecognizedPropertyException.java:51)
	at com.fasterxml.jackson.databind.DeserializationContext.reportUnknownProperty(DeserializationContext.java:671)
	at com.fasterxml.jackson.databind.deser.std.StdDeserializer.handleUnknownProperty(StdDeserializer.java:773)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownProperty(BeanDeserializerBase.java:1297)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownVanilla(BeanDeserializerBase.java:1275)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:247)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:118)
	at com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:2965)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:1587)
	at com.fasterxml.jackson.databind.ObjectMapper.treeToValue(ObjectMapper.java:1931)
	at org.apache.usergrid.corepersistence.pipeline.cursor.AbstractCursorSerializer.fromJsonNode(AbstractCursorSerializer.java:48)
	... 74 more
Caused by: rx.exceptions.OnErrorThrowable$OnNextValue: OnError while emitting onNext value: org.apache.usergrid.corepersistence.pipeline.read.FilterResult.class
	at rx.exceptions.OnErrorThrowable.addValueAsLastCause(OnErrorThrowable.java:101)
	at rx.internal.operators.OperatorMap$1.onNext(OperatorMap.java:58)
	... 68 more",,,,,,,,,,,,,,,,,,,,,,,,,,120,120,,0%,120,120,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-08 20:53:40.0,,,,,,,"0|i004b3:",9223372036854775807,,,,,,,,,,,Usergrid 17,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: StaleIndexCleanup Failing due to old version getting returned from search,USERGRID-642,12828156,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,safeldm,safeldm,07/May/15 22:07,12/Jul/15 16:09,29/Oct/20 16:32,08/May/15 19:09,,,,,,,,,2.1.0,,,,,,Stack,,,,,0,,,,,,,,,"java.lang.AssertionError: 
Expected :e2283e4d-f504-11e4-b4ae-324ce75ff58b
Actual   :e2150469-f504-11e4-b4ae-324ce75ff58b
 <Click to see difference>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.usergrid.corepersistence.StaleIndexCleanupTest.testUpdateVersionMaxFirst(StaleIndexCleanupTest.java:187)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.apache.usergrid.CoreApplication$1.evaluate(CoreApplication.java:145)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.apache.usergrid.CoreITSetupImpl$1.evaluate(CoreITSetupImpl.java:76)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
",,,,,,,,,,,,,,,,,,,,,,,,,,180,180,,0%,180,180,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-07 22:07:35.0,,,,,,,"0|i0048n:",9223372036854775807,,,,,,,,,,,Usergrid 17,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Entity does not return from collection when ql=select *,USERGRID-639,12827797,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,,jeffreyawest,jeffreyawest,06/May/15 20:27,02/Feb/16 21:49,29/Oct/20 16:32,20/Aug/15 22:19,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,"Entity exists at /{org}/{app}/{collection}/{name} but is not returned from /{org}/{app}/{collection}?ql=select * 

For reference: https://apigeesc.atlassian.net/browse/APIBAAS-1560",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-817,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-06 20:27:10.0,,,,,,,"0|i0027j:",9223372036854775807,,,,,,,,,,,Usergrid 17,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: NotSubPropertyIT,USERGRID-637,12827504,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,jeffreyawest,jeffreyawest,06/May/15 00:44,12/Jul/15 15:47,29/Oct/20 16:32,07/May/15 17:27,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-06 00:44:58.0,,,,,,,"0|i004af:",9223372036854775807,,,,,,,,,,,Usergrid 17,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: IntersectionUnionPagingIT,USERGRID-636,12827503,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,jeffreyawest,jeffreyawest,06/May/15 00:44,12/Jul/15 15:47,29/Oct/20 16:32,07/May/15 17:19,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-06 00:44:52.0,,,,,,,"0|i004a7:",9223372036854775807,,,,,,,,,,,Usergrid 17,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: IntersectionTransitivePagingIT,USERGRID-635,12827502,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,jeffreyawest,jeffreyawest,06/May/15 00:44,12/Jul/15 15:48,29/Oct/20 16:32,07/May/15 17:17,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-06 00:44:45.0,,,,,,,"0|i0049z:",9223372036854775807,,,,,,,,,,,Usergrid 17,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: EntityManagerFactoryImplIT,USERGRID-634,12827501,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,jeffreyawest,jeffreyawest,06/May/15 00:44,12/Jul/15 15:48,29/Oct/20 16:32,28/May/15 22:59,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-05-06 15:36:49.284,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 15:36:49 UTC 2015,,,,,,,"0|i003kn:",9223372036854775807,,,,,,,,,,,Usergrid 17,Usergrid 19,Usergrid 20,,,,,,,,,1.0,,,,,,,,,,,"06/May/15 15:36;safeldm;Caused by: org.apache.usergrid.exception.NotImplementedException: Implement me
	at org.apache.usergrid.corepersistence.index.IndexServiceImpl.deleteEntityIndexes(IndexServiceImpl.java:185)
	at org.apache.usergrid.corepersistence.asyncevents.InMemoryAsyncEventService.queueEntityDelete(InMemoryAsyncEventService.java:107)
	at org.apache.usergrid.corepersistence.CpEntityManager.deleteAsync(CpEntityManager.java:652)
	at org.apache.usergrid.corepersistence.CpEntityManager.delete(CpEntityManager.java:620)
	at org.apache.usergrid.corepersistence.CpEntityManagerFactory.lambda$migrateAppInfo$16(CpEntityManagerFactory.java:381)
	... 59 more",,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: PermissionsIT,USERGRID-633,12827500,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,rbridges,jeffreyawest,jeffreyawest,06/May/15 00:44,12/Jul/15 15:48,29/Oct/20 16:32,28/May/15 22:59,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-05-06 15:34:43.464,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 15:34:43 UTC 2015,,,,,,,"0|i003kf:",9223372036854775807,,,,,,,,,,,Usergrid 17,Usergrid 19,Usergrid 20,,,,,,,,,0.0,,,,,,,,,,,"06/May/15 15:34;safeldm; @Override
    public Observable<IndexOperationMessage> deleteEntityIndexes( final ApplicationScope applicationScope,
                                                                  final Id entityId ) {

        //TODO query ES and remove this entityId
        throw new NotImplementedException( ""Implement me"" );
    }
",,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: PerformanceEntityRebuildIndexTest,USERGRID-632,12827499,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,rbridges,jeffreyawest,jeffreyawest,06/May/15 00:44,12/Jul/15 15:48,29/Oct/20 16:32,26/May/15 17:13,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-05-07 17:34:22.903,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 16:38:22 UTC 2015,,,,,,,"0|i003jj:",9223372036854775807,,,,,,,,,,,Usergrid 17,Usergrid 18,Usergrid 19,Usergrid 20,,,,,,,,1.0,,,,,,,,,,,"07/May/15 17:34;safeldm;2015-05-07 11:31:40,180 ERROR (main) PerformanceEntityRebuildIndexTest - Error rebuilding index
java.lang.UnsupportedOperationException: Implement me
	at org.apache.usergrid.corepersistence.CpEntityManagerFactory.rebuildCollectionIndex(CpEntityManagerFactory.java:693)
	at org.apache.usergrid.persistence.PerformanceEntityRebuildIndexTest.rebuildOneCollectionIndex(PerformanceEntityRebuildIndexTest.java:218)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.apache.usergrid.CoreApplication$1.evaluate(CoreApplication.java:145)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.apache.usergrid.CoreITSetupImpl$1.evaluate(CoreITSetupImpl.java:76)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
","26/May/15 16:38;rbridges;This test is passing as of rev 53563e83b45b85c30a756530f366b6312f6d45ac",,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: PathQueryIT,USERGRID-631,12827498,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,jeffreyawest,jeffreyawest,06/May/15 00:44,12/Jul/15 15:48,29/Oct/20 16:32,07/May/15 17:12,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-06 00:44:14.0,,,,,,,"0|i0049r:",9223372036854775807,,,,,,,,,,,Usergrid 17,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: IndexIT,USERGRID-630,12827496,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,jeffreyawest,jeffreyawest,06/May/15 00:43,07/May/15 17:26,29/Oct/20 16:32,07/May/15 17:26,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-06 00:43:58.0,,,,,,,"0|i004bj:",9223372036854775807,,,,,,,,,,,Usergrid 17,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: GeoQueryBooleanTest,USERGRID-629,12827495,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,jeffreyawest,jeffreyawest,06/May/15 00:43,07/May/15 17:28,29/Oct/20 16:32,07/May/15 17:28,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-06 00:43:52.0,,,,,,,"0|i004bb:",9223372036854775807,,,,,,,,,,,Usergrid 17,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: GeoIT,USERGRID-628,12827494,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,tnine,jeffreyawest,jeffreyawest,06/May/15 00:43,02/Feb/16 21:49,29/Oct/20 16:32,01/Jun/15 21:52,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-05-07 17:24:50.181,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 16:52:58 UTC 2015,,,,,,,"0|i003l3:",9223372036854775807,,,,,,,,,,,Usergrid 17,Usergrid 19,Usergrid 20,,,,,,,,,1.0,,,,,,,,,,,"07/May/15 17:24;safeldm;
org.apache.usergrid.exception.NotImplementedException: Implement me
	at org.apache.usergrid.corepersistence.index.IndexServiceImpl.deleteEntityIndexes(IndexServiceImpl.java:182)
	at org.apache.usergrid.corepersistence.asyncevents.InMemoryAsyncEventService.queueEntityDelete(InMemoryAsyncEventService.java:107)
	at org.apache.usergrid.corepersistence.CpEntityManager.deleteAsync(CpEntityManager.java:652)
	at org.apache.usergrid.corepersistence.CpEntityManager.delete(CpEntityManager.java:620)
	at org.apache.usergrid.persistence.GeoIT.testRemovedLocationQuery(GeoIT.java:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.usergrid.CoreApplication$1.evaluate(CoreApplication.java:145)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.apache.usergrid.CoreITSetupImpl$1.evaluate(CoreITSetupImpl.java:76)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
","26/May/15 16:56;rbridges;The only reason this test is failing is because deleteEntityIndexes is not implemented in the index service.  We can remove remove the calls to delete entities since they have no practical application to the tests at hand, or wait until this method has been implemented.","26/May/15 17:26;rbridges;These tests will pass once USERGRID-608 is completed.","29/May/15 16:52;tnine;Took a look at this after delete was completed.  We still have 1 test failing when all the entities are located at the same point.  I modified our default sort semantics to sort by geo field name, then edge timestamp, then entityid. 

This exposed a secondary bug where the old UUIDUtils was still in use.  This class does not correctly generate sequential time UUIDs in the same millisecond when under heavy load.  I modified this call to delegate to the new UUIDGenerator class, which fixes the issue.",,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: EntityManagerIT,USERGRID-627,12827493,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,jeffreyawest,jeffreyawest,06/May/15 00:43,29/May/15 22:57,29/Oct/20 16:32,28/May/15 22:50,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-05-06 15:33:45.151,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 15:33:45 UTC 2015,,,,,,,"0|i003kv:",9223372036854775807,,,,,,,,,,,Usergrid 17,Usergrid 19,Usergrid 20,,,,,,,,,0.0,,,,,,,,,,,"06/May/15 15:33;safeldm;fails due to 
    @Override
    public Observable<IndexOperationMessage> deleteIndexEdge( final ApplicationScope applicationScope,
                                                              final Edge edge ) {


        //TODO, query ES and remove this edge

        throw new NotImplementedException( ""Implement me"" );
    }


    @Override
    public Observable<IndexOperationMessage> deleteEntityIndexes( final ApplicationScope applicationScope,
                                                                  final Id entityId ) {

        //TODO query ES and remove this entityId
        throw new NotImplementedException( ""Implement me"" );
    }",,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: EntityDictionaryIT,USERGRID-626,12827492,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,jeffreyawest,jeffreyawest,06/May/15 00:43,07/May/15 17:21,29/Oct/20 16:32,07/May/15 17:21,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-06 00:43:31.0,,,,,,,"0|i004av:",9223372036854775807,,,,,,,,,,,Usergrid 17,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: CountingMutatorIT,USERGRID-625,12827491,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,jeffreyawest,jeffreyawest,06/May/15 00:43,07/May/15 17:20,29/Oct/20 16:32,07/May/15 17:20,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-06 00:43:23.0,,,,,,,"0|i004an:",9223372036854775807,,,,,,,,,,,Usergrid 17,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: MessagesIT,USERGRID-624,12827490,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,jeffreyawest,jeffreyawest,06/May/15 00:43,07/May/15 16:54,29/Oct/20 16:32,07/May/15 16:54,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-05-06 22:55:39.257,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 07 16:54:19 UTC 2015,,,,,,,"0|i0049j:",9223372036854775807,,,,,,,,,,,Usergrid 17,,,,,,,,,,,2.0,,,,,,,,,,,"06/May/15 22:55;safeldm;me.prettyprint.hector.api.exceptions.HInvalidRequestException: InvalidRequestException(why:unconfigured columnfamily Entity_Properties)
	at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:52)
	at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:260)
	at me.prettyprint.cassandra.model.ExecutingVirtualKeyspace.doExecuteOperation(ExecutingVirtualKeyspace.java:66)
	at me.prettyprint.cassandra.model.MutatorImpl.execute(MutatorImpl.java:243)
	at org.apache.usergrid.persistence.hector.CountingMutator.execute(CountingMutator.java:220)
	at org.apache.usergrid.persistence.cassandra.CassandraPersistenceUtils.batchExecute(CassandraPersistenceUtils.java:233)
	at org.apache.usergrid.mq.cassandra.QueueManagerImpl.postToQueue(QueueManagerImpl.java:248)
	at org.apache.usergrid.mq.MessagesIT.testConsumer(MessagesIT.java:183)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.apache.usergrid.CoreApplication$1.evaluate(CoreApplication.java:145)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.apache.usergrid.CoreITSetupImpl$1.evaluate(CoreITSetupImpl.java:76)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
Caused by: InvalidRequestException(why:unconfigured columnfamily Entity_Properties)
	at org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:20919)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:973)
	at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:959)
	at me.prettyprint.cassandra.model.thrift.AbstractThriftClientWrapper.batch_mutate(AbstractThriftClientWrapper.java:64)
	at me.prettyprint.cassandra.service.VirtualKeyspaceCassandraClient.batch_mutate(VirtualKeyspaceCassandraClient.java:105)
	at me.prettyprint.cassandra.model.MutatorImpl$3.execute(MutatorImpl.java:246)
	at me.prettyprint.cassandra.model.MutatorImpl$3.execute(MutatorImpl.java:243)
	at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:104)
	at me.prettyprint.cassandra.service.VirtualKeyspaceOperation.executeAndSetResult(VirtualKeyspaceOperation.java:50)
	at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:253)
	... 37 more

2015-05-06 16:54:12,174 INFO (main) CoreApplication - Test testSubscriberSearch(org.apache.usergrid.mq.MessagesIT): finish with application
2015-05-06 16:54:12,175 INFO (main) CoreITSetupImpl - Tearing down for org.apache.usergrid.mq.MessagesIT

Process finished with exit code 255
","07/May/15 16:54;safeldm;ignore these tests
",,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: AllEntitiesInSystemObservableIT,USERGRID-623,12827489,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,jeffreyawest,jeffreyawest,06/May/15 00:41,07/May/15 17:13,29/Oct/20 16:32,06/May/15 21:33,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-06 00:41:34.0,,,,,,,"0|i0049b:",9223372036854775807,,,,,,,,,,,Usergrid 17,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: IndexServiceTest,USERGRID-622,12827488,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,jeffreyawest,jeffreyawest,06/May/15 00:41,07/May/15 20:17,29/Oct/20 16:32,07/May/15 20:17,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-05-06 21:21:30.779,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 22:53:33 UTC 2015,,,,,,,"0|i00493:",9223372036854775807,,,,,,,,,,,Usergrid 17,,,,,,,,,,,2.0,,,,,,,,,,,"06/May/15 21:21;safeldm;06 15:21:09,475 DEBUG (main) EsApplicationEntityIndexImpl -    Hit count: 0 Total hits: 0

org.apache.usergrid.exception.NotImplementedException: Implement me
	at org.apache.usergrid.corepersistence.index.IndexServiceImpl.deleteEntityIndexes(IndexServiceImpl.java:182)
	at org.apache.usergrid.corepersistence.asyncevents.InMemoryAsyncEventService.queueEntityDelete(InMemoryAsyncEventService.java:107)
	at org.apache.usergrid.corepersistence.CpEntityManager.deleteAsync(CpEntityManager.java:652)
	at org.apache.usergrid.corepersistence.CpEntityManager.delete(CpEntityManager.java:620)
	at org.apache.usergrid.persistence.GeoIT.testGeoDistanceOfConnection(GeoIT.java:216)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.apache.usergrid.CoreApplication$1.evaluate(CoreApplication.java:145)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.apache.usergrid.CoreITSetupImpl$1.evaluate(CoreITSetupImpl.java:76)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
","06/May/15 22:53;safeldm;something is wrong with the way we query out edges",,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: InMemoryAsyncIndexServiceTest,USERGRID-621,12827487,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,jeffreyawest,jeffreyawest,06/May/15 00:37,07/May/15 22:03,29/Oct/20 16:32,07/May/15 22:03,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-05-06 21:34:14.425,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 21:34:14 UTC 2015,,,,,,,"0|i0048v:",9223372036854775807,,,,,,,,,,,Usergrid 17,,,,,,,,,,,2.0,,,,,,,,,,,"06/May/15 21:34;safeldm;this is the result of a query failing",,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: IteratingQueryIT + collectionIT,USERGRID-587,12822822,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,tnine,safeldm,safeldm,21/Apr/15 22:06,12/Jul/15 16:10,29/Oct/20 16:32,28/May/15 15:10,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-04-21 22:06:26.0,,,,,,,"0|i003lr:",9223372036854775807,,,,,,,,,,,Usergrid 19,Usergrid 20,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: staleIndexCleanup,USERGRID-584,12822815,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,greyes,greyes,21/Apr/15 21:55,12/Jul/15 16:10,29/Oct/20 16:32,28/May/15 22:59,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,Seems like all of the stale entities aren't being cleaned up. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-05-07 15:58:24.913,,,false,USERGRID-638,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 17:07:40 UTC 2015,,,,,,,"0|i003lj:",9223372036854775807,,,,,,,,,,,Usergrid 19,Usergrid 20,,,,,,,,,,1.0,,,,,,,,,,,"07/May/15 15:58;safeldm;org.apache.usergrid.exception.NotImplementedException: Implement me
	at org.apache.usergrid.corepersistence.index.IndexServiceImpl.deleteEntityIndexes(IndexServiceImpl.java:182)
	at org.apache.usergrid.corepersistence.asyncevents.InMemoryAsyncEventService.queueEntityDelete(InMemoryAsyncEventService.java:107)
	at org.apache.usergrid.corepersistence.CpEntityManager.deleteAsync(CpEntityManager.java:652)
	at org.apache.usergrid.corepersistence.CpEntityManager.delete(CpEntityManager.java:620)
	at org.apache.usergrid.corepersistence.StaleIndexCleanupTest.testCleanupOnDelete(StaleIndexCleanupTest.java:370)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.apache.usergrid.CoreApplication$1.evaluate(CoreApplication.java:145)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.apache.usergrid.CoreITSetupImpl$1.evaluate(CoreITSetupImpl.java:76)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)","29/May/15 17:07;greyes;Fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,
ES Cursors not working in 2.1,USERGRID-578,12821910,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,safeldm,safeldm,safeldm,17/Apr/15 15:35,12/Jul/15 16:51,29/Oct/20 16:32,21/Apr/15 21:31,,,,,,,,,2.1.0,,,,,,Stack,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-477,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-04-17 15:35:12.0,,,,,,,"0|i0043b:",9223372036854775807,,,,,,,,,,,Usergrid 14,Usergrid 15,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New Apps not in dropdown until you click on Org Administration Page,USERGRID-570,12821562,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,,,jeffreyawest,jeffreyawest,16/Apr/15 15:57,02/Feb/16 22:09,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,"When creating apps in the portal sometimes it takes a very long time to reflect them in the portal.  Even after /users/me returns the app in the list it is still not reflected in the portal.

When you use the '+ Add New App' at the top of the page the newly created app is not reflected in the App dropdown until you click on the Org Administration page.

See Jeff to get a URL for a recording of this issue being reproduced.

In addition, sometimes the immediate result is 'No Application Access Authorized'.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-353,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-04-16 15:57:19.0,,,,,,,"0|i001pr:000000010cohzzzzzy",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updating default accesstoken ttl for an app gives 404,USERGRID-566,12819897,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,senthilkumar.kj,senthilkumar.kj,10/Apr/15 00:22,02/Feb/16 21:29,29/Oct/20 16:32,,2.1.0,,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"PUT call to an organization's app to update accesstoken ttl gives 404. The REST call is documented here - http://apigee.com/docs/api-baas/content/changing-token-time-live-ttl

This fails in two-dot-o and two-dot-o-dev. The code which handles the api has been updated recently (past month).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-827,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 10 00:29:03 UTC 2015,,,,,,,"0|i001pr:000000010cozi",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"10/Apr/15 00:29;senthilkumar.kj;[~djohnson] could you please take a look at this and comment?",,,,,,,,,,,,,,,,,,,,,,,,,,,
[SPIKE] Cross-collection / graph filtering does not work,USERGRID-551,12819080,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,,,jeffreyawest,jeffreyawest,07/Apr/15 21:24,05/Jan/16 19:21,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,"REST calls of this nature do not work: GET /pets;ql=select * where type='cat'/belongsTo/owners;ql=select * where city='Dallas'
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-818,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-04-07 21:24:51.0,,,,,,,"0|i001pr:000000010cohzzzzzzzr",9223372036854775807,,,,,,,,,,,Usergrid 14,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure SQS Consumers are robust and do not stop consuming messages,USERGRID-546,12818602,Bug,Resolved,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Blocker,Done,mrusso,jeffreyawest,jeffreyawest,06/Apr/15 13:21,12/May/16 08:59,29/Oct/20 16:32,28/Jul/15 21:40,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,"In the past: For some reason the SQS queue consumers stop processing messages.  Please investigate, ensure that the code is robust, and add log messages.

Current: Test and confirm, review with Jeff and/or Todd that the loop for processing SQS messages is robust.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-07-28 21:40:32.444,,,false,USERGRID-731,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 28 21:40:32 UTC 2015,,,,,,,"0|i004fz:",9223372036854775807,,,,,,,,,,,Usergrid 26,,,,,,,,,,,3.0,,,,,,,,,,,"28/Jul/15 21:40;mrusso;Code for queue consumers have been reviewed and tested and they are always running if the system is up.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Configuring Notifier with APNS does not error when a non-p12 certificate is uploaded or used,USERGRID-519,12785630,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,mrusso,jeffreyawest,jeffreyawest,25/Mar/15 18:16,24/Feb/17 22:56,29/Oct/20 16:32,13/Jan/16 00:04,,,,,,,,,2.2.0,,,,,,Stack,,,,,0,,,,,,,,,"I uploaded a x509 certificate instead of a p12 certificate for configuring an APS notifier.  I experienced the following:

1) The UI did not error out when I uploaded a non p12 certificate.
-- Similar to Parse, the console should error out when a non-p12 certificate is attempted to be used.  If we accept an x509 certificate the docs and UI page should be updated.

2) When sending a push notification BaaS reports a success in the message history and notification entity but a push notification was not received
-- It seems that this is not possible to have worked since it was not a p12 certificate.  It looks as if there could be an exception getting swallowed.

3) I cannot delete the notifiers.
-- Dont know if this is related to the certificate or not
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-13 00:04:55.285,,,false,USERGRID-814,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 13 00:04:55 UTC 2016,,,,,,,"0|i001pr:000000010co0002",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"13/Jan/16 00:04;mrusso;In 2.1.1 going forward, the API will try to load the cert as a .p12 file.  If this does not work, then an exception will be thrown.",,,,,,,,,,,,,,,,,,,,,,,,,,,
BaaS Queries returning Entities which do not match the input Query,USERGRID-516,12785599,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Won't Fix,jeffreyawest,jeffreyawest,jeffreyawest,25/Mar/15 17:21,24/Feb/17 22:53,29/Oct/20 16:32,02/Feb/16 21:40,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,Error,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-817,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 18:51:20 UTC 2015,,,,,,,"0|i004sf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"22/Sep/15 18:51;jeffreyawest;bad query",,,,,,,,,,,,,,,,,,,,,,,,,,,
Delete call with a limit of X returns >X resluts,USERGRID-508,12785253,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,lynchlee,jeffreyawest,jeffreyawest,24/Mar/15 18:39,05/Jan/16 19:21,29/Oct/20 16:32,,,,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"When doing a delete call with ql=select * and lmit=100, >100 results are returned.  This may or may not imply that >100 entities were deleted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-838,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-03-24 18:39:12.0,,,,,,,"0|i001pr:000000010cozzzzr",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken link to contribution page,USERGRID-498,12783574,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,yep,yep,20/Mar/15 12:01,02/Nov/15 17:41,29/Oct/20 16:32,,,,,,,,,,,,,,,,Docs,,,,,0,,,,,,,,,"On the usergrid homepage at http://usergrid.incubator.apache.org the ""Contribution Guidelines"" link points to

https://cwiki.apache.org/confluence/display/usergrid/GitHub+Based+Contribution+Workflow

but it should should really be

https://cwiki.apache.org/confluence/display/usergrid/Usergrid+External+Contributors+Guide",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-826,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-03-20 12:01:05.0,,,,,,,"0|i005dr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Long Pauses and Errors from ES on deleteByQuery,USERGRID-471,12780923,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Blocker,Not A Problem,,jeffreyawest,jeffreyawest,10/Mar/15 17:42,24/Feb/17 22:53,29/Oct/20 16:32,20/Aug/15 22:16,,,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"When a single thread is doing PUTs against Usergrid, one after the other, ES can time out on deleteByQuery and block the caller.

The response time from Usergrid goes from 200ms to ~60s during this time.  Note that the error in ES logs indicates a 1 minute timeout.

[2015-03-10 17:37:42,947][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:42,947][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:42,948][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:42,948][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:42,949][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:42,949][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,007][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,008][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,008][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,008][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,008][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,008][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,008][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,012][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,013][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,013][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,013][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,013][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,013][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,013][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,072][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,073][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,073][DEBUG][action.deletebyquery     ] [res003sy] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2015-03-10 17:37:43,073][DEBUG][action.deletebyquery",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-810,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 20 22:16:49 UTC 2015,,,,,,,"0|i0028v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"20/Aug/15 22:16;jeffreyawest;This was a problem with threads and GC collections which is no longer an issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Rejected ES Index jobs are dropped.  ,USERGRID-466,12780559,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Blocker,Fixed,tnine,jeffreyawest,jeffreyawest,09/Mar/15 18:13,12/Jul/15 16:22,29/Oct/20 16:32,12/Mar/15 15:17,,,,,,,,,2.1.0,,,,,,Stack,,,,,0,,,,,,,,,"When the index queue is full and ES rejects index requests Usergrid drops them.  The result could be that entities cannot be returned.

We should handle this soon.  At a minimum, we should put them in an SQS queue or something similar for future handling.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,USERGRID-324,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-03-10 18:14:18.165,,,false,USERGRID-477,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 15:17:44 UTC 2015,,,,,,,"0|i005cv:",9223372036854775807,,,,,,,,,,,Usergrid 9,Usergrid 10,,,,,,,,,,5.0,,,,,,,,,,,"10/Mar/15 18:14;tnine;Using the existing producer/consumer, we can put an SQS queue into them.  This will allow us to handle the backpressure, as well as guarantee eventual consistency.  ","12/Mar/15 15:17;tnine;Done.  We use SQS for signaling and map (cassandra) for storing the data.  We need to get a realtime messaging p2p system in place.  We're currently storing our entity payload 2x, which is inefficient.  Instead we need to send the signal with a message, then perform indexing of the entity from the entity data itself.  This should ultimately occur at a higher level.",,,,,,,,,,,,,,,,,,,,,,,,,,
Content length headers are not being sent on POSTS for ping identity,USERGRID-457,12779945,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,mrusso,rockerston,rockerston,06/Mar/15 00:56,22/Jul/15 20:16,29/Oct/20 16:32,14/Jul/15 22:14,,,,,,,,,2.1.0,,,,,,Stack,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-03-20 03:23:17.841,,,false,USERGRID-804,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 22 20:16:53 UTC 2015,,,,,,,"0|i002of:",9223372036854775807,,,,,,,,,,,Usergrid 27,,,,,,,,,,,3.0,,,,,,,,,,,"20/Mar/15 03:23;githubbot;GitHub user michaelarusso opened a pull request:

    https://github.com/apache/incubator-usergrid/pull/193

    USERGRID-457 - Add explicit Content-Length of 0 for POST request with no...

    ... body.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/michaelarusso/incubator-usergrid USERGRID-457

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-usergrid/pull/193.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #193
    
----
commit 5937f9fb0618d1f1ca38e817b14c7e25cf8b7bd7
Author: Michael Russo <mrusso@apigee.com>
Date:   2015-03-20T03:12:45Z

    USERGRID-457 - Add explicit Content-Length of 0 for POST request with no body.

----
","20/Mar/15 13:12;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-usergrid/pull/193
","20/Mar/15 13:17;githubbot;Github user tnine commented on the pull request:

    https://github.com/apache/incubator-usergrid/pull/193#issuecomment-84012047
  
    Thanks!  I merged this into 2.0 and into 1.0

","20/Mar/15 14:19;hudson;FAILURE: Integrated in usergrid-portal-master #77 (See [https://builds.apache.org/job/usergrid-portal-master/77/])
USERGRID-457 - Add explicit Content-Length of 0 for POST request with no body. (tnine: rev 451dc0b9fd4d39f5f9c0744de275aae89e9212af)
* stack/services/src/main/java/org/apache/usergrid/security/providers/PingIdentityProvider.java
","23/Mar/15 17:04;hudson;SUCCESS: Integrated in usergrid-portal-master #81 (See [https://builds.apache.org/job/usergrid-portal-master/81/])
USERGRID-457 - Add explicit Content-Length of 0 for POST request with no body. (rbridges: rev aadcf4cf4a5e520d6eedeffa5bc9e3b2ccb1fa73)
* stack/services/src/main/java/org/apache/usergrid/security/providers/PingIdentityProvider.java
[USERGRID-457] incrementing version (rbridges: rev 4d7be242c7b1b9cf2cd1ededf3e2dbf56fdb0ec9)
* stack/core/pom.xml
* stack/test-utils/pom.xml
* stack/java-sdk-old/pom.xml
* stack/build-tools/pom.xml
* stack/services/pom.xml
* stack/launcher/pom.xml
* stack/websocket/pom.xml
* stack/tools/pom.xml
* stack/rest/pom.xml
* stack/pom.xml
* stack/mongo-emulator/pom.xml
* stack/config/pom.xml
","27/Mar/15 05:26;githubbot;GitHub user michaelarusso opened a pull request:

    https://github.com/apache/incubator-usergrid/pull/202

    USERGRID-457 - Update PingIdentityProvider to send parameters as formpar...

    ...ams instead of queryparams.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/michaelarusso/incubator-usergrid USERGRID-457

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-usergrid/pull/202.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #202
    
----
commit 0c4b7c1cb2fcdc997659cd7f3056840d6b90db45
Author: Michael Russo <mrusso@apigee.com>
Date:   2015-03-27T05:19:13Z

    USERGRID-457 - Update PingIdentityProvider to send parameters as formparams instead of queryparams.

----
","14/Jul/15 22:14;jeffreyawest;Mike confirmed in the codebase","22/Jul/15 20:16;githubbot;Github user michaelarusso commented on the pull request:

    https://github.com/apache/incubator-usergrid/pull/202#issuecomment-123849808
  
    Already merged into the code base.  Closing.
","22/Jul/15 20:16;githubbot;Github user michaelarusso closed the pull request at:

    https://github.com/apache/incubator-usergrid/pull/202
",,,,,,,,,,,,,,,,,,,
Invalidate token after it has been used to activate a new user account,USERGRID-450,12779182,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,rockerston,rockerston,03/Mar/15 21:15,05/Jan/16 19:21,29/Oct/20 16:32,,,,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"Once token has been used to activate an account, it should be invalidated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-827,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-03-03 21:15:00.0,,,,,,,"0|i001pr:000000010cozzzr",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Admin Portal - ${user} permissions not allowed, but they should be",USERGRID-449,12779120,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,rockerston,rockerston,03/Mar/15 17:14,02/Nov/15 17:41,29/Oct/20 16:32,,,,,,,,,,,,,,,,Portal,,,,,0,,,,,,,,,"In the admin portal, if you try to enter a permission like this:

/users/{$user}/has/stuff/mystuff

It gives this error:

""Path must begin with a slash, path only allows: /, a-z, 0-9, dot, and dash, paths of the format: /path/ or /path//path are not allowed""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-353,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-03-03 17:14:17.0,,,,,,,"0|i00abz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove redundant appinfos collections in ManagementServiceImpl,USERGRID-448,12779087,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,djohnson,djohnson,djohnson,03/Mar/15 14:36,02/Feb/16 21:29,29/Oct/20 16:32,30/Mar/15 20:07,,,,,,,,,2.1.0,,,,,,Stack,,,,,0,,,,,,,,,"There is a flaw in the two-dot-o CpEntityManagerFactory.

The factory stores a collection of ""appinfo"" type entities, but the ManagementServiceImpl stores a redundant collection of ""application_info"" entities. 

The problem becomes evident when you try to delete an application. The application will only be deleted from the ""appinfos"" collection. When you call the management org/apps end-point you will still see the application because the end-point uses the ""application_infos"". 

To fix this:
- Ensure that only one collection is stored
- Add code to migrate the existing app information collections
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-03-04 18:46:36.595,,,false,USERGRID-356,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 30 20:08:08 UTC 2015,,,,,,,"0|i004x3:",9223372036854775807,,,,,,,,,,,Usergrid 9,Usergrid 10,Usergrid 11,Usergrid 12,,,,,,,,3.0,,,,,,,,,,,"04/Mar/15 18:46;githubbot;GitHub user snoopdave opened a pull request:

    https://github.com/apache/incubator-usergrid/pull/173

    Fix for USERGRID-448: Remove redundant appinfos collections in ManagementServiceImpl

    DO NOT MERGE
    
    Needs careful review.
    
    Changes to merge appinfo and application_info collections into one application_info collection. Also:
    - Removed unnecessary system and default applications
    - Now use seek rather than search to load application info entities
    - Uncommented the parts of ApplicationDeleteTest that did not work before the appinfo fix
    - Will migrate old appinfo collection to application_info on startup, but can be configured to skip that
    
    https://issues.apache.org/jira/browse/USERGRID-448

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/snoopdave/incubator-usergrid USERGRID-448-appinfofix

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-usergrid/pull/173.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #173
    
----
commit 67fe099a48b87d6becaae9b439a8d758efc314c4
Author: Dave Johnson <dmjohnson@apigee.com>
Date:   2015-03-03T16:10:37Z

    Implementation of AppDeleteTest and supporting REST test infra structure.

commit 485f5787056e25ce98b28821c52746766239bb81
Author: Dave Johnson <dmjohnson@apigee.com>
Date:   2015-03-03T16:30:59Z

    Removing redundant ""appinfos"" collection, work in progress.

commit 592c730232a5d5156b29c9644e2ed5873e829b7b
Author: Dave Johnson <dmjohnson@apigee.com>
Date:   2015-03-03T17:46:31Z

    Comment out parts of test blocked by USERGRID-448.

commit 6791df6d4037daadd2781d26ac14aa4a213bd3bd
Author: Dave Johnson <dmjohnson@apigee.com>
Date:   2015-03-03T21:35:10Z

    Move ApplicationInfo to Core package, use ""application_info"" type consistently across entire codebase.

commit 3b91da08e9199356c3c62be54f47936ef8bb8ab5
Author: Dave Johnson <dmjohnson@apigee.com>
Date:   2015-03-03T21:46:26Z

    Merge branch 'two-dot-o' into USERGRID-361-appdeletetest

commit 4d8678cfdcb95f811bf7fe3b551a28be37ee76d9
Author: Dave Johnson <dmjohnson@apigee.com>
Date:   2015-03-04T14:07:56Z

    Merge branch 'USERGRID-361-appdeletetest' into USERGRID-448-appinfofix
    
    Conflicts:
    	stack/rest/src/test/java/org/apache/usergrid/rest/test/resource2point0/endpoints/mgmt/ApplicationsResource.java

commit c0d50394e0e4f3b3669fcfd582b78ce49a86a542
Author: Dave Johnson <dmjohnson@apigee.com>
Date:   2015-03-04T18:34:11Z

    Changes to merge appinfo and application_info collections into one application_info collection. Also:
    - Removed unnecessary system and default applications
    - Now use seek rather than search to load application info entities
    - Uncommented the parts of ApplicationDeleteTest that did not work before the appinfo fix
    - Will migrate old appinfo collection to application_info on startup, but can be configured to skip that

----
","04/Mar/15 18:48;djohnson;Pull request is ready for review:

     https://github.com/apache/incubator-usergrid/pull/173/files

Unit tests are looking good but I still need to test a ""live"" migration.","09/Mar/15 15:42;githubbot;Github user snoopdave commented on the pull request:

    https://github.com/apache/incubator-usergrid/pull/173#issuecomment-77878271
  
    Review notes
    ===
    This is a big commit, so here are the key changes to consider.  Many files changed because I moved ApplicationInfo from the services module to the core module, my IDE did some uncalled for formatting (removing trailing spaces) and then I decided to move ApplicationInfo back home to services.
    
    CpEntityManagerFactory
    ---
    The main change here is that we no longer create an ""appinfos"" collection and instead rely solely on the ""application_info"" collection created by the ManagementServiceImpl.
       - System App and Default App have been removed completely, they were unused
       - No longer create redundant ""organization"" Entity
       - New impl. of deleteApplication() that saves connections
       - New impl. of restoreApplication() that restores connections
       - lookupAppication(name) uses alias instead of ES
       - migrationOldAppInfos() reads old ""appinfo"" entities and writes ""application_info"" entities. 
       - New property usergrid.twodoto.appinfo.migration which defaults to true
    
    JobServiceScheduler
    ---
    - Fixes to adapt to new way MetricsFactory is instantiated. Somebody changed it to be instantiated with Guice but did not fix this class.
    
    ManagementServiceImpl
    ---
    - New impl. of deleteApplication() that does some error checks and then calls emf.deleteApplication()
    - New impl. of restoreApplication() calls emf.restoreApplication(), restores token and organization connection, and adds activity
    
    ApplicationDeleteTest
    ---
    - New and comprehensive app delete and restore tests
    
    ApplicationsResource (test infrastructure)
    ---
    - Represents management ApplicationsResource end-point
    
    ManagementResponse (test infrastructure)
    ---
    - Represents JSON response from management end-points

","30/Mar/15 15:47;githubbot;Github user tnine commented on the pull request:

    https://github.com/apache/incubator-usergrid/pull/173#issuecomment-87729978
  
    Hey Dave.  Can you close this and point it at two-dot-o-dev?
","30/Mar/15 18:09;githubbot;Github user snoopdave commented on the pull request:

    https://github.com/apache/incubator-usergrid/pull/173#issuecomment-87776891
  
    This has been merged it two-dot-o-dev
","30/Mar/15 18:09;githubbot;Github user snoopdave closed the pull request at:

    https://github.com/apache/incubator-usergrid/pull/173
","30/Mar/15 20:07;djohnson;work complete","30/Mar/15 20:08;djohnson;merged into two-dot-o-dev",,,,,,,,,,,,,,,,,,,,
Investigate ElasticSearch timeout such that a long-running GC does not pause all ES and Tomcat,USERGRID-442,12778826,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,Fixed,,jeffreyawest,jeffreyawest,02/Mar/15 18:43,02/Feb/16 22:19,29/Oct/20 16:32,02/Feb/16 22:19,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,"I have seen as many as 16 tomcat nodes pausing at the same time across the cluster.  I took a stack trace and have attached it.  

This one jumped out to me:
""http-bio-8080-exec-2"" daemon prio=10 tid=0x00007f0cbc003800 nid=0x20ff waiting on condition [0x00007f0d370dd000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007ee1de4d8> (a org.elasticsearch.common.util.concurrent.BaseFuture$Sync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)
	at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:274)
	at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)
	at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)
	at org.apache.usergrid.persistence.index.impl.EsEntityIndexImpl.search(EsEntityIndexImpl.java:357)
	at org.apache.usergrid.corepersistence.CpRelationManager.searchCollection(CpRelationManager.java:963)
	at org.apache.usergrid.corepersistence.CpEntityManager.searchCollection(CpEntityManager.java:624)
	at org.apache.usergrid.corepersistence.CpEntityManagerFactory.lookupApplication(CpEntityManagerFactory.java:433)
	at org.apache.usergrid.rest.organizations.OrganizationResource.getApplicationByName(OrganizationResource.java:137)
	at sun.reflect.GeneratedMethodAccessor113.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.dispatch(SubLocatorRule.java:197)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.invokeSubLocator(SubLocatorRule.java:183)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:110)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.SubLocatorRule.accept(SubLocatorRule.java:137)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
	at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
	at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1542)
	at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1473)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1419)
	at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1409)
	at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:409)
	at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:540)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:909)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:857)
	at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:811)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)
	at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)
	at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)
	at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)
	at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)
	at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)
	at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)
	at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
	at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:259)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.usergrid.rest.filters.ContentTypeFilter.doFilter(ContentTypeFilter.java:92)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:503)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:170)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
	at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:950)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:421)
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1070)
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)
	at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:314)
	- locked <0x00000007eda23278> (a org.apache.tomcat.util.net.SocketWrapper)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
	at java.lang.Thread.run(Thread.java:722)

and Todd called out this one:
""collectiontasks-18"" daemon prio=10 tid=0x00007f0cfc004000 nid=0x21b7 waiting on condition [0x00007f0cb87c6000]
  java.lang.Thread.State: WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for  <0x00000007ed32ccf0> (a org.elasticsearch.common.util.concurrent.BaseFuture$Sync)
at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)
at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)
at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:274)
at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)
at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)
at org.apache.usergrid.persistence.index.impl.EsEntityIndexImpl.deletePreviousVersions(EsEntityIndexImpl.java:533)
at org.apache.usergrid.corepersistence.events.EntityVersionCreatedHandler.versionCreated(EntityVersionCreatedHandler.java:67)
at org.apache.usergrid.persistence.collection.impl.EntityVersionCreatedTask.fireEvents(EntityVersionCreatedTask.java:97)
at org.apache.usergrid.persistence.collection.impl.EntityVersionCreatedTask.call(EntityVersionCreatedTask.java:83)
at org.apache.usergrid.persistence.collection.impl.EntityVersionCreatedTask.call(EntityVersionCreatedTask.java:38)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
at java.util.concurrent.FutureTask.run(FutureTask.java:166)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:722)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/15 18:44;jeffreyawest;paused_threads.txt;https://issues.apache.org/jira/secure/attachment/12701940/paused_threads.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,2015-03-03 21:17:01.338,,,false,USERGRID-357,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 02 22:19:03 UTC 2016,,,,,,,"0|i00507:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"02/Mar/15 18:45;jeffreyawest;[~tnine] added the stack trace file here","03/Mar/15 21:17;safeldm;look into creating futures for all ES operations.
then try to push it over","03/Mar/15 21:17;safeldm;is there another call that only runs on master","12/Mar/15 15:33;tnine;Was caused by delete by query killing the ES cluster.  Delete by query has been removed.  Closing.","02/Feb/16 22:19;jeffreyawest;The problem here was related to several factors in ES, one of which was the NewRatio.  We tuned the GC settings and we were able to bypass this.",,,,,,,,,,,,,,,,,,,,,,,
Scheduler run failed when system idling,USERGRID-439,12778371,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,Fixed,mrusso,jeffreyawest,jeffreyawest,27/Feb/15 23:21,02/Feb/16 21:49,29/Oct/20 16:32,12/Jan/16 01:46,,,,,,,,,2.1.0,,,,,,Stack,,,,,0,,,,,,,,,"This error happens consistently in some clusters when the system is largely idle:

2015-02-27 23:19:40,060 [JobSchedulerService] ERROR org.apache.usergrid.batch.service.JobSchedulerService- Scheduler run failed, error is
org.apache.usergrid.persistence.exceptions.QueueException: Unable to obtain a lock on queue '/jobs' after '5'seconds
	at org.apache.usergrid.mq.cassandra.io.ConsumerTransaction.getResults(ConsumerTransaction.java:206)
	at org.apache.usergrid.mq.cassandra.QueueManagerImpl.getFromQueue(QueueManagerImpl.java:412)
	at org.apache.usergrid.batch.service.SchedulerServiceImpl.getJobs(SchedulerServiceImpl.java:164)
	at org.apache.usergrid.batch.service.JobSchedulerService.runOneIteration(JobSchedulerService.java:111)
	at com.google.common.util.concurrent.AbstractScheduledService$1$1.run(AbstractScheduledService.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-12 01:46:08.251,,,false,USERGRID-814,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 12 01:46:08 UTC 2016,,,,,,,"0|i266d3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"12/Jan/16 01:46;mrusso;This is actually not an error case if the server cannot get a lock on the queue jobs.  It could happen in a valid scenario.  The log statement has already been switched to debug in master branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Admin Portal - connections listing on data page throws error,USERGRID-435,12778346,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,rockerston,rockerston,27/Feb/15 22:10,02/Nov/15 17:41,29/Oct/20 16:32,,,,,,,,,,,,,,,,Portal,Stack,,,,0,,,,,,,,,"Steps to reproduce:

1. Create entities in two different collections
2. Connect them
3. Read the connection in the data explorer

POST /reviews {""name"":""myreview""}
POST /users {""username"":""fred""}
POST /users/fred/wrote/reviews/myreview
GET /users/fred/wrote/reviews/ <== this call will give the error

This happens because the data explorer tries to make a /indexes call:

GET /users/fred/wrote/reviews/indexes

Which returns a 500 error",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-353,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-02-27 22:10:44.0,,,,,,,"0|i00ac7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[SPIKE] Verify that during an ES re-index, existing documents are force updated",USERGRID-425,12778011,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,Won't Fix,,tnine,tnine,26/Feb/15 20:45,02/Feb/16 22:14,29/Oct/20 16:32,02/Feb/16 22:13,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,"When running a reindex last night, documents that appeared in ES were not returned via search in the CandidateResultSet.  Deleting the document from ES and then re-indexing resolved the issue. We need to ensure we're updating existing documents that exist in ES during a re-index to force an index update for existing entities.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-02 22:13:43.451,,,false,USERGRID-805,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 02 22:13:43 UTC 2016,,,,,,,"0|i001pr:000000010cozzzzi",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"02/Feb/16 22:13;jeffreyawest;Due to some problems with RX we were actually dropping index operations.  This has been resolved.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot build Android SDK,USERGRID-350,12767206,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,szlacko,szlacko,13/Jan/15 18:22,05/Jan/16 19:21,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,"I cannot build the Android SDK in Usergrid 1.0.1.

Printouts with build_sdk_zip.sh modified with ""-U"" and ""-e"" options added (""mvn clean install -U -e""):

ubuntu@ubuntu-VirtualBox:~/Downloads/incubator-usergrid-master/sdks/android$ ./build_release_zip.sh 0.0.8
[INFO] Error stacktraces are turned on.
[INFO] Scanning for projects...
Downloading: http://repo.maven.apache.org/maven2/org/apache/usergrid/usergrid/1.0.0/usergrid-1.0.0.pom
[ERROR] The build could not read 1 project -> [Help 1]
org.apache.maven.project.ProjectBuildingException: Some problems were encountered while processing the POMs:
[FATAL] Non-resolvable parent POM: Could not find artifact org.apache.usergrid:usergrid:pom:1.0.0 in central (http://repo.maven.apache.org/maven2) and 'parent.relativePath' points at wrong local POM @ line 27, column 10

	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:364)
	at org.apache.maven.DefaultMaven.collectProjects(DefaultMaven.java:672)
	at org.apache.maven.DefaultMaven.getProjectsForMavenReactor(DefaultMaven.java:663)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:250)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:213)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:157)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[ERROR]   
[ERROR]   The project org.apache.usergrid:usergrid-android:0.0.8 (/home/ubuntu/Downloads/incubator-usergrid-master/sdks/android/pom.xml) has 1 error
[ERROR]     Non-resolvable parent POM: Could not find artifact org.apache.usergrid:usergrid:pom:1.0.0 in central (http://repo.maven.apache.org/maven2) and 'parent.relativePath' points at wrong local POM @ line 27, column 10 -> [Help 2]
org.apache.maven.model.resolution.UnresolvableModelException: Could not find artifact org.apache.usergrid:usergrid:pom:1.0.0 in central (http://repo.maven.apache.org/maven2)
	at org.apache.maven.project.ProjectModelResolver.resolveModel(ProjectModelResolver.java:159)
	at org.apache.maven.model.building.DefaultModelBuilder.readParentExternally(DefaultModelBuilder.java:817)
	at org.apache.maven.model.building.DefaultModelBuilder.readParent(DefaultModelBuilder.java:669)
	at org.apache.maven.model.building.DefaultModelBuilder.build(DefaultModelBuilder.java:307)
	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:411)
	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:380)
	at org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:344)
	at org.apache.maven.DefaultMaven.collectProjects(DefaultMaven.java:672)
	at org.apache.maven.DefaultMaven.getProjectsForMavenReactor(DefaultMaven.java:663)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:250)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:213)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:157)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.eclipse.aether.resolution.ArtifactResolutionException: Could not find artifact org.apache.usergrid:usergrid:pom:1.0.0 in central (http://repo.maven.apache.org/maven2)
	at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolve(DefaultArtifactResolver.java:459)
	at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolveArtifacts(DefaultArtifactResolver.java:262)
	at org.eclipse.aether.internal.impl.DefaultArtifactResolver.resolveArtifact(DefaultArtifactResolver.java:239)
	at org.eclipse.aether.internal.impl.DefaultRepositorySystem.resolveArtifact(DefaultRepositorySystem.java:295)
	at org.apache.maven.project.ProjectModelResolver.resolveModel(ProjectModelResolver.java:155)
	... 21 more
Caused by: org.eclipse.aether.transfer.ArtifactNotFoundException: Could not find artifact org.apache.usergrid:usergrid:pom:1.0.0 in central (http://repo.maven.apache.org/maven2)
	at org.eclipse.aether.connector.wagon.WagonRepositoryConnector$6.wrap(WagonRepositoryConnector.java:1012)
	at org.eclipse.aether.connector.wagon.WagonRepositoryConnector$6.wrap(WagonRepositoryConnector.java:1004)
	at org.eclipse.aether.connector.wagon.WagonRepositoryConnector$GetTask.run(WagonRepositoryConnector.java:725)
	at org.eclipse.aether.util.concurrency.RunnableErrorForwarder$1.run(RunnableErrorForwarder.java:67)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
[ERROR] 
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException
[ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/UnresolvableModelException
ubuntu@ubuntu-VirtualBox:~/Downloads/incubator-usergrid-master/sdks/android$ 



","Apache Maven 3.2.1 (ea8b2b07643dbb1b84b6d16e1f08391b666bc1e9; 2014-02-14T18:37:52+01:00)
Maven home: /usr/share/maven3
Java version: 1.7.0_72, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-7-oracle/jre
Default locale: en_US, platform encoding: UTF-8
OS name: ""linux"", version: ""3.13.0-32-generic"", arch: ""amd64"", family: ""unix""
(Ubuntu 14.04)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-01-13 18:28:35.118,,,false,USERGRID-834,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 15 08:33:11 UTC 2015,,,,,,,"0|i001pr:000000010cozr",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"13/Jan/15 18:28;johndament;This is because usergrid is not distributed via maven central at this time.  To work around this, please check out the 1.0.0 tag and build that locally.","15/Jan/15 08:33;szlacko;OK, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix PropertiesResourceIT,USERGRID-349,12766595,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,greyes,greyes,10/Jan/15 00:10,11/Aug/15 21:42,29/Oct/20 16:32,13/Jul/15 15:46,,,,,,,,,2.1.0,,,,,,,,,,,0,Fixed?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-603,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 31 17:57:23 UTC 2015,,,,,,,"0|i002pr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"23/Mar/15 19:15;greyes;The test passes when run alone but still needs to be converted over to the new rest test framework.","31/Jul/15 17:57;greyes;Test was refactored into other tests and so it no longer exists",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix AccessTokenIT,USERGRID-347,12766591,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,greyes,greyes,10/Jan/15 00:05,24/Mar/15 20:35,29/Oct/20 16:32,23/Mar/15 18:33,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-03-23 18:34:07.335,,,false,USERGRID-300,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 18:34:07 UTC 2015,,,,,,,"0|i0057r:",9223372036854775807,,,,,,,,,,,Usergrid 11,,,,,,,,,,,3.0,,,,,,,,,,,"23/Mar/15 18:33;greyes;Finished fixing the tests in AccessTokenIT and all passing. The PR is located here https://github.com/apache/incubator-usergrid/pull/197/files","23/Mar/15 18:34;githubbot;Github user GERey commented on the pull request:

    https://github.com/apache/incubator-usergrid/pull/197#issuecomment-85135742
  
    https://issues.apache.org/jira/browse/USERGRID-347
",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix MatrixQueryTests,USERGRID-346,12766589,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,,,greyes,greyes,10/Jan/15 00:00,05/Jan/16 19:21,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-818,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-01-10 00:00:53.0,,,,,,,"0|i001pr:000000010cohzzzzzzx",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix RetrieveUsersTest,USERGRID-345,12766588,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,greyes,greyes,10/Jan/15 00:00,24/Mar/15 20:35,29/Oct/20 16:32,20/Mar/15 19:48,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-03-20 19:49:12.849,,,false,USERGRID-300,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 20 19:49:12 UTC 2015,,,,,,,"0|i0057b:",9223372036854775807,,,,,,,,,,,Usergrid 11,,,,,,,,,,,1.0,,,,,,,,,,,"20/Mar/15 19:49;githubbot;Github user GERey commented on the pull request:

    https://github.com/apache/incubator-usergrid/pull/191#issuecomment-84116673
  
    https://issues.apache.org/jira/browse/USERGRID-345 Also resolves the following issue as well.
",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ApplicationRequestCounterIT,USERGRID-343,12766586,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Duplicate,greyes,greyes,greyes,09/Jan/15 23:50,11/Aug/15 21:42,29/Oct/20 16:32,31/Jul/15 20:27,,,,,,,,,2.1.0,,,,,,,,,,,0,Fixed?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-603,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-01-09 23:50:57.0,,,,,,,"0|i002rr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix AdminEmailEncodingIT,USERGRID-342,12766583,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Done,rbridges,greyes,greyes,09/Jan/15 23:49,28/Apr/15 21:45,29/Oct/20 16:32,02/Apr/15 17:21,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-03-31 19:27:53.763,,,false,USERGRID-300,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 31 20:11:04 UTC 2015,,,,,,,"0|i0041z:",9223372036854775807,,,,,,,,,,,Usergrid 13,Usergrid 14,Usergrid 15,,,,,,,,,1.0,,,,,,,,,,,"31/Mar/15 19:27;githubbot;GitHub user r3b opened a pull request:

    https://github.com/apache/incubator-usergrid/pull/205

    [USERGRID-342] Added unimplemented remove method to ElasticSearchQueryEx...

    ...ecutor. Fixed merge artifact in AdminEmailEncodingIT

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/r3b/usergrid-1 USERGRID-342

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-usergrid/pull/205.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #205
    
----
commit d20267bb5bc179e77f5174f1c338fbf7c7df9b99
Author: ryan bridges <ryanb@apache.org>
Date:   2015-03-27T16:48:43Z

    [USERGRID-342] Added unimplemented remove method to ElasticSearchQueryExecutor. Fixed merge artifact in AdminEmailEncodingIT

----
","31/Mar/15 20:11;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-usergrid/pull/205
",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix BrowserCompatibilityTest,USERGRID-341,12766581,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Not A Problem,greyes,greyes,greyes,09/Jan/15 23:48,23/Mar/15 18:43,29/Oct/20 16:32,23/Mar/15 18:43,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-300,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 18:43:28 UTC 2015,,,,,,,"0|i0056v:",9223372036854775807,,,,,,,,,,,Usergrid 11,,,,,,,,,,,3.0,,,,,,,,,,,"23/Mar/15 18:43;greyes;This was already fixed so no longer an issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix RegistrationIT,USERGRID-340,12766580,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Done,rbridges,greyes,greyes,09/Jan/15 23:43,28/Apr/15 21:44,29/Oct/20 16:32,02/Apr/15 17:19,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-300,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-01-09 23:43:52.0,,,,,,,"0|i0042n:",9223372036854775807,,,,,,,,,,,Usergrid 13,Usergrid 14,Usergrid 15,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix OwnershipResourceIT,USERGRID-338,12766578,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,greyes,greyes,09/Jan/15 23:42,24/Mar/15 20:35,29/Oct/20 16:32,23/Mar/15 19:07,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,Failing the contextualConnectionOwnership test everytime. The others pass.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-03-23 19:06:59.293,,,false,USERGRID-300,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 19:06:59 UTC 2015,,,,,,,"0|i0057j:",9223372036854775807,,,,,,,,,,,Usergrid 11,,,,,,,,,,,1.0,,,,,,,,,,,"23/Mar/15 19:06;githubbot;Github user GERey commented on the pull request:

    https://github.com/apache/incubator-usergrid/pull/197#issuecomment-85151123
  
    Also fixes https://issues.apache.org/jira/browse/USERGRID-338
",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ContentTypeResourceIT,USERGRID-336,12766576,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Done,rbridges,greyes,greyes,09/Jan/15 23:33,28/Apr/15 21:45,29/Oct/20 16:32,02/Apr/15 17:19,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-300,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-01-09 23:33:06.0,,,,,,,"0|i00427:",9223372036854775807,,,,,,,,,,,Usergrid 13,Usergrid 14,Usergrid 15,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Not Passing: CollectionsResourceIT,USERGRID-335,12766573,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,greyes,greyes,09/Jan/15 23:32,12/Jul/15 16:11,29/Oct/20 16:32,20/Mar/15 17:05,,,,,,,,,2.1.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-03-20 17:00:42.406,,,false,USERGRID-300,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 20 17:05:17 UTC 2015,,,,,,,"0|i0056n:",9223372036854775807,,,,,,,,,,,Usergrid 2,Usergrid 3,Usergrid 4,Usergrid 5,Usergrid 6,Usergrid 7,Usergrid 9,Usergrid 11,,,,2.0,,,,,,,,,,,"20/Mar/15 17:00;githubbot;Github user GERey commented on the pull request:

    https://github.com/apache/incubator-usergrid/pull/191#issuecomment-84070468
  
    https://issues.apache.org/jira/browse/USERGRID-335 Issue that this resolves
","20/Mar/15 17:05;greyes;Issue is addressed here: https://github.com/apache/incubator-usergrid/pull/191",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix AssetResourceIT,USERGRID-334,12766572,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,greyes,greyes,09/Jan/15 23:31,11/Aug/15 21:42,29/Oct/20 16:32,13/Jul/15 15:46,,,,,,,,,2.1.0,,,,,,,,,,,0,Fixed?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-603,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 31 17:50:07 UTC 2015,,,,,,,"0|i002rj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"31/Jul/15 17:50;greyes;Was fixed and updated recently",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ApplicationResourceIT,USERGRID-333,12766571,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,djohnson,greyes,greyes,09/Jan/15 23:31,16/Mar/15 16:24,29/Oct/20 16:32,26/Feb/15 17:50,,,,,,,,,OLD,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-02-26 17:50:24.209,,,false,USERGRID-300,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 02 14:38:39 UTC 2015,,,,,,,"0|i009o7:",9223372036854775807,,,,,,,,,,,Usergrid 5,Usergrid 6,Usergrid 7,Usergrid 8,,,,,,,,3.0,,,,,,,,,,,"26/Feb/15 17:50;djohnson;Fixed and merged into two-dot-o.","26/Feb/15 17:50;djohnson;Merged into two-dot-o and therefore closed.","02/Mar/15 14:38;djohnson;Apparently I was wrong, this one has not been merged into two-dot-o.",,,,,,,,,,,,,,,,,,,,,,,,,
Fix ApplicationRequestCounterIT,USERGRID-332,12766570,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,greyes,greyes,greyes,09/Jan/15 23:31,11/Aug/15 21:42,29/Oct/20 16:32,15/Jul/15 19:48,,,,,,,,,2.1.0,,,,,,,,,,,0,Fixed?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,USERGRID-343,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-603,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-01-09 23:31:07.0,,,,,,,"0|i002rz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Usergrid Launcher throws exception if ""initialize database"" unchecked",USERGRID-316,12765227,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,djohnson,djohnson,06/Jan/15 21:51,02/Nov/15 17:41,29/Oct/20 16:32,,1.0.1,,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"If you run the Launcher and you do not check the ""Initialize Database On Start"" button, then the first time you start the server the Launcher will exit with an error message like this:

2015-01-06 16:03:51,476 ERROR (pool-2-thread-1) [org.apache.cassandra.config.DatabaseDescriptor] - Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Cannot locate file:tmp/cassandra.yaml
	at org.apache.cassandra.config.DatabaseDescriptor.getStorageConfigURL(DatabaseDescriptor.java:123)
	at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:140)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:132)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:216)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.usergrid.launcher.EmbeddedServerHelper$CassandraRunner.run(EmbeddedServerHelper.java:190)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-02-06 20:22:21.017,,,false,USERGRID-308,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 06 20:22:21 UTC 2015,,,,,,,"0|i240bb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"06/Feb/15 20:22;lewismc;[~djohnson] is this marked for wrong version?",,,,,,,,,,,,,,,,,,,,,,,,,,,
Build Application and Collection Delete Async Distributed Workflow,USERGRID-286,12761477,Bug,In Progress,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,mrusso,rockerston,rockerston,12/Dec/14 22:40,29/Jul/16 20:37,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,Akka and the Usergrid ActorSystem module is now used in Usergrid from 2.1.1 onwards.  We need to develop an asynch distributed workflow leveraging this ActorSystem for things like data cleanup after app/collection deletion.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,USERGRID-292,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-820,,,,,,,,,,,,,,,,,9223372036854775807,,,2014-12-12 22:40:01.0,,,,,,,"0|hzzzre:dli4",9223372036854775807,,,,,,,,,,,Usergrid 34,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit not honored on subsequent requests with cursors,USERGRID-263,12759470,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Critical,,,tnine,tnine,04/Dec/14 17:17,02/Feb/16 21:29,29/Oct/20 16:32,,2.1.0,,,,,,,,,,,,,,Stack,,,,,1,,,,,,,,,"The following scenario worked in 1.0. We've broken this in 2.0.

Steps to reproduce.

1) Perform a query that returns a cursor, a limit may or may not be supplied.

curl -X GET ""http://localhost:8080/usergrid/test1/users?limit=20""

2) Perform the next page query and change the limit

curl -X GET ""http://localhost:8080/usergrid/test1/users?limit=40&cursor=cXVlcnlBbmRGZXRjaDsxOzk4NzA6UDFXa08zSVRTdGFXNncwRmZ2VkZudzswOw=="" 

*What should happen*

A response with  40 entities, assuming there are 40 to return.

*What actually happens*

A response with 20 entities, which is from the original query, but not what was requested.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-817,,,,,,,,,,,,,,,,,9223372036854775807,,,2014-12-04 17:17:39.0,,,,,,,"0|i001pr:000000010co8zzv",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ES cursor errors should be translated for our users,USERGRID-262,12759325,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,Fixed,,tnine,tnine,04/Dec/14 00:56,03/Jun/16 15:04,29/Oct/20 16:32,03/Jun/16 15:04,,,,,,,,,2.1.0,,,,,,Stack,,,,,0,,,,,,,,,"When advancing beyond a cursor's end, or passing an invalid cursor, the following error is returned to the user.

{code}
body=
{""error"":""elasticsearch_illegal_argument"",""timestamp"":1417654353286,""duration"":0,""error_description"":""Failed to decode scrollId"",""exception"":""org.elasticsearch.ElasticsearchIllegalArgumentException""}

{code}

We should catch this error, and return a friendlier error message to our users.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-03 15:04:13.2,,,false,USERGRID-817,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 03 15:04:13 UTC 2016,,,,,,,"0|i231ev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"03/Jun/16 15:04;mrusso;This is no longer an issue in 2.1.0 as scrolls in Elasticsearch are not being used for paging. ",,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't post on connection,USERGRID-249,12755473,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,rockerston,rockerston,14/Nov/14 20:23,05/Jan/16 19:21,29/Oct/20 16:32,,,,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"After setting up my roles as prescribed, I receive this error in the terminal when trying
to create a connection:

WARNING: Exception occurred during body skip
java.lang.IllegalStateException: Can not skip more bytes than available
	at org.glassfish.grizzly.http.server.io.InputBuffer.skip(InputBuffer.java:600)
	…

I tried issuing this command from the shell in the portal:

post /users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynotes { ""data"": ""Learn Usergrid” }

and

post /users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynotes [{ ""data"": ""Learn Usergrid""}]

Either command returns:

/users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynotes
 
{
  ""action"": ""post"",
  ""application"": ""f2b952fa-22ee-11e4-9b4b-e9ea3d610fab"",
  ""params"": {
    ""access_token"": [
      ""YWMtmZWU0iQUEeScLt1PgUhfegAAAUf7J0uW32RkTiYpwSNVOHBVAtmkMnjFT3s""
    ]
  },
  ""path"": ""/users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynotes"",
  ""uri"": ""http://localhost:8080/test.2/note-pad/users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynotes"",
  ""entities"": [],
  ""timestamp"": 1408063432982,
  ""duration"": 6,
  ""organization"": ""test.2"",
  ""applicationName"": “note-pad""
}

However, no entities are being created. I’ve also tried using curl with the same results:

curl -H ""Authorization: Bearer YWMt2j3JaCQLEeSr1fvx65wFRAAAAUf67ffU8cqvWOyiAVXXIOea177UF05Noa8""
-X POST -d '[ {""data"":""Lear Usergrid""}]' http://localhost:8080/test.2/note-pad/users/me/mynotes

I did execute a post /users/2760f03a-22ef-11e4-8ffd-f3f0018b3726/mynotes by itself as well,
which made no difference.

-charles
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-09-22 21:16:26.275,,,false,USERGRID-356,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 21:16:26 UTC 2015,,,,,,,"0|i001pr:000000010cor",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"22/Sep/15 21:16;jeffreyawest;This needs to be tested and confirmed with 1.0 and 2.1.  Separate tickets should be created for each.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Index Queue should not depend solely on AWS SQS ,USERGRID-241,12748031,Bug,Resolved,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,Fixed,mrusso,djohnson,djohnson,14/Oct/14 13:31,06/Mar/19 04:04,29/Oct/20 16:32,29/Jul/16 20:29,,,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"In the two-dot-o branch the faulty queue implementation has been replaced by a set of Queue interfaces and an AWS SQS implementation of those interfaces. 

Usergrid should not depend on a commercial service and the Usergrid Launcher should be able to run standalone and without external services.

So, what we need is a Queue implementation that we can embed in Usergrid for running JUnit tests that depend on the queue, for running the Usergrid Launcher and (ideally) one that can also be run remotely.

Apache Qpid seems to be a good candidate for our default queue implementation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-06-08 16:56:13.129,,,false,USERGRID-820,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 06 04:04:42 UTC 2019,,,,,,,"0|i002vz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"08/Jun/15 16:56;jeffreyawest;Understood.  We have an in-memory queue option at this time and will pursue QPID in the future.","29/Jul/16 20:29;mrusso;In-Memory queue is available and Usergrid is not dependent on AWS.  Other options will be available in the future for a Usergrid built distributed queue.","06/Mar/19 04:04;xingh;Potential queue implementation that uses cassandra as a backend to the queue. 

[https://github.com/paradoxical-io/cassieq]

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,
two-dot-o: Error in Portal after entity create,USERGRID-237,12745796,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Won't Fix,,djohnson,djohnson,03/Oct/14 18:43,01/Sep/15 18:19,29/Oct/20 16:32,01/Sep/15 18:19,,,,,,,,,,,,,,,Portal,,,,,0,,,,,,,,,"After an entity is created, the Portal attempts to load the entity but gets a 500 error from the server because it forms a bad URL with two slashes after the application is specified.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/14 18:44;djohnson;Usergrid_Admin_Portal.png;https://issues.apache.org/jira/secure/attachment/12672813/Usergrid_Admin_Portal.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,2015-09-01 18:19:18.185,,,false,USERGRID-353,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 01 18:19:18 UTC 2015,,,,,,,"0|i20s4v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"01/Sep/15 18:19;jeffreyawest;Not supporting 2.0",,,,,,,,,,,,,,,,,,,,,,,,,,,
Portal looks for helpJson.json in the wrong place,USERGRID-236,12745785,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,djohnson,djohnson,03/Oct/14 18:04,05/Jan/16 19:21,29/Oct/20 16:32,,,,,,,,,,,,,,,,Portal,,,,,0,,,,,,,,,"If you deploy the portal to a path other than ""/"" then you will get a 404 or possible a 500 error when the portal loads pages because it will not be able to load helpJson.json.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/14 18:04;djohnson;Usergrid_Admin_Portal.png;https://issues.apache.org/jira/secure/attachment/12672799/Usergrid_Admin_Portal.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-353,,,,,,,,,,,,,,,,,9223372036854775807,,,2014-10-03 18:04:07.0,,,,,,,"0|i001pr:000000010cozz",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent missing resource status code,USERGRID-235,12745779,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,,scottganyo,scottganyo,03/Oct/14 17:50,02/Feb/16 21:49,29/Oct/20 16:32,22/Sep/15 19:25,,,,,,,,,2.1.0,,,,,,Stack,,,,,0,,,,,,,,,"Assuming valid credentials, I get a different status code depending on the authorization method.

Bearer token:

curl -X GET -i -H ""Accept: application/json"" -H ""Authorization: Bearer XXX"" 'http://localhost:8080/test-organization/test-app/dogs/fido'

404 error: Service resource not found


Client id & Secret:

curl -X GET -i -H ""Accept: application/json"" 'http://localhost:8080/test-organization/test-app/dogs/fido?client_id=XXX&client_secret=XXX'

HTTP/1.1 401 Unauthorized

Note: In both cases, the client receives the same body content:

{""error"":""service_resource_not_found"",""timestamp"":1412358355742,""duration"":0,""exception"":""org.apache.usergrid.services.exceptions.ServiceResourceNotFoundException"",""error_description"":""Service resource not found""}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-356,,,,,,,,,,,,,,,,,9223372036854775807,,,2014-10-03 17:50:50.0,,,,,,,"0|i20s1b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default collections are not created by database setup,USERGRID-231,12742155,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,djohnson,djohnson,17/Sep/14 14:26,02/Feb/16 21:29,29/Oct/20 16:32,,2.1.0,,,,,,,,,,,,,,Stack,,,,,0,two-dot-o,,,,,,,,"Steps to reproduce:

1) setup a new Usergrid system
2) HTTP get the URL /system/database/setup
3) HTTP get the URL /system/superuser/setup
4) Login to the Portal
5) See that there is only a ""roles"" collection

But there should be these collections:
    /activities
    /assets
    /devices
    /folders
    /groups 
    /roles
    /tokens
    /users
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-356,,,,,,,,,,,,,,,,,9223372036854775807,,,2014-09-17 14:26:30.0,,,,,,,"0|i001x3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unable to run usergrid while off line from internet,USERGRID-230,12742025,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,apps4u,apps4u,17/Sep/14 02:28,11/May/16 06:59,29/Oct/20 16:32,,1.0.0,,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"When trying to run the launcher and your not connected to the internet there is a spring bean error its unable to load the spring xsd e.g.:

http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.1.xsd 

that is on line 22 in usergrid-standalone-context.xml in the launcher module.

Ive tried to run from source builds or from a-127 command and Its the same issue.","os x 9.2 , osx 10.10 , Ubuntu 12.04 , JDK 1.7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-826,,,,,,,,,,,,,,,,,9223372036854775807,,,2014-09-17 02:28:38.0,,,,,,,"0|i001pr:000000010cozy",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible bug in usergrid-lib.min.js TypeError: c in null,USERGRID-215,12734126,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,lewismc,lewismc,14/Aug/14 18:32,11/May/16 06:59,29/Oct/20 16:32,,1.0.0,,,,,,,,,,,,,,Portal,,,,,0,,,,,,,,,"Using UG (master) when I turn on firebug in firefox and log in to the UG portal, I see the following console error

usergrid-lib.min.js (line 27, col 92)

...==this&&c.$$nextSibling))for(;c!==this&&!(d=c.$$nextSibling);)c=c.$parent}while(...

First things first, can anyone else reproduce this? I have no idea about why this is as I have not looked into the JavaScript yet.

	
","Macosx v 10.9.4
Firefox 31.0
Usergrid (master)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-834,,,,,,,,,,,,,,,,,412153,,,2014-08-14 18:32:11.0,,,,,,,"0|i1ywjr:",412142,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix ""start paging"" in the two-dot-o branch",USERGRID-211,12733804,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Won't Fix,djohnson,djohnson,djohnson,13/Aug/14 15:44,01/Sep/15 18:19,29/Oct/20 16:32,01/Sep/15 18:13,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,"This test fails because (apparently) startPaging is only be supported for queues and not for generic collections as this test assumes:

  PagingResourceIT.startPaging:127 expected:<[9]> but was:<[0]>

The test above has been marked as Ignore.

To fix this bug, re-enable the above test and then fix whatever has broken startPaging in generic collections. 

Also, consider this: do we really need start paging?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-09-01 18:13:07.839,,,false,USERGRID-356,,,,,,,,,,,,,,,,,411832,,,Tue Sep 01 18:13:07 UTC 2015,,,,,,,"0|i1yum7:",411823,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"01/Sep/15 18:13;jeffreyawest;Not supporting 2.0",,,,,,,,,,,,,,,,,,,,,,,,,,,
Java documentations are misleading the user(developer).,USERGRID-179,12721389,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,krishanthibs,krishanthibs,16/Jun/14 11:15,02/Feb/16 21:29,29/Oct/20 16:32,,2.1.0,,,,,,,,,,,,,,SDK-Android,,,,,0,,,,,,,,,"Java classes in Usergrid SDKs still referring apigee SDKs classes.

ex: Through the method explanation of logOutAppUserAsync() method in Client.java of Usergrid android SDK, referring to a class called ""DataClient"". There is not a class called ""DataClient"" in Usergrid android SDK, while apigee SDK has.

Therefore it is difficult to figure out by going through the code base.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-829,,,,,,,,,,,,,,,,,399585,,,2014-06-16 11:15:19.0,,,,,,,"0|i1wsdj:",399694,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lack of Documentation on available features and REST APIs,USERGRID-178,12721374,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,krishanthibs,krishanthibs,16/Jun/14 10:03,02/Feb/16 21:29,29/Oct/20 16:32,,2.1.0,,,,,,,,,,,,,,Docs,SDK-Android,SDK-iOS,Stack,,0,,,,,,,,,"There is not a proper documentation for available features and REST APIs. It is bit time consuming to do things with Apache Usergrid without documentation.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-829,,,,,,,,,,,,,,,,,399570,,,2014-06-16 10:03:49.0,,,,,,,"0|i1wsa7:",399679,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some required fields should not be required on the users page of the Admin Portal,USERGRID-161,12717531,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,rockerston,rockerston,30/May/14 13:53,02/Nov/15 17:41,29/Oct/20 16:32,,,,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"On the users page of the Admin portal, there are a several fields that are marked as required but should not be.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-353,,,,,,,,,,,,,,,,,395735,,,2014-05-30 13:53:21.0,,,,,,,"0|i1w4rz:",395854,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
select (filter) queries with dot notation don't return results,USERGRID-160,12717457,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,lynchlee,rockerston,rockerston,30/May/14 02:09,02/Nov/15 17:40,29/Oct/20 16:32,,,,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"Steps to reproduce:

1. Create a collection and populate it with an entity with the following data:
{ ""foo.bar"": ""baz"" }

2. Run a GET query on this collection with a filter:

https://api.usergrid.com/org/app/collection/?ql=select foo.bar

3. No results are returned because the ""select"" filter isn't working with dot-notated key names.

Attached is a real-world use-case of where this is failing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-09-22 21:11:07.479,,,false,USERGRID-817,,,,,,,,,,,,,,,,,395661,,,Tue Sep 22 21:11:07 UTC 2015,,,,,,,"0|i001wf:",386341,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"22/Sep/15 21:11;jeffreyawest;This needs to be tested and confirmed with 2.1 or closed.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot change custom entity name,USERGRID-82,12695295,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,,stliu,scottganyo,scottganyo,14/Feb/14 21:28,05/Jan/16 19:21,29/Oct/20 16:32,,,,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"If you have a custom entity and attempt to update the entity's name (via PUT), the system will accept the request and return success (200), but the name will remain unchanged.

eg. using ugc:
ugc create foo ""name: 'bar'""
ugc update foo/bar ""name: 'baz'""

(Note: Referencing the entity in the PUT via UUID has the same effect.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-04-15 15:04:47.111,,,false,USERGRID-356,,,,,,,,,,,,,,,,,373803,,,Tue Nov 03 22:50:05 UTC 2015,,,,,,,"0|i001pr:000000010cof",374103,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"15/Apr/14 15:04;tnine;We either need to handle this 1 of 2 ways.

1) We throw an exception.  Name is currently immutable because it is an entity's alias.


2) We allow changing the name, which could potentially break any existing URLs to this entity","13/Aug/14 18:34;githubbot;GitHub user stliu opened a pull request:

    https://github.com/apache/incubator-usergrid/pull/46

    USERGRID-82 Cannot change custom entity name

    also, update log4j.xml to avoid duplicate log message
    update spring configuration to configure static attribute

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/stliu/incubator-usergrid USERGRID-82

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-usergrid/pull/46.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #46
    
----
commit 19652732248af2855141371a83452bbd1515bd7a
Author: Strong Liu <stliu@hibernate.org>
Date:   2014-08-13T18:26:54Z

    USERGRID-82 Cannot change custom entity name
    also, update log4j.xml to avoid duplicate log message
    update spring configuration to configure static attribute

----
","13/Aug/14 18:35;stliu;PR created https://github.com/apache/incubator-usergrid/pull/46

please someone take a look","21/Aug/14 05:11;githubbot;Github user GERey commented on the pull request:

    https://github.com/apache/incubator-usergrid/pull/46#issuecomment-52879209
  
    Hi @stliu , this currently PR currently fails ServiceInvocationIT.testServices under the following error 
    
    
    java.lang.IllegalArgumentException: Cannot modify immutable property[foo] of entity user
    	at org.apache.usergrid.services.AbstractService.assertNoImmutableProperty(AbstractService.java:476)
    	at org.apache.usergrid.services.AbstractService.updateEntity(AbstractService.java:458)
    	at org.apache.usergrid.services.AbstractService.updateEntity(AbstractService.java:440)
    	at org.apache.usergrid.services.AbstractCollectionService.putItemById(AbstractCollectionService.java:276)
    	at org.apache.usergrid.services.AbstractService.invokeItemWithId(AbstractService.java:664)
    	at org.apache.usergrid.services.AbstractService.invoke(AbstractService.java:641)
    	at org.apache.usergrid.services.AbstractService.invoke(AbstractService.java:554)
    	at org.apache.usergrid.services.ServiceRequest.execute(ServiceRequest.java:226)
    	at org.apache.usergrid.services.ServiceRequest.execute(ServiceRequest.java:193)
    	at org.apache.usergrid.ServiceApplication.invokeService(ServiceApplication.java:97)
    	at org.apache.usergrid.ServiceApplication.testRequest(ServiceApplication.java:79)
    	at org.apache.usergrid.ServiceApplication.testRequest(ServiceApplication.java:73)
    	at org.apache.usergrid.services.ServiceInvocationIT.testServices(ServiceInvocationIT.java:58)
    
    Could you take a look at it?
","21/Aug/14 09:46;githubbot;Github user stliu commented on the pull request:

    https://github.com/apache/incubator-usergrid/pull/46#issuecomment-52899309
  
    fixed
","21/Aug/14 19:58;githubbot;Github user GERey commented on the pull request:

    https://github.com/apache/incubator-usergrid/pull/46#issuecomment-52973625
  
    Thanks, that fixed that problem. While running the tests I also noticed that UserResourceIT.nameUpdate was also failing. This is the error that I'm seeing
    
    com.sun.jersey.api.client.UniformInterfaceException: PUT http://localhost:10005/test-organization/test-app/users/usernameb2dd4590-296c-11e4-a107-41adc6397962?access_token=YWMtsrMBXilsEeS_ZX1IJiWvkQAAAUgeLUNUJz3s0d1RAscfIxGhp2HVVn5B4hs returned a response status of 400 Bad Request
    	at com.sun.jersey.api.client.WebResource.handle(WebResource.java:688)
    	at com.sun.jersey.api.client.WebResource.access$200(WebResource.java:74)
    	at com.sun.jersey.api.client.WebResource$Builder.put(WebResource.java:539)
    	at org.apache.usergrid.rest.applications.users.UserResourceIT.nameUpdate(UserResourceIT.java:809)
    
    I'm pretty sure this is the last failure that needs to be fixed.
","22/Aug/14 05:07;stliu;ah, running into a problem that I think we need to make a decision:

what I did is trying to find out if there is a immutable property in the PUT request payload, and if there is, then throws IllegalArgumentException.

but, what if the immutable property value is the same as the one stored?

for example, get a User entity by id, then update the user entity's 'nickname' property, then send a PUT request with the whole User entity (see org.apache.usergrid.rest.applications.users.UserResourceIT#nameUpdate).

what should this be response?
clearly, it is not trying to actually *UPDATE* the immutable property

I can see there are two ways to resolve this, but neither of them are perfect IMO

1. for every PUT request with immutable property, we load the entity from db, then compare each immutable property to see if the request is trying to *UPDATE* the value, if it is , then throw exception, but this require we load lots of things when considering batch update

2. just remove the immutable property from request payload and continue, this is easier, but if the request does intent update an immutable property(wrongly), the request would still success

any idea?



","07/Nov/14 19:39;githubbot;Github user rodsimpson commented on the pull request:

    https://github.com/apache/incubator-usergrid/pull/46#issuecomment-62200208
  
    Does this still need to be fixed?
","11/Dec/14 01:52;githubbot;Github user rodsimpson commented on the pull request:

    https://github.com/apache/incubator-usergrid/pull/46#issuecomment-66557618
  
    @stliu is this PR still valid?
","12/Aug/15 22:40;githubbot;Github user GERey commented on the pull request:

    https://github.com/apache/incubator-usergrid/pull/46#issuecomment-130468617
  
    @stliu If you could address the conflicts I'd be happy to take another look at it! Thanks!
","03/Nov/15 22:48;githubbot;Github user mdunker commented on the pull request:

    https://github.com/apache/usergrid/pull/46#issuecomment-153513533
  
    master is now the 2.1+ branch -- this PR therefore is no longer valid. Please submit against 1.x branch if this is still an issue. Thanks!
","03/Nov/15 22:50;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/usergrid/pull/46
",,,,,,,,,,,,,,,,
Application end-point returns incorrect counts,USERGRID-58,12692890,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Duplicate,malaka,djohnson,djohnson,03/Feb/14 16:03,22/Sep/15 21:09,29/Oct/20 16:32,22/Sep/15 21:08,,,,,,,,,,,,,,,Stack,,,,,0,REST,,,,,,,,"Here's an example of the obviously bad counts:

        ""dogs"" : {
          ""title"" : ""Dogs"",
          ""count"" : -248,
          ""name"" : ""dogs"",
          ""type"" : ""dog""
        },
        ""medications"" : {
          ""title"" : ""Medications"",
          ""count"" : 3,
          ""name"" : ""medications"",
          ""type"" : ""medication""
        },
        ""somethings"" : {
          ""title"" : ""Somethings"",
          ""count"" : -12,
          ""name"" : ""somethings"",
          ""type"" : ""something""
        },

I suspect ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/14 18:12;malaka;count.png;https://issues.apache.org/jira/secure/attachment/12646062/count.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,2014-05-21 18:12:12.993,,,false,USERGRID-836,,,,,,,,,,,,,,,,,371476,,,Wed May 21 18:12:12 UTC 2014,,,,,,,"0|i001vr:",371779,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"21/May/14 18:12;malaka;Hi [~djohnson], 

I was unable to recreate this in my local environment with the latest code.
Are there any specific steps that cause the counters to break ? 

Also I noticed that the 'count' is within the entity data itself, I think this 'count' can be any value the user wants it to be (inside the entity of the 'entities' array returned). However the  'count' in the same level as the 'entities' should be the correct count (please see the screenshot attached). 

Please let me know if that is correct.",,,,,,,,,,,,,,,,,,,,,,,,,,,
ActivityStreams should be indexed,USERGRID-40,12691713,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,,,rockerston,rockerston,28/Jan/14 19:41,05/Jan/16 19:21,29/Oct/20 16:32,,,,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"""top-level properties like category, content, title should be indexed by default, so you can do 

/activities?ql=category = 'Report' 


Objects such as actor, object, provider, generator & target should be deep-indexed so you can do queries like: 

/activities?ql=object.uuid = 'ca16c9a1-ab5b-11e1-8b99-1231381c404f' 

/activities?ql=author.name = 'Tim' 

etc.""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-356,,,,,,,,,,,,,,,,,370458,,,2014-01-28 19:41:34.0,,,,,,,"0|i001pr:000000010cohzzzzzzzzzy",359395,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot delete an entity that has a connection to it,USERGRID-35,12691702,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Critical,Cannot Reproduce,greyes,rockerston,rockerston,28/Jan/14 19:08,24/Feb/17 22:56,29/Oct/20 16:32,08/Jan/16 00:32,,,,,,,,,2.2.0,,,,,,Stack,,,,,0,,,,,,,,,"""To repro, attempt to delete an entity that has a connection to it such as: curl -X DELETE """"https://api.com/fdsafdsa/testapp/dog/Dachsund"""" 

Notice that you get error message: 
{""""error"""":""""class_cast"""",""""timestamp"""":1386200483978,""""duration"""":0,""""exception"""":""""java.lang.ClassCastException"""",""""error_description"""":""""org.usergrid.persistence.cassandra.ConnectedEntityRefImpl cannot be cast to org.usergrid.persistence.cassandra.ConnectionRefImpl""""} 

It is expected that the entity is deleted and the connection is deleted as well.""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-08-13 18:38:54.533,,,false,USERGRID-356,,,,,,,,,,,,,,,,,370447,,,Fri Jan 08 00:31:46 UTC 2016,,,,,,,"0|i2o78u:y",365845,,,,,,,,,,,Usergrid 33,,,,,,,,,,,3.0,,,,,,,,,,,"13/Aug/14 18:38;stliu;I saw same issue before but can't remember how do i workarounded it :(
let me check once i have some time.","13/Aug/14 19:16;tnine;Is this still broken?  I thought we fixed this a couple of months ago.","01/Sep/15 18:11;jeffreyawest;[~stliu] dropping priority until this is confirmed.","08/Jan/16 00:31;greyes;Hi All, Just confirmed that this should work in 1.0,2.0 and 2.x if this does reappear again, then please re-open the ticket and I can look into it. Thanks!",,,,,,,,,,,,,,,,,,,,,,,,
"/collection/id/connected/* paths should either return an entity, an empty set or a 404",USERGRID-30,12691539,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,rockerston,rockerston,28/Jan/14 01:11,02/Nov/15 17:40,29/Oct/20 16:32,,,,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"""Not clear what that path is supposed to do but itÕs accepted, and always returns an empty set of results or an error 

Examples: 

curl http://api.usergrid.com/fdsafdsa/sandbox/users/tim/connected/followers 
=> returns empty set despite follower relationships existing 
curl http://api.usergrid.com/fdsafdsa/sandbox/users/tim/connected/users 
=> returns empty set despite users being connected 
curl http://api.usergrid.com/fdsafdsa/sandbox/users/tim/connected/d2740256-e312-11e1-ad17-12313d2b9232 
=> returns an error despite an entity with that UUID being connected 

1. Either we should accept a name or uuid at the end, and return that entity if it exists in the ÒconnectedÓ entities, or a 404 if it does not exist in the list of connected entities 
2. Or we should accept a relationship identifier there and return all connected entities through that identifier, or an empty set if there are none 
3. Or we should accept a collection name / type there and return all connected entities of that type, or an empty set if there are none 
4. Or you should tell me what is supposed to be provided there so we can put that in our docs and SDKs :p""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-356,,,,,,,,,,,,,,,,,370284,,,2014-01-28 01:11:46.0,,,,,,,"0|i001pr:0000i",370585,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We don't seem to be parsing '++' correctly,USERGRID-29,12691537,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,Fixed,mrusso,rockerston,rockerston,28/Jan/14 00:57,24/Mar/17 06:20,29/Oct/20 16:32,24/Mar/17 06:19,1.0.3,2.1.0,,,,,,,2.2.0,,,,,,Stack,,,,,0,,,,,,,,,"""I have made a simple app to test Usergrid's capabilities and I'm having trouble with updating entities that contain html entities. 

i.e. I add C++ as an entity in a collection called language and I also have C in the collection too. Whenever I try to update C++, which I grab via a collection query 

""""select * where owned='testuser' and lang='C++'"""" 

I get back the C entity. 

I have tried encoding the strings , but C%2B%2B 
gets double encoded to C%252B%252B 

This a typical call it makes without me trying to encode anything: 

https://api.usergrid.com/clydebyrdiii... 

Although it seems to encode correctly it returns the entity with lang='C' and not 
lang='C++'; 


And confirmed by Scott: 

Nope. If I create an entity with an attribute = 'c++', I can find it by querying for: 'c*' but not for 'c' or 'c+++'. 

""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-24 06:19:32.54,,,false,USERGRID-817,,,,,,,,,,,,,,,,,370282,,,Fri Mar 24 06:19:32 UTC 2017,,,,,,,"0|i001pr:000000010cohzzzzzzzx",370583,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"24/Mar/17 06:19;mrusso;https://github.com/apache/usergrid/commit/89178326debbd82660f9f2691b3b8cf06d847d4a",,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit on connection query not working,USERGRID-27,12691535,Bug,Closed,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Blocker,Fixed,mrusso,rockerston,rockerston,28/Jan/14 00:56,08/Feb/17 17:21,29/Oct/20 16:32,05/Jan/16 20:05,,,,,,,,,2.1.0,,,,,,Stack,,,,,0,,,,,,,,,"Also ""limit"" doesn't seems to be working for connections for e.g. https://api.usergrid.com/fdsafdsa/test/users/me/connecting/shares?limit=2&access_token=YWMtSQ92bBCsEeOYAQ1TRlcn_AAAAUDuPjgtlh06pENBKFRmoxta8jg1XwRuwVs. Returns all in our case is 3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-11-03 22:52:57.502,,,false,USERGRID-817,,,,,,,,,,,,,,,,,370280,,,Tue Nov 03 22:52:57 UTC 2015,,,,,,,"0|i001pr:000000010co3i",370581,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"03/Nov/15 22:52;mrusso;This is fixed in 2.1.  Here is pulling back all connections:

{code}
➜  ~  curl ""http://localhost:8080/test-organization/test-app/books/book1/connecting/has""
{
  ""action"" : ""get"",
  ""application"" : ""972033e9-8276-11e5-a427-36d323f2ba24"",
  ""params"" : { },
  ""path"" : ""/books/f424b588-827c-11e5-9896-36d323f2ba24/connecting"",
  ""uri"" : ""http://localhost:8080/test-organization/test-app/books/f424b588-827c-11e5-9896-36d323f2ba24/connecting"",
  ""entities"" : [ {
    ""uuid"" : ""bc6f5b41-827f-11e5-9896-36d323f2ba24"",
    ""type"" : ""library"",
    ""name"" : ""library4"",
    ""created"" : 1446592083854,
    ""modified"" : 1446592083854,
    ""metadata"" : {
      ""path"" : ""/books/f424b588-827c-11e5-9896-36d323f2ba24/connecting/bc6f5b41-827f-11e5-9896-36d323f2ba24"",
      ""size"" : 321,
      ""connections"" : {
        ""has"" : ""/books/f424b588-827c-11e5-9896-36d323f2ba24/connecting/bc6f5b41-827f-11e5-9896-36d323f2ba24/has""
      }
    }
  }, {
    ""uuid"" : ""baf00df8-827f-11e5-9896-36d323f2ba24"",
    ""type"" : ""library"",
    ""name"" : ""library3"",
    ""created"" : 1446592081342,
    ""modified"" : 1446592081342,
    ""metadata"" : {
      ""path"" : ""/books/f424b588-827c-11e5-9896-36d323f2ba24/connecting/baf00df8-827f-11e5-9896-36d323f2ba24"",
      ""size"" : 321,
      ""connections"" : {
        ""has"" : ""/books/f424b588-827c-11e5-9896-36d323f2ba24/connecting/baf00df8-827f-11e5-9896-36d323f2ba24/has""
      }
    }
  }, {
    ""uuid"" : ""b98bc3b5-827f-11e5-9896-36d323f2ba24"",
    ""type"" : ""library"",
    ""name"" : ""library2"",
    ""created"" : 1446592079007,
    ""modified"" : 1446592079007,
    ""metadata"" : {
      ""path"" : ""/books/f424b588-827c-11e5-9896-36d323f2ba24/connecting/b98bc3b5-827f-11e5-9896-36d323f2ba24"",
      ""size"" : 321,
      ""connections"" : {
        ""has"" : ""/books/f424b588-827c-11e5-9896-36d323f2ba24/connecting/b98bc3b5-827f-11e5-9896-36d323f2ba24/has""
      }
    }
  }, {
    ""uuid"" : ""eb88ba91-827c-11e5-9896-36d323f2ba24"",
    ""type"" : ""library"",
    ""name"" : ""library1"",
    ""created"" : 1446590874383,
    ""modified"" : 1446590874383,
    ""metadata"" : {
      ""path"" : ""/books/f424b588-827c-11e5-9896-36d323f2ba24/connecting/eb88ba91-827c-11e5-9896-36d323f2ba24"",
      ""size"" : 321,
      ""connections"" : {
        ""has"" : ""/books/f424b588-827c-11e5-9896-36d323f2ba24/connecting/eb88ba91-827c-11e5-9896-36d323f2ba24/has""
      }
    }
  } ],
  ""timestamp"" : 1446592170947,
  ""duration"" : 12,
  ""organization"" : ""test-organization"",
  ""applicationName"" : ""test-app""
}
{code}

Here is pulling back connections using a limit:

{code}
➜  ~  curl ""http://localhost:8080/test-organization/test-app/books/book1/connecting/has?limit=2""
{
  ""action"" : ""get"",
  ""application"" : ""972033e9-8276-11e5-a427-36d323f2ba24"",
  ""params"" : {
    ""limit"" : [ ""2"" ]
  },
  ""path"" : ""/books/f424b588-827c-11e5-9896-36d323f2ba24/connecting"",
  ""uri"" : ""http://localhost:8080/test-organization/test-app/books/f424b588-827c-11e5-9896-36d323f2ba24/connecting"",
  ""entities"" : [ {
    ""uuid"" : ""bc6f5b41-827f-11e5-9896-36d323f2ba24"",
    ""type"" : ""library"",
    ""name"" : ""library4"",
    ""created"" : 1446592083854,
    ""modified"" : 1446592083854,
    ""metadata"" : {
      ""path"" : ""/books/f424b588-827c-11e5-9896-36d323f2ba24/connecting/bc6f5b41-827f-11e5-9896-36d323f2ba24"",
      ""size"" : 321,
      ""connections"" : {
        ""has"" : ""/books/f424b588-827c-11e5-9896-36d323f2ba24/connecting/bc6f5b41-827f-11e5-9896-36d323f2ba24/has""
      }
    }
  }, {
    ""uuid"" : ""baf00df8-827f-11e5-9896-36d323f2ba24"",
    ""type"" : ""library"",
    ""name"" : ""library3"",
    ""created"" : 1446592081342,
    ""modified"" : 1446592081342,
    ""metadata"" : {
      ""path"" : ""/books/f424b588-827c-11e5-9896-36d323f2ba24/connecting/baf00df8-827f-11e5-9896-36d323f2ba24"",
      ""size"" : 321,
      ""connections"" : {
        ""has"" : ""/books/f424b588-827c-11e5-9896-36d323f2ba24/connecting/baf00df8-827f-11e5-9896-36d323f2ba24/has""
      }
    }
  } ],
  ""timestamp"" : 1446592112416,
  ""duration"" : 12,
  ""organization"" : ""test-organization"",
  ""applicationName"" : ""test-app""
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,
Offer an option to delete all inbound and/or outbound connections on entity delete,USERGRID-26,12691534,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,rockerston,rockerston,28/Jan/14 00:54,02/Nov/15 17:40,29/Oct/20 16:32,,,,,,,,,,,,,,,,Stack,,,,,1,,,,,,,,,"""Per Ed, all inbound connections should be cleaned up when an entity is deleted: 

For example, if you have entity rock and entity paper who both like entity scissors, both likes connections to scissors should be deleted if scissors is deleted. 


It would be great if there was an option to delete all inbound connections when an entity is deleted (the same flag should also be possible on deletes by queries).""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-809,,,,,,,,,,,,,,,,,370279,,,2014-01-28 00:54:53.0,,,,,,,"0|i001pr:0002",370580,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collection counters on App endpoint are completely off,USERGRID-24,12691530,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,,,rockerston,rockerston,28/Jan/14 00:48,05/Jan/16 19:21,29/Oct/20 16:32,,,,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"""The counts of items in each collection returned in a call to /org/app are completely off (usually by a factor of 10). This is possibly related to USERGRID-17. Can we do something to fix this?

Possible solution in two parts:
1. Investigate all code paths to ensure that counters are being invoked when needed

2. Create a job that can be invoked automatically or manually that will recount all entities in a collection
""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,USERGRID-58,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-05-21 18:15:07.54,,,false,USERGRID-836,,,,,,,,,,,,,,,,,370275,,,Tue Sep 22 21:10:25 UTC 2015,,,,,,,"0|i001pr:000000010cohzzzzzzzzzx",370576,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"21/May/14 18:15;malaka;Hi [~rockerston], 

Is this the same as USERGRID-58 ?","22/Sep/15 21:09;jeffreyawest;We should look at doing this with ElasticSearch in 2.1","22/Sep/15 21:10;jeffreyawest;We have a new endpoint for this, but for the App Page we should implement the GET to ES.",,,,,,,,,,,,,,,,,,,,,,,,,
Fresh Admin user token won't work on /management/users/me,USERGRID-23,12691529,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Minor,,,rockerston,rockerston,28/Jan/14 00:47,02/Nov/15 17:41,29/Oct/20 16:32,,,,,,,,,,,,,,,,Stack,,,,,0,,,,,,,,,"""After logging on:

curl -X POST """"https://api.usergrid.com/management/token"""" -d '{""""grant_type"""":""""password"""",""""username"""":""""fdsafdsa"""",""""password"""":""""fdsafdsafdsa""""}' 


The user is not able to log in with the token:

curl -X GET -i -H """"Authorization: Bearer YWMt997XKhAyEeO7oblzQPxkkAAAAUDrIyfHE8GIZlo8IhJztOX2JQ7kXpbFTHc"""" """"https://api.usergrid.com/management/usersfdsafdsa""""


But the token works with other endpoints.

***Update***: Looks like the problem is that the /management/users/<username or email> endpoint is not case-insensitive on the username or email address. We need to update that API call so that it is case-insensitive.




""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-354,,,,,,,,,,,,,,,,,370274,,,2014-01-28 00:47:24.0,,,,,,,"0|i1r0cv:",370575,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad geo query returns entire collection,USERGRID-19,12691524,Bug,Open,USERGRID,Usergrid,software,mrusso,"<h3><a name=""Usergrid""></a>Welcome to the Usergrid project</h3>
<p>Usergrid is a multi-tenant Backend-as-a-Service stack for web & mobile applications, based on RESTful APIs.</p>",http://usergrid.apache.org/,Major,,,rockerston,rockerston,28/Jan/14 00:43,11/Jan/16 20:45,29/Oct/20 16:32,,,,,,,,,,,,,,,,Stack,,,,,0,retest,,,,,,,,"""When a badly formed geo query is sent with a GET, the API returns the entire collection, when it should return nothing, or return some sort of query parse error. 

For example, this is missing the 'location' param of the query statement: 

within 16000 of 37.774989,-122.419413 

and returns the first 10 entities in the collection even though the query can't be parsed. 
""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,USERGRID-817,,,,,,,,,,,,,,,,,370269,,,2014-01-28 00:43:53.0,,,,,,,"0|i001pr:000000010cohzzzzzzzi",370570,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Random segmentation fault during training,MXNET-1108,13192302,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,Fixed,yuxihu,apeforest,apeforest,17/Oct/18 16:59,30/Oct/18 17:01,29/Oct/20 16:32,30/Oct/18 17:01,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-1105,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-10-17 16:59:09.0,,,,,,,"0|i3zbin:",9223372036854775807,,,,,,,,,,,Carl Friedrich	Gauss,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BUG in MultiBoxTargetForward when there is single box label,MXNET-1033,13190010,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,,apeforest,apeforest,apeforest,08/Oct/18 06:28,19/Oct/18 07:41,29/Oct/20 16:32,,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,"apeforest opened a new pull request #12840: [MXNET-1033] Fix a bug in MultiboxTarget GPU implementation
URL: https://github.com/apache/incubator-mxnet/pull/12840
 
 
   ## Description ##
   Fix issue https://github.com/apache/incubator-mxnet/issues/9733
   The contrib.MultiBoxTarget operator unnecessarily checks for number of labels in the GPU implementation. It is inconsistent with CPU implementation. 
   
   ## Checklist ##
   ### Essentials ###
   Please feel free to remove inapplicable items for your PR.
   - [X] The PR title starts with [MXNET-$JIRA_ID], where $JIRA_ID refers to the relevant [JIRA issue](https://issues.apache.org/jira/projects/MXNET/issues) created (except PRs with tiny changes)
   - [X] Changes are complete (i.e. I finished coding on this PR)
   - [X] All changes have test coverage:
   - Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
   - Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
   - Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
   - [ ] Code is well-documented: 
   - For user-facing API changes, API doc string has been updated. 
   - For new C++ functions in header files, their functionalities and arguments are documented. 
   - For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
   - Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
   - [ ] To the my best knowledge, examples are either not affected by this change, or have been fixed to be compatible with this change
   
   ### Changes ###
   Bugfix

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Oct/18 21:10;githubbot;600","apeforest commented on issue #12840: [MXNET-1033] Fix a bug in MultiboxTarget GPU implementation
URL: https://github.com/apache/incubator-mxnet/pull/12840#issuecomment-430402931
 
 
   @zhreshold Please help to review. Thanks

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Oct/18 21:13;githubbot;600","samskalicky commented on issue #12840: [MXNET-1033] Fix a bug in MultiboxTarget GPU implementation
URL: https://github.com/apache/incubator-mxnet/pull/12840#issuecomment-430404312
 
 
   @apeforest Can you provide more info on why this code was there originally? or why it wasnt there in the CPU implementation? What is the problem with the check and why do you have to remove it?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Oct/18 21:17;githubbot;600","zhreshold commented on issue #12840: [MXNET-1033] Fix a bug in MultiboxTarget GPU implementation
URL: https://github.com/apache/incubator-mxnet/pull/12840#issuecomment-430415374
 
 
   The check was there to ensure the buffer is large enough for cuda merge sort implementation. I double checked the buffer is actually from the temp space https://github.com/apache/incubator-mxnet/blob/9c78437d895e9035ef1f30e247ffd0d9b4231829/src/operator/contrib/multibox_target-inl.h#L119 that has at least (11 - 4) * batch size * num_anchor (since I am using temp_space[4] as the buffer), it is safe to remove the check IMO. 
   
   But I would like to see a test case covering this. @apeforest Could you help me verify that with a unittest?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Oct/18 21:56;githubbot;600","Roshrini commented on issue #12840: [MXNET-1033] Fix a bug in MultiboxTarget GPU implementation
URL: https://github.com/apache/incubator-mxnet/pull/12840#issuecomment-430829695
 
 
   @apeforest CI seems to have failed. Can you retrigger this build?
   @mxnet-label-bot [Operator, pr-awaiting-testing]

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Oct/18 23:59;githubbot;600","Roshrini commented on issue #12840: [MXNET-1033] Fix a bug in MultiboxTarget GPU implementation
URL: https://github.com/apache/incubator-mxnet/pull/12840#issuecomment-431167248
 
 
   @zhreshold Can you take a look again and merge if looks good? Thanks!

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Oct/18 21:18;githubbot;600","zhreshold closed pull request #12840: [MXNET-1033] Fix a bug in MultiboxTarget GPU implementation
URL: https://github.com/apache/incubator-mxnet/pull/12840
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/src/operator/contrib/multibox_target.cu b/src/operator/contrib/multibox_target.cu
index c70dce3c1d7..ca0428348a6 100644
--- a/src/operator/contrib/multibox_target.cu
+++ b/src/operator/contrib/multibox_target.cu
@@ -356,7 +356,6 @@ inline void MultiBoxTargetForward(const Tensor<gpu, 2, DType> &loc_target,
   const int num_anchors = anchors.size(0);
   const int num_classes = cls_preds.size(1);
   CHECK_GE(num_batches, 1);
-  CHECK_GT(num_labels, 2);
   CHECK_GE(num_anchors, 1);
   CHECK_EQ(variances.ndim(), 4);
 
diff --git a/tests/python/gpu/test_operator_gpu.py b/tests/python/gpu/test_operator_gpu.py
index dd7ec985c7c..02895cd920e 100644
--- a/tests/python/gpu/test_operator_gpu.py
+++ b/tests/python/gpu/test_operator_gpu.py
@@ -42,6 +42,7 @@
 from test_sparse_operator import *
 from test_ndarray import *
 from test_subgraph_op import *
+from test_contrib_operator import test_multibox_target_op
 
 set_default_context(mx.gpu(0))
 del test_support_vector_machine_l1_svm  # noqa
diff --git a/tests/python/unittest/test_contrib_operator.py b/tests/python/unittest/test_contrib_operator.py
index 76efe305bce..58728d8dad6 100644
--- a/tests/python/unittest/test_contrib_operator.py
+++ b/tests/python/unittest/test_contrib_operator.py
@@ -244,6 +244,22 @@ def assert_match(inputs, x, y, threshold, is_ascend=False):
     assert_match([[0.5, 0.6], [0.1, 0.2], [0.3, 0.4]], [1, -1, 0], [2, 0], 1e-12, False)
     assert_match([[0.5, 0.6], [0.1, 0.2], [0.3, 0.4]], [-1, 0, 1], [1, 2], 100, True)
 
+def test_multibox_target_op():
+    anchors = mx.nd.array([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]], ctx=default_context()).reshape((1, -1, 4))
+    cls_pred = mx.nd.array(list(range(10)), ctx=default_context()).reshape((1, -1, 2))
+    label = mx.nd.array([1, 0.1, 0.1, 0.5, 0.6], ctx=default_context()).reshape((1, -1, 5))
+
+    loc_target, loc_mask, cls_target = \
+        mx.nd.contrib.MultiBoxTarget(anchors, label, cls_pred,
+                                     overlap_threshold=0.5,
+                                     negative_mining_ratio=3,
+                                     negative_mining_thresh=0.4)
+    expected_loc_target = np.array([[5.0, 2.5000005, 3.4657357, 4.581454, 0., 0., 0., 0.]])
+    expected_loc_mask = np.array([[1, 1, 1, 1, 0, 0, 0, 0]])
+    expected_cls_target = np.array([[2, 0]])
+    assert_allclose(loc_target.asnumpy(), expected_loc_target, rtol=1e-5, atol=1e-5)
+    assert_array_equal(loc_mask.asnumpy(), expected_loc_mask)
+    assert_array_equal(cls_target.asnumpy(), expected_cls_target)
 
 if __name__ == '__main__':
     import nose


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Oct/18 06:17;githubbot;600","zhreshold commented on issue #12840: [MXNET-1033] Fix a bug in MultiboxTarget GPU implementation
URL: https://github.com/apache/incubator-mxnet/pull/12840#issuecomment-431256691
 
 
   merged, thank you @apeforest 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Oct/18 06:18;githubbot;600",,,,,,,,,,,,,,,,,,0,4800,,,0,4800,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-10-08 06:28:10.0,,,,,,,"0|i3wzrw:9i",9223372036854775807,,,,,,,,,,,Bernhard Riemann,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Investigate roi_align operator issue,MXNET-974,13186471,Bug,In Progress,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,,yuxihu,apeforest,apeforest,20/Sep/18 21:43,08/Oct/18 17:53,29/Oct/20 16:32,,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-09-20 21:43:18.0,,,,,,,"0|i3wzrw:a",9223372036854775807,,,,,,,,,,,Ada Lovelace,Bernhard Riemann,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix the bug for matrices of multiple dimension, with one dimension much larger ",MXNET-921,13184156,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,,apeforest,apeforest,apeforest,10/Sep/18 20:50,05/Oct/18 20:15,29/Oct/20 16:32,,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-09-10 20:50:07.0,,,,,,,"0|i3wzrw:f",9223372036854775807,,,,,,,,,,,Ada Lovelace,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InferShape return false not caught in Symbolic mode,MXNET-865,13181448,Bug,To Do,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,,apeforest,apeforest,apeforest,27/Aug/18 20:22,27/Aug/18 20:22,29/Oct/20 16:32,,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-08-27 20:22:13.0,,,,,,,"0|i3xh2n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
how to convert parameters dtype from float32 to float64 in gluon?,MXNET-816,13178643,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,,apeforest,apeforest,apeforest,13/Aug/18 16:55,14/Sep/18 22:12,29/Oct/20 16:32,,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-08-13 16:55:29.0,,,,,,,"0|i3wzru:",9223372036854775807,,,,,,,,,,,Backend (08/13-08/27),Backend (08/27-09/07),Backend (09/10-09/24),,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
different behaviour of customop in latest MXNet,MXNET-812,13178637,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,Cannot Reproduce,samskalicky,apeforest,apeforest,13/Aug/18 16:52,07/Sep/18 18:20,29/Oct/20 16:32,07/Sep/18 18:20,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-08-13 16:52:26.0,,,,,,,"0|i3wzsf:",9223372036854775807,,,,,,,,,,,Backend (08/27-09/07),,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_arange failure - Jetson TX2 (CPU),MXNET-811,13178636,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,,haojin2,apeforest,apeforest,13/Aug/18 16:51,21/Aug/18 17:12,29/Oct/20 16:32,,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-08-13 16:51:45.0,,,,,,,"0|i3wu5y:i",9223372036854775807,,,,,,,,,,,Backend (08/13-08/27),,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
src/operator/./bilinear_sampler-inl.h:105: Have not implemented the data req combinations! gdata_req=0 ggrid_req=1,MXNET-810,13178635,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,,haojin2,apeforest,apeforest,13/Aug/18 16:51,11/Sep/18 17:36,29/Oct/20 16:32,,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,"haojin2 opened a new pull request #12386: [MXNET-810] [WIP] Add support for more req patterns for bilinear sampler backward
URL: https://github.com/apache/incubator-mxnet/pull/12386
 
 
   ## Description ##
   Fix #8866.
   
   ## Checklist ##
   ### Essentials ###
   - [x] Changes are complete (i.e. I finished coding on this PR)
   - [x] All changes have test coverage:
   - Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
   - Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
   - Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
   - [x] Code is well-documented: 
   - For user-facing API changes, API doc string has been updated. 
   - For new C++ functions in header files, their functionalities and arguments are documented. 
   - For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
   - Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
   - [x] To the my best knowledge, examples are either not affected by this change, or have been fixed to be compatible with this change
   
   ### Changes ###
   - [x] Support more req type patterns for backward of bilinear sampler
   - [ ] Corresponding unit tests
   
   ## Comments ##
   Currently test_bilinear_sampler is still marked as flaky, cannot un-leash the unit test for that, so blocked at this moment.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Aug/18 23:02;githubbot;600","haojin2 commented on issue #12386: [MXNET-810] [WIP] Add support for more req patterns for bilinear sampler backward
URL: https://github.com/apache/incubator-mxnet/pull/12386#issuecomment-416768267
 
 
   @eric-haibin-lin @reminisce @piiswrong @anirudh2290 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Aug/18 23:02;githubbot;600","anirudh2290 commented on a change in pull request #12386: [MXNET-810] Add support for more req patterns for bilinear sampler backward
URL: https://github.com/apache/incubator-mxnet/pull/12386#discussion_r216106843
 
 

 ##########
 File path: tests/python/gpu/test_operator_gpu.py
 ##########
 @@ -1975,6 +1975,19 @@ def test_bilinear_sampler_versions():
         assert_almost_equal(exe_list[ref_idx].grad_dict['data'].asnumpy(), data_grad + data_initial_grad, rtol=1e-3, atol=1e-5)
         assert_almost_equal(exe_list[ref_idx].grad_dict['grid'].asnumpy(), grid_grad + grid_initial_grad, rtol=1e-3, atol=1e-5)
 
+        req_dict = {'data' : 'null', 'grid' : 'write'}
 
 Review comment:
   not testing req1 != Null ?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Sep/18 23:04;githubbot;600","eric-haibin-lin commented on a change in pull request #12386: [MXNET-810] Add support for more req patterns for bilinear sampler backward
URL: https://github.com/apache/incubator-mxnet/pull/12386#discussion_r216107182
 
 

 ##########
 File path: src/operator/mxnet_op.h
 ##########
 @@ -111,6 +111,33 @@ inline int get_num_threads<cpu>(const int N) {
   }
 
 
+/*! \brief operator request type switch */
+#define MXNET_REQ_TYPE_SWITCH(req, ReqType, ...)  \
 
 Review comment:
   Why is this added?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Sep/18 23:08;githubbot;600","haojin2 commented on a change in pull request #12386: [MXNET-810] Add support for more req patterns for bilinear sampler backward
URL: https://github.com/apache/incubator-mxnet/pull/12386#discussion_r216107245
 
 

 ##########
 File path: tests/python/gpu/test_operator_gpu.py
 ##########
 @@ -1975,6 +1975,19 @@ def test_bilinear_sampler_versions():
         assert_almost_equal(exe_list[ref_idx].grad_dict['data'].asnumpy(), data_grad + data_initial_grad, rtol=1e-3, atol=1e-5)
         assert_almost_equal(exe_list[ref_idx].grad_dict['grid'].asnumpy(), grid_grad + grid_initial_grad, rtol=1e-3, atol=1e-5)
 
+        req_dict = {'data' : 'null', 'grid' : 'write'}
 
 Review comment:
   Well the actual use case reported on the issues is this one, but maybe I should add the other one even if it's not a reported use case at all.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Sep/18 23:08;githubbot;600","haojin2 commented on a change in pull request #12386: [MXNET-810] Add support for more req patterns for bilinear sampler backward
URL: https://github.com/apache/incubator-mxnet/pull/12386#discussion_r216108611
 
 

 ##########
 File path: src/operator/mxnet_op.h
 ##########
 @@ -111,6 +111,33 @@ inline int get_num_threads<cpu>(const int N) {
   }
 
 
+/*! \brief operator request type switch */
+#define MXNET_REQ_TYPE_SWITCH(req, ReqType, ...)  \
 
 Review comment:
   because the one above will not generate any code when one of the input req type is kNullOp: https://github.com/apache/incubator-mxnet/pull/12386/files#diff-2e98465e383a92cb3c9a53e9f32658ceR92

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Sep/18 23:21;githubbot;600","haojin2 commented on a change in pull request #12386: [MXNET-810] Add support for more req patterns for bilinear sampler backward
URL: https://github.com/apache/incubator-mxnet/pull/12386#discussion_r216497807
 
 

 ##########
 File path: tests/python/gpu/test_operator_gpu.py
 ##########
 @@ -1975,6 +1975,19 @@ def test_bilinear_sampler_versions():
         assert_almost_equal(exe_list[ref_idx].grad_dict['data'].asnumpy(), data_grad + data_initial_grad, rtol=1e-3, atol=1e-5)
         assert_almost_equal(exe_list[ref_idx].grad_dict['grid'].asnumpy(), grid_grad + grid_initial_grad, rtol=1e-3, atol=1e-5)
 
+        req_dict = {'data' : 'null', 'grid' : 'write'}
 
 Review comment:
   Added extra test case as per your review, should be good for merge.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Sep/18 22:43;githubbot;600","haojin2 commented on issue #12386: [MXNET-810] Add support for more req patterns for bilinear sampler backward
URL: https://github.com/apache/incubator-mxnet/pull/12386#issuecomment-420138751
 
 
   @anirudh2290 @eric-haibin-lin Should be good for merge?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Sep/18 03:58;githubbot;600","anirudh2290 closed pull request #12386: [MXNET-810] Add support for more req patterns for bilinear sampler backward
URL: https://github.com/apache/incubator-mxnet/pull/12386
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/src/operator/bilinear_sampler-inl.h b/src/operator/bilinear_sampler-inl.h
index e0b4db7b367..499d2339620 100644
--- a/src/operator/bilinear_sampler-inl.h
+++ b/src/operator/bilinear_sampler-inl.h
@@ -95,19 +95,16 @@ class BilinearSamplerOp : public Operator {
     Tensor<xpu, 4, DType> gdata = in_grad[bs::kData].get<xpu, 4, DType>(s);
     Tensor<xpu, 4, DType> ggrid = in_grad[bs::kGrid].get<xpu, 4, DType>(s);
     Tensor<xpu, 4, DType> grad = out_grad[bs::kOut].get<xpu, 4, DType>(s);
-    if (req[bs::kData] != kNullOp && req[bs::kGrid] != kNullOp) {
+    if (req[bs::kData] == kNullOp && req[bs::kGrid] == kNullOp) {
+      return;
+    } else {
       if (req[bs::kData] == kWriteTo) {
         gdata = scalar<DType>(0.0f);
       }
       if (req[bs::kGrid] == kWriteTo) {
         ggrid = scalar<DType>(0.0f);
       }
-      BilinearSamplerBackward(gdata, ggrid, grad, data, grid);
-    } else if (req[bs::kData] == kNullOp && req[bs::kGrid] == kNullOp) {
-      return;
-    } else {
-      LOG(FATAL) << ""Have not implemented the data req combinations! gdata_req=""
-                 << req[bs::kData] << "" ggrid_req="" << req[bs::kGrid];
+      BilinearSamplerBackward(gdata, ggrid, grad, data, grid, req[bs::kData], req[bs::kGrid]);
     }
   }
 
diff --git a/src/operator/bilinear_sampler.cc b/src/operator/bilinear_sampler.cc
index 3365d98bb4d..a3b7d576424 100644
--- a/src/operator/bilinear_sampler.cc
+++ b/src/operator/bilinear_sampler.cc
@@ -78,10 +78,12 @@ inline void BilinearSamplerForward(const Tensor<cpu, 4, DType> &output,
 
 template<typename DType>
 inline void BilinearSamplerBackward(const Tensor<cpu, 4, DType> &gdata,
-                                     const Tensor<cpu, 4, DType> &ggrid,
-                                     const Tensor<cpu, 4, DType> &output_grad,
-                                     const Tensor<cpu, 4, DType> &input_data,
-                                     const Tensor<cpu, 4, DType> &grid) {
+                                    const Tensor<cpu, 4, DType> &ggrid,
+                                    const Tensor<cpu, 4, DType> &output_grad,
+                                    const Tensor<cpu, 4, DType> &input_data,
+                                    const Tensor<cpu, 4, DType> &grid,
+                                    const mxnet::OpReqType data_req,
+                                    const mxnet::OpReqType grid_req) {
   DType *g_input = gdata.dptr_;
   DType *grad_grid = ggrid.dptr_;
   const DType *grid_src = grid.dptr_;
@@ -104,8 +106,7 @@ inline void BilinearSamplerBackward(const Tensor<cpu, 4, DType> &gdata,
           DType top_left_x_w = 1.0 - (x_real - top_left_x);
           for (index_t c = 0; c < static_cast<index_t>(o_c); ++c) {
             index_t grad_index = n * o_c * o_h * o_w + c * o_h * o_w + h * o_w + w;
-            int data_index = n * i_c * i_h * i_w + c * i_h * i_w + top_left_y * i_w
-                                  + top_left_x;
+            int data_index = n * i_c * i_h * i_w + c * i_h * i_w + top_left_y * i_w + top_left_x;
             // calc 4 vertex value in input data
             DType top_left_v = 0;
             DType top_right_v = 0;
@@ -113,22 +114,30 @@ inline void BilinearSamplerBackward(const Tensor<cpu, 4, DType> &gdata,
             DType bottom_right_v = 0;
             // calc input grad
             if (between(top_left_x, 0, i_w-1) && between(top_left_y, 0, i_h-1)) {
-              *(g_input + data_index) += *(grad + grad_index) * top_left_y_w * top_left_x_w;
+              if (data_req != mxnet::kNullOp) {
+                *(g_input + data_index) += *(grad + grad_index) * top_left_y_w * top_left_x_w;
+              }
               top_left_v = *(data + data_index);
             }
             if (between(top_left_x+1, 0, i_w-1) && between(top_left_y, 0, i_h-1)) {
-              *(g_input + data_index + 1) += *(grad + grad_index) * top_left_y_w
-                                              * (1.0 - top_left_x_w);
+              if (data_req != mxnet::kNullOp) {
+                *(g_input + data_index + 1) +=
+                  *(grad + grad_index) * top_left_y_w * (1.0 - top_left_x_w);
+              }
               top_right_v = *(data + data_index + 1);
             }
             if (between(top_left_x, 0, i_w-1) && between(top_left_y+1, 0, i_h-1)) {
-              *(g_input + data_index+ i_w) += *(grad + grad_index) * (1.0 - top_left_y_w)
-                                              * top_left_x_w;
+              if (data_req != mxnet::kNullOp) {
+                *(g_input + data_index+ i_w) +=
+                  *(grad + grad_index) * (1.0 - top_left_y_w) * top_left_x_w;
+              }
               bottom_left_v = *(data + data_index + i_w);
             }
             if (between(top_left_x+1, 0, i_w-1) && between(top_left_y+1, 0, i_h-1)) {
-              *(g_input + data_index+ i_w + 1) += *(grad + grad_index) * (1.0 - top_left_y_w)
-                                                  * (1.0 - top_left_x_w);
+              if (data_req != mxnet::kNullOp) {
+                *(g_input + data_index+ i_w + 1) +=
+                  *(grad + grad_index) * (1.0 - top_left_y_w) * (1.0 - top_left_x_w);
+              }
               bottom_right_v = *(data + data_index + i_w + 1);
             }
             // calc weight grad of top_left_w, then multiple -1 is the grad of grid_src
@@ -139,9 +148,11 @@ inline void BilinearSamplerBackward(const Tensor<cpu, 4, DType> &gdata,
                               (top_left_v - top_right_v - bottom_left_v + bottom_right_v)
                               * top_left_y_w);
           }
-          // calc grad of grid
-          *(grad_grid + grid_src_index + o_h * o_w) += top_left_y_gw * (i_h - 1) / 2;
-          *(grad_grid + grid_src_index) += top_left_x_gw * (i_w - 1) / 2;
+          if (grid_req != mxnet::kNullOp) {
+            // calc grad of grid
+            *(grad_grid + grid_src_index + o_h * o_w) += top_left_y_gw * (i_h - 1) / 2;
+            *(grad_grid + grid_src_index) += top_left_x_gw * (i_w - 1) / 2;
+          }
         }
       }
     }
diff --git a/src/operator/bilinear_sampler.cu b/src/operator/bilinear_sampler.cu
index e1f205258a2..2e6be3e1ef3 100644
--- a/src/operator/bilinear_sampler.cu
+++ b/src/operator/bilinear_sampler.cu
@@ -79,7 +79,7 @@ __global__ void BilinearSamplerForwardKernel(const int i_c, const int i_h,
   }
 }
 
-template<typename DType>
+template<typename DType, int Req1, int Req2>
 __global__ void BilinearSamplerBackwardKernel(const int i_c, const int i_h,
                                               const int i_w, const DType* grad,
                                               const DType* data, const int o_n,
@@ -114,22 +114,30 @@ __global__ void BilinearSamplerBackwardKernel(const int i_c, const int i_h,
       DType bottom_right_v = 0;
       // calc input grad
       if (between(top_left_x, 0, i_w-1) && between(top_left_y, 0, i_h-1)) {
-        atomicAdd(&g_input[data_index], *(grad + grad_index) * top_left_y_w * top_left_x_w);
+        if (Req1 != mxnet::kNullOp) {
+          atomicAdd(&g_input[data_index], *(grad + grad_index) * top_left_y_w * top_left_x_w);
+        }
         top_left_v = *(data + data_index);
       }
       if (between(top_left_x+1, 0, i_w-1) && between(top_left_y, 0, i_h-1)) {
-        atomicAdd(&g_input[data_index + 1], *(grad + grad_index) * top_left_y_w
-                                        * (1.0 - top_left_x_w));
+        if (Req1 != mxnet::kNullOp) {
+          atomicAdd(&g_input[data_index + 1],
+                    *(grad + grad_index) * top_left_y_w * (1.0 - top_left_x_w));
+        }
         top_right_v = *(data + data_index + 1);
       }
       if (between(top_left_x, 0, i_w-1) && between(top_left_y+1, 0, i_h-1)) {
-        atomicAdd(&g_input[data_index+ i_w], *(grad + grad_index) * (1.0 - top_left_y_w)
-                                        * top_left_x_w);
+        if (Req1 != mxnet::kNullOp) {
+          atomicAdd(&g_input[data_index+ i_w],
+                    *(grad + grad_index) * (1.0 - top_left_y_w) * top_left_x_w);
+        }
         bottom_left_v = *(data + data_index + i_w);
       }
       if (between(top_left_x+1, 0, i_w-1) && between(top_left_y+1, 0, i_h-1)) {
-        atomicAdd(&g_input[data_index+ i_w + 1], *(grad + grad_index) * (1.0 - top_left_y_w)
-                                            * (1.0 - top_left_x_w));
+        if (Req1 != mxnet::kNullOp) {
+          atomicAdd(&g_input[data_index+ i_w + 1],
+                    *(grad + grad_index) * (1.0 - top_left_y_w) * (1.0 - top_left_x_w));
+        }
         bottom_right_v = *(data + data_index + i_w + 1);
       }
       // calc weight grad of top_left_w, then multiple -1 is the grad of grid_src
@@ -140,9 +148,11 @@ __global__ void BilinearSamplerBackwardKernel(const int i_c, const int i_h,
                         (top_left_v - top_right_v - bottom_left_v + bottom_right_v)
                         * top_left_y_w);
     }
-    // calc grad of grid
-    *(grad_grid + grid_src_index + o_h * o_w) += top_left_y_gw * (i_h - 1) / 2;
-    *(grad_grid + grid_src_index) += top_left_x_gw * (i_w - 1) / 2;
+    if (Req2 != mxnet::kNullOp) {
+      // calc grad of grid
+      *(grad_grid + grid_src_index + o_h * o_w) += top_left_y_gw * (i_h - 1) / 2;
+      *(grad_grid + grid_src_index) += top_left_x_gw * (i_w - 1) / 2;
+    }
   }
 }
 }  // namespace cuda
@@ -174,10 +184,13 @@ inline void BilinearSamplerForward(const Tensor<gpu, 4, DType> &output,
 
 template<typename DType>
 inline void BilinearSamplerBackward(const Tensor<gpu, 4, DType> &input_grad,
-                                     const Tensor<gpu, 4, DType> &ggrid,
-                                     const Tensor<gpu, 4, DType> &output_grad,
-                                     const Tensor<gpu, 4, DType> &input_data,
-                                     const Tensor<gpu, 4, DType> &grid) {
+                                    const Tensor<gpu, 4, DType> &ggrid,
+                                    const Tensor<gpu, 4, DType> &output_grad,
+                                    const Tensor<gpu, 4, DType> &input_data,
+                                    const Tensor<gpu, 4, DType> &grid,
+                                    const mxnet::OpReqType data_req,
+                                    const mxnet::OpReqType grid_req) {
+  using namespace mxnet;
   DType *g_input = input_grad.dptr_;
   DType *grad_grid = ggrid.dptr_;
   const DType *grid_src = grid.dptr_;
@@ -196,8 +209,13 @@ inline void BilinearSamplerBackward(const Tensor<gpu, 4, DType> &input_grad,
   dim3 threads_per_block(kMaxThreadsPerBlock);
   CheckLaunchParam(num_blocks, threads_per_block, ""bilinear sampler backward"");
   cudaStream_t stream = Stream<gpu>::GetStream(input_grad.stream_);
-  cuda::BilinearSamplerBackwardKernel<DType> << <num_blocks, threads_per_block, 0, stream >> >(
-    i_c, i_h, i_w, grad, data, o_n, o_c, o_h, o_w, g_input, grid_src, grad_grid);
+  MXNET_REQ_TYPE_SWITCH(data_req, Req1, {
+    MXNET_REQ_TYPE_SWITCH(grid_req, Req2, {
+      cuda::BilinearSamplerBackwardKernel<DType, Req1, Req2>
+      <<<num_blocks, threads_per_block, 0, stream >>>(
+        i_c, i_h, i_w, grad, data, o_n, o_c, o_h, o_w, g_input, grid_src, grad_grid);
+    });
+  });
   // post kernel check
   cudaError err = cudaPeekAtLastError();
   CHECK_EQ(err, cudaSuccess) << cudaGetErrorString(err);
diff --git a/src/operator/mxnet_op.h b/src/operator/mxnet_op.h
index f11a497c564..e77569671eb 100644
--- a/src/operator/mxnet_op.h
+++ b/src/operator/mxnet_op.h
@@ -111,6 +111,33 @@ inline int get_num_threads<cpu>(const int N) {
   }
 
 
+/*! \brief operator request type switch */
+#define MXNET_REQ_TYPE_SWITCH(req, ReqType, ...)  \
+  switch (req) {                                    \
+  case kNullOp:                                     \
+    {                                               \
+      const OpReqType ReqType = kNullOp;            \
+      {__VA_ARGS__}                                 \
+    }                                               \
+    break;                                          \
+  case kWriteInplace:                               \
+  case kWriteTo:                                    \
+    {                                               \
+      const OpReqType ReqType = kWriteTo;           \
+      {__VA_ARGS__}                                 \
+    }                                               \
+    break;                                          \
+  case kAddTo:                                      \
+    {                                               \
+      const OpReqType ReqType = kAddTo;             \
+      {__VA_ARGS__}                                 \
+    }                                               \
+    break;                                          \
+  default:                                          \
+    break;                                          \
+  }
+
+
 #define MXNET_NDIM_SWITCH(NDim, ndim, ...)         \
   if (NDim == 0) {                                 \
   } else if (NDim == 1) {                          \
diff --git a/tests/python/gpu/test_operator_gpu.py b/tests/python/gpu/test_operator_gpu.py
index 1fc2c8e922d..d201a2e09c6 100644
--- a/tests/python/gpu/test_operator_gpu.py
+++ b/tests/python/gpu/test_operator_gpu.py
@@ -1945,7 +1945,7 @@ def test_bilinear_sampler_versions():
             exe.arg_dict['data'][:] = test_data
             exe.arg_dict['grid'][:] = test_grid
             exe.forward(is_train=True)
-            assert_almost_equal(exe_list[0].outputs[0].asnumpy(), exe.outputs[0].asnumpy(), rtol=1e-3, atol=1e-5)
+            assert_almost_equal(exe_list[ref_idx].outputs[0].asnumpy(), exe.outputs[0].asnumpy(), rtol=1e-3, atol=1e-5)
 
         out_grad = np.random.uniform(low=-0.01, high=0.01,size=data_shape[:2] + grid_shape[2:]).astype(np.float32)
         for exe in exe_list:
@@ -1975,6 +1975,22 @@ def test_bilinear_sampler_versions():
         assert_almost_equal(exe_list[ref_idx].grad_dict['data'].asnumpy(), data_grad + data_initial_grad, rtol=1e-3, atol=1e-5)
         assert_almost_equal(exe_list[ref_idx].grad_dict['grid'].asnumpy(), grid_grad + grid_initial_grad, rtol=1e-3, atol=1e-5)
 
+        for req_dict in [{'data' : 'null', 'grid' : 'write'}, {'data' : 'write', 'grid' : 'null'}]:
+            # Mixture of kWriteTo and kNullOp
+            exe_cpu_mix = sym1.simple_bind(data=data_shape, grid=grid_shape, ctx=mx.cpu(), grad_req=req_dict)
+            exe_gpu_mix = sym2.simple_bind(data=data_shape, grid=grid_shape, ctx=default_context(), grad_req=req_dict)
+            exe_cudnn_mix = sym3.simple_bind(data=data_shape, grid=grid_shape, ctx=default_context(), grad_req=req_dict)
+            exe_list = [exe_cpu_mix, exe_gpu_mix, exe_cudnn_mix]
+            for exe in exe_list:
+                exe.arg_dict['data'][:] = test_data
+                exe.arg_dict['grid'][:] = test_grid
+                exe.forward(is_train=True)
+                exe.backward(mx.nd.array(out_grad))
+                if req_dict['data'] is 'write':
+                    assert_almost_equal(exe.grad_dict['data'].asnumpy(), exe_list[ref_idx].grad_dict['data'].asnumpy(), rtol=1e-3, atol=1e-5)
+                if req_dict['grid'] is 'write':
+                    assert_almost_equal(exe.grad_dict['grid'].asnumpy(), exe_list[ref_idx].grad_dict['grid'].asnumpy(), rtol=1e-3, atol=1e-5)
+
 
 def test_context_num_gpus():
     # Test that num_gpus reports at least one GPU, as the test is run on a GPU host.


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Sep/18 17:20;githubbot;600",,,,,,,,,,,,,,,,,0,5400,,,0,5400,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-08-13 16:51:13.0,,,,,,,"0|i3wzrx:",9223372036854775807,,,,,,,,,,,Backend (08/27-09/07),Backend (09/10-09/24),,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Why does a tanh activation layer generates values greater than 1?,MXNET-809,13178634,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,Cannot Reproduce,samskalicky,apeforest,apeforest,13/Aug/18 16:50,07/Sep/18 18:21,29/Oct/20 16:32,07/Sep/18 18:21,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-08-13 16:50:42.0,,,,,,,"0|i3wzsk:",9223372036854775807,,,,,,,,,,,Backend (08/27-09/07),,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Undefined Behavior of mx.sym.where with shape-mismatched cond,MXNET-806,13178631,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,,apeforest,apeforest,apeforest,13/Aug/18 16:47,23/Aug/18 22:07,29/Oct/20 16:32,,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,"apeforest opened a new pull request #12174: [MXNET-806] Report error when shape mismatch in ""where"" operator
URL: https://github.com/apache/incubator-mxnet/pull/12174
 
 
   ## Description ##
   ```mx.sym.where(cond, x, y)``` still functions, given shape-mismatched cond, whereas ```mx.nd.where(cond, x, y)``` will throw exception. We should make sure the behavior is the same.
   
   ## Checklist ##
   ### Essentials ###
   Please feel free to remove inapplicable items for your PR.
   - [X] The PR title starts with [MXNET-$JIRA_ID], where $JIRA_ID refers to the relevant [JIRA issue](https://issues.apache.org/jira/projects/MXNET/issues) created (except PRs with tiny changes)
   - [X] Changes are complete (i.e. I finished coding on this PR)
   - [X] All changes have test coverage:
   - Unit tests are added for small changes to verify correctness
   test_operator.py:test_where():test_invalid_shape()
   - [ ] Code is well-documented: 
   - This change is consistent with the current API documentation. Therefore, no additional documentation should be needed.
   - Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
   - [X] To the my best knowledge, examples are either not affected by this change, or have been fixed to be compatible with this change
   
   ### Changes ###
   - [X] where operator with mismatched shape should throw exception
   
   ## Comments ##

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/18 00:24;githubbot;600","anirudh2290 commented on a change in pull request #12174: [MXNET-806] Report error when shape mismatch in ""where"" operator
URL: https://github.com/apache/incubator-mxnet/pull/12174#discussion_r210270908
 
 

 ##########
 File path: tests/python/unittest/test_operator.py
 ##########
 @@ -4442,6 +4442,18 @@ def test_where_numeric_gradient(shape, same_shape):
             condition_np, x_np, y_np = get_forward_inputs_condition_vector(shape)
         check_numeric_gradient(where_sym, [condition_np, x_np, y_np], grad_nodes=['x', 'y'])
 
+    def test_invalid_shape():
+        condition = mx.sym.Variable('condition')
+        x = mx.sym.Variable('x')
+        y = mx.sym.Variable('y')
+        where_sym = mx.sym.where(condition, x, y)
 
 Review comment:
   can you add similar tests for mx.nd.where

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/18 13:38;githubbot;600","apeforest commented on a change in pull request #12174: [MXNET-806] Report error when shape mismatch in ""where"" operator
URL: https://github.com/apache/incubator-mxnet/pull/12174#discussion_r210339535
 
 

 ##########
 File path: tests/python/unittest/test_operator.py
 ##########
 @@ -4442,6 +4442,18 @@ def test_where_numeric_gradient(shape, same_shape):
             condition_np, x_np, y_np = get_forward_inputs_condition_vector(shape)
         check_numeric_gradient(where_sym, [condition_np, x_np, y_np], grad_nodes=['x', 'y'])
 
+    def test_invalid_shape():
+        condition = mx.sym.Variable('condition')
+        x = mx.sym.Variable('x')
+        y = mx.sym.Variable('y')
+        where_sym = mx.sym.where(condition, x, y)
 
 Review comment:
   @anirudh2290 Added test for mx.nd.where.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/18 17:13;githubbot;600","apeforest commented on issue #12174: [MXNET-806] Report error when shape mismatch in ""where"" operator
URL: https://github.com/apache/incubator-mxnet/pull/12174#issuecomment-413988270
 
 
   @anirudh2290  Can we merge this PR if no other reviewers comments?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Aug/18 21:06;githubbot;600","sandeep-krishnamurthy closed pull request #12174: [MXNET-806] Report error when shape mismatch in ""where"" operator
URL: https://github.com/apache/incubator-mxnet/pull/12174
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/src/operator/tensor/control_flow_op.h b/src/operator/tensor/control_flow_op.h
index 503bc7c4abb..94e65109c35 100644
--- a/src/operator/tensor/control_flow_op.h
+++ b/src/operator/tensor/control_flow_op.h
@@ -188,7 +188,7 @@ inline bool WhereOpShape(const nnvm::NodeAttrs& attrs,
     SHAPE_ASSIGN_CHECK(*in_attrs, 0, tshape);
     return true;
   } else if ((*in_attrs)[0].ndim() == 1) {
-    return (*in_attrs)[0].Size() == static_cast<size_t>(tshape[0]);
+    CHECK_EQ((*in_attrs)[0].Size(), static_cast<size_t>(tshape[0]));
   }
   return false;
 }
diff --git a/tests/python/unittest/test_operator.py b/tests/python/unittest/test_operator.py
index f1aec12ccc3..a1307cf868f 100644
--- a/tests/python/unittest/test_operator.py
+++ b/tests/python/unittest/test_operator.py
@@ -4471,6 +4471,20 @@ def test_where_numeric_gradient(shape, same_shape):
             condition_np, x_np, y_np = get_forward_inputs_condition_vector(shape)
         check_numeric_gradient(where_sym, [condition_np, x_np, y_np], grad_nodes=['x', 'y'])
 
+    def test_invalid_shape():
+        condition = mx.sym.Variable('condition')
+        x = mx.sym.Variable('x')
+        y = mx.sym.Variable('y')
+        where_sym = mx.sym.where(condition, x, y)
+	
+        assert_exception(lambda: where_sym.eval(x=mx.nd.array([[2,3],[4,5],[6,7]]),
+                                                y=mx.nd.array([[8,9],[10,11],[12,13]]),
+                                                condition=mx.nd.array([1,0])), MXNetError)
+
+        assert_exception(lambda: mx.nd.where(x=mx.nd.array([[2,3],[4,5],[6,7]]),
+                                             y=mx.nd.array([[8,9],[10,11],[12,13]]),
+                                             condition=mx.nd.array([1,0])), MXNetError)
+
     test_where_helper((5, 9), True)
     test_where_helper((5, 9), False)
     test_where_helper((5, 7, 9), True)
@@ -4481,7 +4495,7 @@ def test_where_numeric_gradient(shape, same_shape):
     test_where_numeric_gradient((5, 9), False)
     test_where_numeric_gradient((5, 7, 9), True)
     test_where_numeric_gradient((5, 7, 9), False)
-
+    test_invalid_shape()
 
 @with_seed()
 def test_new_softmax():


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Aug/18 00:23;githubbot;600","zheng-da commented on a change in pull request #12174: [MXNET-806] Report error when shape mismatch in ""where"" operator
URL: https://github.com/apache/incubator-mxnet/pull/12174#discussion_r212472034
 
 

 ##########
 File path: src/operator/tensor/control_flow_op.h
 ##########
 @@ -188,7 +188,7 @@ inline bool WhereOpShape(const nnvm::NodeAttrs& attrs,
     SHAPE_ASSIGN_CHECK(*in_attrs, 0, tshape);
     return true;
   } else if ((*in_attrs)[0].ndim() == 1) {
-    return (*in_attrs)[0].Size() == static_cast<size_t>(tshape[0]);
+    CHECK_EQ((*in_attrs)[0].Size(), static_cast<size_t>(tshape[0]));
 
 Review comment:
   this is wrong. what is the problem of the original code? If the first dimension doesn't match, infer shape fails, so it should return false.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Aug/18 22:07;githubbot;600",,,,,,,,,,,,,,,,,,,,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 15 00:25:01 UTC 2018,,,,,,,"0|i3wu5y:li",9223372036854775807,,,,,,,,,,,Backend (08/13-08/27),,,,,,,,,,,2.0,,,,,,,,,,,"15/Aug/18 00:25;apeforest;PR under review: https://github.com/apache/incubator-mxnet/pull/12174",,,,,,,,,,,,,,,,,,,,,,,,,,,
mx.nd.topk does not work with ndarray of type float16,MXNET-805,13178630,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,Later,samskalicky,apeforest,apeforest,13/Aug/18 16:47,23/Aug/18 23:47,29/Oct/20 16:32,23/Aug/18 23:47,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-08-13 16:47:11.0,,,,,,,"0|i3wu5v:",9223372036854775807,,,,,,,,,,,Backend (08/13-08/27),,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nd.softmax() doesn't support grad_req='add',MXNET-804,13178629,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,,haojin2,apeforest,apeforest,13/Aug/18 16:45,21/Aug/18 17:12,29/Oct/20 16:32,,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-08-13 16:45:54.0,,,,,,,"0|i3wu5y:k",9223372036854775807,,,,,,,,,,,Backend (08/13-08/27),,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent / wrong output from sum,MXNET-803,13178628,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,,haojin2,apeforest,apeforest,13/Aug/18 16:45,06/Sep/18 21:06,29/Oct/20 16:32,,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-08-13 16:45:15.0,,,,,,,"0|i3wu5u:",9223372036854775807,,,,,,,,,,,Backend (08/13-08/27),Backend (08/27-09/07),,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Autograd fails when using `take` operator repeatedly,MXNET-802,13178627,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,,haojin2,apeforest,apeforest,13/Aug/18 16:44,16/Aug/18 22:30,29/Oct/20 16:32,,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-08-13 16:44:15.0,,,,,,,"0|i3wu5s:",9223372036854775807,,,,,,,,,,,Backend (08/13-08/27),,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
non-deterministic backward of scatter_nd,MXNET-801,13178626,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,Not A Bug,haojin2,apeforest,apeforest,13/Aug/18 16:43,15/Aug/18 17:28,29/Oct/20 16:32,15/Aug/18 17:28,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-08-13 16:43:10.0,,,,,,,"0|i3wu5r:",9223372036854775807,,,,,,,,,,,Backend (08/13-08/27),,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gradient function not returning enough gradient,MXNET-799,13178153,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,,apeforest,apeforest,apeforest,09/Aug/18 23:34,15/Aug/18 21:21,29/Oct/20 16:32,,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 15 21:21:38 UTC 2018,,,,,,,"0|i3wu5y:l",9223372036854775807,,,,,,,,,,,Backend (08/13-08/27),,,,,,,,,,,5.0,,,,,,,,,,,"15/Aug/18 21:21;apeforest;Not able to reproduce.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Dangling outputs and dtype != float32: Gradient computation fails,MXNET-798,13178152,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,,apeforest,apeforest,apeforest,09/Aug/18 23:32,14/Sep/18 22:12,29/Oct/20 16:32,,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,"apeforest opened a new pull request #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290
 
 
   ## Description ##
   In imperative mode, the gradient computation for multiout operators would fail when the dtype is not equal to float32 and one of the outputs is dont-care. The rootcause of this problem is a zeros operator would be automatically derived in the nnvm::Graph where the default dtype (float32) is used in the zeros operator.
   
   This change fix the problem by inferring dtype from other inputs in the graph when the operator is a zero operator.
   
   ## Checklist ##
   ### Essentials ###
   Please feel free to remove inapplicable items for your PR.
   - [X] The PR title starts with [MXNET-$JIRA_ID], where $JIRA_ID refers to the relevant [JIRA issue](https://issues.apache.org/jira/projects/MXNET/issues) created (except PRs with tiny changes)
   - [X] Changes are complete (i.e. I finished coding on this PR)
   - [X] All changes have test coverage:
   - Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
   - Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
   - Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
   - [X] Code is well-documented: 
   - For user-facing API changes, API doc string has been updated. 
   - For new C++ functions in header files, their functionalities and arguments are documented. 
   - For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
   - Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
   - [X] To the my best knowledge, examples are either not affected by this change, or have been fixed to be compatible with this change
   
   ### Changes ###
   - [X] Change the way to infer type for auto-derived zero operator in nnvm::Graph
   - [X] Added a unittest for operators with multioutput.
   
   ## Comments ##
   - This seems to be a general problem for all multioutput operators when computing gradient in imperative mode. A simple example is copied from the original issue below:
   - Although the change is small, the impact could be large. Thus thorough review is solicited.
   ```
   import mxnet as mx
   from mxnet import autograd
   
   
   data = mx.nd.arange(16, dtype='float64').reshape((4, 4))
   data.attach_grad()
   
   with autograd.record():
       y = mx.nd.split(data, axis=0, num_outputs=2)
   y[0].backward()
   print(data.grad)
   ```

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/18 17:28;githubbot;600","apeforest commented on issue #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#issuecomment-415113463
 
 
   @eric-haibin-lin Please review

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/18 17:30;githubbot;600","apeforest edited a comment on issue #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#issuecomment-415113463
 
 
   @eric-haibin-lin @piiswrong Will appreciate your review.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/18 17:48;githubbot;600","apeforest edited a comment on issue #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#issuecomment-415113463
 
 
   @eric-haibin-lin @piiswrong @haojin2 Will appreciate your review.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/18 17:51;githubbot;600","apeforest edited a comment on issue #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#issuecomment-415113463
 
 
   @eric-haibin-lin @piiswrong @haojin2 I will appreciate your review.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/18 17:51;githubbot;600","haojin2 commented on a change in pull request #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#discussion_r212053845
 
 

 ##########
 File path: tests/python/unittest/test_infer_type.py
 ##########
 @@ -0,0 +1,36 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+# pylint: skip-file
+import mxnet as mx
+import numpy as np
+from common import models
+from mxnet import autograd
+from nose.tools import *
+
+def test_infer_multiout_op():
+    data = mx.nd.arange(16, dtype='float64').reshape((4, 4))
+    data.attach_grad()
+
+    with autograd.record():
+        y = mx.nd.split(data, axis=0, num_outputs=2)
+    y[0].backward()
+    assert data.grad.dtype == np.float64
+
+
+if __name__ == ""__main__"":
+    test_infer_multiout_op()
 
 Review comment:
   I think this should go to something like test_operator.py instead of creating a separate file for it? And, please see https://github.com/apache/incubator-mxnet/blob/master/tests/python/unittest/test_operator.py#L7017-L7018 for how to use nosetests.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/18 18:10;githubbot;600","haojin2 commented on a change in pull request #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#discussion_r212053948
 
 

 ##########
 File path: tests/python/unittest/test_infer_type.py
 ##########
 @@ -0,0 +1,55 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+# pylint: skip-file
+import mxnet as mx
+import numpy as np
+from common import models
+from mxnet import autograd
+from nose.tools import *
+
+def test_infer_multiout_op():
+    data = mx.nd.arange(16, dtype='float64').reshape((4, 4))
+    data.attach_grad()
+
+    with autograd.record():
+        y = mx.nd.split(data, axis=0, num_outputs=2)
+    y[0].backward()
+    assert data.grad.dtype == np.float64
+
+def test_infer_multiout_op2():
+    def test_func(a):
+        q, l = mx.nd.linalg.gelqf(a)
+        return mx.nd.sum(l)
+
+    data32 = mx.nd.random.normal(shape=(2, 3), ctx=mx.cpu(), dtype='float32')
+    data32.attach_grad()
+    with autograd.record():
+        test32 = test_func(data32)
+        test32.backward()
+
+    data64 = mx.nd.Cast(data32, dtype='float64')
+    data64.attach_grad()
+    with autograd.record():
+        test64 = test_func(data64)
+        test64.backward()
+    assert_almost_equal(data64.grad.asnumpy().all(), data32.grad.asnumpy().all())
+
+
 
 Review comment:
   I think this should go to something like test_operator.py instead of creating a separate file for it? And, please see https://github.com/apache/incubator-mxnet/blob/master/tests/python/unittest/test_operator.py#L7017-L7018 for how to use nosetests.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/18 18:10;githubbot;600","apeforest commented on a change in pull request #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#discussion_r212058959
 
 

 ##########
 File path: tests/python/unittest/test_infer_type.py
 ##########
 @@ -0,0 +1,55 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+# pylint: skip-file
+import mxnet as mx
+import numpy as np
+from common import models
+from mxnet import autograd
+from nose.tools import *
+
+def test_infer_multiout_op():
+    data = mx.nd.arange(16, dtype='float64').reshape((4, 4))
+    data.attach_grad()
+
+    with autograd.record():
+        y = mx.nd.split(data, axis=0, num_outputs=2)
+    y[0].backward()
+    assert data.grad.dtype == np.float64
+
+def test_infer_multiout_op2():
+    def test_func(a):
+        q, l = mx.nd.linalg.gelqf(a)
+        return mx.nd.sum(l)
+
+    data32 = mx.nd.random.normal(shape=(2, 3), ctx=mx.cpu(), dtype='float32')
+    data32.attach_grad()
+    with autograd.record():
+        test32 = test_func(data32)
+        test32.backward()
+
+    data64 = mx.nd.Cast(data32, dtype='float64')
+    data64.attach_grad()
+    with autograd.record():
+        test64 = test_func(data64)
+        test64.backward()
+    assert_almost_equal(data64.grad.asnumpy().all(), data32.grad.asnumpy().all())
+
+
 
 Review comment:
   This is not to test the functionality of the operator but a general type casting issue for all multioutput operators. I inclined to add it in the infer type tests but would like to hear more suggestions.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/18 18:24;githubbot;600","apeforest commented on a change in pull request #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#discussion_r212059180
 
 

 ##########
 File path: tests/python/unittest/test_infer_type.py
 ##########
 @@ -0,0 +1,55 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+# pylint: skip-file
+import mxnet as mx
+import numpy as np
+from common import models
+from mxnet import autograd
+from nose.tools import *
+
+def test_infer_multiout_op():
+    data = mx.nd.arange(16, dtype='float64').reshape((4, 4))
+    data.attach_grad()
+
+    with autograd.record():
+        y = mx.nd.split(data, axis=0, num_outputs=2)
+    y[0].backward()
+    assert data.grad.dtype == np.float64
+
+def test_infer_multiout_op2():
+    def test_func(a):
+        q, l = mx.nd.linalg.gelqf(a)
+        return mx.nd.sum(l)
+
+    data32 = mx.nd.random.normal(shape=(2, 3), ctx=mx.cpu(), dtype='float32')
+    data32.attach_grad()
+    with autograd.record():
+        test32 = test_func(data32)
+        test32.backward()
+
+    data64 = mx.nd.Cast(data32, dtype='float64')
+    data64.attach_grad()
+    with autograd.record():
+        test64 = test_func(data64)
+        test64.backward()
+    assert_almost_equal(data64.grad.asnumpy().all(), data32.grad.asnumpy().all())
+
+
 
 Review comment:
   Changed test to run nose runmodule

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/18 18:24;githubbot;600","apeforest commented on issue #12290: [MXNET-798][WIP] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#issuecomment-415196751
 
 
   Change to [WIP] to fix some platform dependent unit test failure.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Aug/18 22:04;githubbot;600","eric-haibin-lin commented on a change in pull request #12290: [MXNET-798][WIP] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#discussion_r212494268
 
 

 ##########
 File path: src/executor/infer_graph_attr_pass.cc
 ##########
 @@ -254,7 +254,8 @@ nnvm::Graph InferAttr(nnvm::Graph &&ret,
         dispatch_mode = &dispatch_modes[nid];
         if (dispatch_modes[nid] == DispatchMode::kUndefined) forward_known = false;
       }
-      auto finfer = finfer_shape.get(inode.source->op(), fdefault);
+      auto finfer = (inode.source->op() == Op::Get(""_zeros"")) ? fdefault :
+        finfer_shape.get(inode.source->op(), fdefault);
 
 Review comment:
   Are you sure about this? This affects all _zero ops, not just for the case you mentioned.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Aug/18 00:33;githubbot;600","apeforest commented on a change in pull request #12290: [MXNET-798][WIP] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#discussion_r212695363
 
 

 ##########
 File path: src/executor/infer_graph_attr_pass.cc
 ##########
 @@ -254,7 +254,8 @@ nnvm::Graph InferAttr(nnvm::Graph &&ret,
         dispatch_mode = &dispatch_modes[nid];
         if (dispatch_modes[nid] == DispatchMode::kUndefined) forward_known = false;
       }
-      auto finfer = finfer_shape.get(inode.source->op(), fdefault);
+      auto finfer = (inode.source->op() == Op::Get(""_zeros"")) ? fdefault :
+        finfer_shape.get(inode.source->op(), fdefault);
 
 Review comment:
   You are right, this is breaking some unit test (however, due to unittest of master branch is broken in MacOS, I wan't able to verify before checkin). I have changed the PR to WIP. 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;24/Aug/18 17:14;githubbot;600","apeforest commented on issue #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#issuecomment-420716016
 
 
   @eric-haibin-lin Please review this new implementation. Thanks for your suggestion!

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Sep/18 16:39;githubbot;600","eric-haibin-lin commented on issue #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#issuecomment-421090364
 
 
   What's up with the build? 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Sep/18 17:37;githubbot;600","apeforest commented on issue #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#issuecomment-421098047
 
 
   @eric-haibin-lin Not sure exactly. An earlier build passed https://github.com/apache/incubator-mxnet/pull/12290/commits/dcc5f7877ec1fad7ad4c1f3d2ab3a3c5f52909ca). After I renamed some variables the build on ARM7 failed. I can submit an empty change to trigger the build again. 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Sep/18 18:02;githubbot;600","anirudh2290 commented on a change in pull request #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#discussion_r217784815
 
 

 ##########
 File path: tests/python/unittest/test_infer_type.py
 ##########
 @@ -0,0 +1,55 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+# pylint: skip-file
+import mxnet as mx
+import numpy as np
+from common import models
+from mxnet import autograd
+from nose.tools import *
+
+def test_infer_multiout_op():
+    data = mx.nd.arange(16, dtype=np.float64).reshape((4, 4))
+    data.attach_grad()
+
+    with autograd.record():
+        y = mx.nd.split(data, axis=0, num_outputs=2)
+    y[0].backward()
+    assert data.grad.dtype == np.float64
+
+def test_infer_multiout_op2():
+    def test_func(a):
+        q, l = mx.nd.linalg.gelqf(a)
+        return mx.nd.sum(l)
+
+    data32 = mx.nd.random.normal(shape=(2, 3), ctx=mx.cpu(), dtype=np.float32)
+    data32.attach_grad()
+    with autograd.record():
+        test32 = test_func(data32)
+        test32.backward()
+
+    data64 = mx.nd.Cast(data32, dtype=np.float64)
+    data64.attach_grad()
+    with autograd.record():
+        test64 = test_func(data64)
+        test64.backward()
+    assert_almost_equal(data64.grad.asnumpy().all(), data32.grad.asnumpy().all())
 
 Review comment:
   can you set rtol and atol to some bigger value than default here ?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Sep/18 17:22;githubbot;600","anirudh2290 commented on issue #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#issuecomment-421430543
 
 
   Also,maybe we should add zeros to APIs that may be good to break for 2.0 https://github.com/apache/incubator-mxnet/issues/9686

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Sep/18 17:33;githubbot;600","apeforest commented on a change in pull request #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#discussion_r217807491
 
 

 ##########
 File path: tests/python/unittest/test_infer_type.py
 ##########
 @@ -0,0 +1,55 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+# pylint: skip-file
+import mxnet as mx
+import numpy as np
+from common import models
+from mxnet import autograd
+from nose.tools import *
+
+def test_infer_multiout_op():
+    data = mx.nd.arange(16, dtype=np.float64).reshape((4, 4))
+    data.attach_grad()
+
+    with autograd.record():
+        y = mx.nd.split(data, axis=0, num_outputs=2)
+    y[0].backward()
+    assert data.grad.dtype == np.float64
+
+def test_infer_multiout_op2():
+    def test_func(a):
+        q, l = mx.nd.linalg.gelqf(a)
+        return mx.nd.sum(l)
+
+    data32 = mx.nd.random.normal(shape=(2, 3), ctx=mx.cpu(), dtype=np.float32)
+    data32.attach_grad()
+    with autograd.record():
+        test32 = test_func(data32)
+        test32.backward()
+
+    data64 = mx.nd.Cast(data32, dtype=np.float64)
+    data64.attach_grad()
+    with autograd.record():
+        test64 = test_func(data64)
+        test64.backward()
+    assert_almost_equal(data64.grad.asnumpy().all(), data32.grad.asnumpy().all())
 
 Review comment:
   Why increase the rtol and atol if the unit test can pass with the default one? 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Sep/18 18:42;githubbot;600","apeforest commented on issue #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#issuecomment-421449750
 
 
   @anirudh2290 The _zeros_without_dtype operator is a private operator used only in building nnvm graph. It is not meant to be exposed to users.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Sep/18 18:43;githubbot;600","anirudh2290 commented on a change in pull request #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#discussion_r217808644
 
 

 ##########
 File path: tests/python/unittest/test_infer_type.py
 ##########
 @@ -0,0 +1,55 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+# pylint: skip-file
+import mxnet as mx
+import numpy as np
+from common import models
+from mxnet import autograd
+from nose.tools import *
+
+def test_infer_multiout_op():
+    data = mx.nd.arange(16, dtype=np.float64).reshape((4, 4))
+    data.attach_grad()
+
+    with autograd.record():
+        y = mx.nd.split(data, axis=0, num_outputs=2)
+    y[0].backward()
+    assert data.grad.dtype == np.float64
+
+def test_infer_multiout_op2():
+    def test_func(a):
+        q, l = mx.nd.linalg.gelqf(a)
+        return mx.nd.sum(l)
+
+    data32 = mx.nd.random.normal(shape=(2, 3), ctx=mx.cpu(), dtype=np.float32)
+    data32.attach_grad()
+    with autograd.record():
+        test32 = test_func(data32)
+        test32.backward()
+
+    data64 = mx.nd.Cast(data32, dtype=np.float64)
+    data64.attach_grad()
+    with autograd.record():
+        test64 = test_func(data64)
+        test64.backward()
+    assert_almost_equal(data64.grad.asnumpy().all(), data32.grad.asnumpy().all())
 
 Review comment:
   This can be flaky. you are comparing a float32 numpy to a float64 numpy and the atol and rtol  defaults are small.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Sep/18 18:46;githubbot;600","anirudh2290 commented on issue #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#issuecomment-421450841
 
 
   @apeforest what i meant is we can change the dtype default to -1 for zeros operator for 2.0.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Sep/18 18:48;githubbot;600","apeforest commented on issue #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#issuecomment-421464171
 
 
   @anirudh2290  Thanks for the clarification. I have increased atol and rtol values in unit test. As to changing the dtype default to -1 for zeros, I think it is not related to this PR and may cause a backward compatibility issue with old models. Therefore, I would prefer doing that in a separate PR. Please let me know what you think. Thanks.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Sep/18 19:41;githubbot;600","anirudh2290 commented on issue #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290#issuecomment-421464826
 
 
   Not suggesting to do it in this PR. Just wanted to document it in the APIs to break for 2.0 and we can do it before 2.0 release.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Sep/18 19:43;githubbot;600","anirudh2290 closed pull request #12290: [MXNET-798] Fix the dtype cast from non float32 in Gradient computation
URL: https://github.com/apache/incubator-mxnet/pull/12290
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/src/executor/graph_executor.cc b/src/executor/graph_executor.cc
index 265554ab391..54a8d224ff4 100644
--- a/src/executor/graph_executor.cc
+++ b/src/executor/graph_executor.cc
@@ -141,8 +141,8 @@ nnvm::NodeEntry AggregateGradient(std::vector<nnvm::NodeEntry>&& v) {
 
   if (v.empty()) {
     nnvm::NodePtr ng = nnvm::Node::Create();
-    ng->attrs.op = zeros_op;
-    ng->attrs.name = ""zeros"";
+    ng->attrs.op = Op::Get(""_zeros_without_dtype"");
+    ng->attrs.name = ""zeros_without_dtype"";
     ng->attrs.op->attr_parser(&(ng->attrs));
     return nnvm::NodeEntry{ng, 0, 0};
   }
diff --git a/src/operator/tensor/init_op.cc b/src/operator/tensor/init_op.cc
index bb23f5d44f6..8554ba85417 100644
--- a/src/operator/tensor/init_op.cc
+++ b/src/operator/tensor/init_op.cc
@@ -30,9 +30,22 @@ namespace op {
 
 DMLC_REGISTER_PARAMETER(InitOpParam);
 DMLC_REGISTER_PARAMETER(InitOpWithScalarParam);
+DMLC_REGISTER_PARAMETER(InitOpWithoutDTypeParam);
 DMLC_REGISTER_PARAMETER(RangeParam);
 DMLC_REGISTER_PARAMETER(EyeParam);
 
+NNVM_REGISTER_OP(_zeros_without_dtype)
+.describe(""fill target with zeros without default dtype"")
+.set_num_inputs(0)
+.set_num_outputs(1)
+.set_attr_parser(ParamParser<InitOpWithoutDTypeParam>)
+.set_attr<nnvm::FInferShape>(""FInferShape"", InitShape<InitOpWithoutDTypeParam>)
+.set_attr<nnvm::FInferType>(""FInferType"", InitType<InitOpWithoutDTypeParam>)
+.set_attr<FInferStorageType>(""FInferStorageType"",
+  InitStorageType<InitOpWithoutDTypeParam, true, true>)
+.set_attr<FCompute>(""FCompute<cpu>"", FillCompute<cpu, 0>)
+.set_attr<FComputeEx>(""FComputeEx<cpu>"", FillComputeZerosEx<cpu>)
+.add_arguments(InitOpWithoutDTypeParam::__FIELDS__());
 
 NNVM_REGISTER_OP(_zeros)
 .describe(""fill target with zeros"")
diff --git a/src/operator/tensor/init_op.cu b/src/operator/tensor/init_op.cu
index 81d835ee3bd..902b567516b 100644
--- a/src/operator/tensor/init_op.cu
+++ b/src/operator/tensor/init_op.cu
@@ -44,6 +44,9 @@ void FillZerosCsrImpl(mshadow::Stream<mshadow::gpu> *s, const NDArray& dst) {
   });
 }
 
+NNVM_REGISTER_OP(_zeros_without_dtype)
+.set_attr<FCompute>(""FCompute<gpu>"", FillCompute<gpu, 0>)
+.set_attr<FComputeEx>(""FComputeEx<gpu>"", FillComputeZerosEx<gpu>);
 
 NNVM_REGISTER_OP(_zeros)
 .set_attr<FCompute>(""FCompute<gpu>"", FillCompute<gpu, 0>)
diff --git a/src/operator/tensor/init_op.h b/src/operator/tensor/init_op.h
index 304911a02a7..1a4790acdb2 100644
--- a/src/operator/tensor/init_op.h
+++ b/src/operator/tensor/init_op.h
@@ -61,6 +61,24 @@ struct InitOpParam : public dmlc::Parameter<InitOpParam> {
   }
 };
 
+struct InitOpWithoutDTypeParam : public dmlc::Parameter<InitOpWithoutDTypeParam> {
+  TShape shape;
+  std::string ctx;
+  int dtype;
+  DMLC_DECLARE_PARAMETER(InitOpWithoutDTypeParam) {
+    DMLC_DECLARE_FIELD(shape)
+    .set_default(TShape())
+    .describe(""The shape of the output"");
+    DMLC_DECLARE_FIELD(ctx)
+    .set_default("""")
+    .describe(""Context of output, in format [cpu|gpu|cpu_pinned](n).""
+              ""Only used for imperative calls."");
+    DMLC_DECLARE_FIELD(dtype)
+    .set_default(-1)
+    .describe(""Target data type."");
+  }
+};
+
 struct EyeParam : public dmlc::Parameter<EyeParam> {
   nnvm::dim_t N;
   nnvm::dim_t M;
diff --git a/tests/python/unittest/test_infer_type.py b/tests/python/unittest/test_infer_type.py
new file mode 100644
index 00000000000..bad83f3ef01
--- /dev/null
+++ b/tests/python/unittest/test_infer_type.py
@@ -0,0 +1,58 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+# pylint: skip-file
+import mxnet as mx
+import numpy as np
+from common import models, with_seed
+from mxnet import autograd
+from nose.tools import *
+from mxnet.test_utils import assert_almost_equal
+
+@with_seed()
+def test_infer_multiout_op():
+    data = mx.nd.arange(16, dtype=np.float64).reshape((4, 4))
+    data.attach_grad()
+
+    with autograd.record():
+        y = mx.nd.split(data, axis=0, num_outputs=2)
+    y[0].backward()
+    assert data.grad.dtype == np.float64
+
+@with_seed()
+def test_infer_multiout_op2():
+    def test_func(a):
+        q, l = mx.nd.linalg.gelqf(a)
+        return mx.nd.sum(l)
+
+    data32 = mx.nd.random.normal(shape=(2, 3), ctx=mx.cpu(), dtype=np.float32)
+    data32.attach_grad()
+    with autograd.record():
+        test32 = test_func(data32)
+        test32.backward()
+
+    data64 = mx.nd.Cast(data32, dtype=np.float64)
+    data64.attach_grad()
+    with autograd.record():
+        test64 = test_func(data64)
+        test64.backward()
+    assert_almost_equal(data64.grad.asnumpy(), data32.grad.asnumpy(), atol=1e-5, rtol=1e-5)
+
+
+if __name__ == '__main__':
+    import nose
+    nose.runmodule()
\ No newline at end of file


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Sep/18 21:45;githubbot;600",,0,14400,,,0,14400,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-08-09 23:32:58.0,,,,,,,"0|i3wzrr:",9223372036854775807,,,,,,,,,,,Backend (08/13-08/27),Backend (08/27-09/07),Backend (09/10-09/24),,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prelu activation compution fault in expand_shape function,MXNET-797,13178151,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,Fixed,bapac,apeforest,apeforest,09/Aug/18 23:24,05/Oct/18 21:12,29/Oct/20 16:32,05/Oct/18 21:12,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-08-09 23:24:02.0,,,,,,,"0|i3wzrw:b",9223372036854775807,,,,,,,,,,,Ada Lovelace,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dropout may mask values even when ratio=0.0,MXNET-792,13177725,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,Fixed,haibinlin,samskalicky,samskalicky,08/Aug/18 16:55,21/Aug/18 17:36,29/Oct/20 16:32,21/Aug/18 17:36,,,,,,,,,,,,,,,Apache MXNet Backend,,,,,0,,,,,,,,,,,"samskalicky opened a new pull request #12091: [MXNET-792] Fix for issue #9816
URL: https://github.com/apache/incubator-mxnet/pull/12091
 
 
   ## Description ##
   Fix for issue #9816 Dropout may mask values even when ratio=0.0
   
   as mentioned by @DickJC123, the problem is that on the CPU and GPU the random number generator works slightly differently. On the CPU the numbers are generated in the range [0,1) and on the GPU they are (0,1]. Therefor on the GPU a value of 1 can be generated and using the existing thresholding mshadow operator this results in an incorrect value since its only performing < operation. 
   
   This fix adds a <= comparison for dropout to fix this issue. 
   
   Another possible solution is to match the random number generator behavior between CPU and GPU, for which there is already a PR in the mshadow repo: https://github.com/dmlc/mshadow/pull/338
   
   When this PR gets merged the behavior of dropout with this fix will still be the same. So this fix works now and in the future.
   
   ## Checklist ##
   ### Essentials ###
   Please feel free to remove inapplicable items for your PR.
   - [X] The PR title starts with [MXNET-$JIRA_ID], where $JIRA_ID refers to the relevant [JIRA issue](https://issues.apache.org/jira/projects/MXNET/issues) created (except PRs with tiny changes)
   - [X] Changes are complete (i.e. I finished coding on this PR)
   - [X] All changes have test coverage:
   - Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
   - Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
   - Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
   - [X] Code is well-documented: 
   - For user-facing API changes, API doc string has been updated. 
   - For new C++ functions in header files, their functionalities and arguments are documented. 
   - For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
   - Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
   - [X] To the my best knowledge, examples are either not affected by this change, or have been fixed to be compatible with this change
   
   ### Changes ###
   added mshadow op for threshold_eq (theshold currently does <, this will do <=)
   
   modified dropout operator to use threshold_eq instead of theshold this will ensure equivalent behavior for the random numbers generated on CPU [0, 1) and GPU (0, 1]
   
   removed fixed seed for test_dropout
   
   ## Comments ##
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Aug/18 17:00;githubbot;600","eric-haibin-lin commented on issue #12091: [MXNET-792] Fix for issue #9816
URL: https://github.com/apache/incubator-mxnet/pull/12091#issuecomment-411608463
 
 
   @DickJC123 could you also help review?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Aug/18 01:32;githubbot;600","samskalicky commented on issue #12091: [MXNET-792] Fix for issue #9816 with dropout operator and RNG
URL: https://github.com/apache/incubator-mxnet/pull/12091#issuecomment-412575286
 
 
   Thanks @apeforest, thats a good suggestions. I updated the title to be a little more descriptive.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Aug/18 16:16;githubbot;600","eric-haibin-lin commented on issue #12091: [MXNET-792] Fix for issue #9816 with dropout operator and RNG
URL: https://github.com/apache/incubator-mxnet/pull/12091#issuecomment-412632483
 
 
   This will result in incorrect values when ratio=1 on cpu. let's fix that

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Aug/18 19:17;githubbot;600","samskalicky commented on issue #12091: [MXNET-792] Fix for issue #9816 with dropout operator and RNG
URL: https://github.com/apache/incubator-mxnet/pull/12091#issuecomment-413056672
 
 
   @eric-haibin-lin I did some more investigation and it seems that the CPU RNG ranges from (0,1] rather than the [0,1) as initially thought. I was able to get random values of 1.0 using seed 976064129 for the test_operator.py:test_dropout on the CPU. Meaning that if dropout=0, then pkeep=1 (goal is to keep everything, no dropout) and the value will get dropped with the random value of 1.0 when using less-than thresholding. Changing the code to use <= for GPU and < for CPU doesnt work. 
   
   If dropout=1, then pkeep=0. This triggers the 2nd term in the mask computation: (1.0f/pkeep). In this case this term is now NaN (divide by zero) propagating out to the output values. 
   
   The current code that is checked in where <= is used for both CPU and GPU is what is currently valid and currently passing for both CPU and GPU test_dropout evaluations.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Aug/18 00:21;githubbot;600","haojin2 commented on a change in pull request #12091: [MXNET-792] Fix for issue #9816 with dropout operator and RNG
URL: https://github.com/apache/incubator-mxnet/pull/12091#discussion_r210476089
 
 

 ##########
 File path: tests/python/unittest/test_operator.py
 ##########
 @@ -5514,7 +5514,7 @@ def test_stack():
 
 
 # test fails with seed 990952066: 0 output seen with dropout ratio=0. See issue #9816
 
 Review comment:
   If no longer flaky please get rid of this line too.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Aug/18 04:49;githubbot;600","samskalicky commented on issue #12091: [MXNET-792] Fix for issue #9816 with dropout operator and RNG
URL: https://github.com/apache/incubator-mxnet/pull/12091#issuecomment-413942250
 
 
   Thanks @haojin2, i removed that unnecessary comment

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Aug/18 17:54;githubbot;600","anirudh2290 closed pull request #12091: [MXNET-792] Fix for issue #9816 with dropout operator and RNG
URL: https://github.com/apache/incubator-mxnet/pull/12091
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/src/operator/mshadow_op.h b/src/operator/mshadow_op.h
index 7a2032df758..14f34be5bf3 100644
--- a/src/operator/mshadow_op.h
+++ b/src/operator/mshadow_op.h
@@ -265,6 +265,7 @@ MXNET_UNARY_MATH_OP(square_grad, 2.0f * math::id(a));
 
 /*! \brief used for generate Bernoulli mask */
 MXNET_BINARY_MATH_OP_NC(threshold, a < b ? DType(1) : DType(0));
+MXNET_BINARY_MATH_OP_NC(threshold_eq, a <= b ? DType(1) : DType(0));
 
 /*! \brief used for generate element of abs */
 MXNET_UNARY_MATH_OP(abs, math::fabs(a)); // NOLINT(*)
diff --git a/src/operator/nn/dropout-inl.h b/src/operator/nn/dropout-inl.h
index 8e4aac61354..b7c40fbdf52 100644
--- a/src/operator/nn/dropout-inl.h
+++ b/src/operator/nn/dropout-inl.h
@@ -206,7 +206,7 @@ class DropoutOp {
                                     const real_t pkeep) {
       RNG_KERNEL_LOOP(xpu, DType, id, gen, N, step, {
         const real_t rand_num = static_cast<real_t>(genImpl.uniform());
-        mask_out[i] = mshadow_op::threshold::Map<real_t>(rand_num, pkeep) * (1.0f / pkeep);
+        mask_out[i] = mshadow_op::threshold_eq::Map<real_t>(rand_num, pkeep) * (1.0f / pkeep);
         dropout_out[i] = input_data[i] * mask_out[i];
       });
     }
@@ -258,6 +258,7 @@ class DropoutOp {
                                         this->pkeep_);
             return;
           }
+
           // initialize the mask
           LaunchRNG<BernoulliKernel, xpu>(s, pgen, mask.Size(),
                                           mask.dptr<DType>(),
diff --git a/tests/python/unittest/test_operator.py b/tests/python/unittest/test_operator.py
index 90e85d123d5..b1408455206 100644
--- a/tests/python/unittest/test_operator.py
+++ b/tests/python/unittest/test_operator.py
@@ -5513,8 +5513,7 @@ def test_stack():
         check_numeric_gradient(out, inputs)
 
 
-# test fails with seed 990952066: 0 output seen with dropout ratio=0. See issue #9816
-@with_seed(1234)
+@with_seed()
 def test_dropout():
     def zero_count(array, ratio):
         zeros = 0
@@ -5566,6 +5565,7 @@ def check_dropout_ratio(ratio, shape):
 
         exe.arg_arrays[0][:] = 1
         exe.forward(is_train=True)
+
         if not math.isnan(max_value):
             assert exe.outputs[0].asnumpy().max() > 0
         else:


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Aug/18 23:21;githubbot;600","haojin2 commented on issue #12091: [MXNET-792] Fix for issue #9816 with dropout operator and RNG
URL: https://github.com/apache/incubator-mxnet/pull/12091#issuecomment-414495840
 
 
   @samskalicky Just merged, please remember to follow up on the related issues. Thanks!

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Aug/18 23:23;githubbot;600",,,,,,,,,,,,,,,,,0,5400,,,0,5400,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-800,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-08-08 16:55:59.0,,,,,,,"0|i3wu5y:",9223372036854775807,,,,,,,,,,,Backend (08/13-08/27),,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gluon raises error if the user does not call nd.waitall(),MXNET-579,13168186,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,,apeforest,apeforest,apeforest,25/Jun/18 22:26,15/Aug/18 21:07,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-568,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 19 20:34:39 UTC 2018,,,,,,,"0|i3v7xj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,"19/Jul/18 20:34;apeforest;No longer reproducible",,,,,,,,,,,,,,,,,,,,,,,,,,,
3D dilation support not working,MXNET-578,13168185,Bug,To Do,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,,,apeforest,apeforest,25/Jun/18 22:26,27/Jun/18 16:07,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-568,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-06-25 22:26:14.0,,,,,,,"0|i3v7xb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Crash while running gluon image-classification.py example with float16,MXNET-577,13168183,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,,apeforest,apeforest,apeforest,25/Jun/18 22:22,20/Jul/18 15:16,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-568,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 19 20:25:14 UTC 2018,,,,,,,"0|i3v7wv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,"19/Jul/18 20:25;apeforest;The issue has been fixed in the latest release. Verified on p2.8xlarge instance:


python example/gluon/image_classification.py --dataset dummy --gpus 0 --epochs 10 --mode imperative --model resnet50_v2 --batch-size 128 --log-interval 1 --dtype float16
",,,,,,,,,,,,,,,,,,,,,,,,,,,
Symbolic .json file not compatible with .params file generated since MXNet 1.2,MXNET-576,13168182,Bug,Done,MXNET,Apache MXNet,software,cjolivier01,,https://mxnet.apache.org/,Major,Fixed,roshrini,apeforest,apeforest,25/Jun/18 22:22,28/Jun/18 21:43,29/Oct/20 16:32,28/Jun/18 21:43,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MXNET-568,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-06-25 22:22:50.0,,,,,,,"0|i3v7wn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reactivating a draining agent leaves the agent in draining state.,MESOS-10096,13284685,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bmahler,bmahler,bmahler,11/Feb/20 22:23,26/Feb/20 18:03,29/Oct/20 16:32,26/Feb/20 17:47,,,,,,,,,1.10.0,1.9.1,,,,,agent,master,,,,0,,,,,,,,,"When reactivating an agent that's in the draining state, the master erases it from its draining maps, and erases its estimated drain time.

However, it doesn't send any message to the agent, so if the agent is still draining and waiting for tasks to terminate, it will stay in that state, ultimately making any tasks that then get launched get DROPPED due to the agent still being in a draining state.

Seems like we should either:

* Disallow the user from reactivating if still in draining, or
* Send a message to the agent, and have the agent move itself out of draining.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-9753,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 26 17:47:28 UTC 2020,,,,,,,"0|z0be28:",9223372036854775807,,,,,,,,,,,Studio 4: RI-23 64,,,,,,,,,,,3.0,,1.9.1,,,,,,,,,"13/Feb/20 03:17;bmahler;https://reviews.apache.org/r/72125/
https://reviews.apache.org/r/72126/","26/Feb/20 17:47;bmahler;{noformat}
commit cba5ea9ac64d0a430b7d37dbd1b408c0879faa02
Author: Benjamin Mahler <bmahler@apache.org>
Date:   Wed Feb 12 22:09:57 2020 -0500

    Disallow reactivating a DRAINING agent.

    When reactivating an agent that's in the draining state, the master
    erases it from its draining maps, and erases its estimated drain time.
    However, it doesn't send any message to the agent, so if the agent is
    still draining and waiting for tasks to terminate, it will stay in
    that state, ultimately making any tasks that then get launched get
    DROPPED due to the agent still being in a draining state.

    To avoid this confusing scenario, we disallow reactivating a DRAINING
    agent, since that seems to have been the original design intent.

    Review: https://reviews.apache.org/r/72125
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Master's agent draining VLOG prints incorrect task counts.,MESOS-10094,13284665,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bmahler,bmahler,bmahler,11/Feb/20 20:58,26/Feb/20 18:03,29/Oct/20 16:32,26/Feb/20 17:46,1.9.0,,,,,,,,1.10.0,1.9.1,,,,,master,,,,,0,,,,,,,,,"This logic is printing the framework counts of these maps rather than the task counts:

https://github.com/apache/mesos/blob/4575c9b452c25f64e6c6cc3eddc12ed3b1f8538b/src/master/master.cpp#L6318-L6319

{code}
  // Check if the agent has any tasks running or operations pending.
  if (!slave->pendingTasks.empty() ||
      !slave->tasks.empty() ||
      !slave->operations.empty()) {
    VLOG(1)
      << ""DRAINING Agent "" << slaveId << "" has ""
      << slave->pendingTasks.size() << "" pending tasks, ""
      << slave->tasks.size() << "" tasks, and ""
      << slave->operations.size() << "" operations"";
    return;
  }
{code}

Since these are {{hashmap<FrameworkID, hashmap<TaskID, Task>>}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-9753,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 26 17:46:45 UTC 2020,,,,,,,"0|z0bdxs:",9223372036854775807,,,,,,,,,,,Studio 4: RI-23 64,,,,,,,,,,,1.0,,1.9.1,,,,,,,,,"11/Feb/20 20:58;bmahler;https://reviews.apache.org/r/72116/","26/Feb/20 17:46;bmahler;{noformat}
commit a699b7d55a2ac1eb40599e9d908fa80bec352baf
Author: Benjamin Mahler <bmahler@apache.org>
Date:   Tue Feb 11 15:50:27 2020 -0500

    Fixed incorrect task count logging in master's agent draining logic.

    The code was incorrectly printing the framework count, since these
    are hashmap<FrameworkID, hashmap<TaskID, Task>>.

    Review: https://reviews.apache.org/r/72116
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Libprocess does not properly escape subprocess argument strings on Windows,MESOS-10093,13283589,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,greggomann,greggomann,05/Feb/20 22:48,06/May/20 15:39,29/Oct/20 16:32,,1.9.0,,,,,,,,,,,,,,,,,,,0,containerization,docker,mesosphere,windows,,,,,"When running some tests of Mesos on Windows, I discovered that the following command would not execute successfully when passed to the Docker containerizer in {{TaskInfo.command}}:
{noformat}
python -c ""print('hello world')""
{noformat}

The following error is found in the task sandbox:
{noformat}
  File ""<string>"", line 1
    ""print('hello
                ^
SyntaxError: EOL while scanning string literal
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-04-02 00:04:05.409,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 15:39:48 UTC 2020,,,,,,,"0|z0b7ag:",9223372036854775807,,,,,,,,,,,Studio 1: RI-24 65,Studio 1: RI-23 66,Studio 1: RI-23 67,Studio 1: RI-23 68,,,,,,,,2.0,,,,,,,,,,,"05/Feb/20 22:50;greggomann;On windows, we execute shell commands as arguments to {{cmd.exe}}. I ran the following test in the command prompt:
{noformat}
C:\Users\Administrator>cmd /c ""python -c \""print('hello world')\""""
  File ""<string>"", line 1
    ""print('hello
                ^
SyntaxError: EOL while scanning string literal

C:\Users\Administrator>cmd /c ""python -c ^""print('hello world')^""""
hello world
{noformat}

In libprocess, it looks like we currently escape double quotes using a backslash: https://github.com/apache/mesos/blob/4990d2cd6e76da340b30e200be0d700124dac2b1/3rdparty/stout/include/stout/os/windows/shell.hpp#L188-L191

Based on the above test, it appears that we should be escaping them with a caret instead.

NOTE that before merging such a change, we should confirm that changing this escaping behavior doesn't break Mesos containerizer tasks.","06/Feb/20 15:00;greggomann;I heard from [~kaysoky] that this may have been a relic from earlier days in Mesos-on-Windows development, when PowerShell was the intended default shell on that platform. This was later changed to {{cmd}} to reduce the resource overhead.","11/Feb/20 08:54;greggomann;https://reviews.apache.org/r/72107/","02/Apr/20 00:04;bmahler;Did some research and synced with [~greggomann] and [~asekretenko]. On windows processes can't actually receive an argument array and they're started using a full string command line; it's up to them to parse these command lines into arguments. Most programs use {{CommandLineToArgvW}}, especially those programs that are not windows only (e.g. some libc, java, or python program). However, cmd.exe *does not* use {{CommandLineToArgvW}} and therefore applying the quoting is not correct.

The fix will be to surface direct command line string utilities (windows only) alongside the argv style, and callers will have to choose when to use the command line string versions (e.g. if using cmd.exe).

We may need to do something special for docker style commands (e.g. ""docker run ... <command>""), but not sure yet.

Some cleanup patches to prepare for a fix:

https://reviews.apache.org/r/72273/
https://reviews.apache.org/r/72285/
https://reviews.apache.org/r/72286/
https://reviews.apache.org/r/72303/
https://reviews.apache.org/r/72304/","01/May/20 22:11;bmahler;Cleanups have landed, I have to move on to other work, but here are two WIP patches of fixes at the stout and libprocess layer to make this easier when we pick this back up:

https://reviews.apache.org/r/72460/
https://reviews.apache.org/r/72461/

Also needed is a patch for Mesos in the docker wrapper code, and in the containerization launch related code. Then we need to document for users up in CommandInfo how shell=true and shell=false works for Windows in particular.","06/May/20 15:39;asekretenko;Removed ""TargetVersion = 1.10"", as there seems to be no hard need  on one hand and no resources on the other hand to land this into 1.10.",,,,,,,,,,,,,,,,,,,,,,
CSI plugins reporting duplicated volumes will crash the agent.,MESOS-9956,13253789,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,chhsia0,chhsia0,chhsia0,29/Aug/19 21:52,27/Nov/19 12:06,29/Oct/20 16:32,30/Aug/19 11:07,,,,,,,,,1.8.2,1.9.0,,,,,storage,,,,,0,mesosphere,storage,,,,,,,"The CSI spec requires volumes to be uniquely identifiable by ID, and thus SLRP currently assumes that a {{ListVolumes}} call does not return duplicated volumes. However, if a SLRP uses a non-conforming CSI plugin that reports duplicated volumes, these volumes would corrupt the SLRP checkpoint and cause the agent to crash at the next reconciliation:
{noformat}
 F0829 07:13:55.171332 12721 provider.cpp:1089] Check failed: !checkpointedMap.contains(resource.disk().source().id()){noformat}
MESOS-9254 introduces periodic reconciliation which make this problem much easier to manifest.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-08-30 11:07:13.166,,,false,MESOS-9744,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 27 12:06:05 UTC 2019,,,,,,,"0|z06668:",9223372036854775807,,,,,bennoe,,,,,,Storage: RI-18 54,,,,,,,,,,,2.0,,1.9.0,,,,,,,,,"30/Aug/19 00:50;chhsia0;Review: [https://reviews.apache.org/r/71414/]","30/Aug/19 11:07;bennoe;{noformat}
commit 43b86da531a889b1c4b1d7ca6acb2eb924ea01e1 (origin/master, master)
Author: Chun-Hung Hsiao <chhsiao@apache.org>
Date:   Fri Aug 30 13:04:22 2019 +0200

    Gracefully handled duplicated volumes from non-conforming CSI plugins.
    
    If the SLRP uses a plugin that does not conform to the CSI spec and
    reports duplicated volumes, the duplicate would be removed.
    
    Review: https://reviews.apache.org/r/71414/


commit b18ce53fe8e49e6f030efe89e0976a9f72ad8b50 (1.9.x, origin/1.9.x)
Author: Chun-Hung Hsiao <chhsiao@apache.org>
Date:   Fri Aug 30 13:05:37 2019 +0200

    Gracefully handled duplicated volumes from non-conforming CSI plugins.
    
    If the SLRP uses a plugin that does not conform to the CSI spec and
    reports duplicated volumes, the duplicate would be removed.
    
    Review: https://reviews.apache.org/r/71414/
{noformat}","14/Nov/19 18:14;bbannier;Review for backport to {{1.8.x}}: https://reviews.apache.org/r/71769/","27/Nov/19 12:06;bbannier;Backport to {{1.8.x}}
{noformat}
commit 4ce75b824620d30fa8a0a50b2d493b96cd5e714d
Author: Chun-Hung Hsiao <chhsiao@apache.org>
Date:   Fri Aug 30 13:04:22 2019 +0200

    Gracefully handled duplicated volumes from non-conforming CSI plugins.
    
    If the SLRP uses a plugin that does not conform to the CSI spec and
    reports duplicated volumes, the duplicate would be removed.
    
    This is a backport of `43b86da531a889b1c4b1d7ca6acb2eb924ea01e1`. We
    are not backporting test changes as the original patch relies on
    periodic plugin polling.
    
    Review: https://reviews.apache.org/r/71769
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,
Master does not handle returning unreachable agents as draining/deactivated,MESOS-9934,13250344,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,kaysoky,kaysoky,kaysoky,12/Aug/19 18:11,13/Aug/19 23:29,29/Oct/20 16:32,13/Aug/19 23:29,,,,,,,,,1.9.0,,,,,,master,,,,,0,foundations,,,,,,,,"The master has two code paths for handling agent reregistration messages, one culminating in {{Master::___reregisterSlave}} and the other in {{Master::}}{{__reregisterSlave}}. The two paths are not continuations of each other.  Looks like we missed the double-underscore case in the initial implementation.  This is the path that unreachable agents take, when/if they come back to the cluster.  The result is that when unreachable agents are marked for draining, they do not get sent the appropriate message unless they are forced to reregister again (i.e. restarted manually).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-9753,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 13 23:29:48 UTC 2019,,,,,,,"0|z05l20:",9223372036854775807,,,,,greggomann,,,,,,Foundations: RI-17 Sprint 52,,,,,,,,,,,3.0,,,,,,,,,,,"13/Aug/19 02:36;kaysoky;Fix: https://reviews.apache.org/r/71275/","13/Aug/19 23:29;kaysoky;{code}
commit 6fcbd99108b19c8dd1cf7a0f45a59d48d7aacd7e
Author: Joseph Wu <josephwu@apache.org>
Date:   Mon Aug 12 18:36:59 2019 -0700

    Fixed agent draining when returning from unreachable state.
    
    This logic was missing from the initial implementation of agent
    draining.  When an agent became unreachable, and then reregistered
    with the master, the master would not properly deactivate or drain
    the agent.
    
    This also fixes a potential problem with checking the agent
    drain state too early in the case of pending operations.
    Operations are not reported to the master until after the agent
    reregisters, so agents with the RESOURCE_PROVIDER capability
    cannot be considered DRAINED until after the first
    UpdateSlaveMessage has arrived.
    
    Review: https://reviews.apache.org/r/71275
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
SlaveTest.DrainingAgentRejectLaunch is flaky,MESOS-9895,13245226,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,bbannier,bbannier,16/Jul/19 14:18,17/Jul/19 14:22,29/Oct/20 16:32,17/Jul/19 14:22,1.9.0,,,,,,,,1.9.0,,,,,,,,,,,0,flaky-test,test,,,,,,,"We saw {{SlaveTest.DrainingAgentRejectLaunch}} fail repeatedly on ASF Jenkins CI.
{noformat}
../../src/tests/slave_tests.cpp:12408: Failure
Failed to wait 15secs for runningUpdate2
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/19 14:17;bbannier;consoleText.txt;https://issues.apache.org/jira/secure/attachment/12974828/consoleText.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-9753,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 17 14:22:55 UTC 2019,,,,,,,"0|z04pyg:",9223372036854775807,,,,,greggomann,,,,,,Mesos Foundations: RI-16 Sp 50,,,,,,,,,,,1.0,,1.9.0,,,,,,,,,"17/Jul/19 12:03;bbannier;Review: https://reviews.apache.org/r/71092/","17/Jul/19 14:22;bbannier;{noformat}
commit 04a1992c289584481a1b0aa5b937bdc68c3d56fb
Author: Benjamin Bannier <bbannier@apache.org>
Date:   Wed Jul 17 13:54:24 2019 +0200

    Added additional synchronization to a previously flaky test.
    
    This test did not make sure that the agent's task status update manager
    had received a task status update acknowledgemtn before launching a
    subsequent task. The agent then, given its state, correctly rejected the
    task.
    
    This patch adds additional synchronization so the agent has finished
    processing the task status update like expected.
    
    Review: https://reviews.apache.org/r/71092

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos.UpdateFrameworkV0Test.SuppressedRoles is flaky.,MESOS-9882,13243104,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,asekretenko,mzhu,mzhu,04/Jul/19 00:13,17/Jul/19 16:09,29/Oct/20 16:32,08/Jul/19 14:35,,,,,,,,,1.9.0,,,,,,flaky,,,,,0,resource-management,,,,,,,,"Observed in CI, log attached.

{noformat}
mesos-ec2-ubuntu-14.04-SSL.Mesos.UpdateFrameworkV0Test.SuppressedRoles (from UpdateFrameworkV0Test)


Error Message
../../src/tests/master/update_framework_tests.cpp:1117
Mock function called more times than expected - returning directly.
    Function call: agentAdded(@0x7fb254001c40 32-byte object <90-7A 6C-85 B2-7F 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 F0-85 00-54 B2-7F 00-00>)
         Expected: to be called once
           Actual: called twice - over-saturated and active
Stacktrace
../../src/tests/master/update_framework_tests.cpp:1117
Mock function called more times than expected - returning directly.
    Function call: agentAdded(@0x7fb254001c40 32-byte object <90-7A 6C-85 B2-7F 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 F0-85 00-54 B2-7F 00-00>)
         Expected: to be called once
           Actual: called twice - over-saturated and active
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/19 00:13;mzhu;UpdateFrameworkV0Test.SuppressedRoles_badrun.txt;https://issues.apache.org/jira/secure/attachment/12973622/UpdateFrameworkV0Test.SuppressedRoles_badrun.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,2019-07-04 14:00:06.042,,,false,MESOS-8500,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 04 14:00:06 UTC 2019,,,,,,,"0|z04czk:",9223372036854775807,,,,,,,,,,,Resource Mgmt: RI-16 Sp 50,,,,,,,,,,,1.0,,1.9.0,,,,,,,,,"04/Jul/19 14:00;asekretenko;Fix: https://reviews.apache.org/r/71013/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos did not respond correctly when operations should fail,MESOS-9875,13242621,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,greggomann,yifan_xing,yifan_xing,01/Jul/19 23:29,10/Oct/19 01:05,29/Oct/20 16:32,25/Aug/19 04:07,,,,,,,,,1.9.0,,,,,,agent,,,,,0,foundations,mesosphere,,,,,,,"For testing persistent volumes with {{OPERATION_FAILED/ERROR}} feedbacks, we sshed into the mesos-agent and made it unable to create subdirectories in {{/srv/mesos/work/volumes}}, however, mesos did not respond any operation failed response. Instead, we received {{OPERATION_FINISHED}} feedback.

Steps to recreate the issue:

1. Ssh into a magent.
 2. Make it impossible to create a persistent volume (we expect the agent to crash and reregister, and the master to release that the operation is {{OPERATION_DROPPED}}):
 * cd /srv/mesos/work (if it doesn't exist mkdir /srv/mesos/work/volumes)
 * chattr -RV +i volumes (then no subdirectories can be created)

3. Launch a service with persistent volumes with the constraint of only using the magent modified above.

 

 

Logs for the scheduler for receiving `OPERATION_FINISHED`:

(Also see screenshot)

 

2019-06-27 21:57:11.879 [12768651|rdar://12768651] [Jarvis-mesos-dispatcher-105] INFO c.a.j.s.ServicePodInstance - Stored operation=4g3k02s1gjb0q_5f912b59-a32d-462c-9c46-8401eba4d2c1 and feedback=OPERATION_FINISHED in podInstanceID=4g3k02s1gjb0q on serviceID=yifan-badagents-1

 

* 2019-06-27 21:55:23: task reached state TASK_FAILED for mesos reason: REASON_CONTAINER_LAUNCH_FAILED with mesos message: Failed to launch container: Failed to change the ownership of the persistent volume at '/srv/mesos/work/volumes/roles/test-2/19b564e8-3a90-4f2f-981d-b3dd2a5d9f90' with uid 264 and gid 264: No such file or directory",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-9977,,,,,,,,"02/Jul/19 18:05;yifan_xing;Screen Shot 2019-06-27 at 15.07.20.png;https://issues.apache.org/jira/secure/attachment/12973464/Screen+Shot+2019-06-27+at+15.07.20.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,2019-07-02 07:08:07.773,,,false,MESOS-9705,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 25 04:07:19 UTC 2019,,,,,,,"0|hzzxgq:",9223372036854775807,,,,,jpeach@apache.org,,,,,,Foundations: RI-17 Sprint 52,Foundations: RI-17 Sprint 53,,,,,,,,,,8.0,,1.9.0,,,,,,,,,"02/Jul/19 07:08;jamespeach;{{f9330006-d885-4ef0-b2c7-c9c6fcc239e5}} is the persistence ID.
{{5fa5c810-2dd3-41cb-9633-a3ef404b08c4}} is the operation UUID.
{{honvr62494cqk_ff4e953f-0eca-4b41-a08d-ddea27980b14}} is the operation ID.

{noformat}

I0627 22:03:17.360236 3529210 slave.cpp:4282] Updated checkpointed operations from [ cfd6b624-996f-45d7-9aaf-9a13ab9714b4 (RESERVE for framework efd8f75d-25a9-4346-8c7b-d8c8c95ba328-22525, ID: honvr62494cqk_a5b92fff-5491-4616-8970-8c390265c009, latest state: OPERATION_FINISHED) ] to [ cfd6b624-996f-45d7-9aaf-9a13ab9714b4 (RESERVE for framework efd8f75d-25a9-4346-8c7b-d8c8c95ba328-22525, ID: honvr62494cqk_a5b92fff-5491-4616-8970-8c390265c009, latest state: OPERATION_FINISHED), 5fa5c810-2dd3-41cb-9633-a3ef404b08c4 (CREATE for framework efd8f75d-25a9-4346-8c7b-d8c8c95ba328-22525, ID: honvr62494cqk_ff4e953f-0eca-4b41-a08d-ddea27980b14, latest state: OPERATION_PENDING) ]
...
I0627 22:03:17.360723 3529210 slave.cpp:8670] Updating the state of operation 'honvr62494cqk_ff4e953f-0eca-4b41-a08d-ddea27980b14' (uuid: 5fa5c810-2dd3-41cb-9633-a3ef404b08c4) for framework efd8f75d-25a9-4346-8c7b-d8c8c95ba328-22525 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)
...
E0627 22:03:17.365811 3529210 slave.cpp:4257] EXIT with status 1: Failed to sync checkpointed resources: Failed to create the persistent volume f9330006-d885-4ef0-b2c7-c9c6fcc239e5 at '/srv/mesos/work/volumes/roles/test-3/f9330006-d885-4ef0-b2c7-c9c6fcc239e5': Operation not permitted
{noformat}


The relevant code sequence is in Slave::applyOperation, and looks roughly like this:

{noformat}
    track the new operation

    checkpointResourceState() (1)

    apply the operation (2)
    report that the operation was applied

    checkpointResourceState() (3)
{noformat}

The operation is checkpointed as pending in (1), but no resource changes are made yet. In (3), the operation is applied by making changes to the agent resources. At (3) the checkpointed resources discrepancy is discovered and the agent tries to create the persistent volume and fails.
","02/Jul/19 17:49;greggomann;Perhaps we can fix this in the short-term by simply moving the {{updateOperation()}} call after the call to {{checkpointResourceState()}}… although with current agent behavior, this would result in the agent crashing, then reconciling with master, and the scheduler would receive an {{OPERATION_DROPPED}} update for that operation, which isn’t accurate (but better than {{FINISHED}} I would say).

I think our current code isn’t going to handle this type of operation failure well; rather than crashing when checkpointing fails, I think we could simply send an {{OPERATION_FAILED}} update and allow the agent to continue running.","06/Aug/19 10:27;greggomann;It looks like the {{OPERATION_FINISHED}} update should only be sent after the agent fails over and recovers its checkpointed operations. We need to make sure that if the agent's call to `syncCheckpointedResources()` fails, which is the function that actually creates the persistent volume, then the operation in state OPERATION_FINISHED should not be recovered by the agent. Currently, it looks like the agent will fail to create the persistent volume, crash, and then recover the operation in state OPERATION_FINISHED and send the update.","06/Aug/19 16:12;greggomann;I'm trying to figure out how to address this with the current information that we checkpoint on the agent. The old-style checkpointing on the agent went like this:
1) Checkpoint resources to a ""target file""
2) Sync checkpointed resources to disk, which creates persistent volumes
3) If #2 succeeds, move the ""target file"" to the actual checkpoint location

When implementing operation feedback, we thought we could get away without this two-phase checkpointing, since we now have the operation feedback streams which we can use as another source of information. When recovering in the agent, we have some logic which inspects both the checkpointed resources/operations as well as the operation streams checkpointed by the operation status update manager in order to recover properly.

It's possible that we could use the old-style checkpointed resource files in order to accomplish recovery now (we still write those to disk to enable agent downgrades), but I'm worried that this will be confusing. But perhaps it's already confusing :)

I'll try to have a patch up by EOD with a solution for you to look at.","08/Aug/19 00:02;jamespeach;{noformat}
f9330006-d885-4ef0-b2c7-c9c6fcc239e5 is the persistence ID.
5fa5c810-2dd3-41cb-9633-a3ef404b08c4 is the operation UUID.
honvr62494cqk_ff4e953f-0eca-4b41-a08d-ddea27980b14 is the operation ID.

I0627 22:03:17.360236 3529210 slave.cpp:4282] Updated checkpointed operations from [ cfd6b624-996f-45d7-9aaf-9a13ab9714b4 (RESERVE for framework efd8f75d-25a9-4346-8c7b-d8c8c95ba328-22525, ID: honvr62494cqk_a5b92fff-5491-4616-8970-8c390265c009, latest state: OPERATION_FINISHED) ] to [ cfd6b624-996f-45d7-9aaf-9a13ab9714b4 (RESERVE for framework efd8f75d-25a9-4346-8c7b-d8c8c95ba328-22525, ID: honvr62494cqk_a5b92fff-5491-4616-8970-8c390265c009, latest state: OPERATION_FINISHED), 5fa5c810-2dd3-41cb-9633-a3ef404b08c4 (CREATE for framework efd8f75d-25a9-4346-8c7b-d8c8c95ba328-22525, ID: honvr62494cqk_ff4e953f-0eca-4b41-a08d-ddea27980b14, latest state: OPERATION_PENDING) ]

I0627 22:03:17.360723 3529210 slave.cpp:8670] Updating the state of operation 'honvr62494cqk_ff4e953f-0eca-4b41-a08d-ddea27980b14' (uuid: 5fa5c810-2dd3-41cb-9633-a3ef404b08c4) for framework efd8f75d-25a9-4346-8c7b-d8c8c95ba328-22525 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)

E0627 22:03:17.365811 3529210 slave.cpp:4257] EXIT with status 1: Failed to sync checkpointed resources: Failed to create the persistent volume f9330006-d885-4ef0-b2c7-c9c6fcc239e5 at '/srv/mesos/work/volumes/roles/test-3/f9330006-d885-4ef0-b2c7-c9c6fcc239e5': Operation not permitted

{noformat}","14/Aug/19 00:54;greggomann;Review: https://reviews.apache.org/r/71285/","25/Aug/19 04:07;jamespeach;{noformat}
commit 75a93fde264d2a591ec9c024354908e85012c271
Author: Greg Mann <greg@mesosphere.io>
Date:   Sat Aug 24 18:29:28 2019 -0700

    Fixed recovery of agent resources and operations after crash.

    Fixes an issue where the agent may incorrectly send an
    OPERATION_FINISHED update for a failed offer operation
    following agent failover and recovery.

    The agent previously relied on the difference between the
    set of checkpointed operations and the set of operation
    IDs recovered from the operation status update manager to
    apply any operations which had not been applied due to an
    ill-timed agent failover.

    However, this approach did not work in the case where a
    persistent volume failed to be successfully created by
    syncCheckpointedResources(). In order to handle this
    case, this patch changes the agent code to continue with
    the old approach of a two-phase-commit of persistent
    volumes to disk, where the agent will fail to complete
    recovery if syncCheckpointedResources() cannot be
    executed successfully after failover.

    Review: https://reviews.apache.org/r/71285/
{noformat}",,,,,,,,,,,,,,,,,,,,,
Simultaneous adding/removal of a role from framework's roles and its suppressed roles crashes the master.,MESOS-9870,13241985,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,asekretenko,asekretenko,asekretenko,27/Jun/19 16:17,03/Jul/19 22:01,29/Oct/20 16:32,03/Jul/19 21:09,,,,,,,,,1.5.4,1.6.3,1.7.3,1.8.1,1.9.0,,,,,,,0,resource-management,,,,,,,,"Calling UPDATE_FRAMEWORK with a new role added both to 'FrameworkInfo.roles` and `suppressed_roles` crashes the master.

The first place which doesn't expect this is increasing a `suppressed` allocator metric:
[https://github.com/apache/mesos/blob/fe7be9701e92d863734621ae1a3d339bb8598044/src/master/allocator/mesos/hierarchical.cpp#L507]
[
https://github.com/apache/mesos/blob/fe7be9701e92d863734621ae1a3d339bb8598044/src/master/allocator/mesos/metrics.cpp#L255]

Probably there are other similar places.
Adding a new role in a suppressed state via re-subscribing  should also trigger this bug - haven't checked it",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-9793,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-06-27 16:20:50.742,,,false,MESOS-8500,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 27 20:21:15 UTC 2019,,,,,,,"0|hzzxj8:",9223372036854775807,,,,,,,,,,,Resource Mgmt: RI 15 Sp 49,Resource Mgmt: RI-16 Sp 50,,,,,,,,,,5.0,,1.5.4,1.6.3,1.7.3,1.8.1,1.9.0,,,,,"27/Jun/19 16:20;bmahler;Marked this as a blocker for the 1.9.0 release.","27/Jun/19 16:42;asekretenko;Test for exposing the bug: [https://reviews.apache.org/r/70965/]

Looks like there are a couple more affected places in allocator's updateFramework().","27/Jun/19 20:21;asekretenko;There is one more problem which might be much more serious: removing a role from framework's roles and framework's suppressed roles in the same call also crashes the master.

Test via UPDATE_FRAMEWORK: [https://reviews.apache.org/r/70966/]

Re-subscribing also crashes (no wonder: they use the same code path) - I don't have the test completed.

Patches with a fix:
-[https://reviews.apache.org/r/70967/]-
-[https://reviews.apache.org/r/70968/]-

[https://reviews.apache.org/r/70994/]
[https://reviews.apache.org/r/70995/]",,,,,,,,,,,,,,,,,,,,,,,,,
Make PushGauges support floating point stats.,MESOS-9861,13241356,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bmahler,mzhu,mzhu,24/Jun/19 22:50,30/Jul/19 01:46,29/Oct/20 16:32,30/Jul/19 01:46,,,,,,,,,1.9.0,,,,,,metrics,,,,,0,foundations,resource-management,,,,,,,"Currently, PushGauges are modeled against counters. Thus it does not support floating point stats. This prevents many existing PullGauges to use it. We need to add support for floating point stat.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-07-29 21:58:13.702,,,false,MESOS-8914,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 30 01:46:23 UTC 2019,,,,,,,"0|z0427s:",9223372036854775807,,,,,,,,,,,Resource Mgmt: RI-16 51,,,,,,,,,,,1.0,,,,,,,,,,,"29/Jul/19 21:58;bmahler;https://reviews.apache.org/r/71187/","30/Jul/19 01:46;bmahler;{noformat}
commit db864ff4579371a7b7086f3789093ba27bb62279
Author: Benjamin Mahler <bmahler@apache.org>
Date:   Mon Jul 29 17:38:11 2019 -0400

    Updated PushGauge to work with double.

    This enables adoption in Mesos code, since many PullGauges are
    set with double values.

    Review: https://reviews.apache.org/r/71187
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
REVIVE call with specified role(s) clears filters for all roles of a framework.,MESOS-9856,13240914,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,asekretenko,bmahler,bmahler,21/Jun/19 16:49,03/Jul/19 15:34,29/Oct/20 16:32,21/Jun/19 19:24,,,,,,,,,1.5.4,1.6.3,1.7.3,1.8.1,1.9.0,,allocation,,,,,0,resource-management,,,,,,,,"As pointed out by [~asekretenko], the REVIVE implementation in the allocator incorrectly clears decline filters for all of the framework's roles, rather than only those that were specified in the REVIVE call:

https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L1392

This should only clear filters for the roles specified in the REVIVE call.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-06-21 17:41:33.338,,,false,MESOS-8500,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 21 17:41:33 UTC 2019,,,,,,,"0|z03zhs:",9223372036854775807,,,,,,,,,,,Resource Mgmt: RI 15 Sp 49,,,,,,,,,,,3.0,,1.5.4,1.6.3,1.7.3,1.8.1,1.9.0,,,,,"21/Jun/19 17:41;asekretenko;https://reviews.apache.org/r/70925/
https://reviews.apache.org/r/70926/",,,,,,,,,,,,,,,,,,,,,,,,,,,
/roles endpoint should return both guarantees and limits. ,MESOS-9854,13240757,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,mzhu,mzhu,mzhu,21/Jun/19 00:16,25/Jun/19 21:50,29/Oct/20 16:32,25/Jun/19 21:50,,,,,,,,,1.9.0,,,,,,,,,,,0,resource-management,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8068,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 25 21:49:54 UTC 2019,,,,,,,"0|z03yjc:",9223372036854775807,,,,,,,,,,,Resource Mgmt: RI 15 Sp 49,,,,,,,,,,,2.0,,,,,,,,,,,"25/Jun/19 21:49;mzhu;{noformat}
commit b23b4e52a24637231a85faf2416b75180cfd9063
Author: Meng Zhu mzhu@mesosphere.io
Date:   Thu Jun 20 17:17:41 2019 -0700


Made `/roles` endpoint also return quota limits.

Now that guarantees are decoupled from limits, we should
return limits and guarantees separately in the `/roles` endpoint.

Three incompatible changes are introduced:

- The `principal` field is removed. This legacy field was used to
record the principal of the operator who configured the quota.
So that later, if a different operator with a different principal
wants to modify the quota, the action can be properly authorized.
This use case has since been deprecated and the principal field
will no longer be filled going forward.

- Resources with zero quantity will no longer be included in
the `guarantee` field.

- The `guarantee` field will continue to be filled.
However, since we are decoupling the quota guarantee from the limit.
One can no longer assume that the limit will be the same as guarantee.
A separate `limit` field is introduced.

Before, the response might contain:
```
{
  ""quota"": {
    ""guarantee"": {
      ""cpus"": 1,
      ""disk"": 0,
      ""gpus"": 0,
      ""mem"": 512
    },
    ""principal"": ""test-principal"",
    ""role"": ""foo""
  }
}
```

After:
```
{
  ""quota"": {
    ""guarantee"": {
      ""cpus"": 1,
      ""mem"": 512
    },
    ""limit"": {
      ""cpus"": 1,
      ""mem"": 512
    },
    ""role"": ""foo""
  }
}
```

Also fixed an affected test.

Review: https://reviews.apache.org/r/70915
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate allocator metrics to PushGauge.,MESOS-9851,13240332,Bug,Open,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,mzhu,mzhu,19/Jun/19 01:22,31/Jul/19 16:28,29/Oct/20 16:32,,,,,,,,,,,,,,,,allocation,,,,,0,resource-management,,,,,,,,We should migrate all metrics in the master actor to use PushGauges instead of PullGauges for better performance.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8914,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-06-19 01:22:50.0,,,,,,,"0|hzzxeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Master should not report disconnected resource providers.,MESOS-9831,13237820,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,chhsia0,chhsia0,chhsia0,05/Jun/19 21:36,31/Jul/19 16:59,29/Oct/20 16:32,06/Jun/19 19:13,1.8.0,,,,,,,,1.8.1,1.9.0,,,,,master,,,,,0,mesosphere,mesosphere-dss-ga,storage,,,,,,"MESOS-9384 attempted to make the master to garbage-collect disconnected resource providers. However, if there are disconnected resource providers but none of the connected ones changes, the following code snippet would make the master ignore the agent update and skip the garbage collection:
https://github.com/apache/mesos/blob/2ae1296c668686d234be92b00bd7abbc0a6194b0/src/master/master.cpp#L8186-L8234
The condition to ignore the agent update will be triggered in one of the following conditions:
1. The resource provider has no resource, so the agent's total resource remains the same.
2. When the agent restarts and reregisters, its resource provider resources will be reset.

As a result, the master will still keep records for the disconnected resource providers and report them.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 06 19:13:56 UTC 2019,,,,,,,"0|hzzxf7:",9223372036854775807,,,,,bbannier,,,,,,,,,,,,,,,,,2.0,,1.8.1,1.9.0,,,,,,,,"06/Jun/19 04:56;chhsia0;Review: https://reviews.apache.org/r/70788/","06/Jun/19 19:13;chhsia0;{noformat}
commit 4c82acba45e31d2426adebdb7783b8bd762b6b0d
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Wed Jun 5 21:47:07 2019 -0700

Garbage-collected disappeared RPs when agent resources remain unchanged.

Previously when there is a missing resource provider in an
`UpdateSlaveMessage` but the agent's total resources remain unchanged,
the update message will be completely ignored, so the missing resource
provider will still be cached in the master's state, which is not the
desired behavior. This patch ensures that the master's state gets
updated if any resource provider is missing.

Review: https://reviews.apache.org/r/70788{noformat}
Backported to 1.8.1:
{noformat}
commit 9ad93574b54841b2dd30df58916159a0125a70b1{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Don't use reverse DNS for hostname validation,MESOS-9811,13237362,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bennoe,bennoe,bennoe,03/Jun/19 23:01,05/Jul/19 13:58,29/Oct/20 16:32,05/Jul/19 13:58,,,,,,,,,1.9.0,,,,,,,,,,,0,foundations,libprocess,ssl,,,,,,"Upon connection we first resolve the hostname and forget about it

https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/http.cpp#L1462-L1504

then later use reverse DNS on the remote address to get back a hostname

https://github.com/apache/mesos/blob/4708c2a368e12a89669135f47777d0dd05d9b0b2/3rdparty/libprocess/src/posix/libevent/libevent_ssl_socket.cpp#L548-L556

and verify the server certificate against *that*.

Instead, we should verify the server certificate against the hostname that was used by t he client to initiate the connection.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-9784,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 05 13:58:30 UTC 2019,,,,,,,"0|z03do0:",9223372036854775807,,,,,,,,,,,Mesos Foundations: RI15 Sp 48,Mesos Foundations: RI 15 Sp 49,Mesos Foundations: RI-16 Sp 50,,,,,,,,,5.0,,,,,,,,,,,"19/Jun/19 14:48;bennoe;Review: https://reviews.apache.org/r/70749/","05/Jul/19 13:58;bennoe;{noformat}
commit 0a081e01a3f4af8141a8085ed2f97ee85ea48fe1
Author: Benno Evers <bevers@mesosphere.com>
Date:   Wed Jun 19 15:49:11 2019 +0200

    Introduced RFC6125-compliant hostname validation scheme.
    
    This commit introduces a new libprocess SSL flag
    `hostname_validation_scheme`, which can be set to 'legacy'
    to select the previous hostname validation behaviour or to
    'openssl' to use standardized OpenSSL algorithms to handle
    hostname validation as part of the TLS handshake.
    
    As a nice side-effect, the new scheme gets rid of reverse DNS
    lookups during TLS connection establishment, which used to be
    a common source of hard-to-debug unresponsiveness in Mesos
    components.
    
    See `docs/ssl.md` in the follow-up commit for details of and
    differences between the schemes.
    
    Review: https://reviews.apache.org/r/70749
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak caused by an infinite chain of futures in `UriDiskProfileAdaptor`.,MESOS-9803,13236392,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,chhsia0,chhsia0,chhsia0,30/May/19 03:06,31/Jul/19 16:59,29/Oct/20 16:32,06/Jun/19 19:11,1.7.0,1.7.1,1.7.2,1.8.0,,,,,1.7.3,1.8.1,1.9.0,,,,storage,,,,,0,mesosphere,mesosphere-dss-ga,storage,,,,,,"Before MESOS-8906, {{UriDiskProfileAdaptor}} only update its promise for watchers if the polled profile matrix becomes larger in size, and this prevents the following code in the {{watch}} function from creating an infinite chain of futures when the profile matrix keeps the same:
https://github.com/apache/mesos/blob/fa410f2fb8efb988590f4da2d4cfffbb2ce70637/src/resource_provider/storage/uri_disk_profile_adaptor.cpp#L159-L160

However, the patch of MESOS-8906 removes the size check in the {{notify}} function to allow profile selectors to be updated. As a result, once the watch function is called, the returned future will be chained with a new promise every time a poll is made, hence creating a memory leak.

A jemalloc call graph for a 2hr trace is attached.",Review: https://reviews.apache.org/r/70766/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/19 03:16;chhsia0;_tmp_libprocess.dazuT6_profile.svg;https://issues.apache.org/jira/secure/attachment/12970263/_tmp_libprocess.dazuT6_profile.svg",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 06 19:11:56 UTC 2019,,,,,,,"0|hzzxf6:i",9223372036854775807,,,,,bbannier,,,,,,Storage: RI-14 Sprint 46,,,,,,,,,,,2.0,,1.7.3,1.8.1,1.9.0,,,,,,,"31/May/19 03:19;chhsia0;Review: https://reviews.apache.org/r/70766/

Also made a repro test in https://reviews.apache.org/r/70764/ for local testing.","06/Jun/19 19:11;chhsia0;{noformat}
commit 343b6d7f0d4b3024692fa7ed5669c9819658e382
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu May 30 15:13:44 2019 -0700

Fixed chaining futures infinitely in `UriDiskProfileAdaptor`.

Previously it is possible to have an infinite chain of futures when
`UriDiskProfileAdaptor::watch` is called: if the set of profiles remains
fixed for every poll, each poll would satisfy a promise that triggers
an asynchronous recursive call to `UriDiskProfileAdaptor::watch` again.

This patch fixes the problem by removing the asynchronous recursion.
Instead, we maintain a separated promise for each watcher that is never
associated to another promise. After each poll, we check if the current
set of profiles differs from the known set for a watcher, and satisfy
its own promise if so.

Review: https://reviews.apache.org/r/70766{noformat}
Backported to 1.8.1:
{noformat}
commit 1188b131a09e086f3a1510f16fe3053f0ae3a46a{noformat}
Backported to 1.7.3:
{noformat}
commit bc406048928095c21c0e1c8389ce60ab5549e84c{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Unpublishing a volume that is failed to publish crashes the agent with CSI v1.,MESOS-9729,13227798,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,chhsia0,chhsia0,chhsia0,12/Apr/19 20:57,12/Apr/19 22:23,29/Oct/20 16:32,12/Apr/19 22:23,1.8.0,,,,,,,,1.8.0,,,,,,,,,,,0,mesosphere,mesosphere-dss-ga,storage,,,,,,"The CSI v1 volume manager recovers a failed `publishVolume` call through `unpublishVolume`, which mistakenly assume that the target path, which is supposed to be created by the CSI plugin after a successful publishing, always exists. If volume publishing fails, a subsequent unpublishing would crash the agent with the following message:
{noformat}
F0412 20:20:12.254420  7540 v1_volume_manager.cpp:1161] Check failed: os::exists(targetPath){noformat}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-9534,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 12 22:23:26 UTC 2019,,,,,,,"0|z01qso:",9223372036854775807,,,,,bbannier,,,,,,Storage: RI-13 Sprint 44,,,,,,,,,,,1.0,,1.8.1,,,,,,,,,"12/Apr/19 21:59;chhsia0;Review: [https://reviews.apache.org/r/70468/]","12/Apr/19 22:23;chhsia0;{noformat}
commit dff1eac2e0d7485181b49d4ee93e50ac6ba83e63
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Fri Apr 12 14:43:09 2019 -0700

Fixed crash when recovering a volume failed to publish with CSI v1.

The CSI v1 volume manager falsely assumed that the target path always
exists when unpublishing a volume, which is not true if there is a
failure when publishing the volume.

Review: https://reviews.apache.org/r/70468{noformat}
Backported to 1.8.0:
{noformat}
commit 134eda9b1d537683994fa87acac0a96fdca4c730{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Agent crashes when SLRP recovers dropped operations.,MESOS-9661,13222387,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,chhsia0,chhsia0,chhsia0,18/Mar/19 18:12,18/Mar/19 20:47,29/Oct/20 16:32,18/Mar/19 20:47,1.7.2,,,,,,,,1.7.3,1.8.0,,,,,storage,,,,,0,mesosphere,mesosphere-dss-beta,storage,,,,,,"MESOS-9537 is fixed by persisting dropped operations in SLRP, but the recovery codepath doesn't account for that:
[https://github.com/apache/mesos/blob/master/src/resource_provider/storage/provider.cpp#L1278]
Which caused the agent to crash with the following message during SLRP recovery:
{noformat}
Reached unreachable statement at /pkg/src/mesos/src/resource_provider/storage/provider.cpp:1283{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 18 20:47:41 UTC 2019,,,,,,,"0|z00tvk:",9223372036854775807,,,,,bbannier,,,,,,Storage R11 Sprint 40,,,,,,,,,,,1.0,,1.7.3,1.8.0,,,,,,,,"18/Mar/19 18:20;chhsia0;Review: https://reviews.apache.org/r/70232/","18/Mar/19 20:47;chhsia0;{noformat}
commit 75ce143ada048b3cce75e470fdda3207d8e48957
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Mon Mar 18 11:14:36 2019 -0700

Fixed an agent crash bug when SLRP recovers dropped operations.

Review: https://reviews.apache.org/r/70232{noformat}
Backported to 1.7.3:
{noformat}
commit 9ce1418b79e92e8bbd1526902e4f70827907ba39{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Resource provider manager assumes all operations are triggered by frameworks,MESOS-9612,13218136,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,nfnt,bbannier,bbannier,26/Feb/19 17:14,01/Apr/19 10:45,29/Oct/20 16:32,27/Mar/19 12:55,,,,,,,,,1.8.0,,,,,,agent,,,,,0,mesosphere,mesosphere-dss-ga,storage,,,,,,"When the agent tries to apply an operation to resource provider resources, it invokes {{ResourceProviderManager::applyOperation}} which in turn invokes {{ResourceProviderManagerProcess::applyOperation}}. That function currently assumes that the received message contains a valid {{FrameworkID}},
{noformat}
 void ResourceProviderManagerProcess::applyOperation(
      const ApplyOperationMessage& message)                                                                                                                                                                                                     {
    const Offer::Operation& operation = message.operation_info();                                                                                                                                                                                 
    const FrameworkID& frameworkId = message.framework_id(); // `framework_id` is `optional`.
{noformat}

Since {{FrameworkID}} is not a trivial proto types, but instead one with a {{required}} field {{value}}, the message composed with the {{frameworkId}} below cannot be serialized which leads to a failure below which in turn triggers a {{CHECK}} failure in the agent's function interfacing with the manager.

A typical scenario where we would want to support operator API calls here is to destroy leftover persistent volumes or reservations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-03-08 13:35:15.997,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 27 12:55:59 UTC 2019,,,,,,,"0|z003qg:",9223372036854775807,,,,,bbannier,,,,,,Storage R11 Sprint 40,Storage: RI-12 Sprint 43,,,,,,,,,,5.0,,1.8.0,,,,,,,,,"08/Mar/19 13:35;nfnt;https://reviews.apache.org/r/70165/","27/Mar/19 12:55;bbannier;{noformat}
commit eb505c1a7d00fc1d591d9fad53a95995601e13ba
Author: Jan Schlicht <jan@mesosphere.io>
Date: Wed Mar 27 09:40:25 2019 +0100

Fixed operator operation handling with resource provider resources.

The resource provider manager didn't allow operations originating from
operator API calls. For speculatively applied operations, this is
allowed now.

Review: https://reviews.apache.org/r/70165/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Removing a resource provider with consumers breaks resource publishing.,MESOS-9607,13217870,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,chhsia0,bbannier,bbannier,25/Feb/19 16:38,05/Mar/19 04:53,29/Oct/20 16:32,05/Mar/19 04:53,,,,,,,,,1.7.3,1.8.0,,,,,agent,resource provider,,,,0,mesosphere-dss-ga,,,,,,,,"Currently, the agent publishes all resources considered ""used"" via the resource provider manager whenever it is asked to publish a subportion. If a resource provider with active users (e.g., tasks or even just executors) was removed, but a user stays around this will fail _any resource publishing_ on that node since a ""used"" resource provider is not subscribed.

We should either update the agent code to just deltas, or provide a workaround of the same effect in the resource provider manager.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-02-25 17:40:49.571,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 05 04:53:58 UTC 2019,,,,,,,"0|z00240:",9223372036854775807,,,,,bbannier,,,,,,Storage R11 Sprint 40,,,,,,,,,,,2.0,,1.7.3,1.8.0,,,,,,,,"25/Feb/19 17:40;chhsia0;We cannot use delta if we want to support resources without identifiers.

This could be solved via https://issues.apache.org/jira/browse/MESOS-9387?focusedCommentId=16695092&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16695092 and a few changes in {{Slave::publish resources}}.","28/Feb/19 00:31;chhsia0;As a first step, let's not block task launching if it does not use resources from a failed resource provider. And we can further improve this through MESOS-9387.","01/Mar/19 05:53;chhsia0;Reviews:
https://reviews.apache.org/r/70080/
https://reviews.apache.org/r/70081/","05/Mar/19 04:53;chhsia0;{noformat}
commit e2b16c1d0066cddaf1f4e5405c8a2e0a431e0197
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu Feb 28 21:23:13 2019 -0800

Changed the `getResourceProviderId` helper to take a `Resources`.

Review: https://reviews.apache.org/r/70080{noformat}
{noformat}
commit c2b094f1352e8ba9d5fc973269ea83271738d9a9
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Fri Mar 1 12:09:06 2019 -0800

Do not fail a task if it doesn't use resources from a failed provider.

`Slave::publishResources` will no longer ask all resource providers to
publish all allocated resources. Instead, it only asks those of the
task's resources to publish resources, so a failed resource provider
would only fail tasks that want to use its resources.

Review: https://reviews.apache.org/r/70081{noformat}
Backported to 1.7.3:
{noformat}
commit 79ed75719e98ba4e58803c2f64ccbbe6ac52b3f1
commit 8649cb0950dd63bc850c77f7d3dabbd443ce9c58{noformat}",,,,,,,,,,,,,,,,,,,,,,,,
"Status update streams for operations affecting agent default resources should be stored under ""meta/slaves/<slave_id>/operations/""",MESOS-9597,13217332,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gkleiman,gkleiman,gkleiman,22/Feb/19 01:55,27/Feb/19 00:15,29/Oct/20 16:32,27/Feb/19 00:15,,,,,,,,,1.8.0,,,,,,,,,,,0,foundations,mesosphere,,,,,,,"The streams are currently created under {{meta/operations/}} but not recovered if {{meta/slaves/latest}} doesn't exist.

After discussing this with [~greggomann] and with [~kaysoky], we agreed that they should be created under {{meta/slaves/<slave_id>/operations/}} instead.

NOTE: don't forget to add the corresponding entry in the ascii drawing in {{slave/paths.hpp}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8054,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 27 00:15:08 UTC 2019,,,,,,,"0|yi195c:",9223372036854775807,,,,,greggomann,,,,,,Mesos Foundations RI11 Sp 40,,,,,,,,,,,2.0,,1.8.0,,,,,,,,,"23/Feb/19 01:34;gkleiman;https://reviews.apache.org/r/70044/","27/Feb/19 00:15;gkleiman;{noformat}
commit 6324134f6a51d0227a06c233b1aa8dba16295851
Author: Gastón Kleiman <gaston@apache.org>
Date: Tue Feb 26 16:07:45 2019 -0800

Moved status update streams of operations on agent's default resources.

This patch moves the operation status update streams for operations
affecting agent default resources from the root of the metadata
directory to the agent's metadata directory, i.e.,
`meta/operations/<operation_uuid>` to
`meta/slaves/<slave_id>/operations/<operation_uuid>`.

Review: https://reviews.apache.org/r/70044/{noformat}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,
Operation status update streams are not properly garbage collected.,MESOS-9574,13215617,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gkleiman,gkleiman,gkleiman,13/Feb/19 22:48,27/Feb/19 00:14,29/Oct/20 16:32,27/Feb/19 00:14,,,,,,,,,1.8.0,,,,,,agent,,,,,0,foundations,mesosphere,,,,,,,"After successfully handling the acknowledgment of a terminal operation status update for an operation affecting agent's default resources, the agent should garbage collect the corresponding operation status update stream.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8054,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 27 00:14:31 UTC 2019,,,,,,,"0|yi0vn2:",9223372036854775807,,,,,greggomann,,,,,,Mesos Foundations RI11 Sp 40,,,,,,,,,,,2.0,,,,,,,,,,,"13/Feb/19 23:04;gkleiman;https://reviews.apache.org/r/69978/","27/Feb/19 00:14;gkleiman;{noformat}
commit 2835cdcb9c3167e862f991589c4ec2a27d8ba258
Author: Gastón Kleiman <gaston@apache.org>
Date: Tue Feb 26 16:07:39 2019 -0800

Added garbage collection of terminated operations status update streams.

Make the agent garbage collect an operation status update stream once
a terminal status update is acknowledged.

This patch also improves the logging of acknowledgement failures and the
readability of the `Slave::operationStatusAcknowledgement` method.

Review: https://reviews.apache.org/r/69978/{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Agent should not try to recover operation status update streams that haven't been created yet.,MESOS-9573,13215616,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gkleiman,gkleiman,gkleiman,13/Feb/19 22:44,27/Feb/19 00:14,29/Oct/20 16:32,27/Feb/19 00:14,,,,,,,,,1.8.0,,,,,,agent,,,,,0,foundations,mesosphere,,,,,,,"If the agent fails over after having checkpointed a new operation but before the operation status update stream is created, the recovery process will fail.

This happens because agent will try to recover the operation status update streams even if it hasn't been created yet.

In order to prevent recovery failures, the agent should obtain the ids of the streams to recover by walking the directory in which operation status updates streams are stored.

The agent should also garbage collect streams if the checkpointed state doesn't contain a corresponding operation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8054,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 27 00:14:04 UTC 2019,,,,,,,"0|yi0vn0:",9223372036854775807,,,,,greggomann,,,,,,Mesos Foundations RI11 Sp 40,,,,,,,,,,,2.0,,,,,,,,,,,"13/Feb/19 22:45;gkleiman;https://reviews.apache.org/r/69977/diff/1#index_header","27/Feb/19 00:14;gkleiman;{noformat}
commit dc3ad332131c39f57b557cb20ae4fc2fa3e17989
Author: Gastón Kleiman <gaston@apache.org>
Date:   Tue Feb 26 16:07:32 2019 -0800

    Improved agent operation recovery process.

    This patch makes the agent walk the operation status update streams
    directories in order to generate the list of streams to recover, instead
    of generating it from the checkpointed `ResourceState` message.

    This prevents the agent from asking the operation status update manager
    to recover streams that haven't been created yet.

    The patch also makes the agent garbage collect operation status update
    streams if no correspondng operation is present in the checkpointed
    state. This can happen after the agent fails over while processing the
    acknowledgement of a terminal operation status update.

    Review: https://reviews.apache.org/r/69977/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
SLRP does not clean up mount directories for destroyed MOUNT disks.,MESOS-9568,13215368,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,chhsia0,chhsia0,chhsia0,13/Feb/19 01:37,01/Apr/19 10:45,29/Oct/20 16:32,05/Mar/19 19:56,1.5.0,1.5.1,1.5.2,1.6.0,1.6.1,1.7.0,1.7.1,1.7.2,1.7.3,1.8.0,,,,,storage,,,,,0,mesosphere,mesosphere-dss-ga,storage,,,,,,"When staging or publishing a CSI volume, SLRP will create the following mount points for these operations:
{noformat}
<work_dir>/csi/<rp_type>/<rp_name>/mounts/<volume_id>/staging
<work_dir>/csi/<rp_type>/<rp_name>/mounts/<volume_id>/target
{noformat}
These directories are cleaned up when the volume is unpublished/unstaged. However, their parent directory, namly {{<work_dir>/csi/<rp_type>/<rp_name>/mounts/<volume_id>}} is never cleaned up.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 05 19:56:47 UTC 2019,,,,,,,"0|yi0x3k:",9223372036854775807,,,,,bbannier,,,,,,Storage R10 Sprint 39,Storage R11 Sprint 40,Storage: RI-12 Sprint 43,,,,,,,,,2.0,,1.7.3,1.8.0,,,,,,,,"13/Feb/19 05:56;chhsia0;Review: https://reviews.apache.org/r/69970/","05/Mar/19 19:56;chhsia0;{noformat}
commit 991cd7886dc4175c268faf47262dc420bb6cf4ad
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Tue Feb 12 20:49:42 2019 -0800

Made SLRP clean up mount directories for destroyed MOUNT disks.

When the SLRP stage/publish a CSI volume, it creates the staging/target
paths, but only the target path is cleaned up during unpublish.
Moreover, even if the CSI plugin does not support node-staging, the
current cleanup is still not enough, as the parent directory of the
target path is not removed. This patch fixes this problem.

Review: https://reviews.apache.org/r/69970{noformat}
Backported to 1.7.3:
{noformat}
commit d336d4add48a2776b9336dfea0e2ca915cc88e09{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
OPERATION_UNREACHABLE and OPERATION_GONE_BY_OPERATOR updates don't include the agent/RP IDs,MESOS-9559,13214262,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gkleiman,gkleiman,gkleiman,06/Feb/19 21:57,12/Feb/19 05:59,29/Oct/20 16:32,12/Feb/19 05:59,,,,,,,,,1.8.0,,,,,,master,,,,,0,foundations,mesosphere,operation-feedback,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-02-12 05:59:29.099,,,false,MESOS-8054,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 12 05:59:29 UTC 2019,,,,,,,"0|yi0qao:",9223372036854775807,,,,,greggomann,,,,,,Mesos Foundations RI10 Sp 39,,,,,,,,,,,2.0,,,,,,,,,,,"06/Feb/19 21:58;gkleiman;https://reviews.apache.org/r/69912/","12/Feb/19 05:59;greggomann;{code}
commit a77bf5ec1238e9147f5bdb3297886230f6a6fc62
Author: Gastón Kleiman <gaston@mesosphere.io>
Date:   Fri Feb 8 18:29:50 2019 -0800

    Added agent/RP IDs to some operation updates generated by the master.

    This patch makes the master include the agent and resource provider IDs
    in the OPERATION_GONE_BY_OPERATOR and OPERATION_UNREACHABLE operation
    status updates that it generates.

    Review: https://reviews.apache.org/r/69912/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Operations are leaked in Framework struct when agents are removed,MESOS-9557,13214218,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,kaysoky,greggomann,greggomann,06/Feb/19 18:12,13/Feb/19 22:47,29/Oct/20 16:32,13/Feb/19 22:47,,,,,,,,,1.8.0,,,,,,master,,,,,0,foundations,mesosphere,,,,,,,"Currently, when agents are removed from the master, their operations are not removed from the {{Framework}} structs. We should ensure that this occurs in all cases.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-02-13 02:06:20.364,,,false,MESOS-8054,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 13 22:47:35 UTC 2019,,,,,,,"0|hzzxxc:008",9223372036854775807,,,,,greggomann,,,,,,Mesos Foundations RI10 Sp 39,,,,,,,,,,,2.0,,,,,,,,,,,"13/Feb/19 02:06;kaysoky;Relatively small change, although it does make some logic in MESOS-9542 a bit simpler, since that cleanup will not need to worry about unknown agents anymore:
https://reviews.apache.org/r/69968/","13/Feb/19 22:47;kaysoky;{code}
commit 51db083d899ff8f444c8842b5349cc21b59995c4
Author: Joseph Wu <josephwu@apache.org>
Date:   Tue Feb 12 17:54:33 2019 -0800

    Copied operation removal logic in agent removal code path.
    
    There are two code paths the master can follow to remove an agent
    from its memory:
    - When the agent sends an UnregisterSlaveMessage or tries to register
      as a new agent,
    - Or, when an agent is marked unreachable/gone.
    
    The first code path did not clean up operation state completely,
    resulting in leaking some memory in the master's Framework structs.
    
    This is direct copy of the operation cleanup logic in
    `Master::__removeSlave()`.
    
    Review: https://reviews.apache.org/r/69968
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
SLRP does not clean up destroyed persistent volumes.,MESOS-9544,13212968,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,chhsia0,chhsia0,chhsia0,31/Jan/19 02:29,13/Feb/19 03:57,29/Oct/20 16:32,13/Feb/19 03:57,1.5.0,1.5.1,1.5.2,1.6.0,1.6.1,1.7.0,1.7.1,,1.7.2,1.8.0,,,,,storage,,,,,0,mesosphere,mesosphere-dss-beta,storage,,,,,,"When a persistent volume created on a {{ROOT}} disk is destroyed, the agent will clean up its data: https://github.com/apache/mesos/blob/f44535bca811720fc272c9abad2bc78652d61fe3/src/slave/slave.cpp#L4397
However, this is not the case for PVs on SLRP disks. The agent relies on the SLRP to do the cleanup:
https://github.com/apache/mesos/blob/f44535bca811720fc272c9abad2bc78652d61fe3/src/slave/slave.cpp#L4472
But SLRP simply updates its metadata and do nothing:
https://github.com/apache/mesos/blob/f44535bca811720fc272c9abad2bc78652d61fe3/src/resource_provider/storage/provider.cpp#L2805

This would lead to data leakage if the framework does not call `CREATE_DISK` but just unreserve it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-9565,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 13 03:57:49 UTC 2019,,,,,,,"0|yi0ibs:",9223372036854775807,,,,,bbannier,,,,,,Storage R10 Sprint 39,,,,,,,,,,,5.0,,1.7.2,1.8.0,,,,,,,,"05/Feb/19 08:09;chhsia0;Patches targeted for backporting:

https://reviews.apache.org/r/69892/
https://reviews.apache.org/r/69893/
https://reviews.apache.org/r/69894/

Patches for testing:

https://reviews.apache.org/r/69895/
https://reviews.apache.org/r/69896/
https://reviews.apache.org/r/69897/
https://reviews.apache.org/r/69898/

Planning to add the following more tests:

DestroyPersistentMountVolumeFailed
DestroyUnpublishedPersistentMountVolume
DestroyUnpublishedPersistentMountVolumeWithRecovery
DestroyUnpublishedPersistentMountVolumeWithReboot
CreatePersistentBlockVolume
RecoverPublishedVolumeFailed","06/Feb/19 05:39;chhsia0;Two more patches for testing:

https://reviews.apache.org/r/69904/ (refactoring)
https://reviews.apache.org/r/69905/ (DestroyPersistentMountVolumeFailed)","12/Feb/19 05:30;chhsia0;Two more patches for testing:

https://reviews.apache.org/r/69954/ (CreateDestroyPersistentBlockVolume)
https://reviews.apache.org/r/69955/ (DestroyUbpublishedPersistentVolume{,WithRecovery,WithReboot}","12/Feb/19 05:47;chhsia0;I've paused the development of the {{RecoverPublishedVolumeFailed}} test. I have some WIP patches and plan to integrate them into MESOS-8745, then we can use the {{GET_RESOURCE_PROVIDRES}} call to verify the resource provider recovery failure in the test.","12/Feb/19 05:58;chhsia0;To track the work more easily, I've broken this ticket into two. This one would represent the fix that needs to be backported. The work for unit tests will be tracked in MESOS-9565.","13/Feb/19 03:57;chhsia0;{noformat}
commit 7642851988e29f42f311175fb2d99e8e9736f8c6
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Fri Feb 1 15:29:33 2019 -0800

Made SLRP recover node-published volumes after reboot.

If a CSI volume has been node-published before a reboot, SLRP will now
try to bring it back to node-published again. This is important to
perform synchronous persistent volume cleanup for `DESTROY`.

To achieve this, in addition to keeping track of the boot ID when a CSI
volume is node-staged in `VolumeState.vol_ready_boot_id` (formerly
`VolumeState.boot_id`), SLRP now also keeps track of the boot ID when
the volume is node-published. This helps SLRP to better determine if a
volume has been published before reboot.

Review: https://reviews.apache.org/r/69892{noformat}
{noformat}
commit 0be5d4ffbefc8f2a7c71f76c2e6bd4785df2ed3c
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu Jan 31 14:36:11 2019 -0800

Cleaned up persistent volumes on SLRP disks.

This patch limits SLRP to only support persistent volumes on MOUNT
disks, and makes it clean up data in persistent volumes when processing
`DESTROY` operations.

NOTE: Persistent volumes backed by CSI disks that are created before
upgrading to a Mesos version that does not include this fix are subject
to data leakage. To ensure data security, these persistent volume must
be consumed by a task at least once after the upgrade before being
destroyed.

Review: https://reviews.apache.org/r/69893{noformat}
{noformat}
commit c60fd87e0e2871afa2508d77358cfad29aba86fd
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Mon Feb 4 23:26:20 2019 -0800

Disallowed `DESTROY_DISK` on persistent volumes.

`DESTROY_DISK` would bypass persistent volume cleanup and directly ask
the CSI plugin to delete the backed volume. Since the CSI spec does not
require the plugin to do data cleanup, to avoid data leakage, we require
that if there is persistent volume on the CSI volume, it should be
destroyed first.

Review: https://reviews.apache.org/r/69894{noformat}
Backported to 1.7.2:
{noformat}
commit 5fe817fab4e8b85a517a0190014b1eb2c17c9f76
commit d6c9c0377c70d47ed5ed941fbaa54bfab0a1fd8c
commit d8458ae078862e706f985b2749d987f5c5cb3823{noformat}
Unit tests will be added in MESOS-9565.",,,,,,,,,,,,,,,,,,,,,,
Hierarchical allocator check failure when an operation on a shutdown framework finishes,MESOS-9542,13212521,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,kaysoky,bbannier,bbannier,29/Jan/19 12:09,06/Mar/19 20:07,29/Oct/20 16:32,26/Feb/19 19:51,1.5.0,1.5.1,1.5.2,1.6.0,1.6.1,1.7.0,1.7.1,1.8.0,1.8.0,,,,,,master,,,,,0,foundations,mesosphere,mesosphere-dss-ga,operation-feedback,,,,,"When a non-speculated operation like e.g., {{CREATE_DISK}} becomes terminal after the originating framework was torn down, we run into an assertion failure in the allocator.
{noformat}
I0129 11:55:35.764394 57857 master.cpp:11373] Updating the state of operation 'operation' (uuid: 10a782bd-9e60-42da-90d6-c00997a25645) for framework a4d0499b-c0d3-4abf-8458-73e595d061ce-0000 (latest state: OPERATION_PENDING, status update state: OPERATION_FINISHED)
F0129 11:55:35.764744 57925 hierarchical.cpp:834] Check failed: frameworks.contains(frameworkId){noformat}
With non-speculated operations like e.g., {{CREATE_DISK}} it became possible that operations outlive their originating framework. This was not possible with speculated operations like {{RESERVE}} which were always applied immediately by the master.

The master does not take this into account, but instead unconditionally calls {{Allocator::updateAllocation}} which asserts that the framework is still known to the allocator.

Reproducer:
 * register a framework with the master.
 * add a master with a resource provider.
 * let the framework trigger a non-speculated operation like {{CREATE_DISK.}}
 * tear down the framework before a terminal operation status update reaches the master; this causes the master to e.g., remove the framework from the allocator.
 * let a terminal, successful operation status update reach the master
 * 💥 

To solve this we should cleanup the lifetimes of operations. Since operations can outlive their framework (unlike e.g., tasks), we probably need a different approach here.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-02-13 02:12:38.713,,,false,MESOS-8054,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 26 20:30:31 UTC 2019,,,,,,,"0|hzzxxc:07",9223372036854775807,,,,,greggomann,,,,,,Mesos Foundations RI10 Sp 39,Mesos Foundations RI11 Sp 40,,,,,,,,,,5.0,,1.8.0,,,,,,,,,"13/Feb/19 02:12;kaysoky;Still in progress, but some reviews are up starting at: https://reviews.apache.org/r/69960/","26/Feb/19 19:51;kaysoky;{code}
commit aa75504c4b7baaf2c6d4af4829f3ea98982e291e
Author: Joseph Wu <josephwu@apache.org>
Date:   Tue Feb 12 11:48:44 2019 -0800

    Added the concept of ""orphaned operations"" to the master.
    
    An orphaned operation is a non-speculative operation whose
    originating framework is unknown.  These operations will consume
    resources until they are terminated, but will have no entry
    in the allocator because their associated framework does not exist.
    
    To account for resources used by orphaned operations, the operation's
    resources are removed from the agent's total resources upon being
    orphaned.
    
    This commit handles one of the two possible code paths which can
    introduce orphaned operations.
    
    Review: https://reviews.apache.org/r/69960
{code}
{code}
commit 32e4a404174e8d6b32b23a64569ae44ecd8e4351
Author: Joseph Wu <josephwu@apache.org>
Date:   Tue Feb 12 14:06:31 2019 -0800

    Handle possible orphaned operations after master/agent failover.
    
    This is one of two possible code paths which can introduce orphaned
    operations.
    
    When a master failover occurs, all agents and frameworks must
    reregister with the master.  Agents that reregister will report their
    operations with an UpdateSlaveMessage.  Any operations without a
    known framework will be considered orphans.  Known frameworks are
    discovered when the framework reregisters, or an agent running a task
    under the framework reregisters.  The race between these reregistrations
    will be addressed in a separate commit.
    
    Agent failover can also introduce orphans, if the agent has a pending
    operation during failover, and then is migrated to a separate master
    before restarting.  This will be handled the same way as agent
    reregistration after a master failover.
    
    Review: https://reviews.apache.org/r/69961
{code}
{code}
commit 2f0ed6dcd7762eee52d0152d09210b138ae8628a
Author: Joseph Wu <josephwu@apache.org>
Date:   Tue Feb 12 14:19:05 2019 -0800

    Added cleanup logic for orphaned operations.
    
    The resources used by orphaned operations are not accounted for
    in the Slave struct's `usedResources`, and must be treated differently
    when cleaning up the operation.  Removal of non-terminal orphan
    operations will instead augment the `totalResources`.
    
    NOTE: The only codepath that can remove non-terminal operations is
    when removing the agent or a resource provider.  In this case,
    there is no need to update the allocator with the augmented
    `totalResources` because the removal codepaths will already
    update the allocator.
    
    Review: https://reviews.apache.org/r/69962
{code}
{code}
commit df9be400cefe99f73fb5f2af9c51127caede845c
Author: Joseph Wu <josephwu@apache.org>
Date:   Tue Feb 12 14:58:33 2019 -0800

    Handled terminal operation status updates for orphans.
    
    When an orphaned operation is transitioned from non-terminal to
    terminal, the operation's resources must be added back to the agent's
    total resources, while accounting for resource conversion if the
    operation is successful.
    
    There is an odd case where an orphan is transitioned to terminal
    via an UpdateSlaveMessage (instead of UpdateOperationStatusMessage).
    When this happens, the required resource math is actually done by
    the agent.
    
    Review: https://reviews.apache.org/r/69963
{code}
{code}
commit fd9d99cea0915b6006a396a5926322ca1352ccd4
Author: Joseph Wu <josephwu@apache.org>
Date:   Tue Feb 12 16:08:32 2019 -0800

    Added a recovery path for orphan operation.
    
    An orphan can be recovered if the originating framework reregisters
    with the master.  When this happens, the resource accounting is
    reversed and resources are added back to the agent's total
    and the allocator.
    
    Review: https://reviews.apache.org/r/69967
{code}
{code}
commit a0baa345b2eb6483648fcbcd7af8aa9eac9148c3
Author: Joseph Wu <josephwu@apache.org>
Date:   Wed Feb 13 15:00:51 2019 -0800

    Modified when master responds to operation status updates.
    
    When dealing with orphaned operation status updates, there are two
    cases the master must deal with:
    - The simple case is when the master knows the framework is completed.
      These status updates can be acknowledged by the master.
    - However, a completed framework can be rotated out of the master's
      memory.  In addition, after master failover, if an agent reregisters
      before the framework, an operation can appear to be orphaned until
      the framework reregisters.
    
    This adds a fixed delay between agent reregistration and when the
    master acknowledges operation status updates from unknown frameworks.
    The delay should give frameworks ample time to reregister.
    
    The delay is based on agent reregistration in order to mitigate the
    delay of acknowledging status updates of frameworks rotated out of
    the completed frameworks buffer.
    
    Review: https://reviews.apache.org/r/69980
{code}
{code}
commit 44c439baae0c273025f183ecf26e4b1bc4be8262
Author: Joseph Wu <josephwu@apache.org>
Date:   Tue Feb 19 14:44:47 2019 -0800

    Removed operations when removing resource providers.
    
    When a resource provider explicitly disconnects from the agent, the
    agent will send a `UpdateSlaveMessage` to the master, telling the
    master to remove the resource provider.  If there are any operations
    associated with the resource provider, they must be removed too,
    because there is no way to make forward progress on resource provider
    operations without a resource provider.
    
    This removes a potential memory leak in the master's Framework structs.
    
    Review: https://reviews.apache.org/r/70014
{code}
{code}
commit 529ec2783159b9b1517a49271e8b30e87deebfec
Author: Joseph Wu <josephwu@apache.org>
Date:   Wed Jan 30 18:32:38 2019 -0800

    Accounted for possible lack of OperationID in RP responses.
    
    Operations sent to a resource provider may include an OperationID
    in addition to an Operation UUID.  The UUID is sufficient to uniquely
    identify the operation, so it is possible for the resource provider
    to omit the OperationID in its responses to the agent.
    
    This commit lets the agent fill in any missing ID before updates
    are acted upon, such as being sent to the master.
    
    Review: https://reviews.apache.org/r/69872
{code}
{code}
commit 01d4c74f30cf32431c0bd70e5743dcdffab91b0e
Author: Joseph Wu <josephwu@apache.org>
Date:   Wed Jan 30 17:32:31 2019 -0800

    Added test for tearing down frameworks while creating disks.
    
    The CREATE_DISK and DESTROY_DISK operations are ""non-speculative""
    operations, which means the master must wait for the operations to
    complete successfully before the master can update its resources.
    Because the master must wait to update the results of non-speculative
    operations, it is possible for the framework making the
    CREATE/DESTROY_DISK to be torn down before the operation completes.
    
    This commit adds a test to make sure the master can gracefully handle
    such a case.
    
    Review: https://reviews.apache.org/r/69869
{code}
{code}
commit 8174ccf0983c82e21ca103bc87a5f78d5bff4971
Author: Joseph Wu <josephwu@apache.org>
Date:   Tue Feb 19 16:41:51 2019 -0800

    Added test for terminal operation updates after master failover.
    
    This test covers a corner case where an agent reregisters with the
    master with a pending operation, but the operation's originating
    framework is unknown.  This can occur in a variety of situations like:
      * the master fails over and a framework never reregisters,
      * a completed framework is rotated out of the master's memory with
        pending operations, or
      * an agent with pending operations is migrated from one cluster to
        another.
    
    In this case, the master should ""adopt"" the orphan operation only
    after a delay.  This gives the framework some time to reregister.
    But if the framework does not reregister in time, the master will
    be in charge of acknowledging operation status updates.
    
    Review: https://reviews.apache.org/r/70040
{code}","26/Feb/19 20:30;bbannier;Note that while this crash is disruptive, after the master is restarted it will reconcile with the agent in question just fine and correctly reflect its state.",,,,,,,,,,,,,,,,,,,,,,,,,
SLRP sends inconsistent status updates for dropped operations.,MESOS-9537,13211924,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,chhsia0,chhsia0,chhsia0,25/Jan/19 20:21,13/Feb/19 03:54,29/Oct/20 16:32,13/Feb/19 03:54,1.5.0,1.5.1,1.5.2,1.6.0,1.6.1,1.7.0,1.7.1,,1.7.2,1.8.0,,,,,storage,,,,,0,mesosphere,mesosphere-dss-ga,storage,,,,,,"The bug manifests in the following scenario:
1. Upon receiving profile updates, the SLRP sends an {{UPDATE_STATE}} to the agent with a new resource version.
2. At the same time, the agent sends an {{APPLY_OPERATION}} to the SLRP with the original resource version.
3. The SLRP asks the status update manager (SUM) to reply with an {{OPERATION_DROPPED}} to the framework because of the resource version mismatch. The status update is required to be acked. Then, it simply discards the operation (i.e., no bookkeeping).
4. The agent finds a missing operation in the {{UPDATE_STATE}} so it sends a {{RECONCILE_OPERATIONS}}.
5. The SLRP asks the SUM to reply with an {{OPERATION_DROPPED}} to the agent (without a framework ID set) because it no longer knows about the operation.
6. The SUM returns an error because the latter {{OPERATION_DROPPED}} is inconsistent with the earlier one since it does not have a framework ID.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 13 03:54:38 UTC 2019,,,,,,,"0|yi0bxk:",9223372036854775807,,,,,bbannier,,,,,,Storage R10 Sprint 38,Storage R10 Sprint 39,,,,,,,,,,3.0,,1.7.2,1.8.0,,,,,,,,"30/Jan/19 22:50;chhsia0;Reviews:
https://reviews.apache.org/r/69858/
https://reviews.apache.org/r/69866/","13/Feb/19 03:54;chhsia0;{noformat}
commit 087b2358496f08dd6a8b8d9852793977a7c574e5
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Tue Jan 29 21:28:48 2019 -0800

Persisted intentionally dropped operations in SLRP.

If an operation is dropped intentionally (e.g., because of a resource
version mismatch), the operation should be persisted so no conflicting
status update would be generated for operation reconciliation.

Review: https://reviews.apache.org/r/69858{noformat}
{noformat}
commit e7f4e6100c7987fc2c2b031b2ec5ba17737ef76e
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Tue Jan 29 15:48:36 2019 -0800

Updated SLRP test `ProfileDisappeared` to request operation feedback.

This patch updates `StorageLocalResourceProviderTest.ProfileDisappeared`
to use the v1 scheduler API to request operation feedback, so MESOS-9537
would be triggered when an outstanding `UPDATE_STATE` call from the
resource provider races with an offer operation.

Review: https://reviews.apache.org/r/69866{noformat}
Backported to 1.7.2:
{noformat}
commit 1d2471d7c1eba8f046aec635054662430f2653df{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
CniIsolatorTest.ROOT_CleanupAfterReboot is flaky.,MESOS-9533,13211147,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,23/Jan/19 01:00,13/Feb/19 17:37,29/Oct/20 16:32,13/Feb/19 17:37,1.8.0,,,,,,,,1.4.3,1.5.3,1.6.2,1.7.2,1.8.0,,cni,containerization,,,,0,cni,flaky-test,,,,,,,"{noformat}
Error Message
../../src/tests/containerizer/cni_isolator_tests.cpp:2685
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7fffc7c05aa0, @0x7fe637918430 136-byte object <80-24 29-45 E6-7F 00-00 00-00 00-00 00-00 00-00 3E-E8 00-00 00-00 00-00 00-B8 0E-20 F0-55 00-00 C0-03 07-18 E6-7F 00-00 20-17 05-18 E6-7F 00-00 10-50 05-18 E6-7F 00-00 50-D1 04-18 E6-7F 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 F0-89 16-E9 58-2B D7-41 00-00 00-00 01-00 00-00 18-00 00-00 0B-00 00-00>)
         Expected: to be called 3 times
           Actual: called 4 times - over-saturated and active
Stacktrace
../../src/tests/containerizer/cni_isolator_tests.cpp:2685
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7fffc7c05aa0, @0x7fe637918430 136-byte object <80-24 29-45 E6-7F 00-00 00-00 00-00 00-00 00-00 3E-E8 00-00 00-00 00-00 00-B8 0E-20 F0-55 00-00 C0-03 07-18 E6-7F 00-00 20-17 05-18 E6-7F 00-00 10-50 05-18 E6-7F 00-00 50-D1 04-18 E6-7F 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 F0-89 16-E9 58-2B D7-41 00-00 00-00 01-00 00-00 18-00 00-00 0B-00 00-00>)
         Expected: to be called 3 times
           Actual: called 4 times - over-saturated and active
{noformat}

It was from this commit https://github.com/apache/mesos/commit/c338f5ada0123c0558658c6452ac3402d9fbec29",centos-6 with SSL enabled,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-01-30 08:35:03.7,,,false,MESOS-6755,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 30 19:45:30 UTC 2019,,,,,,,"0|yi0754:",9223372036854775807,,,,,jieyu,,,,,,Containerization RI10 Spr 38,Containerization RI10 Spr 39,,,,,,,,,,2.0,,,,,,,,,,,"24/Jan/19 00:16;gilbert;commit b54b493d6d6ae479d2154ed05f9615a11d662391
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Jan 22 17:13:15 2019 -0800

    Fixed a test flakiness in ROOT_CleanupAfterReboot.
    
    Review: https://reviews.apache.org/r/69809","30/Jan/19 08:35;alexr;I've reopened this because I have observed the same failure on the {{1.7.x}} branch. I've also set up fix versions to match those in MESOS-9518 since I suppose that are the branches where the test have been back introduced.","30/Jan/19 19:03;gilbert;yea, did not realize the test was backported. I will backport the fix now","30/Jan/19 19:45;gilbert;Done with backporting:

commit 53d99735229d377f47582d93df2435b8f7970778
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jan 30 11:37:42 2019 -0800

    Added MESOS-9533 to 1.4.3 CHANGELOG.

commit 3acd35c9fa58f1888efa70dc9c118a9b923cc052
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jan 30 11:37:26 2019 -0800

    Added MESOS-9533 to 1.5.3 CHANGELOG.

commit 16eff2bb1c1acf1bd5022e585f06ef8e12cdc569
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jan 30 11:37:09 2019 -0800

    Added MESOS-9533 to 1.6.2 CHANGELOG.

commit 94796bcab7b1382ead699278e4dfb9505d9dd017
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jan 30 11:06:21 2019 -0800

    Added MESOS-9533 to 1.7.2 CHANGELOG.",,,,,,,,,,,,,,,,,,,,,,,,
"SLRP should treat gRPC timeouts as non-terminal errors, instead of reporting OPERATION_FAILED.",MESOS-9517,13208579,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,chhsia0,jdef,jdef,09/Jan/19 16:49,29/Jan/19 21:26,29/Oct/20 16:32,29/Jan/19 21:23,1.5.0,1.5.1,1.5.2,1.6.0,1.6.1,1.7.0,1.7.1,,1.7.2,1.8.0,,,,,resource provider,storage,,,,0,mesosphere,mesosphere-dss-ga,storage,,,,,,"1. framework executes a CREATE_DISK operation.
2. The SLRP issues a CreateVolume RPC to the plugin
3. The RPC call times out
4. The agent/SLRP translates non-terminal gRPC timeout errors (DeadlineExceeded) for ""CreateVolume"" calls into OPERATION_FAILED, which is terminal.
5. framework receives a *terminal* OPERATION_FAILED status, so it executes another CREATE_DISK operation.
6. The second CREATE_DISK operation does not timeout.
7. The first CREATE_DISK operation was actually completed by the plugin, unbeknownst to the SLRP.
8. There's now an orphan volume in the storage system that no one is tracking.

Proposed solution: the SLRP makes more intelligent decisions about non-terminal gRPC errors. For example, timeouts are likely expected for potentially long-running storage operations and should not be considered terminal. In such cases, the SLRP should NOT report OPERATION_FAILED and instead should re-issue the **same** (idempotent) CreateVolume call to the plugin to ascertain the status of the requested volume creation.

Agent logs for the 3 orphan vols above:
{code}
[jdefelice@ec101 DCOS-46889]$ grep -e 3bd1a1a9-43d3-485c-9275-59cebd64b07c agent.log
Jan 09 11:10:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:10:27.896306 13189 provider.cpp:1548] Received CREATE_DISK operation 'a1BdfrEhy4ZLSNPZbDrzp1h-0' (uuid: 3bd1a1a9-43d3-485c-9275-59cebd64b07c)
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: E0109 11:11:27.904057 13190 provider.cpp:1605] Failed to apply operation (uuid: 3bd1a1a9-43d3-485c-9275-59cebd64b07c): Deadline Exceeded
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.904058 13192 status_update_manager_process.hpp:152] Received operation status update OPERATION_FAILED (Status UUID: 8c1ddad1-4adb-4df5-91fe-235d265a71d8) for operation UUID 3bd1a1a9-43d3-485c-9275-59cebd64b07c (framework-supplied ID 'a1BdfrEhy4ZLSNPZbDrzp1h-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.904331 13192 status_update_manager_process.hpp:929] Checkpointing UPDATE for operation status update OPERATION_FAILED (Status UUID: 8c1ddad1-4adb-4df5-91fe-235d265a71d8) for operation UUID 3bd1a1a9-43d3-485c-9275-59cebd64b07c (framework-supplied ID 'a1BdfrEhy4ZLSNPZbDrzp1h-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.947286 13189 slave.cpp:7696] Handling resource provider message 'UPDATE_OPERATION_STATUS: (uuid: 3bd1a1a9-43d3-485c-9275-59cebd64b07c) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 (latest state: OPERATION_FAILED, status update state: OPERATION_FAILED)'
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.947376 13189 slave.cpp:8034] Updating the state of operation 'a1BdfrEhy4ZLSNPZbDrzp1h-0' (uuid: 3bd1a1a9-43d3-485c-9275-59cebd64b07c) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 (latest state: OPERATION_FAILED, status update state: OPERATION_FAILED)
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.947407 13189 slave.cpp:7890] Forwarding status update of operation 'a1BdfrEhy4ZLSNPZbDrzp1h-0' (operation_uuid: 3bd1a1a9-43d3-485c-9275-59cebd64b07c) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.952689 13193 status_update_manager_process.hpp:252] Received operation status update acknowledgement (UUID: 8c1ddad1-4adb-4df5-91fe-235d265a71d8) for stream 3bd1a1a9-43d3-485c-9275-59cebd64b07c
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.952725 13193 status_update_manager_process.hpp:929] Checkpointing ACK for operation status update OPERATION_FAILED (Status UUID: 8c1ddad1-4adb-4df5-91fe-235d265a71d8) for operation UUID 3bd1a1a9-43d3-485c-9275-59cebd64b07c (framework-supplied ID 'a1BdfrEhy4ZLSNPZbDrzp1h-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
[jdefelice@ec101 DCOS-46889]$ grep -e 4acf1495-1a36-4939-a71b-75ca5aa73657 agent.log
Jan 09 11:10:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:10:28.452811 13192 provider.cpp:1548] Received CREATE_DISK operation 'a5MU6JqxYpT9IWXM75cwuHO-0' (uuid: 4acf1495-1a36-4939-a71b-75ca5aa73657)
Jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: E0109 11:11:28.460510 13190 provider.cpp:1605] Failed to apply operation (uuid: 4acf1495-1a36-4939-a71b-75ca5aa73657): Deadline Exceeded
Jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:28.460511 13186 status_update_manager_process.hpp:152] Received operation status update OPERATION_FAILED (Status UUID: e810608b-58ac-47eb-bf19-9abcca6907a2) for operation UUID 4acf1495-1a36-4939-a71b-75ca5aa73657 (framework-supplied ID 'a5MU6JqxYpT9IWXM75cwuHO-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
Jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:28.460793 13186 status_update_manager_process.hpp:929] Checkpointing UPDATE for operation status update OPERATION_FAILED (Status UUID: e810608b-58ac-47eb-bf19-9abcca6907a2) for operation UUID 4acf1495-1a36-4939-a71b-75ca5aa73657 (framework-supplied ID 'a5MU6JqxYpT9IWXM75cwuHO-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
Jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:28.504062 13191 slave.cpp:7696] Handling resource provider message 'UPDATE_OPERATION_STATUS: (uuid: 4acf1495-1a36-4939-a71b-75ca5aa73657) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 (latest state: OPERATION_FAILED, status update state: OPERATION_FAILED)'
Jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:28.504133 13191 slave.cpp:8034] Updating the state of operation 'a5MU6JqxYpT9IWXM75cwuHO-0' (uuid: 4acf1495-1a36-4939-a71b-75ca5aa73657) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 (latest state: OPERATION_FAILED, status update state: OPERATION_FAILED)
Jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:28.504159 13191 slave.cpp:7890] Forwarding status update of operation 'a5MU6JqxYpT9IWXM75cwuHO-0' (operation_uuid: 4acf1495-1a36-4939-a71b-75ca5aa73657) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002
Jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:28.509495 13194 status_update_manager_process.hpp:252] Received operation status update acknowledgement (UUID: e810608b-58ac-47eb-bf19-9abcca6907a2) for stream 4acf1495-1a36-4939-a71b-75ca5aa73657
Jan 09 11:11:28 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:28.509521 13194 status_update_manager_process.hpp:929] Checkpointing ACK for operation status update OPERATION_FAILED (Status UUID: e810608b-58ac-47eb-bf19-9abcca6907a2) for operation UUID 4acf1495-1a36-4939-a71b-75ca5aa73657 (framework-supplied ID 'a5MU6JqxYpT9IWXM75cwuHO-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
[jdefelice@ec101 DCOS-46889]$ grep -e ca2bed2f-480e-4d35-af9e-1161a44c5b9b agent.log
Jan 09 11:10:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:10:27.458933 13186 provider.cpp:1548] Received CREATE_DISK operation 'a3AvAF97UsHU6zIIPhyGdrY-0' (uuid: ca2bed2f-480e-4d35-af9e-1161a44c5b9b)
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: E0109 11:11:27.469853 13189 provider.cpp:1605] Failed to apply operation (uuid: ca2bed2f-480e-4d35-af9e-1161a44c5b9b): Deadline Exceeded
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.469859 13186 status_update_manager_process.hpp:152] Received operation status update OPERATION_FAILED (Status UUID: bb7807e8-dc2f-4f64-b611-d24a1e559317) for operation UUID ca2bed2f-480e-4d35-af9e-1161a44c5b9b (framework-supplied ID 'a3AvAF97UsHU6zIIPhyGdrY-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.470120 13186 status_update_manager_process.hpp:929] Checkpointing UPDATE for operation status update OPERATION_FAILED (Status UUID: bb7807e8-dc2f-4f64-b611-d24a1e559317) for operation UUID ca2bed2f-480e-4d35-af9e-1161a44c5b9b (framework-supplied ID 'a3AvAF97UsHU6zIIPhyGdrY-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.513059 13192 slave.cpp:7696] Handling resource provider message 'UPDATE_OPERATION_STATUS: (uuid: ca2bed2f-480e-4d35-af9e-1161a44c5b9b) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 (latest state: OPERATION_FAILED, status update state: OPERATION_FAILED)'
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.513129 13192 slave.cpp:8034] Updating the state of operation 'a3AvAF97UsHU6zIIPhyGdrY-0' (uuid: ca2bed2f-480e-4d35-af9e-1161a44c5b9b) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002 (latest state: OPERATION_FAILED, status update state: OPERATION_FAILED)
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.513147 13192 slave.cpp:7890] Forwarding status update of operation 'a3AvAF97UsHU6zIIPhyGdrY-0' (operation_uuid: ca2bed2f-480e-4d35-af9e-1161a44c5b9b) for framework c0b7cc7e-db35-450d-bf25-9e3183a07161-0002
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.518623 13191 status_update_manager_process.hpp:252] Received operation status update acknowledgement (UUID: bb7807e8-dc2f-4f64-b611-d24a1e559317) for stream ca2bed2f-480e-4d35-af9e-1161a44c5b9b
Jan 09 11:11:27 ip-10-10-0-28.us-west-2.compute.internal mesos-agent[13170]: I0109 11:11:27.518656 13191 status_update_manager_process.hpp:929] Checkpointing ACK for operation status update OPERATION_FAILED (Status UUID: bb7807e8-dc2f-4f64-b611-d24a1e559317) for operation UUID ca2bed2f-480e-4d35-af9e-1161a44c5b9b (framework-supplied ID 'a3AvAF97UsHU6zIIPhyGdrY-0') of framework 'c0b7cc7e-db35-450d-bf25-9e3183a07161-0002' on agent c0b7cc7e-db35-450d-bf25-9e3183a07161-S1
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-01-09 19:55:23.478,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 29 21:23:46 UTC 2019,,,,,,,"0|u00o94:",9223372036854775807,,,,,bbannier,,,,,,Storage R10 Sprint 38,,,,,,,,,,,8.0,,1.7.2,1.8.0,,,,,,,,"09/Jan/19 19:55;chhsia0;It seems to me that we should address this with MESOS-8400.","25/Jan/19 22:17;chhsia0;Patches intended to backport to 1.7.2:
https://reviews.apache.org/r/69811/
https://reviews.apache.org/r/69812/
https://reviews.apache.org/r/69827/

Patches needed not to backport:
https://reviews.apache.org/r/69813/
https://reviews.apache.org/r/69787/
https://reviews.apache.org/r/69814/
https://reviews.apache.org/r/69815/","29/Jan/19 21:23;chhsia0;{noformat}
commit 5ae178611246b4a1ea928ff45f29cef4f9b55bf7
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Tue Jan 22 20:18:06 2019 -0800

Preliminary SLRP refactoring for RPC retry.

This patch refactors the `StorageLocalResourceProvider::call` function
to obtain the latest service future through `getService` before making
the actual RPC call. The subsequent patch would utilize this to support
RPC retry across plugin restarts.

Review: https://reviews.apache.org/r/69811{noformat}
{noformat}
commit 9fb42b0fa8c79e432089ef56099a03765e65fb38
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Tue Jan 22 23:00:43 2019 -0800

Implemented the RPC retry logic for SLRP.

For CSI calls triggered by offer operations, i.e., `CreateVolume` and
`DeleteVolume`, if the plugin returns retryable errors (`UNAVAILABLE` or
`DEADLINE_EXCEEDED`), SLRP will now retry indefinitely with a random
exponential backoff. With this, frameworks will know that the operations
are terminated with deterministic volume states when getting
`OPERATION_FAILED`.

Review: https://reviews.apache.org/r/69812{noformat}
{noformat}
commit e9d2a2960817476513ae3c47c71f9a0b2af698fd
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Wed Jan 23 21:44:10 2019 -0800

Exposed `StorageLocalResourceProviderProcess` for testing purpose.

This patch moves the declaration of the SLRP process into an internal
header file and add a `__call` function, so a follow-up test could use
`FUTURE_DISPATCH` to capture a dispatch on an RPC retry.

To simplify the declarations, it also internalizes `RPCTraits` and
introduce new type aliases, and moves `DEFAULT_CSI_RETRY_BACKOFF_FACTOR`
and `DEFAULT_CSI_RETRY_INTERVAL_MAX` to the new header for testing.

Review: https://reviews.apache.org/r/69827{noformat}
{noformat}
commit f8d9a9334fe9c838310319a3bcd35be9d3b3fb5f
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu Jan 17 21:40:45 2019 -0800

Added a forwarding mode to the test CSI plugin.

If the `--forward` flag is set, the test CSI plugin would forward all gRPC
requests to the specified gRPC server URI, and return the response to
the caller. This can be used to forward all CSI calls to a mock CSI
plugin object in the test process.

Review: https://reviews.apache.org/r/69787{noformat}
{noformat}
commit 39c4a7791cf938ccbacff4858d9e00d3e6866798
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Tue Jan 22 14:08:35 2019 -0800

Improved the default actions of the mock CSI plugin.

This patch removes the macros used to generate the default actions for
the mock CSI plugin, and makes it returns all possible capabilities by
default so it can be used to test SLRP w/ a proxy-mode test CSI plugin.

Review: https://reviews.apache.org/r/69814{noformat}
{noformat}
commit 4839917fc219398a2929fa56ca094b32b2f3a75d
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Tue Jan 22 23:03:02 2019 -0800

Added a unit test for RPC retry in SLRP.

This patch adds a unit test to verify that SLRP will retry
`CreateVolume` and `DeleteVolume` CSI calls with a random exponential
backoff upon receiving `DEADLINE_EXCEEDED` or `UNAVAILABLE`.

Review: https://reviews.apache.org/r/69815{noformat}
The first 4 patches have been backported to 1.7.2:
{noformat}
commit 994f2089da451981f2e5a236b2a69965d0e745fc
commit 7c7a874121413453294a4f0b4f06a08115b1b202
commit 2877a0070b108033644817ec1b8f99085f73ab4b
commit 80c72a1b895ea1fbca6a977d0599033d27d80c75{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,
IOswitchboard cleanup could get stuck due to FD leak from a race.,MESOS-9502,13206734,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,abudnik,mzhu,mzhu,28/Dec/18 06:17,11/Jan/19 21:06,29/Oct/20 16:32,09/Jan/19 19:39,1.7.0,,,,,,,,1.4.3,1.5.2,1.6.2,1.7.1,1.8.0,,containerization,,,,,0,containerizer,,,,,,,,"Our check container got stuck during destroy which in turned stucks the parent container. It is blocked by the I/O switchboard cleanup:

1223 18:04:41.000000 16269 switchboard.cpp:814] Sending SIGTERM to I/O switchboard server (pid: 62854) since container 4d4074fa-bc87-471b-8659-08e519b68e13.16d02532-675a-4acb-964d-57459ecf6b67.check-e91521a3-bf72-4ac4-8ead-3950e31cf09e is being destroyed
....
1227 04:45:38.000000  5189 switchboard.cpp:916] I/O switchboard server process for container 4d4074fa-bc87-471b-8659-08e519b68e13.16d02532-675a-4acb-964d-57459ecf6b67.check-e91521a3-bf72-4ac4-8ead-3950e31cf09e has terminated (status=N/A)

Note the timestamp.

*Root Cause:*
Fundamentally, this is caused by a race between *.discard()* triggered by Check Container TIMEOUT and IOSB extracting ContainerIO object. This race could be exposed by overloaded/slow agent process. Please see how this race be triggered below:

# Right after IOSB server process is running, Check container Timed out and the checker process returns a failure, which would close the HTTP connection with agent.
# From the agent side, if the connection breaks, the handler will trigger a discard on the returned future and that will result in containerizer->launch()'s future transitioned to DISCARDED state.
# In containerizer, the DISCARDED state will be propagated back to IOSB prepare(), which stop its continuation on *extracting the containerIO* (it implies the object being cleaned up and FDs(one end of pipes created in IOSB) being closed in its destructor).
# Agent starts to destroy the container due to its discarded launch result, and asks IOSB to cleanup the container.
# IOSB server is still running, so agent sends a SIGTERM.
# SIGTERM handler unblocks the IOSB from redirecting (to redirect stdout/stderr from container to logger before exiting).
# io::redirect() calls io::splice() and reads the other end of those pipes forever.

This issue is *not easy to reproduce unless* on a busy agent, because the timeout has to happen exactly *AFTER* IOSB server is running and *BEFORE* IOSB extracts containerIO.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6632,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-01-09 19:39:09.479,,,false,MESOS-7103,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 09 19:39:09 UTC 2019,,,,,,,"0|hzzxxg:",9223372036854775807,,,,,gilbert,,,,,,Containerization R9 Sprint 37,,,,,,,,,,,8.0,,,,,,,,,,,"28/Dec/18 06:24;mzhu;[~jieyu] mentioned that one possibility is pid reuse. When the io switchboard terminates and agent restarts, the old pid for io switchboard will be reaped by init. The agent will reap on the old pid after failover, in common case, this will return None() immediately. However, in the corner case, if the pid is reused, the agent can get stuck.","28/Dec/18 19:51;mzhu;[~abudnik] pointed out that this is likely caused by MESOS-6632, which I can confirm with the log.

When a container launch gets discarded halfway but after IOSB starts to redirect to the container, the IOSB will be stuck there until agent fails over and all fds are closed. I can confirm that stuck ISOB always comes back and terminates after an agent failover.","09/Jan/19 19:39;gilbert;commit 6938af6e8edc15b24846adface1eeb98032e3463
Author: Andrei Budnik <abudnik@mesosphere.com>
Date:   Wed Jan 9 11:09:08 2019 -0800

    Fixed the FD leak if containerizer::_launch() failed or discarded.
    
    For the period between IOSB server is up and container process exec,
    if the containerizer launch returns failure or discarded, the FD used
    for signaling from the container process to the IOSB finish redirect
    will be leaked, which would cause the IOSB stuck at `io::redirect`
    forever. It would block the containerizer from cleaning up this
    container at IOSB.
    
    This issue could be exposed if there are frequent check containers
    launch on an agent with heavy loads.
    
    This patch fixes the issue by handling discard of a `launch` future,
    so the container IO is cleaned up and therefore all FDs are closed.
    
    Review: https://reviews.apache.org/r/69684/",,,,,,,,,,,,,,,,,,,,,,,,,
SLRP does not set RP ID in produced OperationStatus.,MESOS-9479,13204483,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,bbannier,bbannier,14/Dec/18 14:00,18/Dec/18 15:12,29/Oct/20 16:32,18/Dec/18 15:12,1.8.0,,,,,,,,1.7.1,1.8.0,,,,,,,,,,0,storage,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 18 15:12:31 UTC 2018,,,,,,,"0|s01j0w:",9223372036854775807,,,,,chhsia0,,,,,,Storage R8 Sprint 35,,,,,,,,,,,1.0,,1.8.0,,,,,,,,,"14/Dec/18 15:17;bbannier;Review: https://reviews.apache.org/r/69569/","18/Dec/18 15:12;bbannier;{noformat}
commit 49fc7fcf1e26d803aa0775f66f36763e39d695e9
Author: Benjamin Bannier <bbannier@apache.org>
Date:   Tue Dec 18 15:08:18 2018 +0100

    Validated that resource providers use correct ID in operation states.
    
    We expect resource providers to set their IDs in operation status
    messages. This patch adds some assertion that the IDs match our
    expectations, and adjusts some test code to honor these assumptions.
    
    Review: https://reviews.apache.org/r/69572/

commit 9dea6df03761ea0c34704268e0e50c79121fb9a7
Author: Benjamin Bannier <bbannier@apache.org>
Date:   Tue Dec 18 15:08:17 2018 +0100

    Added missing code to set resource provider and agent ids.
    
    The helper function to set up the message was set up to receive
    arguments, but we missed implementing the code making use of the values.
    This went undiscovered as previously only used a mock resource provider.
    
    Review: https://reviews.apache.org/r/69569/
{noformat}","18/Dec/18 15:12;bbannier;Backported to {{1.7.x}}.
{noformat}
commit 5b7055ca146aa5196059ad151396955c2b537b9f
Author: Benjamin Bannier <bbannier@apache.org>
Date:   Tue Dec 18 15:08:18 2018 +0100

    Validated that resource providers use correct ID in operation states.
    
    We expect resource providers to set their IDs in operation status
    messages. This patch adds some assertion that the IDs match our
    expectations, and adjusts some test code to honor these assumptions.
    
    Review: https://reviews.apache.org/r/69572/

commit ffe925d26fe05afe3cefb70c31154024cac96987
Author: Benjamin Bannier <bbannier@apache.org>
Date:   Tue Dec 18 15:08:17 2018 +0100

    Added missing code to set resource provider and agent ids.
    
    The helper function to set up the message was set up to receive
    arguments, but we missed implementing the code making use of the values.
    This went undiscovered as previously only used a mock resource provider.
    
    Review: https://reviews.apache.org/r/69569/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,
Master may send `FRAMEWORK_UPDATED` for a new framework ID in operator API.,MESOS-9470,13203836,Bug,Open,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,chhsia0,chhsia0,11/Dec/18 22:14,09/Jan/19 19:05,29/Oct/20 16:32,,1.7.0,,,,,,,,,,,,,,HTTP API,master,,,,0,api,foundations,mesosphere,,,,,,"In the operator streaming API, the master only sends {{FRAMEWORK_ADDED}} if a framework is subscribed with no ID:
[https://github.com/apache/mesos/blob/4803af11bcb038193c238c049ea67c73b1bfd9a1/src/master/master.cpp#L2653-L2679]
[https://github.com/apache/mesos/blob/4803af11bcb038193c238c049ea67c73b1bfd9a1/src/master/master.cpp#L2951-L2988]

That means after a master failover, if a framework is recovered from an agent or subscribed with an ID, a {{FRAMEWORK_UPDATED}} with a framework ID that is previously unknown will be sent to the subscribers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-6007,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-12-11 22:14:48.0,,,,,,,"0|i00a3x:o",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Completed framework update streams may retry forever,MESOS-9434,13201768,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,greggomann,greggomann,01/Dec/18 03:47,06/Feb/19 09:04,29/Oct/20 16:32,06/Feb/19 09:04,1.7.0,,,,,,,,1.8.0,,,,,,agent,resource provider,,,,0,mesosphere,,,,,,,,"Since the agent/RP currently does not GC operation status update streams when frameworks are torn down, it's possible that active update streams associated with completed frameworks may remain and continue retrying forever. We should add a mechanism to complete these streams when the framework becomes completed.

A couple options which have come up during discussion:
* Have the master acknowledge updates associated with completed frameworks. Note that since completed frameworks are currently only tracked by the master in memory, a master failover could prevent this from working perfectly.
* Extend the RP API to allow the GC of particular update streams, and have the agent GC streams associated with a framework when it receives a {{ShutdownFrameworkMessage}}. This would also require the addition of a new method to the status update manager.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-01-07 11:46:42.213,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 06 09:04:26 UTC 2019,,,,,,,"0|s012fk:",9223372036854775807,,,,,greggomann,,,,,,Storage R9 Sprint 36,Storage R9 Sprint 37,Storage R10 Sprint 38,Storage R10 Sprint 39,,,,,,,,2.0,,,,,,,,,,,"07/Jan/19 11:46;bbannier;There exists a similar issue with when a {{ShutdownFrameworkMessage}} can be sent by a master. If an agent is partitioned from the cluster for a long time and does not know that a framework completed (partitioned at time of completion), after a master failover and resubscription of the agent the new master would 1) not know that the framework completed, and even 2) learn about the framework from the resubscribed agent.

As we currently do not reliably handle this case as well, it seems the first suggestion above is more consistent (i.e., have a master acknowledge operations status updates of frameworks it currently knows are removed). Note that should we e.g., persist completed {{FrameworkID}} values in the future this solution would work naturally as well.

Above second suggestion of masters explicitly informing status update managers of framework completion does not work reliable either in cases where status update managers are partitioned at the time of completion and subsequent master failovers.","07/Jan/19 12:25;bbannier;Review: https://reviews.apache.org/r/69680/","16/Jan/19 13:25;bbannier;[~greggomann] [~gkleiman], could you please help move this forward? Reviews would be greatly appreciated.","28/Jan/19 15:16;bbannier;[~greggomann] [~gkleiman] , ping?","06/Feb/19 09:04;bbannier;{noformat}
commit bb545b338f94c1f1e4e00dc6dbdb9a0484d4f163
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Tue Feb 5 19:00:40 2019 +0100

    Have master acknowledge operation updates of completed frameworks.
    
    After a framework was removed and has unacknowledged operations status
    updates, it was impossible to remove terminal operations as nobody could
    acknowledge them.
    
    In this patch we make the master acknowledge operation status updates
    for frameworks it knows are removed so that e.g., terminal operations
    can be removed. Since masters do not persist completed frameworks this
    is not reliable (e.g., an agent was partitioned for a long time and
    still tracks a completed framework's `FrameworkInfo`, and comes back
    only after the master knowing about the framework's completion has
    failed over). We merely extend the existing master behavior (e.g., send
    `ShutdownFrameworkMessage` to all currently registered agents) to
    operations.
    
    Review: https://reviews.apache.org/r/69680/

commit 9b54356d67cbe4bd594c68d8522763eb44ca73d3
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Tue Feb 5 19:00:37 2019 +0100

    Set status update UUID in MockResourceProvider.
    
    Review: https://reviews.apache.org/r/69854/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,
Check failure on `StorageLocalResourceProviderProcess::applyCreateDisk`.,MESOS-9395,13198889,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,Fixed,chhsia0,alexr,alexr,16/Nov/18 12:10,31/Jul/19 16:58,29/Oct/20 16:32,06/Jun/19 19:10,1.5.0,1.5.1,1.5.2,1.6.0,1.6.1,1.7.0,1.7.1,1.7.2,1.8.1,1.9.0,,,,,storage,,,,,0,mesosphere,probability:low,storage,,,,,,"Observed the following agent failure on one of our staging clusters:
{noformat}
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: I1116 11:57:24.641331 26684 http.cpp:1799] Processing GET_AGENT call
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: I1116 11:57:24.650429 26679 http.cpp:1117] HTTP POST for /slave(1)/api/v1/resource_provider from 172.31.8.65:57790
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: I1116 11:57:24.650629 26679 manager.cpp:672] Subscribing resource provider {""attributes"":[{""name"":""lvm-vg-name"",""text"":{""value"":""lvm-double-1540383639""},""type"":""SCALAR""},{""name"":""dss-asset-id"",""text"":{""value"":""6AbZV6W2DrK4YgcIR3ICVo""},""type"":""SCALAR""}],""default_reservations"":[{""principal"":""storage-principal"",""role"":""dcos-storage"",""type"":""DYNAMIC""}],""id"":{""value"":""8326e931-41f2-4f45-9174-13fe35c19300""},""name"":""rp_6AbZV6W2DrK4YgcIR3ICVo"",""storage"":{""plugin"":{""containers"":[{""command"":{""environment"":{""variables"":[{""name"":""PATH"",""type"":""VALUE"",""value"":""/opt/mesosphere/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin""},{""name"":""LD_LIBRARY_PATH"",""type"":""VALUE"",""value"":""/opt/mesosphere/lib""},{""name"":""CONTAINER_LOGGER_DESTINATION_TYPE"",""type"":""VALUE"",""value"":""journald+logrotate""},{""name"":""CONTAINER_LOGGER_EXTRA_LABELS"",""type"":""VALUE"",""value"":""{\""CSI_PLUGIN\"":\""csilvm\""}""}]},""shell"":true,""uris"":[{""executable"":true,""extract"":false,""value"":""<possibly-sensitive>""}],""value"":""echo \""a *:* rwm\"" > /sys/fs/cgroup/devices`cat /proc/self/cgroup | grep devices | cut -d : -f 3`/devices.allow; exec ./csilvm -devices=/dev/xvdk,/dev/xvdj -volume-group=lvm-double-1540383639 -unix-addr-env=CSI_ENDPOINT -tag=6AbZV6W2DrK4YgcIR3ICVo""},""resources"":[{""name"":""cpus"",""scalar"":{""value"":0.1},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":128.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":10.0},""type"":""SCALAR""}],""services"":[""CONTROLLER_SERVICE"",""NODE_SERVICE""]}],""name"":""plugin_6AbZV6W2DrK4YgcIR3ICVo"",""type"":""io.mesosphere.dcos.storage.csilvm""}},""type"":""org.apache.mesos.rp.local.storage""}
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: I1116 11:57:24.690474 26685 provider.cpp:546] Received SUBSCRIBED event
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: I1116 11:57:24.690521 26685 provider.cpp:1492] Subscribed with ID 8326e931-41f2-4f45-9174-13fe35c19300
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: I1116 11:57:24.690657 26681 status_update_manager_process.hpp:314] Recovering operation status update manager
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: F1116 11:57:24.691496 26682 provider.cpp:3121] Check failed: resource.disk().source().has_profile() != resource.disk().source().has_id() (1 vs. 1)
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: *** Check failure stack trace: ***
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb099e9fd  google::LogMessage::Fail()
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb09a082d  google::LogMessage::SendToLog()
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb099e5ec  google::LogMessage::Flush()
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb09a1129  google::LogMessageFatal::~LogMessageFatal()
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb01654ca  mesos::internal::StorageLocalResourceProviderProcess::applyCreateDisk()
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb017c683  mesos::internal::StorageLocalResourceProviderProcess::_applyOperation()
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb017d64a  _ZZN5mesos8internal35StorageLocalResourceProviderProcess26reconcileOperationStatusesEvENKUlRKNS0_26StatusUpdateManagerProcessIN2id4UUIDENS0_27UpdateOperationStatusRecordENS0_28UpdateOperationStatusMessageEE5StateEE_clESA_
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb017dd21  _ZNO6lambda12CallableOnceIFN7process6FutureI7NothingEEvEE10CallableFnINS_8internal7PartialIZN5mesos8internal35StorageLocalResourceProviderProcess26reconcileOperationStatusesEvEUlRKNSB_26StatusUpdateManagerProcessIN2id4UUIDENSB_27UpdateOperationStatusRecordENSB_28UpdateOperationStatusMessageEE5StateEE_ISJ_EEEEclEv
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecafa0ce97  _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8internal8DispatchINS1_6FutureI7NothingEEEclINS0_IFSD_vEEEEESD_RKNS1_4UPIDEOT_EUlSt10unique_ptrINS1_7PromiseISC_EESt14default_deleteISP_EEOSH_S3_E_JSS_SH_St12_PlaceholderILi1EEEEEEclEOS3_
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb08eec51  process::ProcessBase::consume()
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb09056cc  process::ProcessManager::resume()
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecb090b186  _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecad5d8070  (unknown)
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecacdf6e25  start_thread
Nov 16 11:57:24 int-mountvolumeagent2-soak112s.testing.mesosphe.re mesos-agent[26663]: @     0x7fecacb20bad  __clone
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-9312,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-05-10 01:19:38.969,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 06 19:10:14 UTC 2019,,,,,,,"0|hzzxex:",9223372036854775807,,,,,bbannier,,,,,,Storage R7 Sprint 33,Storage R8 Sprint 34,Storage R8 Sprint 35,Storage R9 Sprint 36,Storage R9 Sprint 37,Storage R10 Sprint 38,Storage R10 Sprint 39,Storage R11 Sprint 40,Storage: RI-13 Sprint 44,Storage: RI-13 Sprint 45,Storage: RI-14 Sprint 46,5.0,,1.8.1,1.9.0,,,,,,,,"10/May/19 01:19;chhsia0;Reviews:
[https://reviews.apache.org/r/70620/]
[https://reviews.apache.org/r/70621/]
[https://reviews.apache.org/r/70622/|https://reviews.apache.org/r/70622/diff/1/]","06/Jun/19 19:10;chhsia0;{noformat}
commit fe7010099133962fd3d58ffde18d6c8e7472e01a
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu May 9 17:59:16 2019 -0700

Made SLRP allow changes in volume context.

To make SLRP more robust against non-conforming CSI plugins that change
volume contexts, the `getExistVolumes` method returns a list of resource
conversions consisting of one for converting old volume contexts to new
volume contexts, and one to remove missing volumes and add new volumes.

To make the interfaces consistent, `getStoragePools` now also returns a
list of resource conversions consisting of one conversion.

Review: https://reviews.apache.org/r/70620{noformat}
{noformat}
commit 24c70aa15c566b6f15a3f3636664507bae3af3b6
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu Apr 18 12:50:54 2019 -0700

Used full paths as volume IDs for the test CSI plugin.

The full paths of simulated volumes are now in their ID instead of their
contextual information. This simplifies SLRP tests, and makes it cleaner
if we want to customize the contextual information in the future.

Review: https://reviews.apache.org/r/70621{noformat}
{noformat}
commit a7b98f702ddf4e8c9cfd9a7e14e050d802f80084
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Mon May 6 11:44:40 2019 -0700

Added a unit test to verify if SLRP allows changes in volume context.

Review: https://reviews.apache.org/r/70622{noformat}
Backported to 1.8.1:
{noformat}
commit e0246d1765df1be69d23fc572c24baadae500dc6{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
UCR container launch stuck at PROVISIONING during image fetching.,MESOS-9320,13191770,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Duplicate,gilbert,gilbert,gilbert,16/Oct/18 01:10,31/Oct/18 20:34,29/Oct/20 16:32,31/Oct/18 20:34,,,,,,,,,,,,,,,containerization,,,,,0,containerizer,,,,,,,,"We observed mesos containerizer stuck at PROVISIONING when launching a mesos container using docker image: `kvish/jenkins-dev:595c74f713f609fd1d3b05a40d35113fc03227c9`:

The image pulling never finishes. Insufficient image contents are still in image store staging directory /var/lib/mesos/slave/store/docker/staging/egLYqO, forever.
{noformat}
OK-22:50:06-root@int-agent89-mwst9:/var/lib/mesos/slave/store/docker/staging/egLYqO # ls -alh
total 1.1G
drwx------. 2 root root 4.0K Oct 15 13:02 .
drwxr-xr-x. 3 root root   20 Oct 15 22:40 ..
-rw-r--r--. 1 root root  59K Oct 15 13:02 manifest
-rw-r--r--. 1 root root 2.6K Oct 15 13:02 sha256:08239cb71d7a3e0d8ed680397590b338a2133117250e1a3e2ee5c5c45292db63
-rw-r--r--. 1 root root  440 Oct 15 13:02 sha256:0984904c0e1558248eb25e93d9fc14c47c0052d58569e64c185afca93a060b66
-rw-r--r--. 1 root root  248 Oct 15 13:02 sha256:0bbc7b377a9155696eb0b684bd1999bc43937918552d73fd9697ea50ef46528a
-rw-r--r--. 1 root root  240 Oct 15 13:02 sha256:0c5c0c095e351b976943453c80271f3b75b1208dbad3ca7845332e873361f3bb
-rw-r--r--. 1 root root  562 Oct 15 13:02 sha256:1558b7c35c9e25577ee719529d6fcdddebea68f5bdf8cbdf13d8d75a02f8a5b1
-rw-r--r--. 1 root root  11M Oct 15 13:02 sha256:1ab373b3deaed929a15574ac1912afc6e173f80d400aba0e96c89f6a58961f2d
-rw-r--r--. 1 root root  130 Oct 15 13:02 sha256:1b6c70b3786f72e5255ccd51e27840d1c853a17561b5e94a4359b17d27494d50
-rw-r--r--. 1 root root  176 Oct 15 13:02 sha256:1bf4aab5c3b363b4fdfc46026df9ae854db8858a5cbcccdd4409434817d59312
-rw-r--r--. 1 root root  380 Oct 15 13:02 sha256:213b0c5bb5300df1d2d06df6213ae94448419cf18ecf61358e978a5d25651d5a
-rw-r--r--. 1 root root  71M Oct 15 13:02 sha256:31aaab384e3fa66b73eced4870fc96be590a2376e93fd4f8db5d00f94fb11604
-rw-r--r--. 1 root root 1.4K Oct 15 13:02 sha256:32442b7d159ed2b7f00b00a989ca1d3ee1a3f566df5d5acbd25f0c3dfdad69d1
-rw-r--r--. 1 root root 653K Oct 15 13:02 sha256:340cd692075b636b5e1803fcde9b1a56a2f6e2728e4fb10f7295d39c7d0e0d01
-rw-r--r--. 1 root root  184 Oct 15 13:02 sha256:398819b00c6cbf9cce6c1ed25005c9e1242cace7a6436730e17da052000c7f90
-rw-r--r--. 1 root root 366K Oct 15 13:02 sha256:41d78c0cb1b2a47189068e55f61d6266be14c4fa75935cb021f17668dd8e7f94
-rw-r--r--. 1 root root  23K Oct 15 13:02 sha256:4f5852c22c7ce0155494b6e86a0a4c536c3c95cb87cad84806aa2d56184b95d2
-rw-r--r--. 1 root root 384M Oct 15 13:02 sha256:4fe621515c4d23e33d9850a6cdfc3aa686d790704b9c5569f1726b4469aa30c0
-rw-r--r--. 1 root root 1.5K Oct 15 13:02 sha256:50dcd1d0618b1d42bf6633dc8176e164571081494fa6483ec4489a59637518bc
-rw-r--r--. 1 root root  48M Oct 15 13:02 sha256:57c8de432dbe337bb6cb1ad328e6c564303a3d3fd05b5e872fd9c47c16fdd02c
-rw-r--r--. 1 root root  30M Oct 15 13:02 sha256:63a0f0b6b5d7014b647ac4a164808208229d2e3219f45a39914f0561a4f831bf
-rw-r--r--. 1 root root 306M Oct 15 13:02 sha256:67f41ed73c082c6ffee553a90b0abd56bc74b260d90b9d594d652b66cbcd5e7f
-rw-r--r--. 1 root root  435 Oct 15 13:02 sha256:6cb303e084ed78386ae87cdaf95e8817d48e94b3ce7c0442a28335600f0efa3d
-rw-r--r--. 1 root root 5.5K Oct 15 13:02 sha256:7d4d905c2060a5ec994ec201e6877714ee73030ef4261f9562abdb0f844174d5
-rw-r--r--. 1 root root  39M Oct 15 13:02 sha256:80d923f4b955c2db89e2e8a9f2dcb0c36a29c1520a5b359578ce2f3d0b849d10
-rw-r--r--. 1 root root  615 Oct 15 13:02 sha256:842cc8bd099d94f6f9c082785bbaa35439af965d1cf6a13300830561427c266b
-rw-r--r--. 1 root root  712 Oct 15 13:02 sha256:977c8e6687e0ca5f0682915102c025dc12d7ff71bf70de17aab3502adda25af2
-rw-r--r--. 1 root root  12K Oct 15 13:02 sha256:989ac24c53a1f7951438aa92ac39bc9053c178336bea4ebe6ab733d4975c9728
-rw-r--r--. 1 root root  861 Oct 15 13:02 sha256:a18e3c45bf91ac3bd11a46b489fb647a721417f60eae66c5f605360ccd8d6352
-rw-r--r--. 1 root root   32 Oct 15 13:02 sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4
-rw-r--r--. 1 root root 266K Oct 15 13:02 sha256:b1d3e8de8ec6d87b8485a8a3b66d63125a033cfb0711f8af24b4f600f524e276
-rw-r--r--. 1 root root 1.6K Oct 15 13:02 sha256:b3a122ff7868d2ed9c063df73b0bf67fd77348d3baa2a92368b3479b41f8aa74
-rw-r--r--. 1 root root 4.2M Oct 15 13:02 sha256:b542772b417703c0311c0b90136091369bcd9c2176c0e3ceed5a0114d743ee3c
-rw-r--r--. 1 root root 1.1K Oct 15 13:02 sha256:b6e3599b777bb2dd681fd84f174a7e0ce3cb01f5a84dcd3c771d0e999a39bc58
-rw-r--r--. 1 root root 2.8K Oct 15 13:02 sha256:b970c9afc934d5e6bb524a6057342a1d1cc835972f047a805f436c540ee20747
-rw-r--r--. 1 root root 6.3M Oct 15 13:02 sha256:b984f623b82721cc642c25cd4797f6c3d2c01b6b063c49905a97bb0a7f0725a5
-rw-r--r--. 1 root root 1.8K Oct 15 13:02 sha256:c0cc702ea6bfc6490ccb2edd9d9c8070964ae2023129d14f90729d0b365f6215
-rw-r--r--. 1 root root 4.1K Oct 15 13:02 sha256:cbedea0328015d1baf1efdd06b2417283f6314c0ef671bc0246ada3221ca21ac
-rw-r--r--. 1 root root  355 Oct 15 13:02 sha256:cc7516477cdbfa791d6fd66c9c19b46036cc294f884d8ebbbd0a7fc878433c87
-rw-r--r--. 1 root root 165M Oct 15 13:02 sha256:d9bbcf733166f991331a80e1cd55a91111c4ba96fc7ce1ecabd05b450b7da7a3
-rw-r--r--. 1 root root 872K Oct 15 13:02 sha256:da44f64ae9991a9e8cb7c2af4dfd63608bd4026552b2b6a7f523dcfac960e1ac
-rw-r--r--. 1 root root  431 Oct 15 13:02 sha256:edb369c8c5d7b67e773eee549901a38b80dfa1246597815ae6eb21d1beceec1a
-rw-r--r--. 1 root root  19M Oct 15 13:02 sha256:f4457f4b3bfe0282e803dd9172421048b80168d9c1433da969555fa571a4a1d6
-rw-r--r--. 1 root root  198 Oct 15 13:02 sha256:f67b87ed7ea47d30c673e289d4c2fd28f5e8e3059840152932e8e813183462ec
-rw-r--r--. 1 root root 550K Oct 15 13:02 sha256:f96e7fcceb6e12a816cb49d01574e29767d9cc2b6f92436f314a59570abae320
-rw-r--r--. 1 root root  676 Oct 15 13:02 sha256:fec44d138823b8076f9a49148f93a7c3d6b0e79ca34b489d60b194d7b1c2c2fa
{noformat}

It is not clear yet why the SHA pulling does not finish, so we use the same image on another empty machine with UCR. The other machine has the container RUNNING correctly, and has the following staging directory before moving to the layers dir:
{noformat}
-rw-r--r--. 1 root root 2.6K Oct 15 18:03 sha256:08239cb71d7a3e0d8ed680397590b338a2133117250e1a3e2ee5c5c45292db63
-rw-r--r--. 1 root root  440 Oct 15 18:03 sha256:0984904c0e1558248eb25e93d9fc14c47c0052d58569e64c185afca93a060b66
-rw-r--r--. 1 root root  248 Oct 15 18:03 sha256:0bbc7b377a9155696eb0b684bd1999bc43937918552d73fd9697ea50ef46528a
-rw-r--r--. 1 root root  240 Oct 15 18:03 sha256:0c5c0c095e351b976943453c80271f3b75b1208dbad3ca7845332e873361f3bb
-rw-r--r--. 1 root root  562 Oct 15 18:03 sha256:1558b7c35c9e25577ee719529d6fcdddebea68f5bdf8cbdf13d8d75a02f8a5b1
-rw-r--r--. 1 root root  11M Oct 15 18:03 sha256:1ab373b3deaed929a15574ac1912afc6e173f80d400aba0e96c89f6a58961f2d
-rw-r--r--. 1 root root  130 Oct 15 18:03 sha256:1b6c70b3786f72e5255ccd51e27840d1c853a17561b5e94a4359b17d27494d50
-rw-r--r--. 1 root root  176 Oct 15 18:03 sha256:1bf4aab5c3b363b4fdfc46026df9ae854db8858a5cbcccdd4409434817d59312
-rw-r--r--. 1 root root  380 Oct 15 18:03 sha256:213b0c5bb5300df1d2d06df6213ae94448419cf18ecf61358e978a5d25651d5a
-rw-r--r--. 1 root root  71M Oct 15 18:03 sha256:31aaab384e3fa66b73eced4870fc96be590a2376e93fd4f8db5d00f94fb11604
-rw-r--r--. 1 root root 1.4K Oct 15 18:03 sha256:32442b7d159ed2b7f00b00a989ca1d3ee1a3f566df5d5acbd25f0c3dfdad69d1
-rw-r--r--. 1 root root 653K Oct 15 18:03 sha256:340cd692075b636b5e1803fcde9b1a56a2f6e2728e4fb10f7295d39c7d0e0d01
-rw-r--r--. 1 root root  184 Oct 15 18:03 sha256:398819b00c6cbf9cce6c1ed25005c9e1242cace7a6436730e17da052000c7f90
-rw-r--r--. 1 root root 366K Oct 15 18:03 sha256:41d78c0cb1b2a47189068e55f61d6266be14c4fa75935cb021f17668dd8e7f94
-rw-r--r--. 1 root root  23K Oct 15 18:03 sha256:4f5852c22c7ce0155494b6e86a0a4c536c3c95cb87cad84806aa2d56184b95d2
-rw-r--r--. 1 root root 122M Oct 15 18:03 sha256:4fe621515c4d23e33d9850a6cdfc3aa686d790704b9c5569f1726b4469aa30c0
-rw-r--r--. 1 root root 1.5K Oct 15 18:03 sha256:50dcd1d0618b1d42bf6633dc8176e164571081494fa6483ec4489a59637518bc
-rw-r--r--. 1 root root  48M Oct 15 18:03 sha256:57c8de432dbe337bb6cb1ad328e6c564303a3d3fd05b5e872fd9c47c16fdd02c
-rw-r--r--. 1 root root  30M Oct 15 18:03 sha256:63a0f0b6b5d7014b647ac4a164808208229d2e3219f45a39914f0561a4f831bf
-rw-r--r--. 1 root root  92M Oct 15 18:03 sha256:67f41ed73c082c6ffee553a90b0abd56bc74b260d90b9d594d652b66cbcd5e7f
-rw-r--r--. 1 root root  435 Oct 15 18:03 sha256:6cb303e084ed78386ae87cdaf95e8817d48e94b3ce7c0442a28335600f0efa3d
-rw-r--r--. 1 root root 5.5K Oct 15 18:03 sha256:7d4d905c2060a5ec994ec201e6877714ee73030ef4261f9562abdb0f844174d5
-rw-r--r--. 1 root root  39M Oct 15 18:03 sha256:80d923f4b955c2db89e2e8a9f2dcb0c36a29c1520a5b359578ce2f3d0b849d10
-rw-r--r--. 1 root root  615 Oct 15 18:03 sha256:842cc8bd099d94f6f9c082785bbaa35439af965d1cf6a13300830561427c266b
-rw-r--r--. 1 root root  712 Oct 15 18:03 sha256:977c8e6687e0ca5f0682915102c025dc12d7ff71bf70de17aab3502adda25af2
-rw-r--r--. 1 root root  12K Oct 15 18:03 sha256:989ac24c53a1f7951438aa92ac39bc9053c178336bea4ebe6ab733d4975c9728
-rw-r--r--. 1 root root  861 Oct 15 18:03 sha256:a18e3c45bf91ac3bd11a46b489fb647a721417f60eae66c5f605360ccd8d6352
-rw-r--r--. 1 root root   32 Oct 15 18:03 sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4
-rw-r--r--. 1 root root 266K Oct 15 18:03 sha256:b1d3e8de8ec6d87b8485a8a3b66d63125a033cfb0711f8af24b4f600f524e276
-rw-r--r--. 1 root root 1.6K Oct 15 18:03 sha256:b3a122ff7868d2ed9c063df73b0bf67fd77348d3baa2a92368b3479b41f8aa74
-rw-r--r--. 1 root root 4.2M Oct 15 18:03 sha256:b542772b417703c0311c0b90136091369bcd9c2176c0e3ceed5a0114d743ee3c
-rw-r--r--. 1 root root 1.1K Oct 15 18:03 sha256:b6e3599b777bb2dd681fd84f174a7e0ce3cb01f5a84dcd3c771d0e999a39bc58
-rw-r--r--. 1 root root 2.8K Oct 15 18:03 sha256:b970c9afc934d5e6bb524a6057342a1d1cc835972f047a805f436c540ee20747
-rw-r--r--. 1 root root 6.3M Oct 15 18:03 sha256:b984f623b82721cc642c25cd4797f6c3d2c01b6b063c49905a97bb0a7f0725a5
-rw-r--r--. 1 root root 1.8K Oct 15 18:03 sha256:c0cc702ea6bfc6490ccb2edd9d9c8070964ae2023129d14f90729d0b365f6215
-rw-r--r--. 1 root root  44M Oct 15 18:03 sha256:c73ab1c6897bf5c11da3c95cab103e7ca8cf10a6d041eda2ff836f45a40e3d3b
-rw-r--r--. 1 root root 4.1K Oct 15 18:03 sha256:cbedea0328015d1baf1efdd06b2417283f6314c0ef671bc0246ada3221ca21ac
-rw-r--r--. 1 root root  355 Oct 15 18:03 sha256:cc7516477cdbfa791d6fd66c9c19b46036cc294f884d8ebbbd0a7fc878433c87
-rw-r--r--. 1 root root  82M Oct 15 18:03 sha256:d9bbcf733166f991331a80e1cd55a91111c4ba96fc7ce1ecabd05b450b7da7a3
-rw-r--r--. 1 root root 872K Oct 15 18:03 sha256:da44f64ae9991a9e8cb7c2af4dfd63608bd4026552b2b6a7f523dcfac960e1ac
-rw-r--r--. 1 root root  431 Oct 15 18:03 sha256:edb369c8c5d7b67e773eee549901a38b80dfa1246597815ae6eb21d1beceec1a
-rw-r--r--. 1 root root  19M Oct 15 18:03 sha256:f4457f4b3bfe0282e803dd9172421048b80168d9c1433da969555fa571a4a1d6
-rw-r--r--. 1 root root  198 Oct 15 18:03 sha256:f67b87ed7ea47d30c673e289d4c2fd28f5e8e3059840152932e8e813183462ec
-rw-r--r--. 1 root root 550K Oct 15 18:03 sha256:f96e7fcceb6e12a816cb49d01574e29767d9cc2b6f92436f314a59570abae320
-rw-r--r--. 1 root root  676 Oct 15 18:03 sha256:fec44d138823b8076f9a49148f93a7c3d6b0e79ca34b489d60b194d7b1c2c2fa
{noformat}

By comparing two cases, we can see one layer `8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324` is missing on the problematic agent node, and it is the last layer to fetch.

Here is the manifest as a reference:
{noformat}
OK-17:42:20-root@int-agent89-mwst9:/var/lib/mesos/slave/store/docker/staging/egLYqO # cat manifest 
{
   ""schemaVersion"": 1,
   ""name"": ""kvish/jenkins-dev"",
   ""tag"": ""595c74f713f609fd1d3b05a40d35113fc03227c9"",
   ""architecture"": ""amd64"",
   ""fsLayers"": [
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:0c5c0c095e351b976943453c80271f3b75b1208dbad3ca7845332e873361f3bb""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:4fe621515c4d23e33d9850a6cdfc3aa686d790704b9c5569f1726b4469aa30c0""
      },
      {
         ""blobSum"": ""sha256:c0cc702ea6bfc6490ccb2edd9d9c8070964ae2023129d14f90729d0b365f6215""
      },
      {
         ""blobSum"": ""sha256:4f5852c22c7ce0155494b6e86a0a4c536c3c95cb87cad84806aa2d56184b95d2""
      },
      {
         ""blobSum"": ""sha256:f96e7fcceb6e12a816cb49d01574e29767d9cc2b6f92436f314a59570abae320""
      },
      {
         ""blobSum"": ""sha256:b984f623b82721cc642c25cd4797f6c3d2c01b6b063c49905a97bb0a7f0725a5""
      },
      {
         ""blobSum"": ""sha256:67f41ed73c082c6ffee553a90b0abd56bc74b260d90b9d594d652b66cbcd5e7f""
      },
      {
         ""blobSum"": ""sha256:b6e3599b777bb2dd681fd84f174a7e0ce3cb01f5a84dcd3c771d0e999a39bc58""
      },
      {
         ""blobSum"": ""sha256:6cb303e084ed78386ae87cdaf95e8817d48e94b3ce7c0442a28335600f0efa3d""
      },
      {
         ""blobSum"": ""sha256:cc7516477cdbfa791d6fd66c9c19b46036cc294f884d8ebbbd0a7fc878433c87""
      },
      {
         ""blobSum"": ""sha256:32442b7d159ed2b7f00b00a989ca1d3ee1a3f566df5d5acbd25f0c3dfdad69d1""
      },
      {
         ""blobSum"": ""sha256:fec44d138823b8076f9a49148f93a7c3d6b0e79ca34b489d60b194d7b1c2c2fa""
      },
      {
         ""blobSum"": ""sha256:1bf4aab5c3b363b4fdfc46026df9ae854db8858a5cbcccdd4409434817d59312""
      },
      {
         ""blobSum"": ""sha256:977c8e6687e0ca5f0682915102c025dc12d7ff71bf70de17aab3502adda25af2""
      },
      {
         ""blobSum"": ""sha256:1558b7c35c9e25577ee719529d6fcdddebea68f5bdf8cbdf13d8d75a02f8a5b1""
      },
      {
         ""blobSum"": ""sha256:842cc8bd099d94f6f9c082785bbaa35439af965d1cf6a13300830561427c266b""
      },
      {
         ""blobSum"": ""sha256:08239cb71d7a3e0d8ed680397590b338a2133117250e1a3e2ee5c5c45292db63""
      },
      {
         ""blobSum"": ""sha256:989ac24c53a1f7951438aa92ac39bc9053c178336bea4ebe6ab733d4975c9728""
      },
      {
         ""blobSum"": ""sha256:f67b87ed7ea47d30c673e289d4c2fd28f5e8e3059840152932e8e813183462ec""
      },
      {
         ""blobSum"": ""sha256:63a0f0b6b5d7014b647ac4a164808208229d2e3219f45a39914f0561a4f831bf""
      },
      {
         ""blobSum"": ""sha256:80d923f4b955c2db89e2e8a9f2dcb0c36a29c1520a5b359578ce2f3d0b849d10""
      },
      {
         ""blobSum"": ""sha256:f4457f4b3bfe0282e803dd9172421048b80168d9c1433da969555fa571a4a1d6""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:b970c9afc934d5e6bb524a6057342a1d1cc835972f047a805f436c540ee20747""
      },
      {
         ""blobSum"": ""sha256:b3a122ff7868d2ed9c063df73b0bf67fd77348d3baa2a92368b3479b41f8aa74""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:213b0c5bb5300df1d2d06df6213ae94448419cf18ecf61358e978a5d25651d5a""
      },
      {
         ""blobSum"": ""sha256:a18e3c45bf91ac3bd11a46b489fb647a721417f60eae66c5f605360ccd8d6352""
      },
      {
         ""blobSum"": ""sha256:50dcd1d0618b1d42bf6633dc8176e164571081494fa6483ec4489a59637518bc""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:0984904c0e1558248eb25e93d9fc14c47c0052d58569e64c185afca93a060b66""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:31aaab384e3fa66b73eced4870fc96be590a2376e93fd4f8db5d00f94fb11604""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:edb369c8c5d7b67e773eee549901a38b80dfa1246597815ae6eb21d1beceec1a""
      },
      {
         ""blobSum"": ""sha256:41d78c0cb1b2a47189068e55f61d6266be14c4fa75935cb021f17668dd8e7f94""
      },
      {
         ""blobSum"": ""sha256:7d4d905c2060a5ec994ec201e6877714ee73030ef4261f9562abdb0f844174d5""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:398819b00c6cbf9cce6c1ed25005c9e1242cace7a6436730e17da052000c7f90""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:cbedea0328015d1baf1efdd06b2417283f6314c0ef671bc0246ada3221ca21ac""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:340cd692075b636b5e1803fcde9b1a56a2f6e2728e4fb10f7295d39c7d0e0d01""
      },
      {
         ""blobSum"": ""sha256:b1d3e8de8ec6d87b8485a8a3b66d63125a033cfb0711f8af24b4f600f524e276""
      },
      {
         ""blobSum"": ""sha256:d9bbcf733166f991331a80e1cd55a91111c4ba96fc7ce1ecabd05b450b7da7a3""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:1b6c70b3786f72e5255ccd51e27840d1c853a17561b5e94a4359b17d27494d50""
      },
      {
         ""blobSum"": ""sha256:0bbc7b377a9155696eb0b684bd1999bc43937918552d73fd9697ea50ef46528a""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:da44f64ae9991a9e8cb7c2af4dfd63608bd4026552b2b6a7f523dcfac960e1ac""
      },
      {
         ""blobSum"": ""sha256:57c8de432dbe337bb6cb1ad328e6c564303a3d3fd05b5e872fd9c47c16fdd02c""
      },
      {
         ""blobSum"": ""sha256:b542772b417703c0311c0b90136091369bcd9c2176c0e3ceed5a0114d743ee3c""
      },
      {
         ""blobSum"": ""sha256:1ab373b3deaed929a15574ac1912afc6e173f80d400aba0e96c89f6a58961f2d""
      },
      {
         ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
      },
      {
         ""blobSum"": ""sha256:c73ab1c6897bf5c11da3c95cab103e7ca8cf10a6d041eda2ff836f45a40e3d3b""
      }
   ],
   ""history"": [
      {
         ""v1Compatibility"": ""{\""architecture\"":\""amd64\"",\""config\"":{\""Hostname\"":\""\"",\""Domainname\"":\""\"",\""User\"":\""nobody\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""ExposedPorts\"":{\""50000/tcp\"":{},\""8080/tcp\"":{}},\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"",\""LANG=C.UTF-8\"",\""JAVA_HOME=/docker-java-home\"",\""JAVA_VERSION=8u162\"",\""JAVA_DEBIAN_VERSION=8u162-b12-1~deb9u1\"",\""CA_CERTIFICATES_JAVA_VERSION=20170531+nmu1\"",\""JENKINS_HOME=/var/jenkinsdcos_home\"",\""JENKINS_SLAVE_AGENT_PORT=50000\"",\""JENKINS_VERSION=2.107.2\"",\""JENKINS_UC=https://updates.jenkins.io\"",\""JENKINS_UC_EXPERIMENTAL=https://updates.jenkins.io/experimental\"",\""COPY_REFERENCE_FILE_LOG=/var/jenkinsdcos_home/copy_reference_file.log\"",\""JENKINS_FOLDER=/usr/share/jenkins\"",\""JENKINS_CSP_OPTS=sandbox; default-src 'none'; img-src 'self'; style-src 'self';\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""/usr/local/jenkins/bin/run.sh\""],\""ArgsEscaped\"":true,\""Image\"":\""sha256:c5e3baf9fe6fc4f564fdad4c9c4705587fad40ae28d1999a608b3547625ffefe\"",\""Volumes\"":{\""/var/jenkins_home\"":{}},\""WorkingDir\"":\""/tmp\"",\""Entrypoint\"":[\""/sbin/tini\"",\""--\"",\""/usr/local/bin/jenkins.sh\""],\""OnBuild\"":[],\""Labels\"":null},\""container\"":\""e4111508e68c304ec5b36009773b41384b96fd887b61177cd42935b9757567fd\"",\""container_config\"":{\""Hostname\"":\""e4111508e68c\"",\""Domainname\"":\""\"",\""User\"":\""nobody\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""ExposedPorts\"":{\""50000/tcp\"":{},\""8080/tcp\"":{}},\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"",\""LANG=C.UTF-8\"",\""JAVA_HOME=/docker-java-home\"",\""JAVA_VERSION=8u162\"",\""JAVA_DEBIAN_VERSION=8u162-b12-1~deb9u1\"",\""CA_CERTIFICATES_JAVA_VERSION=20170531+nmu1\"",\""JENKINS_HOME=/var/jenkinsdcos_home\"",\""JENKINS_SLAVE_AGENT_PORT=50000\"",\""JENKINS_VERSION=2.107.2\"",\""JENKINS_UC=https://updates.jenkins.io\"",\""JENKINS_UC_EXPERIMENTAL=https://updates.jenkins.io/experimental\"",\""COPY_REFERENCE_FILE_LOG=/var/jenkinsdcos_home/copy_reference_file.log\"",\""JENKINS_FOLDER=/usr/share/jenkins\"",\""JENKINS_CSP_OPTS=sandbox; default-src 'none'; img-src 'self'; style-src 'self';\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) \"",\""CMD [\\\""/bin/sh\\\"" \\\""-c\\\"" \\\""/usr/local/jenkins/bin/run.sh\\\""]\""],\""ArgsEscaped\"":true,\""Image\"":\""sha256:c5e3baf9fe6fc4f564fdad4c9c4705587fad40ae28d1999a608b3547625ffefe\"",\""Volumes\"":{\""/var/jenkins_home\"":{}},\""WorkingDir\"":\""/tmp\"",\""Entrypoint\"":[\""/sbin/tini\"",\""--\"",\""/usr/local/bin/jenkins.sh\""],\""OnBuild\"":[],\""Labels\"":{}},\""created\"":\""2018-09-26T17:33:57.6822239Z\"",\""docker_version\"":\""18.03.0-ce\"",\""id\"":\""fb401ed0b4f9de5534c224811d0dca94b876225c31ddc3cbb0993ad2faf32cff\"",\""os\"":\""linux\"",\""parent\"":\""bb54e3dc4a692004ece424733d0ff7bbfe930bdc65491776e2f409a461e838f1\"",\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""bb54e3dc4a692004ece424733d0ff7bbfe930bdc65491776e2f409a461e838f1\"",\""parent\"":\""2ef4a3efec8e89b04691b207a4fe3fddd2e3d3a2a539f8e7dc8a645773758e1f\"",\""created\"":\""2018-09-26T17:33:57.3350528Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c echo 2.0 \\u003e /usr/share/jenkins/ref/jenkins.install.UpgradeWizard.state\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""2ef4a3efec8e89b04691b207a4fe3fddd2e3d3a2a539f8e7dc8a645773758e1f\"",\""parent\"":\""36bfc452a3f13511e6f9c0345ffac82054286eea40b02263c83ea791d00a22ea\"",\""created\"":\""2018-09-26T17:33:56.0461597Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  USER nobody\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""36bfc452a3f13511e6f9c0345ffac82054286eea40b02263c83ea791d00a22ea\"",\""parent\"":\""ea550cbe252ca3ca06771b0e837d1f7cc61c50404f7f1920ed5bc6cc816d8a0a\"",\""created\"":\""2018-09-26T17:33:55.6692099Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c chmod -R ugo+rw \\\""$JENKINS_HOME\\\"" \\\""${JENKINS_FOLDER}\\\""     \\u0026\\u0026 chmod -R ugo+r \\\""${JENKINS_STAGING}\\\""     \\u0026\\u0026 chmod -R ugo+rx /usr/local/jenkins/bin/     \\u0026\\u0026 chmod -R ugo+rw /var/jenkins_home/     \\u0026\\u0026 chmod -R ugo+rw /var/lib/nginx/ /var/nginx/ /var/log/nginx     \\u0026\\u0026 chmod ugo+rx /usr/local/jenkins/bin/*\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""ea550cbe252ca3ca06771b0e837d1f7cc61c50404f7f1920ed5bc6cc816d8a0a\"",\""parent\"":\""c4c140687ce95f2d23202b7efaa543ef7d064b226864fb4a0ae68bef283e074f\"",\""created\"":\""2018-09-26T17:33:49.7534514Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c groupadd -g ${gid} nobody     \\u0026\\u0026 usermod -u ${uid} -g ${gid} ${user}     \\u0026\\u0026 usermod -a -G users nobody     \\u0026\\u0026 echo \\\""nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin\\\"" \\u003e\\u003e /etc/passwd\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""c4c140687ce95f2d23202b7efaa543ef7d064b226864fb4a0ae68bef283e074f\"",\""parent\"":\""2f04614030c91b184503348484780b62bda952a2905cc1fb035c5a6f371ca239\"",\""created\"":\""2018-09-26T17:33:48.3150654Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) ADD c84b80e3ceaef7f211a221093369729eeb89e5cfc5f3d0a5cd4917e7b6c7027f in /usr/share/jenkins/ref//plugins/metrics-graphite.hpi \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""2f04614030c91b184503348484780b62bda952a2905cc1fb035c5a6f371ca239\"",\""parent\"":\""a0ad27b653be7a9400d9e46784f897097cf24f157bfc3fb647e49c360b7c12c1\"",\""created\"":\""2018-09-26T17:33:47.8920446Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) ADD f4d41c9bf39651b20107d62d85c101014320946e6a33763e5519ec18aee77858 in /usr/share/jenkins/ref//plugins/prometheus.hpi \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""a0ad27b653be7a9400d9e46784f897097cf24f157bfc3fb647e49c360b7c12c1\"",\""parent\"":\""f9f80b9791fb4dd2e525de37e33e0b730809bed2b2edb7898b802d3fde3d9c08\"",\""created\"":\""2018-09-26T17:33:46.775839Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) ADD 652f0ad5e9ad70b4db10957b64265f808b45c63d8ef07b107d3082450084164c in /usr/share/jenkins/ref//plugins/mesos.hpi \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f9f80b9791fb4dd2e525de37e33e0b730809bed2b2edb7898b802d3fde3d9c08\"",\""parent\"":\""0b648b2545d81712d56536a42a0a99d3d78008bf6f1f04f22d140c427b645b76\"",\""created\"":\""2018-09-26T17:33:45.5611867Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c /usr/local/bin/install-plugins.sh         blueocean-bitbucket-pipeline:${BLUEOCEAN_VERSION}      blueocean-commons:${BLUEOCEAN_VERSION}      blueocean-config:${BLUEOCEAN_VERSION}       blueocean-dashboard:${BLUEOCEAN_VERSION}    blueocean-events:${BLUEOCEAN_VERSION}       blueocean-git-pipeline:${BLUEOCEAN_VERSION}            blueocean-github-pipeline:${BLUEOCEAN_VERSION}         blueocean-i18n:${BLUEOCEAN_VERSION}         blueocean-jwt:${BLUEOCEAN_VERSION}          blueocean-jira:${BLUEOCEAN_VERSION}         blueocean-personalization:${BLUEOCEAN_VERSION}          blueocean-pipeline-api-impl:${BLUEOCEAN_VERSION}        blueocean-pipeline-editor:${BLUEOCEAN_VERSION}          blueocean-pipeline-scm-api:${BLUEOCEAN_VERSION}         blueocean-rest-impl:${BLUEOCEAN_VERSION}    blueocean-rest:${BLUEOCEAN_VERSION}         blueocean-web:${BLUEOCEAN_VERSION}          blueocean:${BLUEOCEAN_VERSION}              ant:1.8                          ansicolor:0.5.2                  antisamy-markup-formatter:1.5    artifactory:2.15.1               authentication-tokens:1.3        azure-credentials:1.6.0          azure-vm-agents:0.7.0            branch-api:2.0.19                build-name-setter:1.6.9          build-timeout:1.19               cloudbees-folder:6.4             conditional-buildstep:1.3.6      config-file-provider:2.18        copyartifact:1.39.1              cvs:2.14                         docker-build-publish:1.3.2       docker-workflow:1.15.1           durable-task:1.22                ec2:1.39                         embeddable-build-status:1.9      external-monitor-job:1.7         ghprb:1.40.0                     git:3.8.0                        git-client:2.7.1                 git-server:1.7                   github:1.29.0                    github-api:1.90                  github-branch-source:2.3.3       github-organization-folder:1.6   gitlab-plugin:1.5.5              gradle:1.28                      greenballs:1.15                  handlebars:1.1.1                 ivy:1.28                         jackson2-api:2.8.11.3            job-dsl:1.68                     jobConfigHistory:2.18            jquery:1.12.4-0                  ldap:1.20                        mapdb-api:1.0.9.0                marathon:1.6.0                   matrix-auth:2.2                  matrix-project:1.13              maven-plugin:3.1.2               metrics:3.1.2.11                 monitoring:1.72.0                nant:1.4.3                       node-iterator-api:1.5.0          pam-auth:1.3                     parameterized-trigger:2.35.2     pipeline-build-step:2.7          pipeline-github-lib:1.0          pipeline-input-step:2.8          pipeline-milestone-step:1.3.1    pipeline-model-api:1.2.8         pipeline-model-definition:1.2.8   pipeline-model-extensions:1.2.8   pipeline-rest-api:2.10           pipeline-stage-step:2.3          pipeline-stage-view:2.10         plain-credentials:1.4            prometheus:1.2.0                 rebuild:1.28                     role-strategy:2.7.0              run-condition:1.0                s3:0.11.0                        saferestart:0.3                  saml:1.0.5                       scm-api:2.2.6                    ssh-agent:1.15                   ssh-slaves:1.26                  subversion:2.10.5                timestamper:1.8.9                translation:1.16                 variant:1.1                      windows-slaves:1.3.1             workflow-aggregator:2.5          workflow-api:2.27                workflow-basic-steps:2.6         workflow-cps:2.48                workflow-cps-global-lib:2.9      workflow-durable-task-step:2.19   workflow-job:2.18                workflow-multibranch:2.17        workflow-scm-step:2.6            workflow-step-api:2.14           workflow-support:2.18\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""0b648b2545d81712d56536a42a0a99d3d78008bf6f1f04f22d140c427b645b76\"",\""parent\"":\""1982429c4258750d3f70dc0f1c563e870725c6d807e9444d9785456d626ef556\"",\""created\"":\""2018-09-26T17:31:24.2544617Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:59ced817d4cd74453e0658c69f937959d2b4d86cfe15d699cd1fdcf2f6867067 in /usr/share/jenkins/ref//init.groovy.d/mesos-auth.groovy \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""1982429c4258750d3f70dc0f1c563e870725c6d807e9444d9785456d626ef556\"",\""parent\"":\""35f1271210258c711e78bd483beee0a7fc9c2d4ee12cf78787d7992277c5a957\"",\""created\"":\""2018-09-26T17:31:23.9384301Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:8ca0529d27d0fa91b7848e39a5d04e55df01746ab31ca6bae1816f062667f8cc in /usr/share/jenkins/ref//nodeMonitors.xml \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""35f1271210258c711e78bd483beee0a7fc9c2d4ee12cf78787d7992277c5a957\"",\""parent\"":\""12d065bb54a4529a4afddab4e580cb4cb78e509a2c7d6faa7e3967510f783887\"",\""created\"":\""2018-09-26T17:31:23.609004Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:beed7a659bf7217db04b70fa4220df32e07015c6f20edf4d73b5cab69354542e in /usr/share/jenkins/ref//jenkins.model.JenkinsLocationConfiguration.xml \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""12d065bb54a4529a4afddab4e580cb4cb78e509a2c7d6faa7e3967510f783887\"",\""parent\"":\""87ea27ae8f877a36014c0eeb3dba7c0a7b29cfc2d779a10bf438dbce8078cc62\"",\""created\"":\""2018-09-26T17:31:23.3055734Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:46468ed2b6fa66eeea868396b18d952f8cbdd0df6529ec2a4d5782a1acc7ee7a in /usr/share/jenkins/ref//config.xml \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""87ea27ae8f877a36014c0eeb3dba7c0a7b29cfc2d779a10bf438dbce8078cc62\"",\""parent\"":\""a13b24c2681da6bacf19dd99041ba2921a11d780fd19edc1b6c0d0b982deb730\"",\""created\"":\""2018-09-26T17:31:23.003904Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:6b54409cf8c3ce4dae538b70b64f8755636613e71806e479c5d8f081224c63e9 in /var/nginx/nginx.conf \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""a13b24c2681da6bacf19dd99041ba2921a11d780fd19edc1b6c0d0b982deb730\"",\""parent\"":\""39807d50654d8e842f4bf754ef8c4e5b72780dc6403db3e78611e7356c3c2173\"",\""created\"":\""2018-09-26T17:31:22.6859214Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c mkdir -p /var/log/nginx/jenkins /var/nginx/\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""39807d50654d8e842f4bf754ef8c4e5b72780dc6403db3e78611e7356c3c2173\"",\""parent\"":\""0bd3eec411704b039cfeb3aec6c6e14882dbc8db68561fe0904548ba708394c5\"",\""created\"":\""2018-09-26T17:31:21.2086534Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:a4cf73ccc8a0e4b1a7acef249766ce76b31bf76d03f97ac157d6eccfab30d4f5 in /usr/local/jenkins/bin/run.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""0bd3eec411704b039cfeb3aec6c6e14882dbc8db68561fe0904548ba708394c5\"",\""parent\"":\""f7ede5d3afbd0518be79d105f5ccb6a1f96117291300ba3acc32d9007c71d6a9\"",\""created\"":\""2018-09-26T17:31:20.9064351Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:3377f08a63084052efa9902be76b1eb669229849b476b52f448697333457e769 in /usr/local/jenkins/bin/dcos-account.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f7ede5d3afbd0518be79d105f5ccb6a1f96117291300ba3acc32d9007c71d6a9\"",\""parent\"":\""2f6f3100494a6050ecd28e7e1a647a3fc1f5e9dbde64a6646dc5e8f418bc7397\"",\""created\"":\""2018-09-26T17:31:20.5594535Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:5814edade36c8c883f19e868796f1ae1d46d6990af813451101abec8196856d4 in /usr/local/jenkins/bin/export-libssl.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""2f6f3100494a6050ecd28e7e1a647a3fc1f5e9dbde64a6646dc5e8f418bc7397\"",\""parent\"":\""ee31e5ffacfc38c46b9fdad7ecb47ab748d5eb9baf82ea8cf2766df3f9e18cdc\"",\""created\"":\""2018-09-26T17:31:20.2349213Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:8206c6af7dc8888193958fd9428ba085ae19c8282c26eb05fb9f4c4f46973a4e in /usr/local/jenkins/bin/bootstrap.py \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""ee31e5ffacfc38c46b9fdad7ecb47ab748d5eb9baf82ea8cf2766df3f9e18cdc\"",\""parent\"":\""9fc63748986386223542f05b4c9685481a303ebb3e30603e174e65121906ea55\"",\""created\"":\""2018-07-09T20:54:30.984299193Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c echo 'networkaddress.cache.ttl=60' \\u003e\\u003e ${JAVA_HOME}/jre/lib/security/java.security\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9fc63748986386223542f05b4c9685481a303ebb3e30603e174e65121906ea55\"",\""parent\"":\""e37610cd26b6212ad9670f101e7d8d70cf108097ead058de50d5d7401cad8b22\"",\""created\"":\""2018-07-09T20:54:29.524404063Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c mkdir -p \\\""${JENKINS_HOME}\\\"" \\\""${JENKINS_FOLDER}/war\\\""\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""e37610cd26b6212ad9670f101e7d8d70cf108097ead058de50d5d7401cad8b22\"",\""parent\"":\""84f29eb0d8745c0510f8d48c347676ff1283ec4921bb2e3e7f462681d8e62ba3\"",\""created\"":\""2018-07-09T20:54:28.236876676Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c echo \\\""deb http://ftp.debian.org/debian testing main\\\"" \\u003e\\u003e /etc/apt/sources.list   \\u0026\\u0026 apt-get update \\u0026\\u0026 apt-get -t testing install -y git\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""84f29eb0d8745c0510f8d48c347676ff1283ec4921bb2e3e7f462681d8e62ba3\"",\""parent\"":\""a0d4487b3138fe03804efb8c6aec561dcd75c6dc45d57458b9e46ca1184b49c2\"",\""created\"":\""2018-07-09T20:54:14.100019856Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c curl -fsSL \\\""$LIBMESOS_DOWNLOAD_URL\\\"" -o libmesos-bundle.tar.gz    \\u0026\\u0026 echo \\\""$LIBMESOS_DOWNLOAD_SHA256 libmesos-bundle.tar.gz\\\"" | sha256sum -c -   \\u0026\\u0026 tar -C / -xzf libmesos-bundle.tar.gz    \\u0026\\u0026 rm libmesos-bundle.tar.gz\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""a0d4487b3138fe03804efb8c6aec561dcd75c6dc45d57458b9e46ca1184b49c2\"",\""parent\"":\""bfc2e6fd97b4b81249e6ad41a0709d1d62de3c35251e1e07b39a855111c6f465\"",\""created\"":\""2018-07-09T20:54:00.580952612Z\"",\""container_config\"":{\""Cmd\"":[\""|11 BLUEOCEAN_VERSION=1.5.0 JENKINS_DCOS_HOME=/var/jenkinsdcos_home JENKINS_STAGING=/usr/share/jenkins/ref/ LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274 LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133 PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8 gid=99 uid=99 user=nobody /bin/sh -c apt-get update \\u0026\\u0026 apt-get install -y nginx python zip jq\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""bfc2e6fd97b4b81249e6ad41a0709d1d62de3c35251e1e07b39a855111c6f465\"",\""parent\"":\""c7344d4d0e1cb95c3e531da5c342fc2fb289e4e51334ca2d1b430de241b28722\"",\""created\"":\""2018-07-09T20:53:46.425927046Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  USER root\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""c7344d4d0e1cb95c3e531da5c342fc2fb289e4e51334ca2d1b430de241b28722\"",\""parent\"":\""662ff9075f47894530c796a8e9b2fafe7f1bc5be7ec38b35d757d442b6833a84\"",\""created\"":\""2018-07-09T20:53:46.096470837Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_CSP_OPTS=sandbox; default-src 'none'; img-src 'self'; style-src 'self';\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""662ff9075f47894530c796a8e9b2fafe7f1bc5be7ec38b35d757d442b6833a84\"",\""parent\"":\""5f2b4cf9791c5f60b455dd4b847028183d61d7d519fc5c6bd1b6fe50ef119e74\"",\""created\"":\""2018-07-09T20:53:45.797188526Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV COPY_REFERENCE_FILE_LOG=/var/jenkinsdcos_home/copy_reference_file.log\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""5f2b4cf9791c5f60b455dd4b847028183d61d7d519fc5c6bd1b6fe50ef119e74\"",\""parent\"":\""25792f867e22dae243cd783474ff4eed5d54a323665c33205058d5a6adf545d9\"",\""created\"":\""2018-07-09T20:53:45.462915577Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_HOME=/var/jenkinsdcos_home\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""25792f867e22dae243cd783474ff4eed5d54a323665c33205058d5a6adf545d9\"",\""parent\"":\""1ff15242962d34fd623e14d7a02db78036b900bb2e526a74d44fc903477cc9e7\"",\""created\"":\""2018-07-09T20:53:45.124088811Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG gid=99\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""1ff15242962d34fd623e14d7a02db78036b900bb2e526a74d44fc903477cc9e7\"",\""parent\"":\""fc1935bf0e7b1a47c7b5947f1ba2c9347472b19f6a5cf4e0553c7c8dd4dac4c2\"",\""created\"":\""2018-07-09T20:53:44.827537014Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG uid=99\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""fc1935bf0e7b1a47c7b5947f1ba2c9347472b19f6a5cf4e0553c7c8dd4dac4c2\"",\""parent\"":\""3d5eaaf246c3dd14cb486d1249fc41cdba012693b3d20dafd9e9a395d104f740\"",\""created\"":\""2018-07-09T20:53:44.458211965Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG user=nobody\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""3d5eaaf246c3dd14cb486d1249fc41cdba012693b3d20dafd9e9a395d104f740\"",\""parent\"":\""ede8ff5b5cfbe778c731602a3da663b93f3c3304b3a34255412d1631d4b77c18\"",\""created\"":\""2018-07-09T20:53:44.10755361Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG JENKINS_DCOS_HOME=/var/jenkinsdcos_home\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""ede8ff5b5cfbe778c731602a3da663b93f3c3304b3a34255412d1631d4b77c18\"",\""parent\"":\""b2579c2af8b8b590584e666fc0a0427e6baa3b46bf9a1ec9b05aa37bbf6fe2fc\"",\""created\"":\""2018-07-09T20:53:43.757033301Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG STATSD_PLUG_HASH=929d4a6cb3d3ce5f1e03af73075b13687d4879c8\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""b2579c2af8b8b590584e666fc0a0427e6baa3b46bf9a1ec9b05aa37bbf6fe2fc\"",\""parent\"":\""d12f6a61c0f00cd2eeb401653c4ec8f52f9fc996514741a6af9e5c57a082f3a8\"",\""created\"":\""2018-07-09T20:53:43.442946812Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG PROMETHEUS_PLUG_HASH=a347bf2c63efe59134c15b8ef83a4a1f627e3b5d\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""d12f6a61c0f00cd2eeb401653c4ec8f52f9fc996514741a6af9e5c57a082f3a8\"",\""parent\"":\""fef12017cd9254dbbbb938bfadd8a9401241149c9a52c347373395a52a1c4ebe\"",\""created\"":\""2018-07-09T20:53:43.116440726Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG MESOS_PLUG_HASH=347c1ac133dc0cb6282a0dde820acd5b4eb21133\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""fef12017cd9254dbbbb938bfadd8a9401241149c9a52c347373395a52a1c4ebe\"",\""parent\"":\""f81de07a754d737714fcaec8c6d6db1656e64d6cca56e347ccebf23776b00617\"",\""created\"":\""2018-04-24T20:52:04.5174488Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG JENKINS_STAGING=/usr/share/jenkins/ref/\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f81de07a754d737714fcaec8c6d6db1656e64d6cca56e347ccebf23776b00617\"",\""parent\"":\""9780d937abc8609fd998fecda4c079d6b097b1cb6a714de993329a2f56548133\"",\""created\"":\""2018-04-24T20:52:04.1863586Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG BLUEOCEAN_VERSION=1.5.0\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9780d937abc8609fd998fecda4c079d6b097b1cb6a714de993329a2f56548133\"",\""parent\"":\""72437e315d169d61ae285cc7cb1fecb5ada9249936e77c39ce28d85e7e6d727d\"",\""created\"":\""2018-04-24T20:52:03.8152478Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG LIBMESOS_DOWNLOAD_SHA256=bd4a785393f0477da7f012bf9624aa7dd65aa243c94d38ffe94adaa10de30274\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""72437e315d169d61ae285cc7cb1fecb5ada9249936e77c39ce28d85e7e6d727d\"",\""parent\"":\""9fa0879e36daa2a5a08776124775e11a7fdb51ca2fb19407baa7dedad3ef97a8\"",\""created\"":\""2018-04-24T20:52:03.4353208Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG LIBMESOS_DOWNLOAD_URL=https://downloads.mesosphere.io/libmesos-bundle/libmesos-bundle-1.11.0.tar.gz\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9fa0879e36daa2a5a08776124775e11a7fdb51ca2fb19407baa7dedad3ef97a8\"",\""parent\"":\""764cacaf206bfbeb6b11777e9b65d16ad6350530f559b9f78ee15413548d7749\"",\""created\"":\""2018-04-24T20:52:03.0719423Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_FOLDER=/usr/share/jenkins\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""764cacaf206bfbeb6b11777e9b65d16ad6350530f559b9f78ee15413548d7749\"",\""parent\"":\""7f59fe46c09c4aa9f59ab9268e34571a0bbc0cfa3c0ac4e4d8e55fdf1392bbd5\"",\""created\"":\""2018-04-24T20:52:02.73463Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) WORKDIR /tmp\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""7f59fe46c09c4aa9f59ab9268e34571a0bbc0cfa3c0ac4e4d8e55fdf1392bbd5\"",\""parent\"":\""35094fea2af3c400c07fc444f7477b90caf1013b87e40696cfa9e57fc0f9c80b\"",\""created\"":\""2018-04-11T10:05:00.283278344Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:2874a36404a19c4075e62bf579a79bf730d317e628e80b03c676af4509481acc in /usr/local/bin/install-plugins.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""35094fea2af3c400c07fc444f7477b90caf1013b87e40696cfa9e57fc0f9c80b\"",\""parent\"":\""9d76182ab3259983e8e14d62c9461f4b08404a709f216b805649ac1a448a1fcc\"",\""created\"":\""2018-04-11T10:04:58.564052111Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:39d6085e6ad132734efabf90a5444f3bc74a21e8bf5a79f4d0176ac18bb98217 in /usr/local/bin/plugins.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9d76182ab3259983e8e14d62c9461f4b08404a709f216b805649ac1a448a1fcc\"",\""parent\"":\""fa49692d5ea14814e6efec3af9517b31313a7461065aeb184acdb68d5df23196\"",\""created\"":\""2018-04-11T10:04:56.647913351Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENTRYPOINT [\\\""/sbin/tini\\\"" \\\""--\\\"" \\\""/usr/local/bin/jenkins.sh\\\""]\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""fa49692d5ea14814e6efec3af9517b31313a7461065aeb184acdb68d5df23196\"",\""parent\"":\""afabf13b1f4f211c64b5535617d2ceb53038f76ceb76b74d8c2c0c66f9a5c9a3\"",\""created\"":\""2018-04-11T10:04:54.736575307Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:dc942ca949bb159f81bbc954773b3491e433d2d3e3ef90bac80ecf48a313c9c9 in /bin/tini \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""afabf13b1f4f211c64b5535617d2ceb53038f76ceb76b74d8c2c0c66f9a5c9a3\"",\""parent\"":\""64c769f00fe141263cbb93af8dded7de9c06f1c96010adbb22f4dc6acd0decc9\"",\""created\"":\""2018-04-11T10:04:51.974150657Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:1a73810a97d134925c37b2276c894e0a9c92125cdd8c750aaf8ef15c3c20aa72 in /usr/local/bin/jenkins.sh \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""64c769f00fe141263cbb93af8dded7de9c06f1c96010adbb22f4dc6acd0decc9\"",\""parent\"":\""0900f664d5ac780bca9e91b18de9f4680eb0eb4842e81cbe26b3a22f3eb8fdec\"",\""created\"":\""2018-04-11T10:04:50.171056466Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:88dd96a27353c9d476981c3cfc6b39c95983c45083324afa7c8bddb682d91bff in /usr/local/bin/jenkins-support \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""0900f664d5ac780bca9e91b18de9f4680eb0eb4842e81cbe26b3a22f3eb8fdec\"",\""parent\"":\""d70c19ee5d688e37e2fce67f01fd691c2509d45dd7903f68ede5ca78d1b7bdc4\"",\""created\"":\""2018-04-11T10:04:48.292041295Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  USER jenkins\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""d70c19ee5d688e37e2fce67f01fd691c2509d45dd7903f68ede5ca78d1b7bdc4\"",\""parent\"":\""c94d46c5aef23a2d56d2ac621b24a6778dcfbd81e1af03f2940ae91dcd991a20\"",\""created\"":\""2018-04-11T10:04:46.288406797Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV COPY_REFERENCE_FILE_LOG=/var/jenkins_home/copy_reference_file.log\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""c94d46c5aef23a2d56d2ac621b24a6778dcfbd81e1af03f2940ae91dcd991a20\"",\""parent\"":\""09e94fdaea35187ca8029b96c2180f04615c8e66a8255dc939f16c9429ba003f\"",\""created\"":\""2018-04-11T10:04:44.37013921Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  EXPOSE 50000\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""09e94fdaea35187ca8029b96c2180f04615c8e66a8255dc939f16c9429ba003f\"",\""parent\"":\""3d98aa3161d75b086c38aee654e4612b83b1bbfdea154785537df95ae157ca5a\"",\""created\"":\""2018-04-11T10:04:42.447771731Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  EXPOSE 8080\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""3d98aa3161d75b086c38aee654e4612b83b1bbfdea154785537df95ae157ca5a\"",\""parent\"":\""6de02152b7dce054eee77ce934fa5652c2142a8a210060e19872db23d9afdacd\"",\""created\"":\""2018-04-11T10:04:40.453492565Z\"",\""container_config\"":{\""Cmd\"":[\""|9 JENKINS_SHA=079ab885be74ea3dd4d2a57dd804a296752fae861f2d7c379bce06b674ae67ed JENKINS_URL=https://repo.jenkins-ci.org/public/org/jenkins-ci/main/jenkins-war/2.107.2/jenkins-war-2.107.2.war TINI_VERSION=v0.16.1 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c chown -R ${user} \\\""$JENKINS_HOME\\\"" /usr/share/jenkins/ref\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""6de02152b7dce054eee77ce934fa5652c2142a8a210060e19872db23d9afdacd\"",\""parent\"":\""c178ed9e80695242e5ae0f6611ddce99a857d3e5a295207bc45d1e680d3c1379\"",\""created\"":\""2018-04-11T10:04:37.42404848Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_UC_EXPERIMENTAL=https://updates.jenkins.io/experimental\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""c178ed9e80695242e5ae0f6611ddce99a857d3e5a295207bc45d1e680d3c1379\"",\""parent\"":\""9aed7a6b86d981cfa853eae0e5a716d16be68b4475884e6b275762e8770946c8\"",\""created\"":\""2018-04-11T10:04:35.309385797Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_UC=https://updates.jenkins.io\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9aed7a6b86d981cfa853eae0e5a716d16be68b4475884e6b275762e8770946c8\"",\""parent\"":\""6345f944b3e6d2f6e763b56af436b46d26c5424282ca0d2c57994beb2d5d1707\"",\""created\"":\""2018-04-11T10:04:33.341878374Z\"",\""container_config\"":{\""Cmd\"":[\""|9 JENKINS_SHA=079ab885be74ea3dd4d2a57dd804a296752fae861f2d7c379bce06b674ae67ed JENKINS_URL=https://repo.jenkins-ci.org/public/org/jenkins-ci/main/jenkins-war/2.107.2/jenkins-war-2.107.2.war TINI_VERSION=v0.16.1 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c curl -fsSL ${JENKINS_URL} -o /usr/share/jenkins/jenkins.war   \\u0026\\u0026 echo \\\""${JENKINS_SHA}  /usr/share/jenkins/jenkins.war\\\"" | sha256sum -c -\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""6345f944b3e6d2f6e763b56af436b46d26c5424282ca0d2c57994beb2d5d1707\"",\""parent\"":\""0b6d17bc569a59dadd8ce806ecf325bc39e0445b5097b004c4c5771d030a754c\"",\""created\"":\""2018-04-11T10:04:28.72473862Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG JENKINS_URL=https://repo.jenkins-ci.org/public/org/jenkins-ci/main/jenkins-war/2.107.2/jenkins-war-2.107.2.war\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""0b6d17bc569a59dadd8ce806ecf325bc39e0445b5097b004c4c5771d030a754c\"",\""parent\"":\""14f6f33e1ddfb9196b4840cc1fb6de989a15de625581d7774c3629a4cc57e48d\"",\""created\"":\""2018-04-11T10:04:26.621369421Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG JENKINS_SHA=2d71b8f87c8417f9303a73d52901a59678ee6c0eefcf7325efed6035ff39372a\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""14f6f33e1ddfb9196b4840cc1fb6de989a15de625581d7774c3629a4cc57e48d\"",\""parent\"":\""3f780bea9189f56ebd2e363fe471a428bd75a2972e7cfc7dd997633b4a8eb951\"",\""created\"":\""2018-04-11T10:04:24.515479866Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_VERSION=2.107.2\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""3f780bea9189f56ebd2e363fe471a428bd75a2972e7cfc7dd997633b4a8eb951\"",\""parent\"":\""8384b8fae6e40ab737b32a849d462b1fedf10b34be86a67ead51536b5e278a90\"",\""created\"":\""2018-04-11T10:04:22.485876008Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG JENKINS_VERSION\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""8384b8fae6e40ab737b32a849d462b1fedf10b34be86a67ead51536b5e278a90\"",\""parent\"":\""14e86ed54896e03f91257436134c46b72b2230612de7487e208d09b2409c9366\"",\""created\"":\""2018-04-11T10:04:20.518174508Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:c84b91c835048a52bb864c1f4662607c56befe3c4b1520b0ea94633103a4554f in /usr/share/jenkins/ref/init.groovy.d/tcp-slave-agent-port.groovy \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""14e86ed54896e03f91257436134c46b72b2230612de7487e208d09b2409c9366\"",\""parent\"":\""f69098d00b0be8a19c2189ff51d0fd286f3d1bba4b02024f5127e3afc65c241d\"",\""created\"":\""2018-04-11T10:04:18.593424219Z\"",\""container_config\"":{\""Cmd\"":[\""|7 TINI_VERSION=v0.16.1 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c curl -fsSL https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini-static-$(dpkg --print-architecture) -o /sbin/tini   \\u0026\\u0026 curl -fsSL https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini-static-$(dpkg --print-architecture).asc -o /sbin/tini.asc   \\u0026\\u0026 gpg --import /var/jenkins_home/tini_pub.gpg   \\u0026\\u0026 gpg --verify /sbin/tini.asc   \\u0026\\u0026 rm -rf /sbin/tini.asc /root/.gnupg   \\u0026\\u0026 chmod +x /sbin/tini\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f69098d00b0be8a19c2189ff51d0fd286f3d1bba4b02024f5127e3afc65c241d\"",\""parent\"":\""8a6f3dcf5ca0ca44988097f598596e70cd0f59d9600c115d65fb35ff330848c2\"",\""created\"":\""2018-04-11T10:04:13.905006564Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) COPY file:653491cb486e752a4c2b4b407a46ec75646a54eabb597634b25c7c2b82a31424 in /var/jenkins_home/tini_pub.gpg \""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""8a6f3dcf5ca0ca44988097f598596e70cd0f59d9600c115d65fb35ff330848c2\"",\""parent\"":\""1c0472056a7e929c90a91ff8b93a3e66caad76310b8431a1efe220ad734e066c\"",\""created\"":\""2018-04-11T10:04:11.747045116Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG TINI_VERSION=v0.16.1\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""1c0472056a7e929c90a91ff8b93a3e66caad76310b8431a1efe220ad734e066c\"",\""parent\"":\""9e7db4035ee231fa280046b53c0513db0df2c9d49938cc74c7fb195d398ce5fb\"",\""created\"":\""2018-04-11T10:04:09.646844829Z\"",\""container_config\"":{\""Cmd\"":[\""|6 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c mkdir -p /usr/share/jenkins/ref/init.groovy.d\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""9e7db4035ee231fa280046b53c0513db0df2c9d49938cc74c7fb195d398ce5fb\"",\""parent\"":\""cd5438b31ee369424fea23648dc89429b4cef574734f0933e48516c7ba9caf65\"",\""created\"":\""2018-04-11T10:04:05.986383436Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  VOLUME [/var/jenkins_home]\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""cd5438b31ee369424fea23648dc89429b4cef574734f0933e48516c7ba9caf65\"",\""parent\"":\""8e94e7a7be85cbc5b57c4f3e140535e297674e8d30df6e0ed6dbf2128b82935a\"",\""created\"":\""2018-04-11T10:04:03.98242692Z\"",\""container_config\"":{\""Cmd\"":[\""|6 agent_port=50000 gid=1000 group=jenkins http_port=8080 uid=1000 user=jenkins /bin/sh -c groupadd -g ${gid} ${group}     \\u0026\\u0026 useradd -d \\\""$JENKINS_HOME\\\"" -u ${uid} -g ${gid} -m -s /bin/bash ${user}\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""8e94e7a7be85cbc5b57c4f3e140535e297674e8d30df6e0ed6dbf2128b82935a\"",\""parent\"":\""f28491aa4d16c9db8733a4efb3588ee73941a4e3e1cfb9f5f50293f894307e96\"",\""created\"":\""2018-04-11T10:04:00.815710832Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_SLAVE_AGENT_PORT=50000\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f28491aa4d16c9db8733a4efb3588ee73941a4e3e1cfb9f5f50293f894307e96\"",\""parent\"":\""5b04eeff8ce581af83220d1ff6f7d89806ddffa65a231b727f94f88ea19d02f0\"",\""created\"":\""2018-04-11T10:03:58.893891854Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JENKINS_HOME=/var/jenkins_home\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""5b04eeff8ce581af83220d1ff6f7d89806ddffa65a231b727f94f88ea19d02f0\"",\""parent\"":\""b53c255204bc7107ee1d6d118ec9369ff868b049ae612f9927425f187df17b72\"",\""created\"":\""2018-04-11T10:03:57.021756845Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG agent_port=50000\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""b53c255204bc7107ee1d6d118ec9369ff868b049ae612f9927425f187df17b72\"",\""parent\"":\""d43650df8b88478fd70464bfdf9812cfcc5c2ae32e753cf62d9c68fe3aacc7bb\"",\""created\"":\""2018-04-11T10:03:55.096596096Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG http_port=8080\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""d43650df8b88478fd70464bfdf9812cfcc5c2ae32e753cf62d9c68fe3aacc7bb\"",\""parent\"":\""35011013d34dc72632cd62d92a28a8f789af62553f69feddb4f8f1699b2288c4\"",\""created\"":\""2018-04-11T10:03:53.140848234Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG gid=1000\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""35011013d34dc72632cd62d92a28a8f789af62553f69feddb4f8f1699b2288c4\"",\""parent\"":\""d9ae2fc7815c25c54be284aac826ee5fc6506626a9b5e839db1dbff5aed85ec7\"",\""created\"":\""2018-04-11T10:03:51.085212134Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG uid=1000\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""d9ae2fc7815c25c54be284aac826ee5fc6506626a9b5e839db1dbff5aed85ec7\"",\""parent\"":\""edaba7f43f101619ec35d84a5362844daa52078110c37ac76913a116b75bb0a7\"",\""created\"":\""2018-04-11T10:03:49.08677048Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG group=jenkins\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""edaba7f43f101619ec35d84a5362844daa52078110c37ac76913a116b75bb0a7\"",\""parent\"":\""27a497071bd1dbbda48bb96b8c4390c76f4b894722330e4f58fad50195326761\"",\""created\"":\""2018-04-11T10:03:47.139089021Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ARG user=jenkins\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""27a497071bd1dbbda48bb96b8c4390c76f4b894722330e4f58fad50195326761\"",\""parent\"":\""558e2b91047ab320c5fae50f79befa3e641fff2f7e2af49e7cc0dfbecc16635b\"",\""created\"":\""2018-04-11T10:03:45.06746326Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c apt-get update \\u0026\\u0026 apt-get install -y git curl \\u0026\\u0026 rm -rf /var/lib/apt/lists/*\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""558e2b91047ab320c5fae50f79befa3e641fff2f7e2af49e7cc0dfbecc16635b\"",\""parent\"":\""180253c9de444b27748a3818b60ac46af5979f9a2b8f714fbe8ee9d8403e4835\"",\""created\"":\""2018-03-19T21:23:43.026367652Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c /var/lib/dpkg/info/ca-certificates-java.postinst configure\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""180253c9de444b27748a3818b60ac46af5979f9a2b8f714fbe8ee9d8403e4835\"",\""parent\"":\""4443d1ea64bf58fcf407017a70cf91b3d2fc25b535f397f3f8d4cbcc21a8def3\"",\""created\"":\""2018-03-19T21:23:40.069312316Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c set -ex; \\t\\tif [ ! -d /usr/share/man/man1 ]; then \\t\\tmkdir -p /usr/share/man/man1; \\tfi; \\t\\tapt-get update; \\tapt-get install -y \\t\\topenjdk-8-jdk=\\\""$JAVA_DEBIAN_VERSION\\\"" \\t\\tca-certificates-java=\\\""$CA_CERTIFICATES_JAVA_VERSION\\\"" \\t; \\trm -rf /var/lib/apt/lists/*; \\t\\t[ \\\""$(readlink -f \\\""$JAVA_HOME\\\"")\\\"" = \\\""$(docker-java-home)\\\"" ]; \\t\\tupdate-alternatives --get-selections | awk -v home=\\\""$(readlink -f \\\""$JAVA_HOME\\\"")\\\"" 'index($3, home) == 1 { $2 = \\\""manual\\\""; print | \\\""update-alternatives --set-selections\\\"" }'; \\tupdate-alternatives --query java | grep -q 'Status: manual'\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""4443d1ea64bf58fcf407017a70cf91b3d2fc25b535f397f3f8d4cbcc21a8def3\"",\""parent\"":\""ad324fb058ede2aae5c0e928616606c62a4a1cf05fde29bff7a54258ef8df607\"",\""created\"":\""2018-03-19T21:22:53.380702822Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV CA_CERTIFICATES_JAVA_VERSION=20170531+nmu1\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""ad324fb058ede2aae5c0e928616606c62a4a1cf05fde29bff7a54258ef8df607\"",\""parent\"":\""7fbcff09cd7a0d880d8a97db3fe60cc283c0eeff7280031e2fab224d604924b3\"",\""created\"":\""2018-03-19T21:22:53.161529652Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JAVA_DEBIAN_VERSION=8u162-b12-1~deb9u1\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""7fbcff09cd7a0d880d8a97db3fe60cc283c0eeff7280031e2fab224d604924b3\"",\""parent\"":\""7cf8101307fa1a243c2fb827fea79335afbd5741cf50082302acca5db55261a0\"",\""created\"":\""2018-03-19T21:22:52.921597489Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JAVA_VERSION=8u162\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""7cf8101307fa1a243c2fb827fea79335afbd5741cf50082302acca5db55261a0\"",\""parent\"":\""a720b859b07e0ada6c73ba160b57de6182c351a951870317963cf2af6fc69d27\"",\""created\"":\""2018-03-14T11:09:02.54085877Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV JAVA_HOME=/docker-java-home\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""a720b859b07e0ada6c73ba160b57de6182c351a951870317963cf2af6fc69d27\"",\""parent\"":\""f9ddd3ece1d40d678cbdbf50a022175acd3ee1f58836eb886a2f44b0ec068523\"",\""created\"":\""2018-03-14T11:09:02.292291489Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c ln -svT \\\""/usr/lib/jvm/java-8-openjdk-$(dpkg --print-architecture)\\\"" /docker-java-home\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""f9ddd3ece1d40d678cbdbf50a022175acd3ee1f58836eb886a2f44b0ec068523\"",\""parent\"":\""b715162a4a7e7b2637f5442c739a99cc20454be35cb453c05ad16d0c1d62cc9b\"",\""created\"":\""2018-03-14T11:09:01.580163972Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c { \\t\\techo '#!/bin/sh'; \\t\\techo 'set -e'; \\t\\techo; \\t\\techo 'dirname \\\""$(dirname \\\""$(readlink -f \\\""$(which javac || which java)\\\"")\\\"")\\\""'; \\t} \\u003e /usr/local/bin/docker-java-home \\t\\u0026\\u0026 chmod +x /usr/local/bin/docker-java-home\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""b715162a4a7e7b2637f5442c739a99cc20454be35cb453c05ad16d0c1d62cc9b\"",\""parent\"":\""800c0f0cafc88aeedbe69b61ad8edf65106dafb4f52b1782de9120b092071cc4\"",\""created\"":\""2018-03-14T11:09:00.816087216Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  ENV LANG=C.UTF-8\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""800c0f0cafc88aeedbe69b61ad8edf65106dafb4f52b1782de9120b092071cc4\"",\""parent\"":\""62ccd3d687be9f840b76a54a1e732cba0761f6af13c3c1840a4c534daf293602\"",\""created\"":\""2018-03-14T11:09:00.593223495Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c apt-get update \\u0026\\u0026 apt-get install -y --no-install-recommends \\t\\tbzip2 \\t\\tunzip \\t\\txz-utils \\t\\u0026\\u0026 rm -rf /var/lib/apt/lists/*\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""62ccd3d687be9f840b76a54a1e732cba0761f6af13c3c1840a4c534daf293602\"",\""parent\"":\""810dccd4311b51f59ddfbd269bda46dacedec3f27bf217c609e84570d49233be\"",\""created\"":\""2018-03-13T23:56:55.333999982Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c apt-get update \\u0026\\u0026 apt-get install -y --no-install-recommends \\t\\tbzr \\t\\tgit \\t\\tmercurial \\t\\topenssh-client \\t\\tsubversion \\t\\t\\t\\tprocps \\t\\u0026\\u0026 rm -rf /var/lib/apt/lists/*\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""810dccd4311b51f59ddfbd269bda46dacedec3f27bf217c609e84570d49233be\"",\""parent\"":\""e373d06b9c7892f565ac0428471923e278834968483972e524a310bf6eb43f67\"",\""created\"":\""2018-03-13T23:56:22.934435097Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c set -ex; \\tif ! command -v gpg \\u003e /dev/null; then \\t\\tapt-get update; \\t\\tapt-get install -y --no-install-recommends \\t\\t\\tgnupg \\t\\t\\tdirmngr \\t\\t; \\t\\trm -rf /var/lib/apt/lists/*; \\tfi\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""e373d06b9c7892f565ac0428471923e278834968483972e524a310bf6eb43f67\"",\""parent\"":\""ae4f7e1d7298f3e0bd9e0aabd310d8217afabb81c2b10bd6a9aa20c7c94de182\"",\""created\"":\""2018-03-13T23:56:19.194216172Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c apt-get update \\u0026\\u0026 apt-get install -y --no-install-recommends \\t\\tca-certificates \\t\\tcurl \\t\\twget \\t\\u0026\\u0026 rm -rf /var/lib/apt/lists/*\""]}}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""ae4f7e1d7298f3e0bd9e0aabd310d8217afabb81c2b10bd6a9aa20c7c94de182\"",\""parent\"":\""8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324\"",\""created\"":\""2018-03-13T22:26:49.547884802Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop)  CMD [\\\""bash\\\""]\""]},\""throwaway\"":true}""
      },
      {
         ""v1Compatibility"": ""{\""id\"":\""8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324\"",\""created\"":\""2018-03-13T22:26:49.153534342Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) ADD file:b380df301ccb5ca09f0d7cd5697ed402fa55f3e9bc5df2f4d489ba31f28de58a in / \""]}}""
      }
   ],
   ""signatures"": [
      {
         ""header"": {
            ""jwk"": {
               ""crv"": ""P-256"",
               ""kid"": ""JTGT:L32L:BI2G:TG3A:RLO2:6H6K:OZXC:HFYY:SPZW:QXEZ:XNK3:2KAL"",
               ""kty"": ""EC"",
               ""x"": ""Q3Qr-lNb0qyOiyFBHzF5v4gxgVp_drIszYInemkB464"",
               ""y"": ""oBzQUsRherctDgDVxwOR0zkij_B7GAL9B20PWVtHzfs""
            },
            ""alg"": ""ES256""
         },
         ""signature"": ""X6BvXE9thNyPHIvyH_0GE1blPxznEcPbILpB5HBvI2339gSA5t4HAE7GMalgKLyThJbjrNjiq_PQqreFMBpqzA"",
         ""protected"": ""eyJmb3JtYXRMZW5ndGgiOjU4ODg5LCJmb3JtYXRUYWlsIjoiQ24wIiwidGltZSI6IjIwMTgtMTAtMTVUMTM6MDI6MjdaIn0""
      }
   ]
}
{noformat}

This should not be related: when we try to find the extracted layers on the layers dir, we could only find two:
{noformat}
ERROR(130)-22:27:48-root@int-agent89-mwst9:/var/lib/mesos/slave/store/docker/layers # ls -alh | grep 'fb401ed0b4f9de5534c224811d0dca94b876225c31ddc3cbb0993ad2faf32cff\|bb54e3dc4a692004ece424733d0ff7bbfe930bdc65491776e2f409a461e838f1\|2ef4a3efec8e89b04691b207a4fe3fddd2e3d3a2a539f8e7dc8a645773758e1f\|36bfc452a3f13511e6f9c0345ffac82054286eea40b02263c83ea791d00a22ea\|ea550cbe252ca3ca06771b0e837d1f7cc61c50404f7f1920ed5bc6cc816d8a0a\|c4c140687ce95f2d23202b7efaa543ef7d064b226864fb4a0ae68bef283e074f\|2f04614030c91b184503348484780b62bda952a2905cc1fb035c5a6f371ca239\|a0ad27b653be7a9400d9e46784f897097cf24f157bfc3fb647e49c360b7c12c1\|f9f80b9791fb4dd2e525de37e33e0b730809bed2b2edb7898b802d3fde3d9c08\|0b648b2545d81712d56536a42a0a99d3d78008bf6f1f04f22d140c427b645b76\|1982429c4258750d3f70dc0f1c563e870725c6d807e9444d9785456d626ef556\|35f1271210258c711e78bd483beee0a7fc9c2d4ee12cf78787d7992277c5a957\|12d065bb54a4529a4afddab4e580cb4cb78e509a2c7d6faa7e3967510f783887\|87ea27ae8f877a36014c0eeb3dba7c0a7b29cfc2d779a10bf438dbce8078cc62\|a13b24c2681da6bacf19dd99041ba2921a11d780fd19edc1b6c0d0b982deb730\|39807d50654d8e842f4bf754ef8c4e5b72780dc6403db3e78611e7356c3c2173\|0bd3eec411704b039cfeb3aec6c6e14882dbc8db68561fe0904548ba708394c5\|f7ede5d3afbd0518be79d105f5ccb6a1f96117291300ba3acc32d9007c71d6a9\|2f6f3100494a6050ecd28e7e1a647a3fc1f5e9dbde64a6646dc5e8f418bc7397\|ee31e5ffacfc38c46b9fdad7ecb47ab748d5eb9baf82ea8cf2766df3f9e18cdc\|9fc63748986386223542f05b4c9685481a303ebb3e30603e174e65121906ea55\|e37610cd26b6212ad9670f101e7d8d70cf108097ead058de50d5d7401cad8b22\|84f29eb0d8745c0510f8d48c347676ff1283ec4921bb2e3e7f462681d8e62ba3\|a0d4487b3138fe03804efb8c6aec561dcd75c6dc45d57458b9e46ca1184b49c2\|bfc2e6fd97b4b81249e6ad41a0709d1d62de3c35251e1e07b39a855111c6f465\|c7344d4d0e1cb95c3e531da5c342fc2fb289e4e51334ca2d1b430de241b28722\|662ff9075f47894530c796a8e9b2fafe7f1bc5be7ec38b35d757d442b6833a84\|5f2b4cf9791c5f60b455dd4b847028183d61d7d519fc5c6bd1b6fe50ef119e74\|25792f867e22dae243cd783474ff4eed5d54a323665c33205058d5a6adf545d9\|1ff15242962d34fd623e14d7a02db78036b900bb2e526a74d44fc903477cc9e7\|fc1935bf0e7b1a47c7b5947f1ba2c9347472b19f6a5cf4e0553c7c8dd4dac4c2\|3d5eaaf246c3dd14cb486d1249fc41cdba012693b3d20dafd9e9a395d104f740\|ede8ff5b5cfbe778c731602a3da663b93f3c3304b3a34255412d1631d4b77c18\|b2579c2af8b8b590584e666fc0a0427e6baa3b46bf9a1ec9b05aa37bbf6fe2fc\|d12f6a61c0f00cd2eeb401653c4ec8f52f9fc996514741a6af9e5c57a082f3a8\|fef12017cd9254dbbbb938bfadd8a9401241149c9a52c347373395a52a1c4ebe\|f81de07a754d737714fcaec8c6d6db1656e64d6cca56e347ccebf23776b00617\|9780d937abc8609fd998fecda4c079d6b097b1cb6a714de993329a2f56548133\|72437e315d169d61ae285cc7cb1fecb5ada9249936e77c39ce28d85e7e6d727d\|9fa0879e36daa2a5a08776124775e11a7fdb51ca2fb19407baa7dedad3ef97a8\|764cacaf206bfbeb6b11777e9b65d16ad6350530f559b9f78ee15413548d7749\|7f59fe46c09c4aa9f59ab9268e34571a0bbc0cfa3c0ac4e4d8e55fdf1392bbd5\|35094fea2af3c400c07fc444f7477b90caf1013b87e40696cfa9e57fc0f9c80b\|9d76182ab3259983e8e14d62c9461f4b08404a709f216b805649ac1a448a1fcc\|fa49692d5ea14814e6efec3af9517b31313a7461065aeb184acdb68d5df23196\|afabf13b1f4f211c64b5535617d2ceb53038f76ceb76b74d8c2c0c66f9a5c9a3\|64c769f00fe141263cbb93af8dded7de9c06f1c96010adbb22f4dc6acd0decc9\|0900f664d5ac780bca9e91b18de9f4680eb0eb4842e81cbe26b3a22f3eb8fdec\|d70c19ee5d688e37e2fce67f01fd691c2509d45dd7903f68ede5ca78d1b7bdc4\|c94d46c5aef23a2d56d2ac621b24a6778dcfbd81e1af03f2940ae91dcd991a20\|09e94fdaea35187ca8029b96c2180f04615c8e66a8255dc939f16c9429ba003f\|3d98aa3161d75b086c38aee654e4612b83b1bbfdea154785537df95ae157ca5a\|6de02152b7dce054eee77ce934fa5652c2142a8a210060e19872db23d9afdacd\|c178ed9e80695242e5ae0f6611ddce99a857d3e5a295207bc45d1e680d3c1379\|9aed7a6b86d981cfa853eae0e5a716d16be68b4475884e6b275762e8770946c8\|6345f944b3e6d2f6e763b56af436b46d26c5424282ca0d2c57994beb2d5d1707\|0b6d17bc569a59dadd8ce806ecf325bc39e0445b5097b004c4c5771d030a754c\|14f6f33e1ddfb9196b4840cc1fb6de989a15de625581d7774c3629a4cc57e48d\|3f780bea9189f56ebd2e363fe471a428bd75a2972e7cfc7dd997633b4a8eb951\|8384b8fae6e40ab737b32a849d462b1fedf10b34be86a67ead51536b5e278a90\|14e86ed54896e03f91257436134c46b72b2230612de7487e208d09b2409c9366\|f69098d00b0be8a19c2189ff51d0fd286f3d1bba4b02024f5127e3afc65c241d\|8a6f3dcf5ca0ca44988097f598596e70cd0f59d9600c115d65fb35ff330848c2\|1c0472056a7e929c90a91ff8b93a3e66caad76310b8431a1efe220ad734e066c\|9e7db4035ee231fa280046b53c0513db0df2c9d49938cc74c7fb195d398ce5fb\|cd5438b31ee369424fea23648dc89429b4cef574734f0933e48516c7ba9caf65\|8e94e7a7be85cbc5b57c4f3e140535e297674e8d30df6e0ed6dbf2128b82935a\|f28491aa4d16c9db8733a4efb3588ee73941a4e3e1cfb9f5f50293f894307e96\|5b04eeff8ce581af83220d1ff6f7d89806ddffa65a231b727f94f88ea19d02f0\|b53c255204bc7107ee1d6d118ec9369ff868b049ae612f9927425f187df17b72\|d43650df8b88478fd70464bfdf9812cfcc5c2ae32e753cf62d9c68fe3aacc7bb\|35011013d34dc72632cd62d92a28a8f789af62553f69feddb4f8f1699b2288c4\|d9ae2fc7815c25c54be284aac826ee5fc6506626a9b5e839db1dbff5aed85ec7\|edaba7f43f101619ec35d84a5362844daa52078110c37ac76913a116b75bb0a7\|27a497071bd1dbbda48bb96b8c4390c76f4b894722330e4f58fad50195326761\|558e2b91047ab320c5fae50f79befa3e641fff2f7e2af49e7cc0dfbecc16635b\|180253c9de444b27748a3818b60ac46af5979f9a2b8f714fbe8ee9d8403e4835\|4443d1ea64bf58fcf407017a70cf91b3d2fc25b535f397f3f8d4cbcc21a8def3\|ad324fb058ede2aae5c0e928616606c62a4a1cf05fde29bff7a54258ef8df607\|7fbcff09cd7a0d880d8a97db3fe60cc283c0eeff7280031e2fab224d604924b3\|7cf8101307fa1a243c2fb827fea79335afbd5741cf50082302acca5db55261a0\|a720b859b07e0ada6c73ba160b57de6182c351a951870317963cf2af6fc69d27\|f9ddd3ece1d40d678cbdbf50a022175acd3ee1f58836eb886a2f44b0ec068523\|b715162a4a7e7b2637f5442c739a99cc20454be35cb453c05ad16d0c1d62cc9b\|800c0f0cafc88aeedbe69b61ad8edf65106dafb4f52b1782de9120b092071cc4\|62ccd3d687be9f840b76a54a1e732cba0761f6af13c3c1840a4c534daf293602\|810dccd4311b51f59ddfbd269bda46dacedec3f27bf217c609e84570d49233be\|e373d06b9c7892f565ac0428471923e278834968483972e524a310bf6eb43f67\|ae4f7e1d7298f3e0bd9e0aabd310d8217afabb81c2b10bd6a9aa20c7c94de182\|8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324'
drwxr-xr-x.   3 root root  40 Oct 15 10:23 8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324
drwxr-xr-x.   3 root root  40 Oct 15 10:23 ae4f7e1d7298f3e0bd9e0aabd310d8217afabb81c2b10bd6a9aa20c7c94de182
{noformat}

These two are base layers that were downloaded earlier from other images. We still need to figure out why there is one layer fetch not finished. (no curl process and tar process running stuck at background)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-9334,,,MESOS-9334,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 27 00:28:30 UTC 2018,,,,,,,"0|i3z893:",9223372036854775807,,,,,,,,,,,Containerization R7 Sprint 32,,,,,,,,,,,5.0,,,,,,,,,,,"16/Oct/18 22:15;gilbert;Related agent logs:
{noformat}
6-b105-164a40b22d84 because: Container does not exist
Oct 15 13:04:20 int-agent89-mwst9.scaletesting.mesosphe.re mesos-agent[24626]: W1015 13:04:20.038676 24706 containerizer.cpp:2401] Skipping status for container 528b3a48-4c77-4ac6-b105-164a40b22d84 because: Unknown container
Oct 15 13:03:20 int-agent89-mwst9.scaletesting.mesosphe.re mesos-agent[24626]: W1015 13:03:20.084156 24687 containerizer.cpp:2308] Skipping resource statistic for container 528b3a48-4c77-4ac6-b105-164a40b22d84 because: Unknown container
Oct 15 13:03:20 int-agent89-mwst9.scaletesting.mesosphe.re mesos-agent[24626]: W1015 13:03:20.084116 24687 containerizer.cpp:2308] Skipping resource statistic for container 528b3a48-4c77-4ac6-b105-164a40b22d84 because: Unknown container
Oct 15 13:03:20 int-agent89-mwst9.scaletesting.mesosphe.re mesos-agent[24626]: W1015 13:03:20.083827 24676 containerizer.cpp:2401] Skipping status for container 528b3a48-4c77-4ac6-b105-164a40b22d84 because: Container does not exist
Oct 15 13:03:20 int-agent89-mwst9.scaletesting.mesosphe.re mesos-agent[24626]: W1015 13:03:20.083788 24676 containerizer.cpp:2401] Skipping status for container 528b3a48-4c77-4ac6-b105-164a40b22d84 because: Unknown container
Oct 15 13:02:26 int-agent89-mwst9.scaletesting.mesosphe.re mesos-agent[24626]: I1015 13:02:26.119920 24652 containerizer.cpp:1282] Starting container 528b3a48-4c77-4ac6-b105-164a40b22d84
Oct 15 13:02:26 int-agent89-mwst9.scaletesting.mesosphe.re mesos-agent[24626]: I1015 13:02:26.117457 24688 slave.cpp:3530] Launching container 528b3a48-4c77-4ac6-b105-164a40b22d84 for executor 'jenkins156.8e56a300-d07a-11e8-a55b-82d63c4c3187.1' of framework 8fe06737-a867-4265-a059-091e45611af9-0004
Oct 15 13:02:26 int-agent89-mwst9.scaletesting.mesosphe.re mesos-agent[24626]: I1015 13:02:26.116492 24688 slave.cpp:8997] Launching executor 'jenkins156.8e56a300-d07a-11e8-a55b-82d63c4c3187.1' of framework 8fe06737-a867-4265-a059-091e45611af9-0004 with resources [{""allocation_info"":{""role"":""mom-4""},""name"":""cpus"",""scalar"":{""value"":0.1},""type"":""SCALAR""},{""allocation_info"":{""role"":""mom-4""},""name"":""mem"",""scalar"":{""value"":32.0},""type"":""SCALAR""}] in work directory '/var/lib/mesos/slave/slaves/113720af-edaf-4ff1-b0da-b3b381a0a32f-S91/frameworks/8fe06737-a867-4265-a059-091e45611af9-0004/executors/jenkins156.8e56a300-d07a-11e8-a55b-82d63c4c3187.1/runs/528b3a48-4c77-4ac6-b105-164a40b22d84'
Oct 15 13:02:26 int-agent89-mwst9.scaletesting.mesosphe.re mesos-agent[24626]: I1015 13:02:26.116279 24688 paths.cpp:748] Creating sandbox '/var/lib/mesos/slave/meta/slaves/113720af-edaf-4ff1-b0da-b3b381a0a32f-S91/frameworks/8fe06737-a867-4265-a059-091e45611af9-0004/executors/jenkins156.8e56a300-d07a-11e8-a55b-82d63c4c3187.1/runs/528b3a48-4c77-4ac6-b105-164a40b22d84'
Oct 15 13:02:26 int-agent89-mwst9.scaletesting.mesosphe.re mesos-agent[24626]: I1015 13:02:26.115389 24688 paths.cpp:745] Creating sandbox '/var/lib/mesos/slave/slaves/113720af-edaf-4ff1-b0da-b3b381a0a32f-S91/frameworks/8fe06737-a867-4265-a059-091e45611af9-0004/executors/jenkins156.8e56a300-d07a-11e8-a55b-82d63c4c3187.1/runs/528b3a48-4c77-4ac6-b105-164a40b22d84' for user 'root'
{noformat}","27/Oct/18 00:28;gilbert;The root caused is found. This issue shares the same root cause as MESOS-9334, but with different symptom.

*Reproduce*
We could reproduce this issue easily, under certain conditions:
On one single machine, scale up 100 tasks running `sleep 10` command tasks and have the scheduler to relaunch task if any finishes. This creates a situation that there are always some containers being destroyed. Then, launch a container using an image with a lot of layers (`kvish/jenkins-dev:595c74f713f609fd1d3b05a40d35113fc03227c9` in this case with ~90 layers). As expected, this container launch would be stuck.

*Root cause*
This is a race between a container launch while another container is being destroyed. This is triggered by a bug in cgroup listener finalize() method. The event FD is closed before an io::read() future finishes the .onDiscard() call back which clean up the event in libevent poll. To explain the bottom of this issue, libevent supports multiple events pointing to the same FD, but it becomes a race in this case because the new container launch io::read() pick up a FD relief by another container destroy. The libevent would disable the FD when it clean up one of event that is not cleaned up yet. Please see MESOS-9334 for details.

*Fix*
https://reviews.apache.org/r/69123/",,,,,,,,,,,,,,,,,,,,,,,,,,
URI disk profile adaptor could deadlock.,MESOS-9308,13190824,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,chhsia0,jieyu,jieyu,11/Oct/18 05:15,11/Oct/18 15:04,29/Oct/20 16:32,11/Oct/18 05:51,1.5.1,1.6.1,1.7.0,,,,,,1.5.2,1.6.2,1.7.1,1.8.0,,,storage,,,,,0,mesosphere,storage,,,,,,,"The loop here can be infinit:
https://github.com/apache/mesos/blob/1.7.0/src/resource_provider/storage/uri_disk_profile_adaptor.cpp#L61-L80

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-9307,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-10-11 05:51:10.725,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 11 15:04:05 UTC 2018,,,,,,,"0|i3z2g7:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere RI-6 Sprint 2018-30,,,,,,,,,,,1.0,,1.5.2,1.6.2,1.7.1,,,,,,,"11/Oct/18 05:51;chhsia0;Shipped onto 1.8.0:
{noformat}
commit a06c12ebb9bf77c99bd87727206e4a02cee04d09
Author: Chun-Hung Hsiao <chhsiao@umich.edu>
Date:   Wed Oct 10 22:14:52 2018 -0700

    Fixed protobuf map equality check in the URI disk profile adaptor.
    
    Review: https://reviews.apache.org/r/68987{noformat}
Backported to 1.7.1:
{noformat}
commit 096e3daf88cd36978335391709a7824820881c88{noformat}
1.6.2:
{noformat}
commit f1e71c64bb48b99a302e80bddbba0d95b08c63ee{noformat}
1.5.2:
{noformat}
commit df7fa3dfb98c337fd76b469aabee52b10ef8096f{noformat}","11/Oct/18 15:04;jdef;I'm wondering why this wasn't caught by a unit test. Maybe we need more unit tests around helpers like this. Tech debt?",,,,,,,,,,,,,,,,,,,,,,,,,,
Nested container launch could fail if the agent upgrade with new cgroup subsystems.,MESOS-9295,13189612,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,05/Oct/18 00:35,08/Oct/18 17:43,29/Oct/20 16:32,08/Oct/18 17:43,,,,,,,,,1.7.1,1.8.0,,,,,containerization,,,,,0,containerizer,,,,,,,,"Nested container launch could fail if the agent upgrade with new cgroup subsystems, because the new cgroup subsystems do not exist on parent container's cgroup hierarchy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-10-05 15:08:38.035,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 08 17:43:58 UTC 2018,,,,,,,"0|i3yv27:",9223372036854775807,,,,,qianzhang,,,,,,Mesosphere RI-6 Sprint 2018-30,,,,,,,,,,,5.0,,1.7.1,1.8.0,,,,,,,,"05/Oct/18 15:08;vinodkone;https://reviews.apache.org/r/68929/
https://reviews.apache.org/r/68941/","08/Oct/18 17:43;gilbert;commit 36a64c869cb04704047b86d3f8d11f1399aa8a8c
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Fri Oct 5 12:19:01 2018 -0700

    Added an unit test for agent recovery with new cgroup subsystems.
    
    Review: https://reviews.apache.org/r/68941

commit 200a532d33b647cc26d9566bbc1765bc039e699d
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Thu Oct 4 16:54:24 2018 -0700

    Fixed the nested container launch failure on the agent upgrade case.
    
    If there are new cgroup subsystems are added after the agent upgrad
    or recovery, new nested container launched under old containers that
    are launched before the recovery would fail, because it cannot assign
    its pid to the non-existed cgroup hierarchy. We should skip those
    new cgroup subsystems for nested containers under old containers.
    
    Review: https://reviews.apache.org/r/68929",,,,,,,,,,,,,,,,,,,,,,,,,,
If a framework looses operation information it cannot reconcile to acknowledge updates.,MESOS-9293,13189507,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,jdef,jdef,04/Oct/18 17:56,14/Dec/18 21:23,29/Oct/20 16:32,05/Dec/18 22:15,1.7.0,,,,,,,,1.7.1,1.8.0,,,,,,,,,,0,mesosphere,operation-feedback,,,,,,,"Normally, frameworks are expected to checkpoint agent ID and resource provider ID before accepting an offer with an OfferOperation. From this expectation comes the requirement in the v1 scheduler API that a framework must provide the agent ID and resource provider ID when acknowledging an offer operation status update. However, this expectation breaks down:

1. the framework might lose its checkpointed data; it no longer remembers the agent ID or the resource provider ID

2. even if the framework checkpoints data, it could be sent a stale update: maybe the original ACK it sent to Mesos was lost, and it needs to re-ACK. If a framework deleted its checkpointed data after sending the ACK (that's dropped) then upon replay of the status update it no longer has the agent ID or resource provider ID for the operation.

An easy remedy would be to add the agent ID and resource provider ID to the OperationStatus message received by the scheduler so that a framework can build a proper ACK for the update, even if it doesn't have access to its previously checkpointed information.

I'm filing this as a BUG because there's no way to reliably use the offer operation status API until this has been fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-10-25 10:54:14.162,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 05 22:15:00 UTC 2018,,,,,,,"0|i3yuev:",9223372036854775807,,,,,greggomann,,,,,,Mesosphere RI-6 Sprint 2018-31,Storage R7 Sprint 33,Storage R8 Sprint 34,,,,,,,,,3.0,,1.7.1,,,,,,,,,"25/Oct/18 10:54;bbannier;Reviews:

https://reviews.apache.org/r/69162/
https://reviews.apache.org/r/69163/","05/Dec/18 22:15;chhsia0;{noformat}
commit 93373344dab0ba0966871ae1470b8ba95a48e320
Author: Benjamin Bannier benjamin.bannier@mesosphere.io
Date:   Wed Dec 5 13:02:59 2018 -0800


Made agent state consistent with forwarded updates.

When the agent handles an `UpdateOperationStatusMessage` from a resource
provider, it injects its own ID which is (at least conceptually) unknown
to the resource provider before forwarding the message to the master,
and also updates its own tracking for the operation.

This patch makes sure that we first mutate the message before handing it
on for updating the internal operation tracking, while previously we
used the unmodified message. Always using the same message reduces error
potential if in future changes we e.g., introduce agent operation status
update managers.

Review: https://reviews.apache.org/r/69458/{noformat}
{noformat}
commit d5577c52587f9bad0bc98eba4b5cf0206022c42a
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date: Wed Dec 5 13:03:06 2018 -0800

Added agent and resource provider IDs to operation status messages.

This patch adds agent and resource provider IDs to
`UpdateOperationStatus` and `UpdateOperationStatusMessage`. With that
frameworks are able to reconcile enough information after failover to
construct operation acknowledgements.

We will add code to populate these fields in a follow-up patch.

Review: https://reviews.apache.org/r/69162/{noformat}
{noformat}
commit be52c980598406a6f41c6e3f6323b02ecaccc450
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date: Wed Dec 5 13:03:09 2018 -0800

Set agent and/or resource provider ID in operation status updates.

This patch sets agent and/or resource provider ID operation status
update messages. This is not always possible, e.g., some operations
might fail validation so that no corresponding IDs can be extracted.

Since operations failing validation are currently directly rejected by
the master without going through a status update manager, they are not
retried either. If a master status update manager for operations is
introduced at a later point it should be possible to forward
acknowledgements for updates to the master's update manager.

Review: https://reviews.apache.org/r/69163/{noformat}
Backported to 1.7.x:
{noformat}
commit 3c7c4568b1b1d75d43cdf71871599557c54c7656
commit 58c7ff57b70e43da479ef2b4ea7f635c2c4a8b73
commit 9fd65a6f8862e52a6d8c9ed970498f4a8db113f7{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
SLRP gets a stale checkpoint after system crash.,MESOS-9281,13188710,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,chhsia0,chhsia0,chhsia0,01/Oct/18 22:05,31/Oct/18 18:31,29/Oct/20 16:32,31/Oct/18 18:31,1.5.0,1.6.0,1.7.0,1.8.0,,,,,1.7.1,1.8.0,,,,,storage,,,,,0,mesosphere,storage,,,,,,,"SLRP checkpoints a pending operations before issuing the corresponding CSI call through {{slave::state::checkpoint}}, which writes a new checkpoint to a temporary file then do a {{rename}}. However, because we don't do any {{fsync}}, {{rename}} is not atomic w.r.t. system crash. As a result, if the operation is processed during a system crash, it is possible that the CSI call has been executed, but the SLRP gets back a stale checkpoint after reboot and totally doesn't know about the operation.

To address this problem, we need to ensure the followings before issuing the CSI call:
 1. The temp file is synced to the disk.
 2. The rename is committed to the disk.

A possible solution is to do an {{fsync}} after writing the temp file, and do another {{fsync}} on the checkpoint dir after the {{rename}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 31 18:31:30 UTC 2018,,,,,,,"0|i3ypif:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere RI-6 Sprint 2018-30,Mesosphere RI-6 Sprint 2018-31,,,,,,,,,,5.0,,1.7.1,1.8.0,,,,,,,,"12/Oct/18 23:22;chhsia0;Reviews:
[https://reviews.apache.org/r/69009/]
[https://reviews.apache.org/r/69010/]","30/Oct/18 21:09;chhsia0;Addinitonal patch: https://reviews.apache.org/r/69085/","31/Oct/18 18:31;chhsia0;{noformat}
commit 5a10007b86e4c7039a5260f6aacb75376270d57f
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Wed Oct 10 15:37:33 2018 -0700

Stout: Added a sync option for `write` and `rename`.

This patch adds an option for the caller to sync the file and directory
contents to the disk after a write to prevent filesystem inconsistency
against reboots.

Review: https://reviews.apache.org/r/69009{noformat}
{noformat}
commit 80fa7318432c043d4763d900c41b53b440fae459
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Fri Oct 19 09:28:38 2018 -0700

Stout: Added a sync option for `mkdir`.

To ensure the directories created by `mkdir` are commited to their
filesystems, an `fsync` will be called on the parent of each created
directory if the `sync` option is set to true. This option has no
effect on Windows.

Review: https://reviews.apache.org/r/69085{noformat}
{noformat}
commit 16bcf61231b6d14019b1d703d887c55b01b85aee
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Fri Oct 12 15:12:41 2018 -0700

Synced SLRP checkpoints to the filesystem.

Currently if a system crashes, SLRP checkpoints might not be synced to
the filesystem, so it is possible that an old or empty checkpoint will
be read upon recovery. Moreover, if a CSI call has been issued right
before the crash, the recovered state may be inconsistent with the
actual state reported by the plugin. For example, the plugin might have
created a volume but the checkpointed state does not know about it.

To avoid this inconsistency, we always call fsync() when checkpointing
SLRP states.

Review: https://reviews.apache.org/r/69010{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,
SLRP does not clean up plugin containers after it is removed.,MESOS-9228,13184722,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,chhsia0,chhsia0,chhsia0,12/Sep/18 21:00,05/Oct/18 23:38,29/Oct/20 16:32,22/Sep/18 06:43,1.5.0,1.6.0,1.7.0,,,,,,1.7.1,1.8.0,,,,,storage,,,,,0,mesosphere,storage,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 22 06:45:20 UTC 2018,,,,,,,"0|i3y107:",9223372036854775807,,,,,bbannier,,,,,,Mesosphere Sprint 2018-28,Mesosphere Sprint 2018-29,,,,,,,,,,5.0,,1.7.1,,,,,,,,,"19/Sep/18 04:56;chhsia0;Reviews:
https://reviews.apache.org/r/68755/
https://reviews.apache.org/r/68756/
https://reviews.apache.org/r/68757/
https://reviews.apache.org/r/68777/
https://reviews.apache.org/r/68758/
https://reviews.apache.org/r/68790/
https://reviews.apache.org/r/68762/
https://reviews.apache.org/r/68763/","22/Sep/18 06:43;chhsia0;{noformat}
commit 0581437999a262277f695592c6a1be9d30bf8c31
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Tue Sep 18 12:59:52 2018 -0700

Removed unnecessary failure handling in agent HTTP API handlers.

The current agent HTTP API handlers either unnecessarily handle failures
on always-ready futures, or return ""500 Internal Server Error""
unnecessarily. This patch removes those unnecessarily code.

Review: https://reviews.apache.org/r/68755{noformat}
{noformat}
commit 1adceaacc81bb33332a52a9f28a864ac928ac706
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Tue Sep 18 14:40:55 2018 -0700

Performed RP-specific validations when adding/updating RP configs.

Each type of RP might have some specific validations for RP info. For
example, SLRP requires the `storage` field to be set. This patch makes
the local RP daemon to perform such validations when adding/updating
configs, so the `ADD_RESOURCE_PROVIDER_CONFIG` and
`UPDATE_RESOURCE_PROVIDER_CONFIG` calls can fail fast.

Review: https://reviews.apache.org/r/68756{noformat}
{noformat}
commit ae41c14e47dd09db82fb237a9987fde8e100c8be
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Tue Sep 18 11:54:29 2018 -0700

Removed `ROOT` requirements for `AgentResourceProviderConfigApiTest`.

These tests required `ROOT` in order to use `filesystem/linux`
isolation this is not a requirement anymore so we can run the tests in
general. These tests appear to be able to run in parallel as well.

We also changed the `AddConflict` test a bit to make it more robust.

Review: https://reviews.apache.org/r/68757{noformat}
{noformat}
commit c174b9d3a7eb842b6de66256f0373c5e12b00cce
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Wed Sep 19 18:07:50 2018 -0700

Set master/agent flags in `AgentResourceProviderConfigApiTest` fixture.

Review: https://reviews.apache.org/r/68777{noformat}
{noformat}
commit a31509e4206edc12fbc7ed6a8e8f87c36e02f34d
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Tue Sep 18 13:25:06 2018 -0700

Added unit tests for adding/updating invalid resource provider configs.

Review: https://reviews.apache.org/r/68758{noformat}
{noformat}
commit 88b28f2cd900a53a8d54b7be837ccc9813e3b764
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu Sep 20 11:25:11 2018 -0700

Moved the container ID prefix generation to `LocalResourceProvider`.

It is more reasonable to not allow each specific resource provider to
construct their own container ID prefix, otherwise it would be hard to
avoid conflicts. Therefore we now established the convension of how the
prefix is constructed in `LocalResourceProvider`.

Review: https://reviews.apache.org/r/68790{noformat}
{noformat}
commit 6366b5c1e5e60dfda5ca0368d6a22da998f0cfa4
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu Sep 20 15:06:51 2018 -0700

Cleaned up residual containers when removing resource provider configs.

When processing `REMOVE_RESOURCE_PROVIDER_CONFIG`, the local resource
provider daemon now performs a best-effort cleanup by killing all
standalone containers prefixed by the 'cid_prefix' of the resource
provider. During the cleanup, no resource provider config with the same
type and name can be added.

Review: https://reviews.apache.org/r/68763{noformat}
{noformat}
commit 678fa8b44bc9c09f5f9908a3a4511f7195150d7b
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Tue Sep 18 21:40:35 2018 -0700

Tested container cleanup in `AgentResourceProviderConfigApiTest.Remove`.

Review: https://reviews.apache.org/r/68762{noformat}
 ","22/Sep/18 06:45;chhsia0;Backported to 1.7.x:
{noformat}
commit 86bd9e4c5b5c0d807bbbe54e10e53f389cd7a7ec
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Tue Sep 18 12:59:52 2018 -0700

Removed unnecessary failure handling in agent HTTP API handlers.

The current agent HTTP API handlers either unnecessarily handle failures
on always-ready futures, or return ""500 Internal Server Error""
unnecessarily. This patch removes those unnecessarily code.

Review: https://reviews.apache.org/r/68755{noformat}
{noformat}
commit 84fa09e3a7a807edce7c0655622e0662ac97031d
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Tue Sep 18 14:40:55 2018 -0700

Performed RP-specific validations when adding/updating RP configs.

Each type of RP might have some specific validations for RP info. For
example, SLRP requires the `storage` field to be set. This patch makes
the local RP daemon to perform such validations when adding/updating
configs, so the `ADD_RESOURCE_PROVIDER_CONFIG` and
`UPDATE_RESOURCE_PROVIDER_CONFIG` calls can fail fast.

Review: https://reviews.apache.org/r/68756{noformat}
{noformat}
commit 76bc3efe4822f285a6783170ba2c12e3546aec35
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu Sep 20 11:25:11 2018 -0700

Moved the container ID prefix generation to `LocalResourceProvider`.

It is more reasonable to not allow each specific resource provider to
construct their own container ID prefix, otherwise it would be hard to
avoid conflicts. Therefore we now established the convension of how the
prefix is constructed in `LocalResourceProvider`.

Review: https://reviews.apache.org/r/68790{noformat}
{noformat}
commit 39e6911567517689452be23893f9031fb02ceebf
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu Sep 20 15:06:51 2018 -0700

Cleaned up residual containers when removing resource provider configs.

When processing `REMOVE_RESOURCE_PROVIDER_CONFIG`, the local resource
provider daemon now performs a best-effort cleanup by killing all
standalone containers prefixed by the 'cid_prefix' of the resource
provider. During the cleanup, no resource provider config with the same
type and name can be added.

Review: https://reviews.apache.org/r/68763{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,
"If some image layers are large, the image pulling may stuck due to the authorized token expired.",MESOS-9221,13183772,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Duplicate,chhsia0,gilbert,gilbert,08/Sep/18 01:43,27/Oct/18 00:13,29/Oct/20 16:32,27/Oct/18 00:11,1.4.2,1.5.0,,,,,,,,,,,,,containerization,,,,,0,containerizer,,,,,,,,"The image layer blobs pulling happen asynchronously but in the same libprocess process. There is a chance that one layer get the token then the thread switch to another layer curling which may take long. When the original layer curling resumes, the token already expired (e.g., after 60 seconds).

{noformat}
$ sudo cat /var/lib/mesos/slave/store/docker/staging/0gx64f/sha256\:c75480ad9aafadef6c7faf829ede40cf2fa990c9308d6cd354d53041b01a7cda
{""errors"":[{""code"":""UNAUTHORIZED"",""message"":""authentication required"",""detail"":[{""Type"":""repository"",""Class"":"""",""Name"":""mesosphere/dapis"",""Action"":""pull""}]}]}
{noformat}

The impact is the task launch stuck and all subsequent task using this image would also stuck because it waits for the same image pulling future to become ready.

Please note that this issue is not likely to be reproduced, unless on a busy system using images containing large layers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-9320,,,,,,,,,,,"28/Sep/18 23:16;chhsia0;mesos-task.json;https://issues.apache.org/jira/secure/attachment/12941782/mesos-task.json","28/Sep/18 23:07;chhsia0;nginx_registry.conf;https://issues.apache.org/jira/secure/attachment/12941777/nginx_registry.conf",,,,2.0,,,,,,,,,,,,,,,,,,,,2018-09-28 23:38:46.026,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 27 00:11:07 UTC 2018,,,,,,,"0|i3xv7z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,5.0,,1.4.3,1.5.2,,,,,,,,"28/Sep/18 23:38;chhsia0;Typically, Mesos containerizer pulls a blob in 5 steps:
1. {{curl -o <blob> <blob-uri>}}.
2. After getting a 401, do {{curl -i <blob-uri>}} to get the authentication challenge.
3. Do a {{curl -i <auth-server-uri>}} to get the token.
4. Do a {{curl -w ""%\{http_code\}\n%\{redirect_url\}"" -o <blob> -H <auth-token-header> <blob-uri>}}
5. After getting a 307, do {{curl -o <blob> <redirect-url>}} to fetch the blob.

The original hypothesis mentioned in the description is that the error json stored in the blob was generated by Step 4.
However, in this case, a failure would have been returned and the associated docker store promise would have become terminal, which was not what we observed.
Also, currently Docker's auth server returns tokens that expire in 300 seconds, which even make the original hypothesis hard to trigger.

A much more probable hypothesis is that the error json was generated by Step 1.
After that, one of Steps 2--4 was stalled, possibly due to a network issue.


I'm able to reproduce the same symptom in Mesos 1.5 with the following setting:

1. Run a private Docker registry locally:
{noformat}
$ docker run --rm -p5000:5000 registry
{noformat}

2. Push an image into the private registry:
{noformat}
$ docker pull chhsiao/alpine
$ docker tag chhsiao/alpine localhost:5000/alpine
$ docker push localhost:5000/alpine
{noformat}

3. Run an Nginx proxy for the registry with config [^nginx_registry.conf]:
{noformat}
$ docker run --rm -p80:80 --net=host --mount=type=bind,source=`pwd`/nginx_registry.conf,target=/etc/nginx/nginx.conf nginx:perl
{noformat}
This config does the following:
a) Any visits to {{/v2/}} without a proper auth header will get a 401 with an auth challenge.
b) The Nginx server serves the challenge through {{/token}}.
c) Any fetch for the last layer will be pending forever even with a proper auth header.

4. Start a local Mesos master and agent with Docker support enabled:
{noformat}
$ sudo bin/mesos-master.sh --work_dir=$HOME/var/lib/mesos
{noformat}
{noformat}
$ sudo bin/mesos-agent.sh --work_dir=$HOME/var/lib/mesos --master=<master-ip>:5050 --image_providers=docker --isolation=filesystem/linux,docker/runtime
{noformat}
5. Run a UCR task with task info  [^mesos-task.json]:
{noformat}
$ src/mesos-execute --master=<master-ip>:5050 --task=$HOME/mesos-task.json
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0928 16:06:53.719611  8251 parse.hpp:116] Specifying an absolute filename to read a command line option out of without using 'file:// is deprecated and will be removed in a future release. Simply adding 'file://' to the beginning of the path should eliminate this warning.
I0928 16:06:53.750340  8263 scheduler.cpp:188] Version: 1.5.1
I0928 16:06:53.756285  8267 scheduler.cpp:311] Using default 'basic' HTTP authenticatee
I0928 16:06:53.757665  8270 scheduler.cpp:494] New master detected at master@<master-ip>:5050
Subscribed with ID 365f595c-2970-4571-8cdb-b542696038d1-0004
Submitted task 'test' to agent 'fc1cb8d5-2ac4-44d6-bcb3-15dd0afbdacf-S1'
{noformat}

6 This task will be pending forever, even after the executor registration timeout is reached:
{noformat}
I0928 16:07:53.809486 29186 slave.cpp:6816] Terminating executor 'test-1' of framework 365f595c-2970-4571-8cdb-b542696038d1-0004 because it did not register within 1mins
I0928 16:07:53.809718 29175 containerizer.cpp:2365] Destroying container 2e8738a2-c043-45bf-a0d3-57c2e0157c94 in PROVISIONING state
I0928 16:07:53.809880 29175 containerizer.cpp:2979] Transitioning the state of container 2e8738a2-c043-45bf-a0d3-57c2e0157c94 from PROVISIONING to DESTROYING
{noformat}
{noformat}
$ curl -q http://<master-ip>:5050/state | jq .
...
          ""id"": ""test-1"",
          ""name"": ""test"",
          ""framework_id"": ""365f595c-2970-4571-8cdb-b542696038d1-0004"",
          ""executor_id"": """",
          ""slave_id"": ""fc1cb8d5-2ac4-44d6-bcb3-15dd0afbdacf-S1"",
          ""state"": ""TASK_STAGING"",
...
{noformat}
And the docker store staging dir still exists:
{noformat}
$ sudo cat /tmp/mesos/store/docker/staging/JD9vi2/sha256:4fe2ade4980c2dda4fc95858ebb981489baec8c1e4bd282ab1c3560be8ff9bde
{""errors"":[{""code"":""UNAUTHORIZED"",""message"":""authentication required""}]}
{noformat}

7. If we terminate {{mesos-execute}} and start a new one to launch another task, no additional HTTP request will be logged by the Nginx server.

8. Once the Nginx server is stopped, the docker store staging directory will be cleaned up:
{noformat}
$ sudo cat /tmp/mesos/store/docker/staging/JD9vi2/sha256:4fe2ade4980c2dda4fc95858ebb981489baec8c1e4bd282ab1c3560be8ff9bde
cat: /tmp/mesos/store/docker/staging/JD9vi2/sha256:4fe2ade4980c2dda4fc95858ebb981489baec8c1e4bd282ab1c3560be8ff9bde: No such file or directory
{noformat}

If this issue is indeed caused by a pending {{curl}} command, then this is already resolved by https://issues.apache.org/jira/browse/MESOS-8620.","27/Oct/18 00:11;gilbert;Close as duplicate.",,,,,,,,,,,,,,,,,,,,,,,,,,
Removing rootfs mounts may fail with EBUSY.,MESOS-9196,13182257,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,jieyu,gilbert,gilbert,30/Aug/18 22:36,10/Oct/18 23:58,29/Oct/20 16:32,01/Sep/18 08:20,1.4.2,1.5.1,1.6.1,,,,,,1.4.3,1.5.2,1.6.2,1.7.0,,,containerization,,,,,0,containerizer,,,,,,,,"We observed in production environment that this
{code}
Failed to destroy the provisioned rootfs when destroying container: Collect failed: Failed to destroy overlay-mounted rootfs '/var/lib/mesos/slave/provisioner/containers/6332cf3d-9897-475b-88b3-40e983a2a531/containers/e8f36ad7-c9ae-40da-9d14-431e98174735/backends/overlay/rootfses/d601ef1b-11b9-445a-b607-7c6366cd21ec': Failed to unmount '/var/lib/mesos/slave/provisioner/containers/6332cf3d-9897-475b-88b3-40e983a2a531/containers/e8f36ad7-c9ae-40da-9d14-431e98174735/backends/overlay/rootfses/d601ef1b-11b9-445a-b607-7c6366cd21ec': Device or resource busy
{code}

Consider fixing the issue by using detach unmount when unmounting container rootfs. See MESOS-3349 for details.

The root cause on why ""Device or resource busy"" is received when doing rootfs unmount is still unknown.

_UPDATE_: The production environment has a cronjob that scan filesystems to build index (updatedb for mlocate). This can explain the EBUSY we receive when doing `unmount`.

_UPDATE_: Splunk that's scanning `/var/lib/mesos` could also be a source of triggers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-09-01 04:36:37.94,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 05 19:33:35 UTC 2018,,,,,,,"0|i3xm1z:",9223372036854775807,,,,,gilbert,,,,,,Mesosphere Sprint 2018-28,,,,,,,,,,,5.0,,1.5.2,1.6.2,1.7.0,,,,,,,"01/Sep/18 04:36;jieyu;Posted a chain starting here:
https://reviews.apache.org/r/68594/","01/Sep/18 08:20;gilbert;commit 3e4e78009d94c15409735ecf4bc2941ec7523d6b
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Aug 31 22:29:48 2018 -0700

    Made copy backend destroy more robust.
    
    Do not return hard failure if `rm -rf` failed. It's also possible that
    it fails if some other processes are accessing the files there. The
    cleanup will be re-attempted during provisioner destroy and agent
    recovery.
    
    Review: https://reviews.apache.org/r/68597/

commit 576a455149cfe5721a3447a100548dae69b6d49a
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Aug 31 22:29:45 2018 -0700

    Made bind backend destroy more robust.
    
    Use MNT_DETACH for `unmount` so that if there are still processes
    holding files or directories in the rootfs (e.g., regular filesystem
    indexing), the unmount will still be successful. The kernel will cleanup
    the mount when the number of references reach zero. See more details in
    MESOS-9196.
    
    Review: https://reviews.apache.org/r/68596/

commit 8715f135376719613276130f3e3af3b244ff06ee
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Aug 31 22:29:41 2018 -0700

    Made aufs backend destroy more robust.
    
    Use MNT_DETACH for `unmount` so that if there are still processes
    holding files or directories in the rootfs (e.g., regular filesystem
    indexing), the unmount will still be successful. The kernel will cleanup
    the mount when the number of references reach zero. Also, do not return
    hard failure if `rmdir` failed. It's also possible that `rmdir` returns
    EBUSY. See more details in MESOS-9196.
    
    Review: https://reviews.apache.org/r/68595/

commit 33710c0af5662c53ee8b309b4474cfa005eac2c6
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Aug 31 22:29:36 2018 -0700

    Made overlay backend destroy more robust.
    
    Use MNT_DETACH for `unmount` so that if there are still processes
    holding files or directories in the rootfs (e.g., regular filesystem
    indexing), the unmount will still be successful. The kernel will cleanup
    the mount when the number of references reach zero. Also, do not return
    hard failure if `rmdir` failed. It's also possible that `rmdir` returns
    EBUSY. See more details in MESOS-9196.
    
    Review: https://reviews.apache.org/r/68594/","05/Sep/18 19:33;gilbert;commit 2c29298f8cf4c96c68ed115acd5a8f335700d735
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Sat Sep 1 00:13:43 2018 -0700

    Added an unit test for rootfs cleanup EBUSY fix.
    
    Review: https://reviews.apache.org/r/68599",,,,,,,,,,,,,,,,,,,,,,,,,
Docker command executor may stuck at infinite unkillable loop.,MESOS-9191,13181805,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,abudnik,gilbert,gilbert,29/Aug/18 07:05,29/Apr/20 05:01,29/Oct/20 16:32,,,,,,,,,,,,,,,,containerization,docker,,,,0,containerizer,,,,,,,,"Due to the change from https://issues.apache.org/jira/browse/MESOS-8574, the behavior of docker command executor to discard the future of docker stop was changed. If there is a new killTask() invoked and there is an existing docker stop in pending state, the old one would call discard and then execute the new one. This is ok for most of cases.

However, docker stop could take long (depends on grace period and whether the application could handle SIGTERM). If the framework retry killTask more frequently than grace period (depends on killpolicy API, env var, or agent flags), then the executor may be stuck forever with unkillable tasks. Because everytime before the docker stop finishes, the future of docker stop is discarded by the new incoming killTask.

We should consider re-use grace period before calling discard() to a pending docker stop future.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-8574,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-08-29 21:42:58.471,,,false,MESOS-8572,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 28 21:43:35 UTC 2019,,,,,,,"0|i3xj9z:",9223372036854775807,,,,,qianzhang,,,,,,Mesosphere Sprint 2018-29,Mesosphere RI-6 Sprint 2018-30,Mesosphere RI-6 Sprint 2018-31,,,,,,,,,3.0,,1.5.4,1.6.3,1.7.4,1.8.1,1.9.0,,,,,"29/Aug/18 21:42;vinodkone;[~abudnik] Would you have cycles in the next sprint work on this?","14/Feb/19 22:36;chhsia0;Retargeted to 1.7.3.","16/Feb/19 00:13;greggomann;Retargeted 1.6.3.","28/Mar/19 21:43;greggomann;I just did some local testing which is relevant to this ticket. It seems that discarding the {{docker stop}} process (in other words, terminating the process running the {{docker stop}} command) is not sufficient to cancel the pending SIGKILL.

I ran a simple shell script in a docker container which trapped SIGTERM. If I issue {{docker stop -t X <container_id>}} at time T0 and then terminate the {{docker stop}} process, the SIGKILL is still sent by the docker daemon at time T0+X. Similarly, if I issue the docker stop command at time T0, terminate it, and then issue another docker stop command at time T1, the SIGKILL is still issued by the docker daemon at time T0+X.

This means that this kill task retry loop issue is not as bad as expected. If the docker daemon is behaving well, issuing a single {{docker stop}} is sufficient to send the SIGKILL after the grace period, regardless of whether we terminate the docker stop process or not.",,,,,,,,,,,,,,,,,,,,,,,,
Test `StorageLocalResourceProviderTest.ROOT_CreateDestroyDiskRecovery` is flaky.,MESOS-9190,13181766,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,chhsia0,chhsia0,chhsia0,29/Aug/18 01:11,29/Aug/18 01:12,29/Oct/20 16:32,29/Aug/18 01:12,1.7.0,,,,,,,,1.8.0,,,,,,storage,,,,,0,mesosphere,storage,,,,,,,"The test is flaky in 1.7.x:
{noformat}
I0824 22:20:01.018494 4208 provider.cpp:1520] Received DESTROY_DISK operation '' (uuid: 7aaadd15-1f6d-4d4e-9000-4c250495f7ba)
W0824 22:20:01.018517 4208 provider.cpp:3008] Dropping operation (uuid: 7aaadd15-1f6d-4d4e-9000-4c250495f7ba): Cannot apply operation when reconciling storage pools
...
I0824 22:20:01.086668 4209 master.cpp:9445] Sending offers [ 0ab2c552-4d85-40fd-8717-8e4d19c7a65e-O4 ] to framework 0ab2c552-4d85-40fd-8717-8e4d19c7a65e-0000 (default) at scheduler-0af22a76-f591-43ba-8470-f4b863292d61@172.16.10.36:35916
../../src/tests/storage_local_resource_provider_tests.cpp:995: Failure
Mock function called more times than expected - returning directly.
Function call: resourceOffers(0x7ffe7ba8c240, @0x7f04a09808c0 { 160-byte object <98-7C 05-AD 04-7F 00-00 00-00 00-00 00-00 00-00 5F-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 05-00 00-00 05-00 00-00 10-A7 03-84 04-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 ... 90-F2 08-84 04-7F 00-00 10-71 00-84 04-7F 00-00 40-71 00-84 04-7F 00-00 00-51 02-84 04-7F 00-00 C0-6A 03-84 04-7F 00-00 00-00 00-00 00-00 00-00 10-F0 00-84 04-7F 00-00 00-00 00-00 00-00 00-00> })
Expected: to be called once
Actual: called twice - over-saturated and active
{noformat}
This is because of `DESTRY_DISK` races with a profile poll. If the poll finishes first, SLRP will start reconciling storage pools, and drop certain incoming operations during reconciliation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 29 01:12:10 UTC 2018,,,,,,,"0|i3xj1b:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 2018-27,,,,,,,,,,,2.0,,,,,,,,,,,"29/Aug/18 01:12;chhsia0;{noformat}
commit 0a69035deb7a89070c647562c2c2122b2016bf91
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Tue Aug 28 14:42:15 2018 -0700

Fixed flakiness in the `CreateDestroyDiskRecovery` SLRP test.

Review: https://reviews.apache.org/r/68550{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add allocator benchmark to allow multiple framework/agent profiles.,MESOS-9187,13181704,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,karya,karya,karya,28/Aug/18 19:44,14/Sep/18 19:25,29/Oct/20 16:32,14/Sep/18 19:25,,,,,,,,,1.8.0,,,,,,,,,,,0,,,,,,,,,We want to add some test harness that allows us to test allocator performance when run with multiple agent and framework profiles.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-9101,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 14 19:25:57 UTC 2018,,,,,,,"0|i3xinj:",9223372036854775807,,,,,tillt,,,,,,Mesosphere Sprint 2018-27,Mesosphere Sprint 2018-28,,,,,,,,,,5.0,,,,,,,,,,,"28/Aug/18 19:49;karya;RRs:

[https://reviews.apache.org/r/68548/] Added allocator benchmark test harness.

[https://reviews.apache.org/r/68591] Added test.

 

[https://reviews.apache.org/r/68549/] Added pause and resume helpers to the allocator. (Meng's patch)

 
  ","14/Sep/18 19:25;karya;{code:java}
commit d351bed856e50ba0f800d3be79dd2df9e4b76271 (HEAD -> master, apache/master) 
Author: Kapil Arya <kapil@mesosphere.io> 
Date:   Fri Aug 31 18:09:45 2018 -0400 

   Added allocator benchmark for non-homogeneous framework profiles. 
    
   This benchmark launches frameworks with different profiles (number of 
   tasks, task sizes and etc.) and prints out statistics such as total 
   tasks launched, cluster utilization and allocation latency. The test has 
   a timeout of 30 seconds 
    
   Review: https://reviews.apache.org/r/68591 

commit 28abc476380efb81382560ef39169d7fffec5402 
Author: Kapil Arya <kapil@mesosphere.io> 
Date:   Fri Aug 31 18:07:14 2018 -0400 

   Introduced a base class for writing allocator benchmarks. 
    
   The test harness encapsulates the common logic in the allocator 
   benchmarks such has adding frameworks and agents. One only 
   needs to customize the various options in the `Benchmarkconfig` 
   to set up the test cluster. 
    
   Review: https://reviews.apache.org/r/68548 

commit 743ad259a002587ca5f42d7c3b946fad4775f30b 
Author: Meng Zhu <mzhu@mesosphere.io> 
Date:   Tue Aug 21 11:46:22 2018 -0400 

   Added pause and resume helpers to the allocator. 
    
   Currently these helpers are only used in the allocator 
   tests to prevent event-driven allocations that triggered 
   when adding frameworks and agents. 
    
   Review: https://reviews.apache.org/r/68549

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Zookeeper doesn't compile with newer gcc due to format error,MESOS-9170,13180205,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,karya,karya,karya,21/Aug/18 12:51,29/Apr/19 09:26,29/Oct/20 16:32,21/Aug/18 13:24,,,,,,,,,1.4.3,1.5.2,1.6.2,1.7.0,,,build,,,,,0,build,integration,,,,,,,RR: https://reviews.apache.org/r/68370/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-9176,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 21 13:24:26 UTC 2018,,,,,,,"0|i3x9fb:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 2018-27,,,,,,,,,,,2.0,,,,,,,,,,,"21/Aug/18 13:24;karya;{code:java}
commit d78878493ed1685a3eee6fbdaa32d4e204aaeb14 
Author: Kapil Arya <kapil@mesosphere.io> 
Date:   Wed Aug 15 19:26:39 2018 -0400 

   Updated Zookeeper patch to fix format error. 
    
   This is a backport of: 
     https://github.com/apache/zookeeper/pull/559/ 
    
   Review: https://reviews.apache.org/r/68370/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,
`UriDiskProfileAdaptor` should not update profiles when a poll returns a non-OK HTTP status.,MESOS-9163,13179637,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,chhsia0,chhsia0,chhsia0,17/Aug/18 17:16,17/Aug/18 17:57,29/Oct/20 16:32,17/Aug/18 17:57,,,,,,,,,1.7.0,,,,,,,,,,,0,mesosphere,storage,,,,,,,"Currently if the {{UriDiskProfileAdatpor}} receives an non-OK status, e.g., {{404 Not Found}}, from a URL poll, it would still read the response body (which could be empty or malformed) and update the profile matrix. The expected behavior should be skip this poll and retry later.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 17 17:57:51 UTC 2018,,,,,,,"0|i3x5xr:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 2018-27,,,,,,,,,,,1.0,,1.7.0,,,,,,,,,"17/Aug/18 17:57;chhsia0;{noformat}
commit cd0a55250dd32f03b48d584114c8c52c7d797de7
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Fri Aug 17 10:17:12 2018 -0700

Only update profiles when getting `200 OK` in `UriDiskProfileAdaptor`.

Review: https://reviews.apache.org/r/68413{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,
MasterTest.TaskStateMetrics is flaky,MESOS-9154,13178891,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,greggomann,greggomann,greggomann,14/Aug/18 16:05,20/Aug/18 10:01,29/Oct/20 16:32,20/Aug/18 10:01,1.7.0,,,,,,,,1.7.1,,,,,,,,,,,0,flaky,flaky-test,mesosphere,,,,,,"Observed on Ubuntu 16.04, cmake build:
{code}
Mock function called more times than expected - returning directly.
    Function call: offers(0x7fffcf5518d0, @0x7f64d805d440 48-byte object <C0-D3 39-0C 65-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 01-00 00-00 04-00 00-00 D0-A1 08-D8 64-7F 00-00>)
         Expected: to be called once
           Actual: called twice - over-saturated and active
{code}

Full log attached.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/18 16:06;greggomann;mastertest.taskstatemetrics.log.txt;https://issues.apache.org/jira/secure/attachment/12935570/mastertest.taskstatemetrics.log.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,2018-08-20 10:01:16.908,,,false,MESOS-9128,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 20 10:01:16 UTC 2018,,,,,,,"0|i3x1cn:",9223372036854775807,,,,,alexr,,,,,,Mesosphere Sprint 2018-26,Mesosphere Sprint 2018-27,,,,,,,,,,1.0,,,,,,,,,,,"15/Aug/18 17:09;greggomann;Review: https://reviews.apache.org/r/68345/","20/Aug/18 10:01;alexr;{noformat}
commit 325ed061210bb67960d69a6dc4c6a1235f5959a5
Author:     Greg Mann <greg@mesosphere.io>
AuthorDate: Mon Aug 20 11:59:31 2018 +0200
Commit:     Alexander Rukletsov <alexr@apache.org>
CommitDate: Mon Aug 20 11:59:31 2018 +0200

    Fixed an expectation in the master tests.
    
    In `MasterTest.TaskStateMetrics`, since the clock is resumed at the
    end of the test, it's possible that the allocation interval may elapse
    on a heavily-loaded machine and the scheduler may receive additional
    offers.
    
    This patch updates an expectation to accommodate additional offers
    sent to the scheduler.
    
    Review: https://reviews.apache.org/r/68345/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Close all file descriptors except whitelist_fds in posix/subprocess.,MESOS-9152,13178395,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,qianzhang,gilbert,gilbert,10/Aug/18 22:45,08/Nov/18 08:59,29/Oct/20 16:32,08/Nov/18 08:59,,,,,,,,,1.6.2,1.7.1,,,,,,,,,,0,containerizer,mesosphere,,,,,,,"Close all file descriptors except whitelist_fds in posix/subprocess (currently whitelist_fds are not honored yet). This would avoid the fd being leaked. Please follow the steps from this commit to make corresponding change:
 https://issues.apache.org/jira/browse/MESOS-8917?focusedCommentId=16522629&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16522629",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-9164,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-09-06 01:26:05.593,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 08 07:44:38 UTC 2018,,,,,,,"0|i3zekk:",9223372036854775807,,,,,gilbert,,,,,,Mesosphere Sprint 2018-27,Mesosphere Sprint 2018-28,Mesosphere Sprint 2018-29,Mesosphere RI-6 Sprint 2018-31,Containerization R7 Sprint 32,,,,,,,5.0,,,,,,,,,,,"06/Sep/18 01:26;qianzhang;RR: https://reviews.apache.org/r/68642/","08/Nov/18 07:44;qianzhang;commit f539d1eba8c7b0fbc4ab040c9af357e016bfde12
Author: Qian Zhang 
Date:   Wed Aug 29 10:16:43 2018 +0800

    Added `lsof()` into stout.
    
    Review: https://reviews.apache.org/r/68642

commit 00497ab19703fb20f71eea74c2df1ff6346391f5
Author: Qian Zhang 
Date:   Wed Sep 26 16:58:48 2018 +0800

    Added a test `FsTest.Lsof`.
    
    Review: https://reviews.apache.org/r/68991

commit ebdf5106c4871875bf7c97cbec43cfe696275f0c

commit d9a02acb8c9440c29811e6f66fe2e1146a04aa52
Author: Qian Zhang 
Date:   Wed Aug 29 10:17:05 2018 +0800

    Closed all file descriptors except `whitelist_fds` in posix/subprocess.
    
    Review: https://reviews.apache.org/r/68644

commit df0a616e3555767e308a87c787d5ad5cdd4e66c1
Author: Qian Zhang 
Date:   Fri Oct 12 22:04:02 2018 +0800

    Added a test `SubprocessTest.WhiteListFds`.
    
    Review: https://reviews.apache.org/r/69016

commit bb533b784928bca1553b6ed86d10105de26bb76d
Author: Qian Zhang 
Date:   Mon Sep 3 15:09:24 2018 +0800

    Updated IO switchboard to use subprocess's `whitelist_fds` parameter.
    
    Review: https://reviews.apache.org/r/68645

commit 2455543d7534d2c1491854ff6efff1c75a1c4395
Author: Qian Zhang 
Date:   Mon Sep 3 15:11:51 2018 +0800

    Updated launchers to use subprocess's `whitelist_fds` parameter.
    
    Review: https://reviews.apache.org/r/68646

commit face988a52b0775f0c3e959d1f164212c1eba96c
Author: Qian Zhang 
Date:   Mon Oct 8 16:06:31 2018 +0800

    Removed the child hook `UNSET_CLOEXEC`.
    
    We do not need this child hook since any file descripters need
    to unset the close-on-exec flag can be put in the `whitelist_fds`
    parameter of the `subprocess` method.
    
    Review: https://reviews.apache.org/r/68995",,,,,,,,,,,,,,,,,,,,,,,,,,
Container stuck at ISOLATING due to FD leak,MESOS-9151,13178394,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,gilbert,gilbert,gilbert,10/Aug/18 22:41,10/Oct/18 23:58,29/Oct/20 16:32,13/Sep/18 19:37,,,,,,,,,1.4.3,1.5.2,1.6.2,1.7.0,,,containerization,,,,,0,containerizer,container-stuck,mesosphere,,,,,,"When containers are launching on a single agent at scale, one container stuck at ISOLATING could occasionally happen. And this container becomes un-destroyable due to containerizer destroy always wait for isolate() finish to continue.

We add more logging to debug this issue:
{noformat}
Aug 10 17:23:28 ip-10-0-1-129.us-west-2.compute.internal mesos-agent[2974]: I0810 17:23:28.050068  2995 collect.hpp:271] $$$$: AwaitProcess waited invoked for ProcessBase ID: __await__(26651); futures size: 3; future: Ready; future index: 2; ready count: 1
Aug 10 17:23:28 ip-10-0-1-129.us-west-2.compute.internal mesos-agent[2974]: I0810 17:23:28.414436  2998 collect.hpp:271] $$$$: AwaitProcess waited invoked for ProcessBase ID: __await__(26651); futures size: 3; future: Ready; future index: 0; ready count: 2
{noformat} 
which shows that the await() in CNI::attach() stuck at the second future (io::read() for stdout).

By looking at the df of this stdout:
{noformat}
Aug 10 17:23:27 ip-10-0-1-129.us-west-2.compute.internal mesos-agent[2974]: I0810 17:23:27.657501  2995 cni.cpp:1287] !!!!: Start to await for plugin '/opt/mesosphere/active/mesos/libexec/mesos/mesos-cni-port-mapper' to finish for container 1c8abf4c-f71a-4704-9a73-1ab0dd709c62 with pid '16644'; stdout fd: 1781; stderr fd: 1800
{noformat}

We found
{noformat}
core@ip-10-0-1-129 ~ $ ps aux | grep mesos-agent
core      1674  0.0  0.0   6704   864 pts/0    S+   20:00   0:00 grep --colour=auto mesos-agent
root      2974 16.4  2.5 1211096 414048 ?      Ssl  17:02  29:11 /opt/mesosphere/packages/mesos--61265af3be37861f26b657c1f9800293b86a0374/bin/mesos-agent
core@ip-10-0-1-129 ~ $ sudo ls -al /proc/2974/fd/ | grep 1781
l-wx------. 1 root root 64 Aug 10 19:38 1781 -> /var/lib/mesos/slave/meta/slaves/d3089315-8e34-40b4-b1f7-0ac6a624d7db-S0/frameworks/d3089315-8e34-40b4-b1f7-0ac6a624d7db-0000/executors/test2.d820326d-9cc1-11e8-9809-ee15da5c8980/runs/38e9270d-ebda-4758-ad96-40c5b84bffdc/tasks/test2.d820326d-9cc1-11e8-9809-ee15da5c8980/task.updates
{noformat}

{noformat}
core@ip-10-0-1-129 ~ $ ps aux | grep 27981
core      2201  0.0  0.0   6704   884 pts/0    S+   20:06   0:00 grep --colour=auto 27981
root     27981  0.0  0.0   1516     4 ?        Ss   17:25   0:00 sleep 10000
core@ip-10-0-1-129 ~ $ cat /proc/s^C       
core@ip-10-0-1-129 ~ $ sudo -s
ip-10-0-1-129 core # ls -al /proc/27981/fd | grep 275230
lr-x------. 1 root root 64 Aug 10 20:05 1781 -> pipe:[275230]
l-wx------. 1 root root 64 Aug 10 20:05 1787 -> pipe:[275230]
{noformat}

{noformat}
core@ip-10-0-1-129 ~ $ sudo ls -al /proc/2974/fd/ | grep pipe
lr-x------. 1 root root 64 Aug 10 17:02 11 -> pipe:[49380]
l-wx------. 1 root root 64 Aug 10 17:02 14 -> pipe:[49380]
lr-x------. 1 root root 64 Aug 10 17:02 17 -> pipe:[48909]
lr-x------. 1 root root 64 Aug 10 19:38 1708 -> pipe:[275089]
l-wx------. 1 root root 64 Aug 10 19:38 1755 -> pipe:[275089]
lr-x------. 1 root root 64 Aug 10 19:38 1787 -> pipe:[275230]
l-wx------. 1 root root 64 Aug 10 17:02 19 -> pipe:[48909]
{noformat}

pipe 275230 is held by the agent process and the sleep process at the same time!

The reason why the leak is possible is because we don't use `pipe2` to create a pipe with `O_CLOEXEC` in subprocess:
https://github.com/apache/mesos/blob/1.5.x/3rdparty/libprocess/src/subprocess_posix.cpp#L61

Although we do set cloexec on those fds later:
https://github.com/apache/mesos/blob/1.5.x/3rdparty/libprocess/src/subprocess.cpp#L366-L373

There is a race where a fork happens after `pipe()` call, but before cloexec is called later. This is more likely on a busy system (this explains why it's not hard to repo the issue when launching a lot of containers on a single box).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-8128,MESOS-9152,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-08-15 09:27:00.557,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 21 23:44:59 UTC 2018,,,,,,,"0|i3wyaf:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 2018-26,Mesosphere Sprint 2018-27,Mesosphere Sprint 2018-28,,,,,,,,,8.0,,1.4.0,1.5.0,1.6.0,1.7.0,,,,,,"15/Aug/18 09:27;alexr;[~gilbert], [~chhsia0] downgrading it to 'critical' since it is not a regression in 1.7.0.","15/Aug/18 22:20;chhsia0;[~alexr] Do you think we should downgrade  MESOS-8128 as well? cc [~jpeach@apache.org] [~jieyu]","16/Aug/18 23:18;gilbert;https://reviews.apache.org/r/68396/
https://reviews.apache.org/r/68397/","20/Aug/18 23:26;gilbert;commit 3f49cf4351d200e4ba0ac7aa2cb69791b2786a23
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Thu Aug 16 11:05:41 2018 -0700

    Updated the ::pipe() system calls to pipe2 in lib_logrotate.
    
    Review: https://reviews.apache.org/r/68397

commit 7a7a879310a5c01b69c8a56b3c1d85555888ec3e
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Thu Aug 16 11:03:52 2018 -0700

    Updated the ::pipe() system calls to pipe2 in posix subprocess.
    
    Review: https://reviews.apache.org/r/68396","21/Aug/18 23:44;gilbert;commit b9d19cd4e03f70014063341c64f96b5860db8b0f
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Aug 21 14:35:01 2018 -0700

    Fixed the lib_logrotate inappropriate UNSET_CLOEXEC via ChildHook.
    
    Previously, we call os::unsetCloexec() on write fd for both outfds and
    errfds. Basically, this is not needed because all stdout and stderr
    fds will be closed at childMain() method. As a result, no need to do
    unsetCloexec() either from the parent process side or UNSET_CLOEXEC
    via the ChildHook.
    
    Review: https://reviews.apache.org/r/68458",,,,,,,,,,,,,,,,,,,,,,,
Test `StorageLocalResourceProviderTest.ROOT_ContainerTerminationMetric` is flaky.,MESOS-9130,13176582,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,chhsia0,chhsia0,02/Aug/18 22:55,25/Jan/19 09:31,29/Oct/20 16:32,25/Jan/19 09:31,1.6.0,1.7.0,,,,,,,1.8.0,,,,,,resource provider,storage,,,,0,mesosphere,storage,,,,,,,"This test is flaky and can fail with the following error:
{noformat}
../../src/tests/storage_local_resource_provider_tests.cpp:3167
Failed to wait 15secs for pluginRestarted{noformat}
The actual error is the following:
{noformat}
E0802 22:13:37.265038  8216 provider.cpp:1496] Failed to reconcile resource provider b9379982-d990-4f63-8a5b-10edd4f5a1bb: Collect failed: OS Error{noformat}
The root cause is that the SLRP calls {{ListVolumes}} and {{GetCapacity}} when starting up, and if the plugin container is killed when these calls are ongoing, gRPC will return an {{OS Error}} which will lead the SLRP to fail.

This flakiness will be fixed once we finish https://issues.apache.org/jira/browse/MESOS-8400.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/18 10:26;bbannier;test.log;https://issues.apache.org/jira/secure/attachment/12936634/test.log",,,,,1.0,,,,,,,,,,,,,,,,,,,,2019-01-07 16:19:32.891,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 25 09:31:09 UTC 2019,,,,,,,"0|i3wn3r:",9223372036854775807,,,,,chhsia0,,,,,,Storage R9 Sprint 36,Storage R9 Sprint 37,,,,,,,,,,1.0,,,,,,,,,,,"07/Jan/19 16:19;bbannier;Review: https://reviews.apache.org/r/69687/","16/Jan/19 13:23;bbannier;{noformat}
commit 2117f671c450d7c74edc53078e8c0ed6035020aa
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Wed Jan 16 14:07:57 2019 +0100

    Fixed flakiness of resource provider ContainerTerminationMetric test.
    
    Plugin restarts can currently fail if the resource provider is killed
    before it has performed minimal reconciliation steps. This patch adds
    additional synchronization to ensure the plugin container can safely be
    killed to safely perform the desired test.
    
    Review: https://reviews.apache.org/r/69687/
{noformat}","17/Jan/19 11:15;bbannier;Reopening as above fix introduced another flakiness.

Review: https://reviews.apache.org/r/69781/","25/Jan/19 09:31;bbannier;{noformat}
commit 1fea7aef1f1a6a48efbae1073f0b66ae02a32faf
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date: Fri Jan 25 09:23:42 2019 +0100

Fixed flakiness of resource provider ContainerTerminationMetric test.

While we addressed one source of flakiness of this test in
`2117f671c450d7c74edc53078e8c0ed6035020aa` we introduced a new source of
flakiness (agents send more than the expected number of
`UpdateSlaveMessage`s since they failed a timeout in registration with
the master).

This patch ensures that the agent registers successfully before
continuing with the test.

Review: https://reviews.apache.org/r/69781/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,
Virtualenv management in support directory is buggy.,MESOS-9075,13172076,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,ArmandGrillet,ArmandGrillet,ArmandGrillet,13/Jul/18 16:14,13/Jul/18 20:59,29/Oct/20 16:32,13/Jul/18 20:59,1.7.0,,,,,,,,1.7.0,,,,,,,,,,,0,,,,,,,,,"When switching back and forth from Python 2 to 3, the virtualenv does not get correctly reinstalled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-07-13 20:57:38.146,,,false,MESOS-8882,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 13 20:59:19 UTC 2018,,,,,,,"0|i3vvmn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,"13/Jul/18 20:57;klueska;{noformat}
commit 4a02bbca61be7451df99f89fc2b99615cc5a7064
Author: Armand Grillet <agrillet@mesosphere.io>
Date:   Fri Jul 13 22:02:41 2018 +0200

    Fixed `build-virtualenv` to correctly clear virtual environment.
    
    Running `build-virtualenv` with Python 3 followed by running it again
    with Python 2 leaves the directory with both versions of Python in the
    virtualenv. This allows Python 3 to still be used from within the
    virtualenv, which is incorrect.
    
    This is caused by a long outstanding issue in `virtualenv` that we now
    handle by unconditionally removing the 'support/.virtualenv' directory
    before building it again.
    
    Review: https://reviews.apache.org/r/67911/
{noformat}","13/Jul/18 20:59;klueska;{noformat}
commit 6936fb553b463b99bf4bc07de9067dd9ca16457a
Author: Armand Grillet <agrillet@mesosphere.io>
Date:   Fri Jul 13 22:21:00 2018 +0200

    Fixed linting errors that appear with the recently updated pylint.
    
    As part of this, we needed to remove some unnecessary delegate to
    'super' calls that are legal in python2 but not in python3. That said,
    they are also unnecessary in python2 so we remove them in both cases.
    
    Specifically, we previously used to override 'main()' in our Python and
    JS linters with:
    
        def main(self, modified_files):
            super(PyLinter, self).main(modified_files)
    
        def main(self, modified_files):
            super(JsLinter, self).main(modified_files)
    
    However, in python3 this gives us linting errors because such a
    delegation is unecessary. Just using the 'main()' function from the base
    class is sufficient.
    
    Review: http://reviews.apache.org/r/67910/
{noformat}","13/Jul/18 20:59;klueska;{noformat}
    
commit dd07b14df02d1a944004e63a03d97161c413e706
Author: Armand Grillet <agrillet@mesosphere.io>
Date:   Fri Jul 13 22:10:46 2018 +0200

    Fixed virtualenv management in support directory.
    
    The switch from Python 2 to Python 3 creates problems with managing the
    virtual environment in the support directory if a developer regularly
    toggles between Python 2 and 3.  For example, we want the virtual
    environment to always use the same version of Python the user uses to
    invoke the python linter from the command line through mesos-style.py.
    
    This commit fixes the issue by adding a new check in the function of
    `mesos-style.py` called `should_build_virtualenv` to see if the Python
    interpreter version currently used is the one in the virtual
    environement. If not, the virtual environment will get recreated.
    
    Review: https://reviews.apache.org/r/67910/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,
Tox doesn't run in the support virtualenv when using Python 3 mesos-style.py,MESOS-9073,13171935,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,ArmandGrillet,ArmandGrillet,ArmandGrillet,13/Jul/18 08:10,13/Jul/18 21:01,29/Oct/20 16:32,13/Jul/18 21:01,,,,,,,,,1.7.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-07-13 21:00:51.031,,,false,MESOS-8882,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 13 21:00:51 UTC 2018,,,,,,,"0|i3vuy7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,"13/Jul/18 21:00;klueska;{noformat}
commit ec87ab575e698c447477fc6bb957f8270aea8c3f
Author: Armand Grillet <agrillet@mesosphere.io>
Date:   Fri Jul 13 15:32:17 2018 +0200

    Updated tox usage in mesos-style.py to run directly from virtualenv.
    
    We were already doing this in the ptyhon2 mesos-style.py script. This
    commit adds the same support to the python3 version of the script.
    
    Review: https://reviews.apache.org/r/67906/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,
Default executor should commit suicide if it cannot receive HTTP responses for LAUNCH_NESTED_CONTAINER calls.,MESOS-9052,13170321,Bug,Open,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,gkleiman,chhsia0,chhsia0,05/Jul/18 18:05,13/Sep/18 21:44,29/Oct/20 16:32,,1.4.0,1.5.0,1.6.0,1.7.0,,,,,,,,,,,executor,,,,,0,,,,,,,,,"If there is a network problem (e.g., a routing problem), it is possible that the agent has received {{LAUNCH_NESTED_CONTAINER}} calls from the default executor and launched the nested container, but the executor does not get the HTTP response. This would result in tasks stuck at {{TASK_STARTING}} forever. We should consider making the default executor commit suicide if it does not receive the response in a reasonable amount of time. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-07-05 19:53:05.085,,,false,MESOS-8529,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 05 19:53:05 UTC 2018,,,,,,,"0|hzzy55:",9223372036854775807,,,,,qianzhang,,,,,,Mesosphere Sprint 2018-27,,,,,,,,,,,3.0,,,,,,,,,,,"05/Jul/18 19:53;vinodkone;Instead of suicide, it should shutdown the current task group. Since one task/container failing to launch shouldn't impact other task groups.

Also, should this be more generically applied to all calls from executor to agent or just launch? 

cc [~gkleiman]",,,,,,,,,,,,,,,,,,,,,,,,,,,
`UPDATE_STATE` can race with `UPDATE_OPERATION_STATUS` for a resource provider.,MESOS-9010,13167048,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,chhsia0,chhsia0,chhsia0,20/Jun/18 03:09,13/Jul/18 21:47,29/Oct/20 16:32,13/Jul/18 21:47,,,,,,,,,1.7.0,,,,,,storage,,,,,0,mesosphere,storage,,,,,,,"Since a resource provider and its operation status update manager run in different actors, for a completed operation, its `UPDATE_OPERATION_STATUS` call may race with an `UPDATE_STATE`. When the `UPDATE_STATE` arrives to the agent earlier, the total resources will be updated, but the terminal status of the completed operation will be ignored since it is known by both the agent and the resource provider. As a result, when the `UPDATE_OPERATION_STATUS` arrives later, the agent will try to apply the operation, but this is incorrect since the total resources has already been updated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 13 21:47:32 UTC 2018,,,,,,,"0|i3v15b:",9223372036854775807,,,,,bbannier,,,,,,Mesosphere Sprint 2018-22,Mesosphere Sprint 2018-23,Mesosphere Sprint 2018-24,,,,,,,,,2.0,,1.7.0,,,,,,,,,"20/Jun/18 05:40;chhsia0;Review: https://reviews.apache.org/r/67664/","13/Jul/18 21:47;chhsia0;{noformat}
commit 739cedcaf0203e29e990944e799735ca3d9c2260
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Tue Jun 19 20:14:30 2018 -0700

Fixed a race between `UPDATE_STATE` and `UPDATE_OPERATION_STATUS`.

Since a resource provider and its operation status update manager run in
different actors, the `UPDATE_OPERATION_STATUS` call of a completed
operation may race with an `UPDATE_STATE` call.

To deal with this race, the agent should update the latest statuses of
all completed operations received in the `UPDATE_STATE` call to avoid
erroneously applying those operations when receiving
`UPDATE_OPERATION_STATUS`es.

Review: https://reviews.apache.org/r/67664{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Wire `UPDATE_QUOTA` call.,MESOS-8968,13163028,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,mzhu,mzhu,mzhu,30/May/18 23:17,23/Jul/19 22:08,29/Oct/20 16:32,23/Jul/19 22:08,,,,,,,,,1.9.0,,,,,,,,,,,0,allocator,multitenancy,Quota,,,,,,"Wire the existing master, auth, registar, and allocator pieces together to complete the `UPDATE_QUOTA` call.

This would enable the master capability `QUOTA_V2`.

This also fixes the ""ignoring zero resource quota"" bug in the old quota implementation, namely:

Currently, Mesos discards resource object with zero scalar value when parsing resources. This means quota set to zero would be ignored and not enforced. For example, role with quota set to ""cpu:10;mem:10;gpu:0"" intends to get no GPU. Due to the above issue, the allocator can only see the quota as ""cpu:10;mem:10"", and no quota GPU means no guarantee and NO limit. Thus GPUs may still be allocated to this role. 

With the completion of `UPDATE_QUOTA` which takes a map of name, scalar values, zero value will no longer be dropped.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-8945,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-30 23:45:19.802,,,false,MESOS-8068,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 23 22:07:48 UTC 2019,,,,,,,"0|z047nq:",9223372036854775807,,,,,,,,,,,Resource Mgmt: RI-16 Sp 50,Resource Mgmt: RI-16 51,,,,,,,,,,5.0,,,,,,,,,,,"30/May/18 23:45;bmahler;[~jieyu] [~bbannier] FYI","10/Jul/19 23:41;mzhu;{noformat}
commit 0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8 (apache/master)
Author: Meng Zhu <mzhu@mesosphere.io>
Date:   Fri Jul 5 18:05:59 2019 -0700

    Implemented `UPDATE_QUOTA` operator call.

    This patch wires up the master, auth, registar and allocator
    pieces for `UPDATE_QUOTA` call.

    This enables the master capability `QUOTA_V2`. The capability
    implies the quota v2 API is capable of writes (`UPDATE_QUOTA`)
    and the master is capable of recovering from V2 quota
    (`QuotaConfig`) in registry.

    This patch lacks the rescind offer logic. When quota limits
    and guarantees are configured, it might be necessary to
    rescind offers on the fly to satisfy new guarantees or be
    constrained by the new limits. A todo is left and will be
    tackled in subsequent patches.

    Also enabled test `MasterQuotaTest.RecoverQuotaEmptyCluster`.

    Review: https://reviews.apache.org/r/71021
{noformat}

{noformat}
commit dcd73437549413790751d1ff127989dbb29bd753 (HEAD -> update_quota, apache/master)
Author: Meng Zhu <mzhu@mesosphere.io>
Date:   Sun Jul 7 14:27:14 2019 -0700

    Added tests for `UPDATE_QUOTA`.

    These tests reuse the existing tests for `SET_QUOTA` and
    `REMOVE_QUOTA` calls. In general, `UPDATE_QUOTA` request
    should fail where `SET_QUOTA` fails. When the existing
    test expects `SET_QUOTA` call succeeds, we test the
    `UPDATE_QUOTA` call by first remove the set quota and then
    send the `UPDATE_QUOTA` request.

    Review: https://reviews.apache.org/r/71022
{noformat}","10/Jul/19 23:41;mzhu;Leave it open for now, until more tests are landed.","23/Jul/19 22:07;mzhu;{noformat}
commit 7aa2a96fea8a44f673a95b425bae71c946c09f2c (HEAD -> update_quota_working, apache/master)
Author: Meng Zhu <mzhu@mesosphere.io>
Date:   Thu Jul 18 11:32:49 2019 -0700

    Added a test to ensure `UPDATE_QUOTA` is applied all-or-nothing.

    Review: https://reviews.apache.org/r/71119
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,
python3/post-reviews.py errors due to TypeError.,MESOS-8954,13162047,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,ArmandGrillet,ArmandGrillet,ArmandGrillet,25/May/18 10:58,29/May/18 09:49,29/Oct/20 16:32,25/May/18 12:06,,,,,,,,,1.7.0,,,,,,,,,,,0,mesosphere,python,,,,,,,"{code:java}
$ ./support/python3/post-reviews.py 
Running 'rbt post' across all of ...
7a662ee75d706d637f9cc2bcbdd0567c32ccef56 - (HEAD -> alexr/subscribers-health, private/ci/alexr/default) Sent task (health) check updates over the operator streaming API. (2 minutes ago)
Creating diff of:
7a662ee75d706d637f9cc2bcbdd0567c32ccef56 - (HEAD -> alexr/subscribers-health, private/ci/alexr/default) Sent task (health) check updates over the operator streaming API.
Press enter to continue or 'Ctrl-C' to skip.

Traceback (most recent call last):
File ""./support/python3/post-reviews.py"", line 432, in <module>
main()
File ""./support/python3/post-reviews.py"", line 365, in main
sys.stdout.buffer.write(output)
TypeError: a bytes-like object is required, not 'str'{code}
The review still get posted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-29 09:49:04.327,,,false,MESOS-8882,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 29 09:49:04 UTC 2018,,,,,,,"0|i3u61r:",9223372036854775807,,,,,alexr,,,,,,Mesosphere Sprint 2018-21,,,,,,,,,,,1.0,,,,,,,,,,,"25/May/18 11:28;ArmandGrillet;https://reviews.apache.org/r/67321/","29/May/18 09:49;alexr;{noformat}
commit 5984e68e19018890ac6a3d27133aa84b5f859b00
Author:     Armand Grillet agrillet@mesosphere.io
AuthorDate: Fri May 25 14:00:48 2018 +0200
Commit:     Alexander Rukletsov alexr@apache.org
CommitDate: Fri May 25 14:02:26 2018 +0200

Fixed python3/post-reviews.py type error.

The main challenge when switching from Python 2 to Python 3 is
to handle the strings, as many of them are now byte strings. I
have checked the other uses of `sys.stdout.buffer.write` while
working on this commit to check that it was the last issue of
that type in python3/post-reviews.py.

Review: https://reviews.apache.org/r/67321/
{noformat}
{noformat}
commit c8ce487a2f55fcc6daf0968a7b53f12b3bcdc0e8
Author:     Armand Grillet agrillet@mesosphere.io
AuthorDate: Tue May 29 11:43:00 2018 +0200
Commit:     Alexander Rukletsov alexr@apache.org
CommitDate: Tue May 29 11:43:29 2018 +0200

Fixed python3/mesos-style.py type error.

The main challenge when switching from Python 2 to Python 3 is
to handle the strings, as many of them are now byte strings. I
have checked the other uses of `sys.stderr.write` while working
on this commit to check that it was the last issue of that type
in python3/mesos-style.py.

Review: https://reviews.apache.org/r/67340/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Quota guarantee metric does not handle removal correctly.,MESOS-8932,13160174,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,greggomann,bmahler,bmahler,17/May/18 22:04,22/May/18 21:22,29/Oct/20 16:32,22/May/18 21:22,,,,,,,,,1.7.0,,,,,,master,,,,,0,,,,,,,,,"The quota guarantee metric is not removed when the quota gets removed:
https://github.com/apache/mesos/blob/1.6.0/src/master/allocator/mesos/metrics.cpp#L165-L174

The consequence of this is that the metric will hold the initial value that gets set and all subsequent removes / sets will not be exposed via the metric.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-21 22:13:24.363,,,false,MESOS-4664,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 22 21:22:49 UTC 2018,,,,,,,"0|i3tuxb:",9223372036854775807,,,,,gkleiman,,,,,,Mesosphere Sprint 2018-20,,,,,,,,,,,2.0,,,,,,,,,,,"21/May/18 22:13;greggomann;Review: https://reviews.apache.org/r/67238/","22/May/18 21:22;greggomann;{code}
commit 07b90e48cfabd543b0b6c8f6d2bb35afd88d6560
Author: Greg Mann <gregorywmann@gmail.com>
Date:   Mon May 21 14:11:29 2018 -0700

    Fixed a quota-related metrics bug.

    Review: https://reviews.apache.org/r/67238
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Autotools don't work with newer OpenJDK versions,MESOS-8921,13159606,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,karya,karya,karya,16/May/18 05:23,06/May/20 15:30,29/Oct/20 16:32,03/Sep/18 22:15,,,,,,,,,1.4.3,1.5.2,1.6.2,1.7.0,,,build,,,,,1,autotools,ci,integration,,,,,,"There are three distinct issues with modern Java and Linux versions:

1. Mesos configure script expects `libjvm.so` at `$JAVA_HOME/jre/lib/<arch>/server/libjvm.so`, but in the newer openjdk versions, `libjvm.so` is found at `$JAVA_HOME/lib/server/libjvm.so`.

2. On some distros (e.g., Ubuntu 18.04), JAVA_HOME env var might be missing. In such cases, the configure is able to compute it by looking at `java` and `javac` paths and succeeds. However, some maven plugins require JAVA_HOME to be set and could fail if it's not found.
{code:java}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.8.1:jar (build-and-attach-javadocs) on project mesos: MavenReportException: Error while creating archive: Unable to find javadoc command: The environment variable JAVA_HOME is not correctly set. -> [Help 1]
{code}
Because configure scripts generate an automake variable `JAVA_HOME`, we can simply invoke maven in the following way to fix this issue:
{code:java}
JAVA_HOME=$JAVA_HOME mvn ...{code}
 These two behaviors were observed with OpenJDK 1.11 on Ubuntu 18.04 but I suspect that the behavior is present on other distros/OpenJDK versions.

3. `javah` has been removed as of OpenJDK 1.10. Instead `javac -h` is to be used as a replacement. See [http://openjdk.java.net/jeps/313] for more details.","RRs:
 # libjvm path: [https://reviews.apache.org/r/68610/]
 # Maven invocation: [https://reviews.apache.org/r/68611/]
 # Fix of javah: [https://reviews.apache.org/r/68612] & https://reviews.apache.org/r/68613",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-09-02 22:07:01.139,,,false,MESOS-9176,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 15:25:19 UTC 2020,,,,,,,"0|i3trf3:",9223372036854775807,,,,,tillt,,,,,,Mesosphere Sprint 2018-28,,,,,,,,,,,3.0,,,,,,,,,,,"02/Sep/18 22:07;tillt;This somewhat also applies to macOS.

{noformat}
make[1]: /Library/Java/JavaVirtualMachines/jdk-10.0.2.jdk/Contents/Home/bin/javah: No such file or directory
{noformat}","03/Sep/18 22:15;karya;{code:java}
commit 06301a830aa57d99c350f5f241f85450baf74b29 
Author: Kapil Arya <kapil@mesosphere.io> 
Date:   Mon Sep 3 12:57:46 2018 -0400 

   Replaced javah with `javac -c` for newer JDKs. 
    
   Starting with JDK 10, javah is not available. The substitute is to use 
   `javac -h` for generating headers. 
    
   Review: https://reviews.apache.org/r/68613 

commit 146d93b8931e583dbb1ec0c9aaf86825c09258d3 
Author: Kapil Arya <kapil@mesosphere.io> 
Date:   Mon Sep 3 12:20:24 2018 -0400 

   Consolidated Java header targets using makefile patterns. 
    
   Review: https://reviews.apache.org/r/68612 

commit 4670aca10943ff8df83e667691e6611aeac9555b 
Author: Kapil Arya <kapil@mesosphere.io> 
Date:   Mon Sep 3 09:52:45 2018 -0400 

   Fixed maven invocation with proper JAVA_HOME. 
    
   On some distros, JAVA_HOME environment variable is not set by default. 
   However, on such systems, our configure script is able to successfully 
   compute the value of JAVA_HOME based on PATH, etc. We use this computed 
   JAVA_HOME to set the environment variable when invoking maven. 
    
   Review: https://reviews.apache.org/r/68611 

commit 6e5bdc6a9050978cc1c788ec550bd62db49891e0 
Author: Kapil Arya <kapil@mesosphere.io> 
Date:   Tue Jun 12 16:46:12 2018 -0400 

   Updated libjvm search logic for newer openjdk versions. 
    
   Review: https://reviews.apache.org/r/68610

{code}","06/May/20 15:25;matthieu.vergne;Javadoc behaviour still present:
{code:none}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.2.0:jar (attach-javadocs) on project <project name>: MavenReportException: Error while generating Javadoc: Unable to find javadoc command: The environment variable JAVA_HOME is not correctly set. -> [Help 1]
{code}
This happens on a {{mvn clean install}} command, so everything is fine until we reach the Javadoc step.

Note that out of Eclipse, the command runs until BUILD SUCCESS.
* OS Ubuntu 16.04
* Eclipse 4.15.0
* m2e 1.15.0
* JDK adoptopenjdk-11-hotspot-amd64",,,,,,,,,,,,,,,,,,,,,,,,,
Docker image fetcher fails with HTTP/2.,MESOS-8907,13158637,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,tillt,jamespeach,jamespeach,11/May/18 04:08,29/Apr/19 09:27,29/Oct/20 16:32,22/Oct/18 21:32,1.5.1,1.6.1,1.7.0,1.8.0,,,,,1.5.3,1.6.2,1.7.1,1.8.0,,,fetcher,,,,,0,integration,,,,,,,,"{noformat}
[ RUN      ] ImageAlpine/ProvisionerDockerTest.ROOT_INTERNET_CURL_SimpleCommand/2
...
I0510 20:52:00.209815 25010 registry_puller.cpp:287] Pulling image 'quay.io/coreos/alpine-sh' from 'docker-manifest://quay.iocoreos/alpine-sh?latest#https' to '/tmp/ImageAlpine_ProvisionerDockerTest_ROOT_INTERNET_CURL_SimpleCommand_2_wF7EfM/store/docker/staging/qit1Jn'
E0510 20:52:00.756072 25003 slave.cpp:6176] Container '5eb869c5-555c-4dc9-a6ce-ddc2e7dbd01a' for executor 'ad9aa898-026e-47d8-bac6-0ff993ec5904' of framework 7dbe7cd6-8ffe-4bcf-986a-17ba677b5a69-0000 failed to start: Failed to decode HTTP responses: Decoding failed
HTTP/2 200
server: nginx/1.13.12
date: Fri, 11 May 2018 03:52:00 GMT
content-type: application/vnd.docker.distribution.manifest.v1+prettyjws
content-length: 4486
docker-content-digest: sha256:61bd5317a92c3213cfe70e2b629098c51c50728ef48ff984ce929983889ed663
x-frame-options: DENY
strict-transport-security: max-age=63072000; preload
...
{noformat}

Note that curl is saying the HTTP version is ""HTTP/2"". This happens on modern curl that automatically negotiates HTTP/2, but the docker fetcher isn't prepared to parse that.

{noformat}
$ curl -i --raw -L -s -S -o -  'http://quay.io/coreos/alpine-sh?latest#https'
HTTP/1.1 301 Moved Permanently
Content-Type: text/html
Date: Fri, 11 May 2018 04:07:44 GMT
Location: https://quay.io/coreos/alpine-sh?latest
Server: nginx/1.13.12
Content-Length: 186
Connection: keep-alive

HTTP/2 301
server: nginx/1.13.12
date: Fri, 11 May 2018 04:07:45 GMT
content-type: text/html; charset=utf-8
content-length: 287
location: https://quay.io/coreos/alpine-sh/?latest
x-frame-options: DENY
strict-transport-security: max-age=63072000; preload
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-25 10:29:55.304,,,false,MESOS-9176,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 22 21:33:21 UTC 2018,,,,,,,"0|i3tlhj:",9223372036854775807,,,,,,,,,,,Mesosphere RI-6 Sprint 2018-31,,,,,,,,,,,3.0,,1.5.2,1.6.2,1.7.1,,,,,,,"25/May/18 10:29;alexr;I think as a workaround for now we can tell {{curl}} to use HTTP/1.1.","05/Sep/18 20:56;tillt;That option would be {{--http1.1}} [~alexr]","05/Sep/18 21:29;tillt;{noformat}
--http1.1

(HTTP) Tells curl to use HTTP version 1.1.

This option overrides -0, --http1.0 and --http2. Added in 7.33.0.
{noformat}","05/Sep/18 21:49;tillt;Just checked, that version was released around  Oct 13, 2013.","10/Oct/18 21:38;alexr;[~tillt] the fix sounds reasonable to me, however, I'd like to confirm first, that the version of curl used in Ubuntu 18 started using HTTP/2 by default, which was the case for Ubuntu 16.","16/Oct/18 13:00;Kirill P;[~alexr], apologies for jumping in, but we waiting support for 18.04 as it's becoming a show-stopper.
minimal version in bionic is [7.55.1|https://launchpad.net/ubuntu/bionic/+source/curl]
http/2 is default for https in curl starting from [7.47.0|https://curl.haxx.se/docs/http2.html] and also CURL_HTTP_VERSION_2TLS is the default value since [7.62.0|https://curl.haxx.se/libcurl/c/CURLOPT_HTTP_VERSION.html] ","16/Oct/18 17:28;tillt;[~Kirill P] understood - we got this upgrade in our current sprint and so far we expect to match our commitment --> expect a patch within the next two weeks. Sounds good?","17/Oct/18 11:49;Kirill P;[~tillt], ""it's something"" :) a patch to be applied to source is a good start, hopefully next release (1.7.1 or 1.8.0) will have the support out-of-the box.","18/Oct/18 22:58;tillt;https://reviews.apache.org/r/69075/","18/Oct/18 23:30;tillt;Turns out that Centos 6 and 7 come with an ancient {{curl}} and hence we have to lock towards HTTP 1.0 instead of HTTP 1.1.
","18/Oct/18 23:44;tillt;We intend to back port this fix towards 1.7, 1.6 and 1.5.","19/Oct/18 13:56;Kirill P;[~tillt], is `–http1.1` flag in cli curl launch needed for the download as well?

also curl is dynamically linked, isn't it?

if so, then what's about `curl_easy_setopt(CURL *handle, CURLOPT_HTTP_VERSION, long version)` being used as well...","22/Oct/18 17:39;tillt;[~Kirill P] the problem is our current HTTP parser. Only when we try to feed an HTTP result into the parser that in fact was HTTP/2, we hit the described bug. We therefore need to fix all HTTP client invocations that try to feed the HTTP responses into the HTTP parser.
The docker fetcher which does this does not use libcurl but instead calls out to the {{curl}} command line tool.
 

","22/Oct/18 21:32;tillt;1.5.3 backport:
{noformat}
commit 5b2e79ee01fd3e7708d8c9bd50115c6c507d2aea
Author: Till Toenshoff <toenshoff@me.com>
Date:   Mon Oct 22 21:42:53 2018 +0200

    Updated docker image fetcher to enforce HTTP 1.1 where needed.

    Modifies the 'curl' invocation that is returning an http::Response,
    locking it into HTTP 1.1. Our current HTTP parser is unable to process
    HTTP 2 responses.

    With the advent of curl 7.47, HTTPS connections are being enforced
    towards HTTP 2 rather aggressively. As a result, our image fetcher
    fails when recent curl versions are being used for pulling images from
    a registry that supports HTTP 2.

    HTTP 1.1 is chosen as long as the underlying curl supports the
    '--http1.1' flag. If curl is old enough to not support that flag, we
    can deduct that it will not enforce HTTP 2 and therefore need no
    further actions.

    For allowing all the benefits of HTTP 2 where possible, we do not
    adapt any 'curl' invocations that do not attempt to parse headers.

    Review: https://reviews.apache.org/r/69075/
{noformat}

1.6.2 backport:
{noformat}
commit da80df34f2e5c68041cb5a9d7282a88062bfdad1
Author: Till Toenshoff <toenshoff@me.com>
Date:   Mon Oct 22 21:42:53 2018 +0200

    Updated docker image fetcher to enforce HTTP 1.1 where needed.

    Modifies the 'curl' invocation that is returning an http::Response,
    locking it into HTTP 1.1. Our current HTTP parser is unable to process
    HTTP 2 responses.

    With the advent of curl 7.47, HTTPS connections are being enforced
    towards HTTP 2 rather aggressively. As a result, our image fetcher
    fails when recent curl versions are being used for pulling images from
    a registry that supports HTTP 2.

    HTTP 1.1 is chosen as long as the underlying curl supports the
    '--http1.1' flag. If curl is old enough to not support that flag, we
    can deduct that it will not enforce HTTP 2 and therefore need no
    further actions.

    For allowing all the benefits of HTTP 2 where possible, we do not
    adapt any 'curl' invocations that do not attempt to parse headers.

    Review: https://reviews.apache.org/r/69075/
{noformat}

1.7.1 backport:
{noformat}
commit dac745fce6b28538b62fda1a87d464e96d0e84ad
Author: Till Toenshoff <toenshoff@me.com>
Date:   Mon Oct 22 21:42:53 2018 +0200

    Updated docker image fetcher to enforce HTTP 1.1 where needed.

    Modifies the 'curl' invocation that is returning an http::Response,
    locking it into HTTP 1.1. Our current HTTP parser is unable to process
    HTTP 2 responses.

    With the advent of curl 7.47, HTTPS connections are being enforced
    towards HTTP 2 rather aggressively. As a result, our image fetcher
    fails when recent curl versions are being used for pulling images from
    a registry that supports HTTP 2.

    HTTP 1.1 is chosen as long as the underlying curl supports the
    '--http1.1' flag. If curl is old enough to not support that flag, we
    can deduct that it will not enforce HTTP 2 and therefore need no
    further actions.

    For allowing all the benefits of HTTP 2 where possible, we do not
    adapt any 'curl' invocations that do not attempt to parse headers.

    Review: https://reviews.apache.org/r/69075/
{noformat}

","22/Oct/18 21:33;tillt;{noformat}
commit bc55489a18fdaf6c79945704d5c70984cab87c11
Author: Till Toenshoff <toenshoff@me.com>
Date:   Mon Oct 22 21:42:53 2018 +0200

    Updated docker image fetcher to enforce HTTP 1.1 where needed.

    Modifies the 'curl' invocation that is returning an http::Response,
    locking it into HTTP 1.1. Our current HTTP parser is unable to process
    HTTP 2 responses.

    With the advent of curl 7.47, HTTPS connections are being enforced
    towards HTTP 2 rather aggressively. As a result, our image fetcher
    fails when recent curl versions are being used for pulling images from
    a registry that supports HTTP 2.

    HTTP 1.1 is chosen as long as the underlying curl supports the
    '--http1.1' flag. If curl is old enough to not support that flag, we
    can deduct that it will not enforce HTTP 2 and therefore need no
    further actions.

    For allowing all the benefits of HTTP 2 where possible, we do not
    adapt any 'curl' invocations that do not attempt to parse headers.

    Review: https://reviews.apache.org/r/69075/
{noformat}",,,,,,,,,,,,,
`UriDiskProfileAdaptor` fails to update profile selectors.,MESOS-8906,13158632,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,chhsia0,chhsia0,chhsia0,11/May/18 03:23,14/Jun/18 20:10,29/Oct/20 16:32,11/May/18 23:35,1.5.0,1.6.0,,,,,,,1.5.2,1.6.1,1.7.0,,,,storage,,,,,0,mesosphere,storage,,,,,,,"The {{UriDiskProfileAdaptor}} ignores the polled profile matrix if the polled one has the same size as the current one: https://github.com/apache/mesos/blob/1.5.x/src/resource_provider/storage/uri_disk_profile.cpp#L282-L286
{code:cxx}
  // Profiles can only be added, so if the parsed data is the same size,
  // nothing has changed and no notifications need to be sent.
  if (parsed.profile_matrix().size() <= profileMatrix.size()) {
    return;
  }
{code}
However, this prevents the profile selector from being updated, which is not the desired behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 11 23:35:17 UTC 2018,,,,,,,"0|i3uvvj:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 2018-20,,,,,,,,,,,2.0,,1.5.2,1.6.1,1.7.0,,,,,,,"11/May/18 03:32;chhsia0;Review: https://reviews.apache.org/r/67078/","11/May/18 23:35;chhsia0;{noformat}
commit 5e740063d433b8f2b89ca43a4d3e93dfbdeb6d8f
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu May 10 20:27:24 2018 -0700

Made `UriDiskProfileAdaptor` be able to update profile selectors.

Review: https://reviews.apache.org/r/67078{noformat}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,
ResourceProviderManagerHttpApiTest.ResubscribeResourceProvider is flaky.,MESOS-8874,13156736,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,chhsia0,chhsia0,03/May/18 00:14,14/Jun/18 20:10,29/Oct/20 16:32,04/May/18 19:20,1.6.0,,,,,,,,1.6.0,,,,,,test,,,,,0,flaky-test,mesosphere,storage,,,,,,"This test is flaky on CI:
{noformat}
../../src/tests/resource_provider_manager_tests.cpp:1114: Failure
Mock function called more times than expected - taking default action specified at:
../../src/tests/mesos.hpp:2972:
    Function call: subscribed(@0x7f881c00aff0 32-byte object <58-04 98-43 88-7F 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 E0-01 01-1C 88-7F 00-00>)
         Expected: to be called once
           Actual: called twice - over-saturated and active
{noformat}

This is different from https://issues.apache.org/jira/browse/MESOS-8315.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/18 00:13;chhsia0;ResubscribeResourceProvider.txt;https://issues.apache.org/jira/secure/attachment/12921656/ResubscribeResourceProvider.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,2018-05-03 11:46:14.62,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 04 19:20:48 UTC 2018,,,,,,,"0|i3uvvb:",9223372036854775807,,,,,chhsia0,,,,,,Mesosphere Sprint 79,,,,,,,,,,,1.0,,,,,,,,,,,"03/May/18 11:46;bbannier;Review: https://reviews.apache.org/r/66931/","04/May/18 19:20;chhsia0;{noformat}
commit c6a60b81ad74479f197dac82e002d509c1d27b14
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Fri May 4 12:09:20 2018 -0700

    Fixed a race in resource provider resubscription test.

    We previously did not make ensure that after the simulated agent
    failover in
    `ResourceProviderManagerHttpApiTest.ResubscribeResourceProvider` the
    mock resource provider created as part of the test did not reconnect
    to the restarted agent (as opposed to the newly initialized resource
    provider). This lead to unmet test expectations.

    With this patch we now explicitly tear down the mock resource provider
    after we have detected that the agent went away to prevent the race.

    Review: https://reviews.apache.org/r/66931/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
StorageLocalResourceProviderTest.ROOT_ZeroSizedDisk is flaky.,MESOS-8873,13156735,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,chhsia0,chhsia0,chhsia0,03/May/18 00:08,29/Nov/18 03:30,29/Oct/20 16:32,18/Jul/18 16:09,1.6.0,,,,,,,,1.7.0,,,,,,test,,,,,0,flaky-test,mesosphere,storage,,,,,,"This test is flaky on CI:
{noformat}
../../src/tests/storage_local_resource_provider_tests.cpp:406: Failure
Value of: updateSlave2->has_resource_providers()
  Actual: false
Expected: true
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/18 00:09;chhsia0;ZeroSizedDisk.txt;https://issues.apache.org/jira/secure/attachment/12921655/ZeroSizedDisk.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,2018-07-18 15:58:30.011,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 29 03:30:51 UTC 2018,,,,,,,"0|i3uvv3:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 2018-20,Mesosphere Sprint 2018-21,Mesosphere Sprint 2018-22,,,,,,,,,2.0,,1.7.0,,,,,,,,,"08/Jun/18 23:17;chhsia0;This is because this (and some other) tests rely on the assumption that the second `UpdateSlaveMessage` contains resource providers. But the resource estimator might trigger a racy `UpdateSlaveMessage` that breaks this assumption.","12/Jun/18 17:52;chhsia0;{noformat}
commit 8d2cebe5eeb7574caa3d10b7ff6018d8f307fe0e
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Mon Jun 11 16:46:00 2018 -0700

Made `NoopResourceEstimator` return a forever-pending future.

There is no need for the noop resource estimator to return a ready
future of empty resource since it is only activated when the user
does not want to use an resource estimator.

Review: https://reviews.apache.org/r/67541{noformat}","18/Jul/18 15:58;mzhu;-Reopening this because it appears to be flaky again on CI debian-9-SSL_GRPC, attached log ""StorageLocalResourceProviderTest_ROOT_ZeroSizedDisk_badrun.txt"".-","18/Jul/18 16:01;chhsia0;The failed test is not {{ZeroSizedDisk}}, and the failure reason is different. Can you create another ticket and close this one?","18/Jul/18 16:08;mzhu;ah, blind me. Closing this.","29/Nov/18 03:30;chhsia0;Observed the same flaky on test {{StorageLocalResourceProviderTest.ROOT_RetryOperationStatusUpdateAfterRecovery}} on our internal 1.6.x CI:
{noformat}
../../src/tests/storage_local_resource_provider_tests.cpp:2706: Failure
Value of: updateSlave2->has_resource_providers()
Actual: false
Expected: true{noformat}
The root cause seems to be the same as this one: {{NoopResourceEstimator}} returns zero oversubscribed resources and triggers the second {{UpdateSlaveMessage}} containing no resource provider.",,,,,,,,,,,,,,,,,,,,,,
Agent may fail to recover if the agent dies before image store cache checkpointed.,MESOS-8871,13156656,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,netrahat,gilbert,gilbert,02/May/18 16:58,04/Jul/18 01:23,29/Oct/20 16:32,21/Jun/18 20:18,,,,,,,,,1.4.2,1.5.2,1.6.1,1.7.0,,,agent,,,,,1,mesosphere,slave,,,,,,,"{noformat}
E0502 13:51:45.398555 10100 slave.cpp:7305] EXIT with status 1: Failed to perform recovery: Collect failed: Collect failed: Collect failed: Unexpected empty images file '/var/lib/mesos/slave/store/docker/storedImages'
{noformat}

This may happen if the agent dies after the file is created but before the contents are persisted on disk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-03 12:19:13.022,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 21 20:18:26 UTC 2018,,,,,,,"0|i3t9on:",9223372036854775807,,,,,gilbert,,,,,,Mesosphere Sprint 2018-23,,,,,,,,,,,3.0,,,,,,,,,,,"03/May/18 12:19;abudnik;This issue has been reproduced once in our internal testing cluster. Fixed by removing '/var/lib/mesos/slave/store/docker/storedImages' file.","21/Jun/18 20:18;gilbert;commit 84664f11635160f8a8434421670111b54a7cfe88
Author: bin zheng <zhengbin@chinaunicom.cn>
Date:   Thu Jun 21 12:02:23 2018 -0700

    Fixed an issue where agent may fail to recover.
    
    Fixed an issue where agent may fail to recover if the agent dies before
    image store cache to the checkpoint. when images file is empty, ignore
    it and continue.
    
    Review: https://reviews.apache.org/r/67597/",,,,,,,,,,,,,,,,,,,,,,,,,,
Consider validating that resubscribing resource providers do not change their name or type,MESOS-8838,13155087,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,bbannier,bbannier,25/Apr/18 11:15,09/Jul/18 08:46,29/Oct/20 16:32,09/Jul/18 08:46,,,,,,,,,1.7.0,,,,,,agent,,,,,0,,,,,,,,,"The agent currently uses a resource provider's name and type to construct e.g., paths for persisting resource provider state and their recovery. With that we should likely prevent resource providers from changing that information since we might otherwise be unable to recover them successfully.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 09 08:46:48 UTC 2018,,,,,,,"0|i3t03r:",9223372036854775807,,,,,chhsia0,,,,,,Mesosphere Sprint 2018-22,Mesosphere Sprint 2018-23,Mesosphere Sprint 2018-24,,,,,,,,,2.0,,,,,,,,,,,"20/Jun/18 13:15;bbannier;Review: https://reviews.apache.org/r/67671/","09/Jul/18 08:46;bbannier;{noformat}
commit b4e08210d326b62ece0a054d4ad3c4686eb5063e
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Fri Jul 6 09:22:42 2018 +0200

    Prevented resource providers from changing their name or type.
    
    Since the agent uses e.g., a resource provider's name or type to
    construct paths to persist resource provider state, changes to this
    information on resource provider resubscription are not supported.
    
    This patch persists a resource provider's name and type in the
    resource provider registry and rejects a resource provider
    resubscription if incompatible changes are detected. Since we did not
    persist this information previous to mesos-1.7.0 we cannot and do not
    perform validation against resource provider registry information
    stored with earlier versions of Mesos.
    
    Review: https://reviews.apache.org/r/67671/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Add a tests of recovery of the resource provider manager registrars.,MESOS-8836,13155083,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,bbannier,bbannier,25/Apr/18 10:53,14/Jun/18 20:10,29/Oct/20 16:32,17/May/18 15:15,,,,,,,,,1.6.0,,,,,,test,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 17 15:15:09 UTC 2018,,,,,,,"0|i3uvun:",9223372036854775807,,,,,chhsia0,,,,,,Mesosphere Sprint 79,Mesosphere Sprint 2018-20,,,,,,,,,,2.0,,,,,,,,,,,"09/May/18 11:40;bbannier;See https://reviews.apache.org/r/66545/","17/May/18 15:15;bbannier;{noformat}
commit 65ed45fb039e7da64a58094aff67cf0189aa1f6d
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Tue May 1 13:09:19 2018 -0700

    Added admitted resource providers to the manager's registry.
    
    Review: https://reviews.apache.org/r/66545/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
RP-related API should be experimental.,MESOS-8787,13152442,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,chhsia0,chhsia0,chhsia0,14/Apr/18 01:02,14/Jun/18 20:10,29/Oct/20 16:32,18/Apr/18 02:00,1.5.0,,,,,,,,1.5.1,1.6.0,,,,,,,,,,0,mesosphere,storage,,,,,,,The new offer operations and resource provider API introduced in Mesos 1.5.0 should be marked as experimental.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 18 02:00:04 UTC 2018,,,,,,,"0|i3uvtz:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 78,,,,,,,,,,,1.0,,1.5.1,1.6.0,,,,,,,,"18/Apr/18 02:00;chhsia0;{noformat}
commit cbb947d50adeb1d76768910367cd973712b06583
Author: Chun-Hung Hsiao <chhsiao@apache.org>
Date:   Tue Apr 17 18:38:38 2018 -0700

    Marked volume/block creation and destroy operations as experimental.

    This patch marks the `CREATE_VOLUME`, `DESTROY_VOLUME`, `CREATE_BLOCK`
    and `DESTROY_BLOCK` as experimental APIs. It also unifies the way we
    mark experimental APIs.

    Review: https://reviews.apache.org/r/66616/
{noformat}
{noformat}
commit 97cc6656279ed0b785226b3f5166e955f399a1e7
Author: Chun-Hung Hsiao <chhsiao@apache.org>
Date:   Tue Apr 17 18:38:52 2018 -0700

    Marked the resource provider API as experimental.

    This patch marks the resource provider API as experimental, and make the
    v0 and v1 protos consistent.

    Review: https://reviews.apache.org/r/66648/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,
OPERATION_DROPPED operation status updates should include the operation/framework IDs,MESOS-8784,13152131,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,gkleiman,gkleiman,gkleiman,12/Apr/18 23:41,08/May/18 01:40,29/Oct/20 16:32,08/May/18 01:40,1.5.0,1.6.0,,,,,,,1.6.0,,,,,,master,,,,,0,,,,,,,,,The agent should include the operation/framework IDs in operation status updates sent in response to a reconciliation request from the master. These status updates have the operation status: {{OPERATION_DROPPED}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-08 01:40:44.817,,,false,MESOS-8054,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 08 01:40:44 UTC 2018,,,,,,,"0|i3shz3:",9223372036854775807,,,,,greggomann,,,,,,Mesosphere Sprint 78,Mesosphere Sprint 79,,,,,,,,,,3.0,,1.6.0,,,,,,,,,"04/May/18 00:00;gkleiman;https://reviews.apache.org/r/66924/","08/May/18 01:40;greggomann;{code}
commit a570f9436b816d40ba3d01455211f5d61f77d66d
Author: Gaston Kleiman <gaston@mesosphere.io>
Date:   Mon May 7 17:32:56 2018 -0700

    Made the master include the operation ID in OPERATION_DROPPED updates.

    Review: https://reviews.apache.org/r/66924/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Transition pending operations to OPERATION_UNREACHABLE when an agent is removed.,MESOS-8783,13152129,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,bennoe,gkleiman,gkleiman,12/Apr/18 23:39,16/Jan/19 23:16,29/Oct/20 16:32,16/Jan/19 23:16,1.5.0,1.6.0,,,,,,,1.8.0,,,,,,master,,,,,0,foundations,mesosphere,,,,,,,"Pending operations on an agent should be transitioned to `OPERATION_UNREACHABLE` when an agent is marked unreachable. We should also make sure that we pro-actively send operation status updates for these operations when the agent becomes unreachable.

We should also make sure that we send new operation updates if/when the agent reconnects - perhaps this is already accomplished with the existing operation update logic in the agent?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-01-04 19:16:41.429,,,false,MESOS-8054,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 16 23:16:55 UTC 2019,,,,,,,"0|i00a3p:",9223372036854775807,,,,,gkleiman,,,,,,Mesos Foundations R8 Sprint 35,Mesos Foundations R9 Sprint 36,Mesos Foundations R9 Sprint 37,,,,,,,,,3.0,,,,,,,,,,,"04/Jan/19 19:16;bennoe;Opened a review for the first paragraph here: https://reviews.apache.org/r/69669/

The second part needs a bit more consideration, and should probably be done in a separate ticket. It might be not necessary to send updates from the master when the agent reconnects, since at that point the agent can send the updated operation statuses itself.","16/Jan/19 23:16;greggomann;{code}
commit 229f4c65093e6eef872298bc72f95968a7f46bca
Author: Benno Evers bevers@mesosphere.com
Date:   Wed Jan 16 11:42:36 2019 -0800


Notified frameworks when operations are marked as unreachable.

When an agent is being marked as unreachable due to missing
the reregistration timeout, all operations on that agent
are implicilty transitioned to status `OPERATION_UNREACHABLE`.

This commit adds an explicit notification for this transition
to frameworks which opted-in to receive operation feedback.

Review: https://reviews.apache.org/r/69669/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Transition operations to OPERATION_GONE_BY_OPERATOR when marking an agent gone.,MESOS-8782,13152127,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,bennoe,gkleiman,gkleiman,12/Apr/18 23:37,10/Jan/19 18:42,29/Oct/20 16:32,10/Jan/19 18:42,1.5.0,1.6.0,,,,,,,1.8.0,,,,,,master,,,,,0,foundations,,,,,,,,"The master should transition operations to the state {{OPERATION_GONE_BY_OPERATOR}} when an agent is marked gone, sending an operation status update to the frameworks that created them.

We should also remove them from {{Master::frameworks}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-12-19 17:52:35.338,,,false,MESOS-8054,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 10 18:42:07 UTC 2019,,,,,,,"0|i00a3o:",9223372036854775807,,,,,gkleiman,,,,,,Mesos Foundations R8 Sprint 35,Mesos Foundations R9 Sprint 36,Mesos Foundations R9 Sprint 37,,,,,,,,,3.0,,,,,,,,,,,"19/Dec/18 17:52;bennoe;Review: https://reviews.apache.org/r/69575/","10/Jan/19 18:42;greggomann;{code}
commit d4dd325d06444fdc33dd8c581a6edf2235c36cbd
Author: Benno Evers <bevers@mesosphere.com>
Date:   Wed Jan 9 14:31:12 2019 -0800

    Sent operation feedback when agent is marked as gone.

    When an agent is marked as gone through the operator API,
    notify all frameworks with outstanding offer operations on
    that agent that these operations have been transitioned
    to `OPERATION_GONE_BY_OPERATOR`.

    Review: https://reviews.apache.org/r/69575/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos master shouldn't silently drop operations,MESOS-8781,13152125,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gkleiman,gkleiman,gkleiman,12/Apr/18 23:34,23/Apr/18 21:48,29/Oct/20 16:32,23/Apr/18 21:48,1.5.0,1.6.0,,,,,,,1.6.0,,,,,,master,,,,,0,,,,,,,,,"We should make sure that all call places of {{void Master::drop(Framework*, const Offer::Operation&, const string&)}} send a status update if an operation ID was specified. OR we should make sure that they do NOT send one, and make that method send one.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-04-23 21:48:19.692,,,false,MESOS-8054,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 23 21:48:19 UTC 2018,,,,,,,"0|i3shxr:",9223372036854775807,,,,,greggomann,,,,,,Mesosphere Sprint 78,,,,,,,,,,,3.0,,1.6.0,,,,,,,,,"18/Apr/18 00:21;gkleiman;https://reviews.apache.org/r/66679/","23/Apr/18 21:48;greggomann;{code}
commit 3711d66aa9eb70e12b184d3c2f79bf56fbd9cffa (HEAD -> master, origin/master, origin/HEAD, merge)
Author: Gaston Kleiman <gaston@mesosphere.io>
Date:   Mon Apr 23 14:22:26 2018 -0700

    Made the master send operation status updates when dropping operations.

    This patch makes the master send an operation status update to the
    framework with status `OPERATION_ERROR` when an operation with an
    operation ID is dropped.

    Review: https://reviews.apache.org/r/66679/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Agent resource provider config API calls should be idempotent.,MESOS-8742,13148458,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,chhsia0,chhsia0,chhsia0,27/Mar/18 23:26,25/Sep/18 23:14,29/Oct/20 16:32,29/Mar/18 18:07,,,,,,,,,1.5.1,1.6.0,,,,,,,,,,0,mesosphere,storage,,,,,,,"There are some issues w.r.t. using the current agent resource provider config API calls:

1. {{UPDATE_RESOURCE_PROVIDER_CONFIG}}: If the caller fail to receive the HTTP response code, there is no way to retry the operation without triggering an RP restart.
2. {{REMOVE_RESOURCE_PROVIDER_CONFIG}}: If the caller fail to receive the HTTP response code, a retry will return a 404 Not Found. But due to MESOS-7697, there is no way for the caller to know if the 404 is due to a previous successful config removal or not.

To address these issues, we should make these calls idempotent, such that they return 200 OK when the caller retry. It would be nice if {{ADD_RESOURCE_PROVIDER_CONFIG}} is also idempotent for consistency.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 29 18:07:08 UTC 2018,,,,,,,"0|i3uvtj:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 76,,,,,,,,,,,2.0,,1.5.1,,,,,,,,,"29/Mar/18 05:13;chhsia0;{noformat}
commit aa57ae87ee96f013109914a9b83ed8affb024793
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date:   Tue Mar 27 14:08:05 2018 -0700

    Made agent resource provider config API calls idempotent.

    This patch adds descriptions to declare the following agent API calls
    idempotent:
      - `ADD_RESOURCE_PROVIDER_CONFIG`
      - `UPDATE_RESOURCE_PROVIDER_CONFIG`
      - `REMOVE_RESOURCE_PROVIDER_CONFIG`

    Review: https://reviews.apache.org/r/66318
{noformat}
{noformat}
commit c423e2cee766b01d24deece79e366e80e3d141df
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date:   Tue Mar 27 19:58:49 2018 -0700

    Implemented idempotency for agent resource provider config API calls.

    `ADD_RESOURCE_PROVIDER_CONFIG`, `UPDATE_RESOURCE_PROVIDER_CONFIG` and
    `REMOVE_RESOURCE_PROVIDER_CONFIG` now return 200 for identical
    repetivite calls.

    Review: https://reviews.apache.org/r/66325
{noformat}
{noformat}
commit cb528c58d7039aeeccb1db72b122f67107524bce
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date:   Tue Mar 27 20:01:14 2018 -0700

    Added tests for agent resource provider API idempotency.

    This patche adds tests to verify that adding, updating and removing the
    same resource provider config will return 200 OK without triggering any
    resource provider launch/termination.

    Review: https://reviews.apache.org/r/66326
{noformat}","29/Mar/18 18:07;chhsia0;Additional patch for 1.5.1 backporting:
{noformat}
commit b0a33cb782db57d054f68335c8126ecae078b238
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date:   Wed Mar 28 23:15:18 2018 -0700

    Added missing changes for backporting `AgentResourceProviderApiTest`.
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Libprocess: deadlock in process::finalize,MESOS-8729,13147577,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,abudnik,abudnik,abudnik,23/Mar/18 20:02,13/Sep/18 19:39,29/Oct/20 16:32,,1.6.0,,,,,,,,,,,,,,libprocess,,,,,0,deadlock,libprocess,,,,,,,"Since we are calling [`libprocess::finalize()`|https://github.com/apache/mesos/blob/02ebf9986ab5ce883a71df72e9e3392a3e37e40e/src/slave/containerizer/mesos/io/switchboard_main.cpp#L157] before returning from the IOSwitchboard's main function, we expect that all http responses are going to be sent back to clients before IOSwitchboard terminates. However, after [adding|https://reviews.apache.org/r/66147/] `libprocess::finalize()` we have seen that IOSwitchboard might get stuck in `libprocess::finalize()`. See attached stacktrace.","The issue has been reproduced on Ubuntu 16.04, master branch, commit `42848653b2`. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-8545,,,,,,,,MESOS-8723,,,,,,,,,"23/Mar/18 20:02;abudnik;deadlock.txt;https://issues.apache.org/jira/secure/attachment/12915972/deadlock.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,2018-03-27 01:41:10.781,,,false,MESOS-8823,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 23 22:01:11 UTC 2018,,,,,,,"0|i3vc7z:",9223372036854775807,,,,,bmahler,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,"27/Mar/18 01:41;bmahler;Looking at the last stack:
 
{color:#000000}...{color}
{color:#000000}#8 0x00007f09d2ac1aac in synchronize<std::recursive_mutex> () at ../../3rdparty/stout/include/stout/synchronized.hpp:58 #9 0x00007f09d492c37b in process::ProcessManager::use () at ../../../3rdparty/libprocess/src/process.cpp:2520 #10 0x00007f09d492e955 in process::ProcessManager::deliver () at ../../../3rdparty/libprocess/src/process.cpp:2775 // Trying to get a reference but blocked on the lock.{color}
...
#66 0x00007f09d492e988 in process::ProcessManager::deliver () at [../../../3rdparty/libprocess/src/process.cpp:2776 |https://github.com/apache/mesos/blob/2e2e38628c1b580a231ddac5270f9848ea4af7af/3rdparty/libprocess/src/process.cpp?utf8=%E2%9C%93#L2776]// XXX Holds a reference!
...
 
This thread is doing a deliver (while holding a reference) and synchronously calls back into deliver and blocks on the lock while holding a reference. The first thread is therefore stuck spinning under the lock and the reference will never be released.
 
{color:#000000}I understand the issue now but haven't thought through a fix.{color}","27/Mar/18 21:25;bmahler;A couple of additional finalization related issues I noticed while trying to write a small test to reproduce this:

(1) re-initialization after finalize is not supported and crashes
(2) things like spawn() implicitly re-initialize and therefore if things like spawn get called post-finalize() they will crash
(3) even without implicit initialization, a spawn will access the process manager pointer and crash
(4) double-finalize() will crash the program.

Before resolving this particular bug, will create an epic to track these other issues if one doesn't already exist. cc [~kaysoky]","23/Apr/18 22:01;bmahler;Filed MESOS-8823 to capture the broader effort needed to make process::finalize safe for use.",,,,,,,,,,,,,,,,,,,,,,,,,
Enable resource provider agent capability by default,MESOS-8647,13143598,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,bbannier,bbannier,08/Mar/18 16:26,14/Jun/18 20:08,29/Oct/20 16:32,26/Mar/18 15:08,,,,,,,,,1.6.0,,,,,,agent,test,,,,0,,,,,,,,,"In 1.5.0 we introduced a resource provider agent capability which e.g., enables a modified operation protocol. We should enable this capability by default.

 

If tests explicitly depend on the agent being fully operational, they should be adjusted for the modified protocol. It is e.g., not enough to wait for a {{dispatch}} to the agent's recovery method, but instead one should wait for a dedicated {{UpdateSlaveMessage}} from the agent.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 26 15:08:58 UTC 2018,,,,,,,"0|i3uvrz:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 76,,,,,,,,,,,2.0,,,,,,,,,,,"13/Mar/18 17:27;bbannier;Review: https://reviews.apache.org/r/66037/","26/Mar/18 15:08;bbannier;{noformat}
commit ebe92c9b39933136968e4ba3a52527e52b361d22
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date: Mon Mar 26 15:54:02 2018 +0200

Enabled agent resource provider capability by default.

This patch add `RESOURCE_PROVIDER` to the list of default-enabled
agent capabilities. In addition we also adjust tests to accommodate
the change in the agent registration protocol this triggers. We rely
on masters ignoring redundant `UpdateSlaveMessage`s.

Review: https://reviews.apache.org/r/66037/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Terminal task status update will not send if 'docker inspect' is hung,MESOS-8605,13140640,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,abudnik,greggomann,greggomann,23/Feb/18 23:21,22/Mar/19 16:41,29/Oct/20 16:32,03/Mar/18 09:26,1.5.0,,,,,,,,1.4.2,1.5.1,1.6.0,,,,docker,,,,,0,mesosphere,,,,,,,,"When the agent processes a terminal status update for a task, it calls {{containerizer->update()}} on the container before it forwards the update: https://github.com/apache/mesos/blob/9635d4a2d12fc77935c3d5d166469258634c6b7e/src/slave/slave.cpp#L5509-L5514

In the Docker containerizer, {{update()}} calls {{Docker::inspect()}}, which means that if the inspect call hangs, the terminal update will not be sent: https://github.com/apache/mesos/blob/9635d4a2d12fc77935c3d5d166469258634c6b7e/src/slave/containerizer/docker.cpp#L1714",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-03-01 18:13:54.641,,,false,MESOS-8572,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 01 18:13:54 UTC 2018,,,,,,,"0|i3qjpj:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 76,,,,,,,,,,,3.0,,,,,,,,,,,"28/Feb/18 19:18;greggomann;{{DockerContainerizer::udpate()}} uses {{inspect}} in order to grab the container PID, which it needs in order to update cgroups: https://github.com/apache/mesos/blob/a15eb712afd51048756148a5af9cd60e86c8e90b/src/slave/containerizer/docker.cpp#L1722-L1740

We only need to grab this once, rather than doing it every time we run {{update()}}. It looks like the location of the containerizer's first invocation of {{update()}} for a particular container depends on whether we go down the {{launchExecutorProcess}} code path or the {{launchExecutorContainer}} code path: https://github.com/apache/mesos/blob/a15eb712afd51048756148a5af9cd60e86c8e90b/src/slave/containerizer/docker.cpp#L1288-L1366

If we do {{launchExecutorContainer}}, then we call {{update()}} as part of the launch path: https://github.com/apache/mesos/blob/a15eb712afd51048756148a5af9cd60e86c8e90b/src/slave/containerizer/docker.cpp#L1352-L1353
If we do {{launchExecutorProcess}}, then the first invocation of {{update()}} occurs when the executor registers: https://github.com/apache/mesos/blob/a15eb712afd51048756148a5af9cd60e86c8e90b/src/slave/slave.cpp#L4875-L4879

I think the most important issue here is making sure that a task can be killed successfully, even if the {{Docker::inspect()}} call in {{Containerizer::update()}} has not returned. We could do this by storing the Future associated with the initial {{inspect()}} call for a container, and then discarding that Future if it’s pending when the container is destroyed.

We could also optimize by updating the {{docker->inspect()}} call in {{update()}} to retry after some duration.","28/Feb/18 23:09;greggomann;I wrote a test which reproduces this bug: https://reviews.apache.org/r/65849/","01/Mar/18 18:13;abudnik;https://reviews.apache.org/r/65868/",,,,,,,,,,,,,,,,,,,,,,,,,
Allow empty resource provider selector in `UriDiskProfileAdaptor`.,MESOS-8598,13139734,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,chhsia0,chhsia0,chhsia0,20/Feb/18 18:40,14/Jun/18 20:08,29/Oct/20 16:32,23/Feb/18 01:09,,,,,,,,,1.5.1,1.6.0,,,,,storage,,,,,0,mesosphere,storage,,,,,,,"Currently in {{UriDiskProfileAdaptor}}, it is invalid for a profile to have a resource provider selector with 0 resource providers. However, one can put non-existent provider types and names into the selector to achieve the same effect, and this is semantically inconsistent. We should allow an empty list of resource providers directly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-02-23 01:09:55.976,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 23 01:09:55 UTC 2018,,,,,,,"0|i3uvrr:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 75,,,,,,,,,,,1.0,,1.5.1,,,,,,,,,"20/Feb/18 18:43;chhsia0;Review: https://reviews.apache.org/r/65718/","23/Feb/18 01:09;jieyu;commit 874dc2cd54422dab26f72a6d8555e1c339b5e8e1 (origin/master, origin/HEAD, master)
Author: Jie Yu <yujie.jay@gmail.com>
Date: Thu Feb 22 16:57:52 2018 -0800

Added MESOS-8598 to the 1.5.1 CHANGELOG.

commit 2ddc3c6a18de01eb51448779de1dff311d5dbe51
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu Feb 22 16:32:00 2018 -0800

Allowed empty resource provider selector in `UriDiskProfileAdaptor`.

Review: https://reviews.apache.org/r/65718/",,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid failure for invalid profile in `UriDiskProfileAdaptor`,MESOS-8592,13139198,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Not A Problem,,chhsia0,chhsia0,17/Feb/18 00:51,24/Apr/19 15:44,29/Oct/20 16:32,24/Apr/19 15:44,,,,,,,,,,,,,,,storage,,,,,0,mesosphere,mesosphere-dss-ga,storage,,,,,,We should be defensive and not fail the profile module when the user provides an invalid profile in the profile matrix.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 24 15:44:24 UTC 2019,,,,,,,"0|i3qatj:",9223372036854775807,,,,,jieyu,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,"24/Apr/19 15:44;chhsia0;Details needed. We haven't observed problems yet.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Test UriDiskProfileTest.FetchFromHTTP is flaky.,MESOS-8567,13137601,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,chhsia0,chhsia0,chhsia0,10/Feb/18 02:31,13/Jul/18 21:46,29/Oct/20 16:32,13/Jul/18 21:46,1.5.0,,,,,,,,1.7.0,,,,,,storage,,,,,0,disabled-test,mesosphere,storage,,,,,,"The {{UriDiskProfileTest.FetchFromHTTP}} test is flaky on Debian 9:
{noformat}
../../src/tests/disk_profile_tests.cpp:683
Failed to wait 15secs for future
{noformat}

I also run it in repetition and got the following error log (although the test itself is passed):
{noformat}
E0209 18:26:37.030012  7282 uri_disk_profile.cpp:220] Failed to parse result: Failed to parse DiskProfileMapping message: INVALID_ARGUMENT:Unexpected end of string. Expected a value.

^
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-02-12 10:04:06.644,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 13 21:46:11 UTC 2018,,,,,,,"0|hzzy1l:",9223372036854775807,,,,,bbannier,,,,,,Mesosphere Sprint 74,Mesosphere Sprint 75,Mesosphere Sprint 76,Mesosphere Sprint 77,Mesosphere Sprint 78,Mesosphere Sprint 79,Mesosphere Sprint 2018-22,Mesosphere Sprint 2018-23,Mesosphere Sprint 2018-24,,,3.0,,1.7.0,,,,,,,,,"12/Feb/18 10:04;alexr;Observed it today on Ubuntu 16.04.
{noformat}
[ RUN      ] UriDiskProfileTest.FetchFromHTTP
I0211 02:44:47.614900  5392 process.cpp:3515] Handling HTTP event for process '(1466)' with path: '/(1466)/profiles'
I0211 02:44:47.615609  5392 uri_disk_profile.cpp:302] Updated disk profile mapping to 1 total profiles
W0211 02:44:47.624699  5391 uri_disk_profile.cpp:229] Failed to poll URI: Failed to open file: No such file or directory
I0211 02:44:47.625382  5389 process.cpp:3515] Handling HTTP event for process '(1466)' with path: '/(1466)/profiles'
W0211 02:44:47.626112  5395 uri_disk_profile.cpp:229] Failed to poll URI: Failed to open file: No such file or directory
W0211 02:44:47.626336  5392 uri_disk_profile.cpp:251] Fetched profile mapping does not contain profile 'profile'. The fetched mapping will be ignored entirely
../../src/tests/disk_profile_tests.cpp:683: Failure
Failed to wait 15secs for future
../../src/tests/disk_profile_tests.cpp:629: Failure
Actual function call count doesn't match EXPECT_CALL(*server.process, profiles(_))...
         Expected: to be called 3 times
           Actual: called twice - unsatisfied and active
[  FAILED  ] UriDiskProfileTest.FetchFromHTTP (15032 ms)
{noformat}","12/Feb/18 10:04;alexr;Disabling this test for now.","12/Feb/18 22:56;chhsia0;Root cause identified: {{FutureSatisfy(&secondCall)}} (and thus the second {{Clock::advance()}}) races with the dispatching of the second {{UriDiskProfileAdaptor::_poll()}}. If the second {{_poll()}} runs after the clock is advanced, the third {{_poll()}} won't be dispatched.","13/Feb/18 22:17;chhsia0;Review: https://reviews.apache.org/r/65640/","21/Mar/18 15:37;alexr;[~chhsia0] With your patch, can we also re-enable the {{UriDiskProfileTest.DISABLED_FetchFromFile}} test?","21/Mar/18 20:03;chhsia0;Already did in my patch :)","13/Jul/18 21:46;chhsia0;{noformat}
commit afd1647b96abdc9352864f0a8e2f4c24a2f0c7d7
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Tue Feb 13 14:08:00 2018 -0800

Fixed a race condition in `UriDiskProfileAdaptorTests`.

There was a race between `Clock::advance()` in the `FetchFromHTTP` test
and `delay()` in `UriDiskProfileAdaptorProcess::_poll`. This patch
avoids the race by enforcing an order between the dispatch of the
`__poll` function (previously `_poll`) and the clock manipulation
in the test.

Review: https://reviews.apache.org/r/65640{noformat}",,,,,,,,,,,,,,,,,,,,,
Default executor should allow decreasing the escalation grace period of a terminating task,MESOS-8557,13137321,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,gkleiman,gkleiman,09/Feb/18 01:52,06/Feb/19 17:13,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,default-executor,foundations,gracefulshutdown,mesosphere,,,,,"The command executor supports [decreasing the escalation grace period of a terminating task|https://github.com/apache/mesos/blob/c665dd6c22715fa941200020a8f7209f1f5b1ca1/src/launcher/executor.cpp#L800-L803].

For consistency, this should also be supported by the default executor.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-03-27 09:44:21.929,,,false,MESOS-8529,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 27 09:44:21 UTC 2018,,,,,,,"0|i3pz8n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,"27/Mar/18 09:44;alexr;{noformat}
commit 4e7bbe67f55fbaa560466fc1d0a2f5e5bdb6ab32
Author:     Gaston Kleiman <gaston@mesosphere.io>
AuthorDate: Tue Mar 27 11:38:13 2018 +0200
Commit:     Alexander Rukletsov <alexr@apache.org>
CommitDate: Tue Mar 27 11:38:13 2018 +0200

    Added a reference to MESOS-8557 to the default executor.
    
    Review: https://reviews.apache.org/r/66235/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,
Test StorageLocalResourceProviderTest.ROOT_Metrics is flaky,MESOS-8548,13136471,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,chhsia0,chhsia0,chhsia0,06/Feb/18 05:24,22/May/19 15:45,29/Oct/20 16:32,08/Feb/18 06:44,,,,,,,,,1.6.0,,,,,,storage,,,,,0,,,,,,,,,"The SLRP Metrics test is flaky because the agent might got two {{SlaveRegisteredMessage}}s due to its retry logic for registration, and thus it would send two {{UpdateSlaveMessage}}s. As a result, the futures waiting for these messages will be ready before the plugin is actually launched. This will lead to a race between the SIGKILL and LAUNCH_CONTAINER in the test, and if the kill happens before SLRP gets connected to the plugin, SLRP will wait for 1 minutes before giving up, which is too long for the test to wait for a second launch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 08 06:44:31 UTC 2018,,,,,,,"0|i3uvs7:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 74,,,,,,,,,,,2.0,,1.5.1,,,,,,,,,"07/Feb/18 21:02;chhsia0;https://reviews.apache.org/r/65523/","08/Feb/18 06:44;chhsia0;{noformat}
commit 59c7e69c9bc4bc5dfde46535479f61dbc03c08b0
Author: Chun-Hung Hsiao 
Date:   Wed Feb 7 16:04:09 2018 -0800

    Fixed the flakiness in the SLRP metrics test.
    
    The metrics test unnecessarily set up the disk profile module without
    providing the profile config file. This may cause the profile module to
    pull a non-existent file. Also, it could kill the plugin before SLRP
    connecting to the endpoint, making SLRP wait for 1 minute.
    
    Review: https://reviews.apache.org/r/65523/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
The default executor can wrongly indicate that tasks from all task groups are unhealthy,MESOS-8543,13136349,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,gkleiman,gkleiman,05/Feb/18 18:33,06/Feb/19 17:14,29/Oct/20 16:32,,,,,,,,,,,,,,,,executor,,,,,0,default-executor,foundations,health-check,mesosphere,,,,,"The default executor sets a ""global"" [unhealthy|https://github.com/apache/mesos/blob/7ace6c4c263ebdd8f1605b1c478cb2e7eb9e83a1/src/launcher/default_executor.cpp#L1286] field to {{true}} once it kills a task due to failed health checks.

When a task from any task group exits, it will [check|https://github.com/apache/mesos/blob/7ace6c4c263ebdd8f1605b1c478cb2e7eb9e83a1/src/launcher/default_executor.cpp#L877-L882] that ""global"" healthy field and set the task status update's {{healthy}} field accordingly. This means that once an unhealthy task belonging to a task group is killed, task status updates for tasks belonging to other task groups, which can contain only healthy tasks, will be sent with their {{healthy}} field set to {{false}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8529,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 05 18:34:32 UTC 2018,,,,,,,"0|i3pt93:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"05/Feb/18 18:34;gkleiman;cc/ [~abudnik] [~alexr]",,,,,,,,,,,,,,,,,,,,,,,,,,,
Default executor doesn't wait for status updates to be ack'd before shutting down,MESOS-8537,13135878,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,abudnik,gkleiman,gkleiman,02/Feb/18 20:38,19/Mar/20 22:31,29/Oct/20 16:32,03/Feb/20 15:56,1.4.1,1.5.0,,,,,,,1.10.0,1.5.4,1.6.3,1.7.3,1.8.2,1.9.1,executor,,,,,0,containerization,default-executor,mesosphere,,,,,,"The default executor doesn't wait for pending status updates to be acknowledged before shutting down, instead it sleeps for one second and then terminates:

{code}
  void _shutdown()
  {
    const Duration duration = Seconds(1);

    LOG(INFO) << ""Terminating after "" << duration;

    // TODO(qianzhang): Remove this hack since the executor now receives
    // acknowledgements for status updates. The executor can terminate
    // after it receives an ACK for a terminal status update.
    os::sleep(duration);
    terminate(self());
  }
{code}

The event handler should exit if upon receiving a {{Event::ACKNOWLEDGED}} the executor is shutting down, no tasks are running anymore, and all pending status updates have been acknowledged.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-01-20 18:49:34.47,,,false,MESOS-8529,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 03 16:04:28 UTC 2020,,,,,,,"0|hzzxgo:i",9223372036854775807,,,,,,,,,,,Containerization: RI-16 Sp 50,,,,,,,,,,,3.0,,,,,,,,,,,"20/Jan/20 18:49;abudnik;https://reviews.apache.org/r/72029/","03/Feb/20 15:56;abudnik;{code:java}
commit 683dfc1ffb0b1ca758a07d19ab3badd8cac62dc7
Author: Andrei Budnik <abudnik@apache.org>
Date:   Wed Jan 29 19:07:50 2020 +0100

    Changed termination logic of the default executor.

    Previously, the default executor terminated itself after all containers
    had terminated. This could lead to termination of the executor before
    processing of a terminal status update by the agent. In order
    to mitigate this issue, the executor slept for one second to give a
    chance to send all status updates and receive all status update
    acknowledgements before terminating itself. This might have led to
    various race conditions in some circumstances (e.g., on a slow host).
    This patch terminates the default executor if all status updates have
    been acknowledged by the agent and no running containers left.
    Also, this patch increases the timeout from one second to one minute
    for fail-safety.

    Review: https://reviews.apache.org/r/72029
{code}
","03/Feb/20 16:04;abudnik;1.5.x
{code:java}
commit 84b7af3409d8af343da0f0420e168a42de4b110f
Author: Andrei Budnik <abudnik@apache.org>
Date:   Wed Jan 29 19:07:50 2020 +0100

    Changed termination logic of the default executor.

    Previously, the default executor terminated itself after all containers
    had terminated. This could lead to termination of the executor before
    processing of a terminal status update by the agent. In order
    to mitigate this issue, the executor slept for one second to give a
    chance to send all status updates and receive all status update
    acknowledgements before terminating itself. This might have led to
    various race conditions in some circumstances (e.g., on a slow host).
    This patch terminates the default executor if all status updates have
    been acknowledged by the agent and no running containers left.
    Also, this patch increases the timeout from one second to one minute
    for fail-safety.

    Review: https://reviews.apache.org/r/72029
{code}

1.6.x
{code:java}
commit 205525eb56a33e58bed1fc38e0b32189b19d3fbc
Author: Andrei Budnik <abudnik@apache.org>
Date:   Wed Jan 29 19:07:50 2020 +0100

    Changed termination logic of the default executor.

    Previously, the default executor terminated itself after all containers
    had terminated. This could lead to termination of the executor before
    processing of a terminal status update by the agent. In order
    to mitigate this issue, the executor slept for one second to give a
    chance to send all status updates and receive all status update
    acknowledgements before terminating itself. This might have led to
    various race conditions in some circumstances (e.g., on a slow host).
    This patch terminates the default executor if all status updates have
    been acknowledged by the agent and no running containers left.
    Also, this patch increases the timeout from one second to one minute
    for fail-safety.

    Review: https://reviews.apache.org/r/72029
{code}

1.7.x
{code:java}
commit 5b399080eee11ee03f4bc6c09b791c24670da6c1
Author: Andrei Budnik <abudnik@apache.org>
Date:   Wed Jan 29 19:07:50 2020 +0100

    Changed termination logic of the default executor.

    Previously, the default executor terminated itself after all containers
    had terminated. This could lead to termination of the executor before
    processing of a terminal status update by the agent. In order
    to mitigate this issue, the executor slept for one second to give a
    chance to send all status updates and receive all status update
    acknowledgements before terminating itself. This might have led to
    various race conditions in some circumstances (e.g., on a slow host).
    This patch terminates the default executor if all status updates have
    been acknowledged by the agent and no running containers left.
    Also, this patch increases the timeout from one second to one minute
    for fail-safety.

    Review: https://reviews.apache.org/r/72029
{code}

1.8.x
{code:java}
commit a2ca451aab4625e126b9e7b470eb9f7c232dd746
Author: Andrei Budnik <abudnik@apache.org>
Date:   Wed Jan 29 19:07:50 2020 +0100

    Changed termination logic of the default executor.

    Previously, the default executor terminated itself after all containers
    had terminated. This could lead to termination of the executor before
    processing of a terminal status update by the agent. In order
    to mitigate this issue, the executor slept for one second to give a
    chance to send all status updates and receive all status update
    acknowledgements before terminating itself. This might have led to
    various race conditions in some circumstances (e.g., on a slow host).
    This patch terminates the default executor if all status updates have
    been acknowledged by the agent and no running containers left.
    Also, this patch increases the timeout from one second to one minute
    for fail-safety.

    Review: https://reviews.apache.org/r/72029
{code}

1.9.x
{code:java}
commit f37ae68a8f0d23a2e0f31812b8fe4494109769c6
Author: Andrei Budnik <abudnik@apache.org>
Date:   Wed Jan 29 19:07:50 2020 +0100

    Changed termination logic of the default executor.

    Previously, the default executor terminated itself after all containers
    had terminated. This could lead to termination of the executor before
    processing of a terminal status update by the agent. In order
    to mitigate this issue, the executor slept for one second to give a
    chance to send all status updates and receive all status update
    acknowledgements before terminating itself. This might have led to
    various race conditions in some circumstances (e.g., on a slow host).
    This patch terminates the default executor if all status updates have
    been acknowledged by the agent and no running containers left.
    Also, this patch increases the timeout from one second to one minute
    for fail-safety.

    Review: https://reviews.apache.org/r/72029
{code}",,,,,,,,,,,,,,,,,,,,,,,,,
The default executor doesn't retry kills that it initiated,MESOS-8532,13135613,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,gkleiman,gkleiman,01/Feb/18 21:49,06/Feb/19 17:17,29/Oct/20 16:32,,1.2.3,1.3.1,1.4.1,1.5.0,,,,,,,,,,,executor,,,,,0,default-executor,foundations,mesosphere,,,,,,"The default executor might initiate a task kill due to health check failures or to a task in the same task group failing.

If the kill call fails, the executor won't retry it, so the task will get stuck in a killing state.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8529,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-02-01 21:49:24.0,,,,,,,"0|i3popz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some task status updates sent by the default executor don't contain a REASON.,MESOS-8531,13135612,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,gkleiman,gkleiman,01/Feb/18 21:46,06/Feb/19 17:18,29/Oct/20 16:32,,1.2.3,1.3.1,1.4.1,1.5.0,,,,,,,,,,,executor,,,,,0,default-executor,foundations,mesosphere,observability,,,,,"The default executor doesn't set a reason when sending {{TASK_KILLING}}, {{TASK_KILLED}},
 and {{TASK_FAILED}} task status update.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-9298,MESOS-8768,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-04-09 17:41:31.099,,,false,MESOS-8529,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 09 17:41:31 UTC 2018,,,,,,,"0|i3popr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"09/Apr/18 17:41;jamespeach;This refers to the status updates that are sent when the default executor tears down a task group in response to a single task failing. In slack, we discussed defining a separate reason field that would be used to make it more explicit that a particular task was killed because the group failed (in some sense).",,,,,,,,,,,,,,,,,,,,,,,,,,,
Default executor tasks can get stuck in KILLING state,MESOS-8530,13135611,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,gkleiman,gkleiman,gkleiman,01/Feb/18 21:43,27/Mar/18 09:43,29/Oct/20 16:32,27/Mar/18 09:43,1.2.3,1.3.1,1.4.1,1.5.0,,,,,1.6.0,,,,,,executor,,,,,0,default-executor,mesosphere,,,,,,,"The default executor will transition a task to {{TASK_KILLING}} and mark its container as being killed before issuing the {{KILL_NESTED_CONTAINER}} call.

If the kill call fails, the task will get stuck in {{TASK_KILLING}}, and the executor won't allow retrying the kill.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-03-27 09:43:06.584,,,false,MESOS-8529,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 27 09:43:06 UTC 2018,,,,,,,"0|i3popj:",9223372036854775807,,,,,alexr,,,,,,Mesosphere Sprint 74,Mesosphere Sprint 75,Mesosphere Sprint 76,,,,,,,,,5.0,,,,,,,,,,,"17/Feb/18 00:30;gkleiman;https://reviews.apache.org/r/65692/
https://reviews.apache.org/r/65693/
https://reviews.apache.org/r/66232/
https://reviews.apache.org/r/65694/
https://reviews.apache.org/r/66233/
https://reviews.apache.org/r/65962/
https://reviews.apache.org/r/66234/","06/Mar/18 18:30;gkleiman;[~kaysoky] hey, do you think you'll have time to review the chain this week?","27/Mar/18 09:43;alexr;{noformat}
commit 73766e635c6619efe94eeef8fa8b7002ee2b6f50
Author:     Gaston Kleiman <gaston@mesosphere.io>
AuthorDate: Tue Mar 27 11:36:29 2018 +0200
Commit:     Alexander Rukletsov <alexr@apache.org>
CommitDate: Tue Mar 27 11:36:29 2018 +0200

    Changed the signature of a DefaultExecutor kill method.
    
    This method is being passed the container to be killed as
    `Owned<Container>`.
    
    The container object is owned by the executor and stored
    in `LinkedHashMap<TaskID, Owner<Container>> containers`.
    
    We should pass around raw pointers and not make copies of
    the stored `Owned` object, so this patch changes the
    signature of the `kill` method.
    
    Review: https://reviews.apache.org/r/65692/
{noformat}
{noformat}
commit 1e9bfbf3705cc9668a0a40829d311493a679712d
Author:     Gaston Kleiman <gaston@mesosphere.io>
AuthorDate: Tue Mar 27 11:37:06 2018 +0200
Commit:     Alexander Rukletsov <alexr@apache.org>
CommitDate: Tue Mar 27 11:37:06 2018 +0200

    Made the default executor fail kills if the response isn't ""200 OK"".
    
    The default executor's `Future<Nothing> kill(const ContainerID&, int)`
    method returns `Nothing()` if the agent responded to the
    `KILL_NESTED_CONTAINER` call, regardless of the response.
    
    This patch updates the method, so that it returns a failure if the
    response is not ""200 OK"".
    
    Review: https://reviews.apache.org/r/65693/
{noformat}
{noformat}
commit 801aee6b9c95f695d3d33b8ee444e2b875347786
Author:     Gaston Kleiman <gaston@mesosphere.io>
AuthorDate: Tue Mar 27 11:37:15 2018 +0200
Commit:     Alexander Rukletsov <alexr@apache.org>
CommitDate: Tue Mar 27 11:37:15 2018 +0200

    Removed unnecessary/invalid checks from the default executor.
    
    The default executor uses checks to ensure that it is subscribed to the
    agent before killing a task/container; if it is not, it will skip kill
    calls or crash.
    
    When killing a task/container, the default executor performs the
    following actions:
    
    1) Calls `KILL_NESTED_CONTAINER`.
    2) Enqueues a task status update (for tasks only).
    
    None of these actions require an active subscription.
    
    When the checks were added:
    
    1) The default executor supported launching only one task group, so it
    was correct to assume that killing a task would eventually trigger the
    destruction of all the child containers.
    2) It was assumed that the executor will only be unsubscribed for brief
    periods of time, mostly during agent recovery, when kill calls are
    likely to fail.
    3) There was no kill/escalation retry logic.
    
    The checks were added as a way of working around the lack of retry logic
    for kill requests, relying on the fact that crashing the executor leads
    to the destruction of all the child containers.
    
    This chain adds retry logic for failed kill call escalation, making the
    workaround unnecessary.
    
    Now that the default executor supports running multiple task groups, the
    checks are not just unnecessary, but also invalid and dangerous. If a
    check fails, all the containers started by the executor will be killed,
    regardless of which task group they belong to. This is bad and could
    lead to data loss.
    
    This patch removes these unnecessary and sometimes invalid checks from
    the default executor.
    
    Review: https://reviews.apache.org/r/66232/
{noformat}
{noformat}
commit 501eb34467812143a6462441e9d8447be474c675
Author:     Gaston Kleiman <gaston@mesosphere.io>
AuthorDate: Tue Mar 27 11:37:28 2018 +0200
Commit:     Alexander Rukletsov <alexr@apache.org>
CommitDate: Tue Mar 27 11:37:28 2018 +0200

    Made the default executor's handling of kill escalations more robust.
    
    This patch makes the default executor retry SIGKILL escalations if the
    executor is disconnected from the agent or the kill call fails.
    
    Review: https://reviews.apache.org/r/65694/
{noformat}
{noformat}
commit 7d0b4489bb940bf0b116342589b22edc247b65b9
Author:     Gaston Kleiman <gaston@mesosphere.io>
AuthorDate: Tue Mar 27 11:37:44 2018 +0200
Commit:     Alexander Rukletsov <alexr@apache.org>
CommitDate: Tue Mar 27 11:37:44 2018 +0200

    Made the default executor log kill failures.
    
    Review: https://reviews.apache.org/r/66233/
{noformat}
{noformat}
commit cbc7d0e2654310063d6ed856b40e3775901e2333
Author:     Gaston Kleiman <gaston@mesosphere.io>
AuthorDate: Tue Mar 27 11:37:59 2018 +0200
Commit:     Alexander Rukletsov <alexr@apache.org>
CommitDate: Tue Mar 27 11:37:59 2018 +0200

    Avoided copying `Owned` pointers in the default executor.
    
    `Owned` pointers are copied in multiple places of the default executor.
    This violates the semantic of owned pointers and works only because
    `Owned` is currently implemented with `shared_ptr`, it would otherwise
    lead to double-freeing the pointers.
    
    This patch changes those places to use references to the original
    `Owned` objects or raw pointers instead of copies.
    
    Review: https://reviews.apache.org/r/65962/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,
"When `UPDATE_SLAVE` messages are received, offers might not be rescinded due to a race ",MESOS-8524,13135442,Bug,Open,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,kaysoky,nfnt,nfnt,01/Feb/18 14:34,24/Apr/19 15:46,29/Oct/20 16:32,,1.5.0,,,,,,,,,,,,,,allocation,master,,,,0,mesosphere,,,,,,,,"When an agent with enabled {{RESOURCE_PROVIDER}} capability (re-)registers with the master it sends a {{UPDATE_SLAVE}} after being (re-)registered. In the master, the agent is added (back) to the allocator, as soon as it's (re-)registered, i.e. before {{UPDATE_SLAVE}} is being send. This triggers an allocation and offers might get sent out to frameworks. When {{UPDATE_SLAVE}} is being handled in the master, these offers have to be rescinded, as they're based on an outdated agent state.
Internally, the allocator defers a offer callback in the master ({{Master::offer}}). In rare cases a {{UPDATE_SLAVE}} message might arrive at the same time and its handler in the master called before the offer callback (but after the actual allocation took place). In this case the (outdated) offer is still sent to frameworks and never rescinded.

Here's the relevant log lines, this was discovered while working on https://reviews.apache.org/r/65045/:
{noformat}
I0201 14:17:47.041093 242208768 hierarchical.cpp:1517] Performed allocation for 1 agents in 704915ns
I0201 14:17:47.041738 242745344 master.cpp:7235] Received update of agent 53c557e7-3161-449b-bacc-a4f8c02e78e7-S0 at slave(540)@172.18.8.20:60469 (172.18.8.20) with total oversubscribed resources {}
I0201 14:17:47.042778 242745344 master.cpp:8808] Sending 1 offers to framework 53c557e7-3161-449b-bacc-a4f8c02e78e7-0000 (default) at scheduler-798f476b-b099-443e-bd3b-9e7333f29672@172.18.8.20:60469
I0201 14:17:47.043102 243281920 sched.cpp:921] Scheduler::resourceOffers took 40444ns
I0201 14:17:47.043427 243818496 hierarchical.cpp:712] Grew agent 53c557e7-3161-449b-bacc-a4f8c02e78e7-S0 by disk[MOUNT]:200 (total), {  } (used)
I0201 14:17:47.043643 243818496 hierarchical.cpp:669] Agent 53c557e7-3161-449b-bacc-a4f8c02e78e7-S0 (172.18.8.20) updated with total resources disk[MOUNT]:200; cpus:2; mem:1024; disk:1024; ports:[31000-32000]
{noformat}",Master + Agent running with enabled {{RESOURCE_PROVIDER}} capability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-02-01 15:41:49.747,,,false,MESOS-4553,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 17 22:44:19 UTC 2018,,,,,,,"0|i3pnnz:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 74,Mesosphere Sprint 75,Mesosphere Sprint 76,,,,,,,,,2.0,,,,,,,,,,,"01/Feb/18 15:41;bbannier;This seems to likely also be an issue for even non-RP-capable agents with oversubscription enabled. ","17/Apr/18 22:44;gilbert;[~kaysoky][~bmahler], do we still want to land it in 1.5.1?",,,,,,,,,,,,,,,,,,,,,,,,,,
SLRP failed to connect to CSI endpoint.,MESOS-8514,13134949,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,chhsia0,chhsia0,chhsia0,31/Jan/18 01:33,14/Jun/18 20:08,29/Oct/20 16:32,31/Jan/18 20:32,,,,,,,,,1.5.0,,,,,,,,,,,0,mesosphere,storage,,,,,,,"After bumping the gRPC bundle to 1.8.3, there are some flakiness in SLRP tests caused by SLRP not being able to connect to a CSI endpoint. The reason is that it seems to take longer for gRPC 1.8 to prepare a domain socket (i.e., the time between the {{bind}} and {{accept}} calls are longer), and as a result, SLRP cannot talk to a CSI plugin immediately after the socket file is created.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-01-31 20:32:30.44,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 31 20:32:30 UTC 2018,,,,,,,"0|i3uvr3:",9223372036854775807,,,,,greggomann,,,,,,Mesosphere Sprint 73,,,,,,,,,,,2.0,,1.5.0,,,,,,,,,"31/Jan/18 01:33;chhsia0;Review: https://reviews.apache.org/r/65426/","31/Jan/18 20:32;greggomann;{code}
commit d7cd8ef1705fcf6a649f8c8322779ae2835d0cfc
Author: Chun-Hung Hsiao chhsiao@mesosphere.io
Date:   Wed Jan 31 11:22:23 2018 -0800


Made gRPC calls wait for the channel to be ready.

Because of the time needed in gRPC 1.8 to prepare a domain socket, the
SLRP cannot talk to a CSI plugin immediately after the socket file is
created. This patch fixes this problem by asking gRPC calls to wait for
the socket to be ready.

Review: https://reviews.apache.org/r/65426/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
URI disk profile adaptor does not consider plugin type for a profile.,MESOS-8510,13134893,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,chhsia0,jieyu,jieyu,30/Jan/18 21:32,14/Jun/18 20:08,29/Oct/20 16:32,09/Feb/18 01:41,1.5.0,,,,,,,,1.5.1,1.6.0,,,,,,,,,,0,,,,,,,,,"Currently, the URI disk profile adaptor will fetch an URI, the content of which contains a profile matrix. However, there's no field in the profile matrix for the adaptor to tell which plugin type a profile is for.

We should consider adding a `plugin_type` field in `CSIManifest`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-02-08 06:40:58.394,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 09 01:41:32 UTC 2018,,,,,,,"0|i3uvqv:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 73,Mesosphere Sprint 74,,,,,,,,,,3.0,,1.5.1,,,,,,,,,"08/Feb/18 06:40;chhsia0;https://reviews.apache.org/r/65553/
https://reviews.apache.org/r/65554/
https://reviews.apache.org/r/65538/
https://reviews.apache.org/r/65558/
https://reviews.apache.org/r/65559/
https://reviews.apache.org/r/65566/","08/Feb/18 23:06;chhsia0;Updated the documentation: https://reviews.apache.org/r/65578","09/Feb/18 01:41;jieyu;commit 3e9ce2b8e0b99283e61cb851ad1ac6563129b291
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu Feb 8 17:33:25 2018 -0800

Updated documentation for `UriDiskProfileAdaptor`.

Review: [https://reviews.apache.org/r/65578/]

commit a018cf33d5f06f5a9f9099a4c74b2daea00bd0f7
Author: Jie Yu <yujie.jay@gmail.com>
Date: Thu Feb 8 16:23:05 2018 -0800

Made gcc happy on return statement check.

commit 58add5a2c615f0dc5620da9a125ca121c632908c
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu Feb 8 14:41:31 2018 -0800

Returned profiles based on provider selectors in UriDiskProfileAdaptor.

Now the URI disk profile adaptor module will return the set of profiles
 in which each profile is either known to a storage resource provider or
 applies to it (based on the resource provider selector) when it watches
 for profiles.

Review: https://reviews.apache.org/r/65566/

commit 921f61fb785a6a2ac5e55e4b5b3f42ce6d80f7db
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu Feb 8 14:41:28 2018 -0800

Updated tests for resource provider selector support.

Review: [https://reviews.apache.org/r/65559/]

commit 00b9ea0f105c69f00c656418a606c9a13ac7581d
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu Feb 8 14:41:25 2018 -0800

Added helpers for supporting resource provider selectors.

This patches validates that a profile's manifest must have a selector
 and adds a helper function for selecting resource providers.

Review: https://reviews.apache.org/r/65558/

commit 5aabef41cd1531f59380274548e0d01d4f346b42
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu Feb 8 14:41:23 2018 -0800

Added resource provider selectors in `disk_profile.proto`.

This patch adds two selectors to the `CSIManifest` protobuf so that the
 URI disk profile adaptor can be customized to notify each resource
 provider with a different set of profiles.

Review: https://reviews.apache.org/r/65538/

commit 2da758eb03801bce716e87521a65357fa13e2d06
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu Feb 8 14:41:20 2018 -0800

Updated disk profile tests due to changes in the module interface.

Review: https://reviews.apache.org/r/65554/

commit 542776b7293f7e2c7bd95967c724fafc524636c4
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Thu Feb 8 14:41:16 2018 -0800

Passed `ResourceProviderInfo` to disk profile adaptor modules.

Review: https://reviews.apache.org/r/65553/",,,,,,,,,,,,,,,,,,,,,,,,,
Test StorageLocalResourceProviderTest.ROOT_ConvertPreExistingVolume is flaky,MESOS-8474,13132861,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,chhsia0,bbannier,bbannier,22/Jan/18 20:47,14/Jun/18 20:08,29/Oct/20 16:32,08/Feb/18 06:43,1.5.0,,,,,,,,1.6.0,,,,,,storage,test,,,,0,flaky,flaky-test,mesosphere,,,,,,"Observed on our internal CI on ubuntu16.04 with SSL and GRPC enabled,
{noformat}
../../src/tests/storage_local_resource_provider_tests.cpp:1898
      Expected: 2u
      Which is: 2
To be equal to: destroyed.size()
      Which is: 1
{noformat}",Review: https://reviews.apache.org/r/65499/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/18 15:40;bbannier;consoleText.txt;https://issues.apache.org/jira/secure/attachment/12907300/consoleText.txt","22/Jan/18 20:48;bbannier;consoleText.txt;https://issues.apache.org/jira/secure/attachment/12907181/consoleText.txt",,,,2.0,,,,,,,,,,,,,,,,,,,,2018-01-22 21:15:57.607,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 08 06:43:21 UTC 2018,,,,,,,"0|i3uvqn:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 74,,,,,,,,,,,2.0,,,,,,,,,,,"22/Jan/18 21:15;chhsia0;This is caused by a race that the master may send out an offer between {{DESTRY_VOLUME}} and {{DESTROY_BLOCK}}. I'll work on a patch to fix this test, possibly by controlling the clock.","23/Jan/18 15:41;bbannier;This failed again with a different error,

{noformat}
../../src/tests/storage_local_resource_provider_tests.cpp:1877
block is NONE
{noformat}

I attached [the full test log|https://issues.apache.org/jira/secure/attachment/12907300/consoleText.txt].","25/Jan/18 02:52;chhsia0;This is a similar issue, but now a race between {{CREATE_VOLUME}} and {{CREATE_BLOCK}}. Will implement a better synchronization logic in this test.","26/Jan/18 11:24;alexr;Disabled the test for now.","03/Feb/18 01:41;chhsia0;Review: https://reviews.apache.org/r/65499/","08/Feb/18 06:43;chhsia0;{noformat}
commit 182760bd7c8611b0822d6d5fbba5e00df6363c8e
Author: Chun-Hung Hsiao 
Date:   Wed Feb 7 16:03:59 2018 -0800

    Fixed the flakiness of the ROOT_ConvertPreExistingVolume test.
    
    This unit test is flaky because the master might send out an offer
    before both offer operations of a previous `ACCEPT` call are finished.
    This is fixed by manipulating the clock such that no offer won't be
    sent out until the master receives status updates for both operations.
    
    Review: https://reviews.apache.org/r/65499/
{noformat}",,,,,,,,,,,,,,,,,,,,,,
`LAUNCH_GROUP` failure tears down the default executor.,MESOS-8468,13132303,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,gkleiman,chhsia0,chhsia0,19/Jan/18 21:59,14/Feb/18 12:51,29/Oct/20 16:32,14/Feb/18 12:51,1.2.0,1.3.0,1.4.0,1.5.0,,,,,1.5.1,1.6.0,,,,,,,,,,0,default-executor,mesosphere,,,,,,,"The following code in the default executor (https://github.com/apache/mesos/blob/12be4ba002f2f5ff314fbc16af51d095b0d90e56/src/launcher/default_executor.cpp#L525-L535) shows that if a `LAUNCH_NESTED_CONTAINER` call is failed (say, due to a fetcher failure), the whole executor will be shut down:
{code:cpp}
// Check if we received a 200 OK response for all the
// `LAUNCH_NESTED_CONTAINER` calls. Shutdown the executor
// if this is not the case.
foreach (const Response& response, responses.get()) {
  if (response.code != process::http::Status::OK) {
    LOG(ERROR) << ""Received '"" << response.status << ""' (""
               << response.body << "") while launching child container"";
    _shutdown();
    return;
  }
}
{code}

This is not expected by a user. Instead, one would expect that a failed `LAUNCH_GROUP` won't affect other task groups launched by the same executor, similar to the case that a task failure only takes down its own task group. We should adjust the semantics to make a failed `LAUNCH_GROUP` not take down the executor and affect other task groups.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-02-07 21:59:32.575,,,false,MESOS-8529,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 14 12:51:36 UTC 2018,,,,,,,"0|i3p4wn:",9223372036854775807,,,,,qianzhang,,,,,,Mesosphere Sprint 73,Mesosphere Sprint 74,,,,,,,,,,5.0,,1.6.0,,,,,,,,,"07/Feb/18 21:59;gkleiman;https://reviews.apache.org/r/65548/
https://reviews.apache.org/r/65549/
https://reviews.apache.org/r/65550/
https://reviews.apache.org/r/65551/
https://reviews.apache.org/r/65552/
https://reviews.apache.org/r/65556/

 ","14/Feb/18 12:50;qianzhang;https://reviews.apache.org/r/65616/","14/Feb/18 12:51;qianzhang;commit 632ff7f7f8e32d3f9507e9199c8a253ff755224e
Author: Gaston Kleiman <gaston@mesosphere.io>
Date: Wed Feb 14 14:35:34 2018 +0800

Removed outdated executor-wide launched flag from the default executor.
 
 Review: https://reviews.apache.org/r/65616/

src/launcher/default_executor.cpp | 13 ++++---------
 1 file changed, 4 insertions(+), 9 deletions(-)

commit 54b6c5b9c7cb059ebd87ee0f9927cfa6ff73129d
Author: Gaston Kleiman <gaston@mesosphere.io>
Date: Wed Feb 14 14:35:22 2018 +0800

Made the default executor treat agent disconnections more gracefully.
 
 This patch makes the default executor not shutdown if there are active
 child containers, and it fails to connect or is not subscribed to the
 agent when starting to launch a task group.
 
 Review: https://reviews.apache.org/r/65556/

src/launcher/default_executor.cpp | 43 +++++++++++++++++++++++++++++++++++--------
 1 file changed, 35 insertions(+), 8 deletions(-)

commit 656196eeca4ab6449c4b9f329b5b9cac2f69a885
Author: Gaston Kleiman <gaston@mesosphere.io>
Date: Wed Feb 14 14:35:17 2018 +0800

Added a regression test for MESOS-8468.
 
 Review: https://reviews.apache.org/r/65552/

src/tests/default_executor_tests.cpp | 252 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 1 file changed, 252 insertions(+)

commit c3f3542e7ecce82cad8b75fdc2db14fe8c43a5da
Author: Gaston Kleiman <gaston@mesosphere.io>
Date: Wed Feb 14 14:35:11 2018 +0800

Stopped shutting down the whole default executor on task launch failure.

The default executor would be completely shutdown on a
 `LAUNCH_NESTED_CONTAINER` failure.
 
 This patch makes it kill the affected task group instead of shutting
 down and killing all task groups.
 
 Review: https://reviews.apache.org/r/65551/

src/launcher/default_executor.cpp | 165 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++--------------------------------------------------
 1 file changed, 103 insertions(+), 62 deletions(-)

commit 5c8852b244b09b4ae57e00abcd940482927d57e6
Author: Gaston Kleiman <gaston@mesosphere.io>
Date: Wed Feb 14 14:35:01 2018 +0800

Made default executor not shutdown if unsubscribed during task launch.
 
 The default executor would unnecessarily shutdown if, while launching a
 task group, it gets unsubscribed after having successfully launched the
 task group's containers.
 
 Review: https://reviews.apache.org/r/65550/

src/launcher/default_executor.cpp | 24 +++++++++++++-----------
 1 file changed, 13 insertions(+), 11 deletions(-)

commit 2e570b709dc7d15c73c8d728ef0b32e2416b0a08
Author: Gaston Kleiman <gaston@mesosphere.io>
Date: Wed Feb 14 14:34:56 2018 +0800

Improved some default executor log messages.
 
 Review: https://reviews.apache.org/r/65549/

src/launcher/default_executor.cpp | 6 +++---
 1 file changed, 3 insertions(+), 3 deletions(-)

commit 29d1e4e1a1b894da78c2033f1932b282ee794f4b
Author: Gaston Kleiman <gaston@mesosphere.io>
Date: Wed Feb 14 14:34:50 2018 +0800

Added `Event::Update` and `v1::scheduler::TaskStatus` ostream operators.
 
 This operators make gtest print a human-readable representation of the
 protos on test failures.
 
 Review: https://reviews.apache.org/r/65548/

include/mesos/v1/mesos.hpp | 3 +++
 include/mesos/v1/scheduler/scheduler.hpp | 10 ++++++++++
 src/v1/mesos.cpp | 37 +++++++++++++++++++++++++++++++++++++
 3 files changed, 50 insertions(+)",,,,,,,,,,,,,,,,,,,,,,,,,
Destroyed executors might be used after `Slave::publishResource()`.,MESOS-8467,13132254,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,chhsia0,chhsia0,chhsia0,19/Jan/18 18:04,10/Apr/19 21:37,29/Oct/20 16:32,10/Apr/19 21:37,1.5.0,,,,,,,,1.7.3,1.8.0,,,,,,,,,,0,mesosphere,mesosphere-dss-ga,storage,,,,,,"In the following code from [https://github.com/apache/mesos/blob/7b30b9ccd63dbcd3375e012dae6e2ffb9dc6a79f/src/slave/slave.cpp#L2652:]
{code:cpp}
publishResources()
  .then(defer(self(), [=] {
    return containerizer->update(
        executor->containerId,
        executor->allocatedResources());
  }))
{code}
A destroyed executor might be dereferenced if it has been move to {{Framework.completedExecutors}} and kicked out from this circular buffer. We should refactor {{Slave::publishResources()}} and its uses to make the code less fragile.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 10 21:37:23 UTC 2019,,,,,,,"0|i3p4lr:",9223372036854775807,,,,,mzhu,,,,,,Storage R11 Sprint 40,Storage: RI-13 Sprint 44,,,,,,,,,,2.0,,1.7.3,1.8.0,,,,,,,,"02/Mar/19 00:08;chhsia0;Review: https://reviews.apache.org/r/70084/","09/Apr/19 20:42;chhsia0;Retargeting to 1.8.1. Actually, if we fully resolve MESOS-9667 by publishing the resources upon receiving {{RunTaskMessage}}s this patch won't be needed, so I'm not going to land the patch for now although I already got a ship-it.","10/Apr/19 21:07;chhsia0;Retargeting this back to 1.8.0 since we have decided to fix MESOS-9711 in another way that would not resolve this issue together.","10/Apr/19 21:37;chhsia0;{noformat}
commit 23f40bed53490127b9e48bc32995c77504dcdc55
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Fri Mar 1 15:49:32 2019 -0800

Avoid dereferencing removed executors and launching containers for them.

When launching executors and tasks, there is no guarantee that the
executors still remain after `Slave::publishResources` is returned. If
not, the executor struct should not be dereferenced and the executor
containers should not be launched at all.

NOTE: The patch makes `Slave::launchExecutor` called asynchronously even
if there is no secret generator. However this should not affect the
correctness of executor launching.

Review: https://reviews.apache.org/r/70084{noformat}
Backported to 1.8.0:
{noformat}
commit 4774bc29a3fabff282ca304597642315f09264f5{noformat}
Backported to 1.7.x:
{noformat}
commit 9cce93413b691f7627491ac264022f7d15ead9cc{noformat}",,,,,,,,,,,,,,,,,,,,,,,,
Clean up endpoint socket if the container daemon is destroyed while waiting.,MESOS-8429,13129953,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,chhsia0,chhsia0,chhsia0,10/Jan/18 20:02,04/Sep/18 21:43,29/Oct/20 16:32,04/Sep/18 17:52,,,,,,,,,1.7.0,,,,,,,,,,,0,mesosphere,storage,,,,,,,"SLRP uses a post-stop hook to ask the container daemon to clean up the endpoint socket after its plugin container is terminated. However, if the container daemon is destructed while waiting for the container it monitors before the container itself is terminated, the socket file will remain there, making SLRP unable to recover.

There might be two solutions:
1. During SLRP recovery, check if the plugin container is still running.
2. Start the container daemon in the waiting phase.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 04 17:52:22 UTC 2018,,,,,,,"0|hzzy5t:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 2018-28,,,,,,,,,,,3.0,,1.7.0,,,,,,,,,"01/Sep/18 17:33;chhsia0;Reviews:
https://reviews.apache.org/r/68600/
https://reviews.apache.org/r/68601/","04/Sep/18 17:52;chhsia0;{noformat}
commit d0349dc6a71e439f2057fd0211880ddcfd773ce5
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Sat Sep 1 05:05:26 2018 -0700

Added a unit test for plugin crash during an agent failover.

If a CSI plugin is crashed during agent failover, the residual socket
file would exist during SLRP recovery. This test verifies that the
plugin is properly cleaned up during recovery so the plugin can be
restarted.

Review: https://reviews.apache.org/r/68600{noformat}
{noformat}
commit be6809a6fb440b3573328e93badee78b7db64848
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Mon Sep 3 14:52:10 2018 -0700

Implicitly authorized `VIEW_STANDALONE_CONTAINER` for SLRPs.

Review: https://reviews.apache.org/r/68614{noformat}
{noformat}
commit 7e825116cdd5a17bf04b6d9075debbb173cc74b6
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Mon Sep 3 14:53:14 2018 -0700

Added the `devolve` helper for agent v1 API responses.

Review: https://reviews.apache.org/r/68615{noformat}
{noformat}
commit 5c5103df5f5f5952859c0c27616fc2d950468763
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date: Mon Sep 3 15:16:34 2018 -0700

Cleaned up residual CSI endpoint sockets for terminated plugins.

If a CSI plugin is crashed during agent failover, the residual socket
file would exist during SLRP recovery, which may in turn make the plugin
fail to restart. This patch cleans up the residual socket files to avoid
such failures.

Review: https://reviews.apache.org/r/68601{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Validation for resource provider config agent API calls.,MESOS-8425,13129932,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Duplicate,chhsia0,chhsia0,chhsia0,10/Jan/18 18:56,05/Oct/18 23:38,29/Oct/20 16:32,05/Oct/18 23:38,,,,,,,,,,,,,,,,,,,,0,mesosphere,storage,,,,,,,"Currently the API returns 200 OK if the config is put in the resource provider config directory, even if the config is not valid (e.g., don't specify a controller plugin). We should consider validating the config when the call is processed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-9228,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 05 23:38:08 UTC 2018,,,,,,,"0|i3or5b:",9223372036854775807,,,,,jieyu,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,"05/Oct/18 23:38;chhsia0;This one is fixed by https://issues.apache.org/jira/browse/MESOS-9228, so closing it as ""duplicate.""",,,,,,,,,,,,,,,,,,,,,,,,,,,
Master's UpdateSlave handler not correctly updating terminated operations,MESOS-8422,13129722,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,bbannier,gkleiman,gkleiman,10/Jan/18 00:59,14/Jun/18 20:07,29/Oct/20 16:32,13/Jan/18 01:23,1.5.0,,,,,,,,1.5.0,,,,,,master,,,,,0,mesosphere,,,,,,,,"I created a test that verifies that operation status updates are resent to the master after being dropped en route to it (MESOS-8420).

The test does the following:

# Creates a volume from a RAW disk resource.
# Drops the first `UpdateOperationStatusMessage` message from the agent to the master, so that it isn't acknowledged by the master.
# Restarts the agent.
# Verifies that the agent resends the operation status update.

The good news are that the agent is resending the operation status update, the bad news are that it triggers a CHECK failure that crashes the master.

Here are the relevant sections of the log produced by the test:

{noformat}
[ RUN      ] StorageLocalResourceProviderTest.ROOT_RetryOperationStatusUpdateAfterRecovery
[...]
I0109 16:36:08.515882 24106 master.cpp:4284] Processing ACCEPT call for offers: [ 046b3f21-6e97-4a56-9a13-773f7d481efd-O0 ] on agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 at slave(2)@10.0.49.2:40681 (core-dev) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (default) at scheduler-2a48a684-64b4-4b4d-a396-6491adb4f2b1@10.0.49.2:40681
I0109 16:36:08.516487 24106 master.cpp:5260] Processing CREATE_VOLUME operation with source disk(allocated: storage)(reservations: [(DYNAMIC,storage)])[RAW(,volume-default)]:4096 from framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (default) at scheduler-2a48a684-64b4-4b4d-a396-6491adb4f2b1@10.0.49.2:40681 to agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 at slave(2)@10.0.49.2:40681 (core-dev)
I0109 16:36:08.518704 24106 master.cpp:10622] Sending operation '' (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) to agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 at slave(2)@10.0.49.2:40681 (core-dev)
I0109 16:36:08.521210 24130 provider.cpp:504] Received APPLY_OPERATION event
I0109 16:36:08.521276 24130 provider.cpp:1368] Received CREATE_VOLUME operation '' (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408)
I0109 16:36:08.523131 24432 test_csi_plugin.cpp:305] CreateVolumeRequest '{""version"":{""minor"":1},""name"":""18b4c4a5-d162-4dcf-bb21-a13c6ee0f408"",""capacityRange"":{""requiredBytes"":""4294967296"",""limitBytes"":""4294967296""},""volumeCapabilities"":[{""mount"":{},""accessMode"":{""mode"":""SINGLE_NODE_WRITER""}}]}'
I0109 16:36:08.525806 24152 provider.cpp:2635] Applying conversion from 'disk(allocated: storage)(reservations: [(DYNAMIC,storage)])[RAW(,volume-default)]:4096' to 'disk(allocated: storage)(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096' for operation (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408)
I0109 16:36:08.528725 24134 status_update_manager_process.hpp:152] Received operation status update OPERATION_FINISHED (Status UUID: 0c79cdf2-b89d-453b-bb62-57766e968dd0) for operation UUID 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 of framework '046b3f21-6e97-4a56-9a13-773f7d481efd-0000' on agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0
I0109 16:36:08.529207 24134 status_update_manager_process.hpp:929] Checkpointing UPDATE for operation status update OPERATION_FINISHED (Status UUID: 0c79cdf2-b89d-453b-bb62-57766e968dd0) for operation UUID 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 of framework '046b3f21-6e97-4a56-9a13-773f7d481efd-0000' on agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0
I0109 16:36:08.573177 24150 http.cpp:1185] HTTP POST for /slave(2)/api/v1/resource_provider from 10.0.49.2:53598
I0109 16:36:08.573974 24139 slave.cpp:7065] Handling resource provider message 'UPDATE_OPERATION_STATUS: (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)'
I0109 16:36:08.574154 24139 slave.cpp:7409] Updating the state of operation ' with no ID (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)
I0109 16:36:08.574785 24139 slave.cpp:7249] Forwarding status update of operation with no ID (operation_uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000
I0109 16:36:08.583748 24084 slave.cpp:931] Agent terminating
I0109 16:36:08.584115 24144 master.cpp:1305] Agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 at slave(2)@10.0.49.2:40681 (core-dev) disconnected
[...]
I0109 16:36:08.655766 24140 slave.cpp:1378] Re-registered with master master@10.0.49.2:40681
I0109 16:36:08.655936 24117 task_status_update_manager.cpp:188] Resuming sending task status updates
I0109 16:36:08.655995 24149 hierarchical.cpp:669] Agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 (core-dev) updated with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]
I0109 16:36:08.656008 24140 slave.cpp:1423] Forwarding agent update {""operations"":{},""resource_version_uuid"":{""value"":""icuAKyO6TymMt2Y9vyF6Jg==""},""slave_id"":{""value"":""046b3f21-6e97-4a56-9a13-773f7d481efd-S0""},""update_oversubscribed_resources"":true}
I0109 16:36:08.656121 24149 hierarchical.cpp:754] Agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 reactivated
W0109 16:36:08.656481 24113 master.cpp:7277] !!!! update slave message: slave_id {
  value: ""046b3f21-6e97-4a56-9a13-773f7d481efd-S0""
}
update_oversubscribed_resources: true
operations {
}
resource_version_uuid {
  value: ""\211\313\200+#\272O)\214\267f=\277!z&""
}
I0109 16:36:08.656637 24113 master.cpp:7320] Received update of agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 at slave(3)@10.0.49.2:40681 (core-dev) with total oversubscribed resources {}
W0109 16:36:08.657387 24113 master.cpp:7704] Performing explicit reconciliation with agent for known operation 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 since it was not present in original reconciliation message from agent
I0109 16:36:08.657917 24133 hierarchical.cpp:669] Agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 (core-dev) updated with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]
W0109 16:36:08.658048 24125 manager.cpp:472] Dropping operation reconciliation message with operation_uuid 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 because resource provider 605b22f5-e39d-4d9f-950a-e7f44d202c01 is not subscribed
I0109 16:36:08.658609 24143 container_daemon.cpp:119] Launching container 'org-apache-mesos-rp-local-storage-test--org-apache-mesos-csi-test-slrp_test--CONTROLLER_SERVICE-NODE_SERVICE'
[...]
I0109 16:36:08.689859 24130 provider.cpp:3066] Sending UPDATE_STATE call with resources 'disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096' and 1 operations to agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0
I0109 16:36:08.690449 24130 provider.cpp:1042] Resource provider 605b22f5-e39d-4d9f-950a-e7f44d202c01 is in READY state
I0109 16:36:08.690491 24105 status_update_manager_process.hpp:385] Resuming operation status update manager
I0109 16:36:08.690640 24105 status_update_manager_process.hpp:394] Sending operation status update OPERATION_FINISHED (Status UUID: 0c79cdf2-b89d-453b-bb62-57766e968dd0) for operation UUID 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408 of framework '046b3f21-6e97-4a56-9a13-773f7d481efd-0000' on agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0
I0109 16:36:08.693244 24131 http.cpp:1185] HTTP POST for /slave(3)/api/v1/resource_provider from 10.0.49.2:53606
I0109 16:36:08.693912 24140 http.cpp:1185] HTTP POST for /slave(3)/api/v1/resource_provider from 10.0.49.2:53606
I0109 16:36:08.693974 24115 manager.cpp:677] Received UPDATE_STATE call with resources '[{""disk"":{""source"":{""id"":""18b4c4a5-d162-4dcf-bb21-a13c6ee0f408"",""metadata"":{""labels"":[{""key"":""path"",""value"":""\/tmp\/n5thZ3\/test\/4GB-18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""}]},""mount"":{""root"":"".\/csi\/org.apache.mesos.csi.test\/slrp_test\/mounts\/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""},""profile"":""volume-default"",""type"":""MOUNT""}},""name"":""disk"",""provider_id"":{""value"":""605b22f5-e39d-4d9f-950a-e7f44d202c01""},""reservations"":[{""role"":""storage"",""type"":""DYNAMIC""}],""scalar"":{""value"":4096.0},""type"":""SCALAR""}]' and 1 operations from resource provider 605b22f5-e39d-4d9f-950a-e7f44d202c01
I0109 16:36:08.694897 24144 slave.cpp:7065] Handling resource provider message 'UPDATE_STATE: 605b22f5-e39d-4d9f-950a-e7f44d202c01 disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096'
I0109 16:36:08.695184 24144 slave.cpp:7182] Forwarding new total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]; disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096
I0109 16:36:08.696467 24144 slave.cpp:7065] Handling resource provider message 'UPDATE_OPERATION_STATUS: (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)'
I0109 16:36:08.696594 24144 slave.cpp:7409] Updating the state of operation ' with no ID (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)
I0109 16:36:08.696666 24144 slave.cpp:7249] Forwarding status update of operation with no ID (operation_uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) for framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000
W0109 16:36:08.697093 24142 master.cpp:7277] !!!! update slave message: slave_id {
  value: ""046b3f21-6e97-4a56-9a13-773f7d481efd-S0""
}
update_oversubscribed_resources: false
operations {
}
resource_version_uuid {
  value: ""\211\313\200+#\272O)\214\267f=\277!z&""
}
resource_providers {
  providers {
    info {
      id {
        value: ""605b22f5-e39d-4d9f-950a-e7f44d202c01""
      }
      type: ""org.apache.mesos.rp.local.storage""
      name: ""test""
      default_reservations {
        role: ""storage""
        type: DYNAMIC
      }
      storage {
        plugin {
          type: ""org.apache.mesos.csi.test""
          name: ""slrp_test""
          containers {
            [...]
          }
        }
      }
    }
    total_resources {
      name: ""disk""
      type: SCALAR
      scalar {
        value: 4096
      }
      disk {
        source {
          type: MOUNT
          mount {
            root: ""./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
          }
          id: ""18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
          metadata {
            labels {
              key: ""path""
              value: ""/tmp/n5thZ3/test/4GB-18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
            }
          }
          profile: ""volume-default""
        }
      }
      provider_id {
        value: ""605b22f5-e39d-4d9f-950a-e7f44d202c01""
      }
      reservations {
        role: ""storage""
        type: DYNAMIC
      }
    }
    operations {
      operations {
        framework_id {
          value: ""046b3f21-6e97-4a56-9a13-773f7d481efd-0000""
        }
        slave_id {
          value: ""046b3f21-6e97-4a56-9a13-773f7d481efd-S0""
        }
        info {
          type: CREATE_VOLUME
          create_volume {
            source {
              name: ""disk""
              type: SCALAR
              scalar {
                value: 4096
              }
              disk {
                source {
                  type: RAW
                  profile: ""volume-default""
                }
              }
              allocation_info {
                role: ""storage""
              }
              provider_id {
                value: ""605b22f5-e39d-4d9f-950a-e7f44d202c01""
              }
              reservations {
                role: ""storage""
                type: DYNAMIC
              }
            }
            target_type: MOUNT
          }
        }
        latest_status {
          state: OPERATION_FINISHED
          converted_resources {
            name: ""disk""
            type: SCALAR
            scalar {
              value: 4096
            }
            disk {
              source {
                type: MOUNT
                mount {
                  root: ""./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                }
                id: ""18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                metadata {
                  labels {
                    key: ""path""
                    value: ""/tmp/n5thZ3/test/4GB-18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                  }
                }
                profile: ""volume-default""
              }
            }
            allocation_info {
              role: ""storage""
            }
            provider_id {
              value: ""605b22f5-e39d-4d9f-950a-e7f44d202c01""
            }
            reservations {
              role: ""storage""
              type: DYNAMIC
            }
          }
          uuid {
            value: ""\014y\315\362\270\235E;\273bWvn\226\215\320""
          }
        }
        statuses {
          state: OPERATION_FINISHED
          converted_resources {
            name: ""disk""
            type: SCALAR
            scalar {
              value: 4096
            }
            disk {
              source {
                type: MOUNT
                mount {
                  root: ""./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                }
                id: ""18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                metadata {
                  labels {
                    key: ""path""
                    value: ""/tmp/n5thZ3/test/4GB-18b4c4a5-d162-4dcf-bb21-a13c6ee0f408""
                  }
                }
                profile: ""volume-default""
              }
            }
            allocation_info {
              role: ""storage""
            }
            provider_id {
              value: ""605b22f5-e39d-4d9f-950a-e7f44d202c01""
            }
            reservations {
              role: ""storage""
              type: DYNAMIC
            }
          }
          uuid {
            value: ""\014y\315\362\270\235E;\273bWvn\226\215\320""
          }
        }
        uuid {
          value: ""\030\264\304\245\321bM\317\273!\241<n\340\364\010""
        }
      }
    }
    resource_version_uuid {
      value: ""M\250\313j\320\301IG\262\0164e\004\367\304\333""
    }
  }
}
I0109 16:36:08.700137 24142 master.cpp:10411] Updating the state of operation '' (uuid: 18b4c4a5-d162-4dcf-bb21-a13c6ee0f408) of framework 046b3f21-6e97-4a56-9a13-773f7d481efd-0000 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)
I0109 16:36:08.700417 24146 hierarchical.cpp:669] Agent 046b3f21-6e97-4a56-9a13-773f7d481efd-S0 (core-dev) updated with total resources disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096; cpus:2; mem:1024; disk:1024; ports:[31000-32000]
F0109 16:36:08.700610 24142 master.cpp:11687] CHECK_SOME(resources): disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096; cpus:2; mem:1024; disk:1024; ports:[31000-32000] does not contain disk(reservations: [(DYNAMIC,storage)])[RAW(,volume-default)]:4096
*** Check failure stack trace: ***
F0109 16:36:08.700896 24146 hierarchical.cpp:908] CHECK_SOME(updatedTotal): disk(reservations: [(DYNAMIC,storage)])[MOUNT(18b4c4a5-d162-4dcf-bb21-a13c6ee0f408,volume-default):./csi/org.apache.mesos.csi.test/slrp_test/mounts/18b4c4a5-d162-4dcf-bb21-a13c6ee0f408]:4096; cpus:2; mem:1024; disk:1024; ports:[31000-32000] does not contain disk(reservations: [(DYNAMIC,storage)])[RAW(,volume-default)]:4096
*** Check failure stack trace: ***
    @     0x7ff06d3bbe7e  (unknown)
    @     0x7ff06d3bbe7e  (unknown)
    @     0x7ff06d3bbddd  (unknown)
    @     0x7ff06d3bbddd  (unknown)
    @     0x7ff06d3bb7ee  (unknown)
    @     0x7ff06d3bb7ee  (unknown)
    @     0x7ff06d3be522  (unknown)
    @     0x55c1c6c2be77  _ZTSN6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8internal8DispatchINS1_6FutureISt4listIN5mesos5slave13QoSCorrectionESaISF_EEEEEclINS0_IFSI_vEEEEESI_RKNS1_4UPIDEOT_EUlSt10unique_ptrINS1_7PromiseISH_EESt14default_deleteISU_EEOSM_S3_E_ISX_SM_St12_PlaceholderILi1EEEEEEE
    @     0x7ff06d3be522  (unknown)
    @     0x55c1c6c2be77  _ZTSN6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8internal8DispatchINS1_6FutureISt4listIN5mesos5slave13QoSCorrectionESaISF_EEEEEclINS0_IFSI_vEEEEESI_RKNS1_4UPIDEOT_EUlSt10unique_ptrINS1_7PromiseISH_EESt14default_deleteISU_EEOSM_S3_E_ISX_SM_St12_PlaceholderILi1EEEEEEE
    @     0x7ff06b729277  (unknown)
    @     0x55c1c6f3be8a  _ZTSN6lambda12CallableOnceIFvRK6ResultIN5mesos2v117resource_provider5EventEEEE10CallableFnINS_8internal7PartialIZNK7process6FutureIS6_E7onReadyISt5_BindIFSt7_Mem_fnIMSG_FbS8_EESG_St12_PlaceholderILi1EEEEbEERKSG_OT_NSG_6PreferEEUlOSQ_S8_E_ISQ_SO_EEEEE
{noformat}

We can see that once the SLRP reregisters with the agent, the following happens:

# The agent will send an {{UpdateSlave}} message to the master including the converted resources and the {{CREATE_VOLUME}} operation with the status {{OPERATION_FINISHED}}.
# The master will update the agent's resources, including the volume created by the operation.
# The agent will resend the operation status update.
# The master will try to apply the operation and crash, because it already updated the agent's resources on step #2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-01-10 14:47:34.763,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 13 00:58:59 UTC 2018,,,,,,,"0|i3uvq7:",9223372036854775807,,,,,greggomann,,,,,,Mesosphere Sprint 72,,,,,,,,,,,5.0,,1.5.0,,,,,,,,,"10/Jan/18 14:47;bbannier;The issue here seems to be that resources of the resource provider sent as part of the {{UpdateSlaveMessage}} after failover already had the operation applied; applying the same operation again when an offer operation status update is received cannot work.

We should update the master to transition newly terminal offer operations when it learns about that from {{UpdateSlaveMessage}}. Since the master will only update resource state in the offer operation status update handler when the operation is _newly_ terminal, this would prevent this setup from becoming a problem.","10/Jan/18 15:17;bbannier;Review: https://reviews.apache.org/r/65072/","12/Jan/18 22:10;bmahler;Should this be updated to a 'Blocker' or is it ok being re-targeted to 1.5.1?","13/Jan/18 00:58;greggomann;{code}
commit b4372acb1133973e605c9c618e144ba3f4f74c16 (HEAD -> master, merge)
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Fri Jan 12 15:40:46 2018 -0800

    Fixed handling of terminal operations in `updateSlave` handler.

    An offer operation can be become terminal between any previously
    received non-terminal offer operation status update and receiving an
    `UpdateSlaveMessage` (e.g., if the agent failed over, or when the
    agent was partitioned from the master).

    The master will in its offer operations status handler attempt
    to apply operations which became terminal since the last update. At
    the same time, the total resources in an `UpdateSlaveMessage` would
    already contain the result of applying the operation, and we need to
    prevent the master from attempting to apply the same operation twice.

    This patch updates the master handler for `UpdateSlaveMessage` to
    transition pending operations which are reported as terminal without
    also updating the resources on the agent as any update would already
    be reflected in the new total from the `UpdateSlaveMessage.

    Review: https://reviews.apache.org/r/65072/
{code}
{code}
commit 24c71047c6a866bdbc83af90650828f239ee07b0
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Fri Jan 12 15:40:42 2018 -0800

    Fixed master's `updateOperation` for operations without framework ID.

    This patch fixes logging of master's `updateOperation` for operations
    without framework ID. We also add a `CHECK` before the part updating
    resources or the allocator for non-speculated operations; currently
    non-speculated operations can only be initiated from a framework, but
    not from e.g., the operation API, and additional work is needed to
    support this.

    Review: https://reviews.apache.org/r/65096/
{code}
{code}
commit 69930fe93bcb513c16f4f575b504978d6e0d96dc
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Fri Jan 12 15:40:37 2018 -0800

    Made it possible to update an operation without mutating resources.

    In certain situations it can make sense to update the state of an
    operation without also wanting to update resources. In this patch we
    modify the master's `updateOperation` function to take an additional
    parameter signifying whether resources should be updated, or whether
    we only care about updating the operation and tracking of used
    resources.

    We will use this functionality in a subsequent patch to perform more
    contained updates to agent state when processing `UpdateSlaveMessages`
    which contain both resources and operations (and where any terminal
    operations were already applied to the agent's resources).

    Review: https://reviews.apache.org/r/65095/
{code}
{code}
commit dd866f9fd73858ef4d6369edddb4aa9a2f1c1a8f
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Fri Jan 12 15:40:32 2018 -0800

    Simplified master's `updateOperation` function.

    This replaces repetitive code by instead using a helper variable which
    allows us to avoid branching. We can then also replace a `Try<bool>`
    with a plain `bool` further simplifying the code.

    Review: https://reviews.apache.org/r/65093/
{code}",,,,,,,,,,,,,,,,,,,,,,,,
RP manager incorrectly setting framework ID leads to CHECK failure,MESOS-8419,13129474,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,greggomann,greggomann,greggomann,09/Jan/18 09:32,14/Jun/18 20:07,29/Oct/20 16:32,11/Jan/18 22:07,,,,,,,,,1.5.0,,,,,,agent,,,,,0,mesosphere,,,,,,,,"The resource provider manager [unconditionally sets the framework ID|https://github.com/apache/mesos/blob/3290b401d20f2db2933294470ea8a2356a47c305/src/resource_provider/manager.cpp#L637] when forwarding operation status updates to the agent. This is incorrect, for example, when the resource provider [generates OPERATION_DROPPED updates during reconciliation|https://github.com/apache/mesos/blob/3290b401d20f2db2933294470ea8a2356a47c305/src/resource_provider/storage/provider.cpp#L1653-L1657], and leads to protobuf errors in this case since the framework ID's required {{value}} field is left unset.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 11 22:07:30 UTC 2018,,,,,,,"0|i3uvpz:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 72,,,,,,,,,,,1.0,,1.5.0,,,,,,,,,"09/Jan/18 09:34;greggomann;Review here: https://reviews.apache.org/r/65034/","11/Jan/18 22:07;greggomann;{code}
commit 71160110bb9458a6e584350ccae7709ae05a5b0c
Author: Greg Mann greg@mesosphere.io
Date:   Wed Jan 10 10:43:02 2018 -0800


Made resource provider manager conditionally set framework ID.

When forwarding operation status updates from resource providers
to the agent, the manager was setting the framework ID
unconditionally. This is a problem when OPERATION_DROPPED updates
with no framework ID are generated by the resource provider.

Review: https://reviews.apache.org/r/65034/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
CHECK failure if trying to recover nested containers but the framework checkpointing is not enabled.,MESOS-8416,13129394,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,gilbert,gilbert,gilbert,08/Jan/18 23:35,14/Jun/18 19:58,29/Oct/20 16:32,17/Apr/18 18:59,,,,,,,,,1.5.1,1.6.0,,,,,containerization,,,,,0,containerizer,mesosphere,,,,,,,"{noformat}
I0108 23:05:25.313344 31743 slave.cpp:620] Agent attributes: [  ]
I0108 23:05:25.313832 31743 slave.cpp:629] Agent hostname: vagrant-ubuntu-wily-64
I0108 23:05:25.314916 31763 task_status_update_manager.cpp:181] Pausing sending task status updates
I0108 23:05:25.323496 31766 state.cpp:66] Recovering state from '/var/lib/mesos/slave/meta'
I0108 23:05:25.323639 31766 state.cpp:724] No committed checkpointed resources found at '/var/lib/mesos/slave/meta/resources/resources.info'
I0108 23:05:25.326169 31760 task_status_update_manager.cpp:207] Recovering task status update manager
I0108 23:05:25.326954 31759 containerizer.cpp:674] Recovering containerizer
F0108 23:05:25.331529 31759 containerizer.cpp:919] CHECK_SOME(container->directory): is NONE 
*** Check failure stack trace: ***
    @     0x7f769dbc98bd  google::LogMessage::Fail()
    @     0x7f769dbc8c8e  google::LogMessage::SendToLog()
    @     0x7f769dbc958d  google::LogMessage::Flush()
    @     0x7f769dbcca08  google::LogMessageFatal::~LogMessageFatal()
    @     0x556cb4c2b937  _CheckFatal::~_CheckFatal()
    @     0x7f769c5ac653  mesos::internal::slave::MesosContainerizerProcess::recover()
{noformat}

If the framework does not enable the checkpointing. It means there is no slave state checkpointed. But containers are still checkpointed at the runtime dir, which mean recovering a nested container would cause the CHECK failure due to its parent's sandbox dir is unknown.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-04-17 14:11:41.107,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 14 19:58:08 UTC 2018,,,,,,,"0|i3onuf:",9223372036854775807,,,,,qianzhang,,,,,,Mesosphere Sprint 78,,,,,,,,,,,5.0,,1.5.1,1.6.0,,,,,,,,"04/Apr/18 23:03;gilbert;This could be a blocking issue for agent recovering. Users are likely hit this issue if they use nested containers, in two cases:
# agent metadata got delated by operators for some reasons.
# custom framework did not set checkpoint as true but launched some taskgroup and completed.","17/Apr/18 14:11;alexr;[~gilbert] promoted it to the blocker for 1.6.0 per your comment above. Can you please help me estimate the workload and find someone to help fix it before we cut 1.6 branch?","17/Apr/18 18:59;gilbert;commit bd447bb5c0295b7cb9f5773b02d1e4cb52bd154e
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Sun Apr 8 20:01:20 2018 -0700

    Added default executor test for agent recovery without metadata.
    
    Review: https://reviews.apache.org/r/66541

commit eeaf5052f02701833aa7662960d6a24dff7f48ab
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Apr 10 12:22:31 2018 -0700

    Added unit test for recovering nested container without slave state.
    
    Review: https://reviews.apache.org/r/66540

commit f80b0d0b863acbb0681e2f8fc063c226686b45a0
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Apr 10 11:35:03 2018 -0700

    Fixed the agent recovery crash if metadata is missing.
    
    This is the case that is missed when handling orphan containers
    cleanup. When the agent metadata does not exist but the container
    pid is chechpointed under the container runtime dir, then the
    container should be regarded as orphan and should be cleaned up.
    
    Review: https://reviews.apache.org/r/66539","14/Jun/18 19:58;gilbert;A workaround for this issue for containers with persistent volumes:
1. find out the framework ids that have checkpointing as false from the /frameworks endpoint
2. use these framework-ids and traverse all the corresponding container sandboxes, should be `/var/lib/mesos slave/slaves/<slave-id>/frameworks/<the-framework-ids-from-step-1>/executors/<executor-id/runs/<container-id>`, collect all these container ids
3. based on the container ids from step 2, kill these container processes
4. watch the mount points from the host, cat /proc/self/mountinfo, and find out the mount points that under Step 2's containers’ sandboxes (for monitoring
5. do `umount -R /var/lib/mesos/slave/slaves/<slave-id>/frameworks/<each-framework-id-from-Step-1>`, we have to do the umount due to https://issues.apache.org/jira/browse/MESOS-8830
6. watch the mount points again like step 4, and verify container ids from step 2 do not show up on the mount table
7. remove each container runtime dir at /var/run/mesos/containers/<container-ids-from-Step-2>, the agent should be able to recovered then and the old PV should be included in a new offer and being sent to the framework",,,,,,,,,,,,,,,,,,,,,,,,
Use unique ID for CSI plugin containers in SLRP.,MESOS-8399,13128674,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Won't Do,chhsia0,chhsia0,chhsia0,05/Jan/18 02:02,29/Oct/18 21:51,29/Oct/20 16:32,29/Oct/18 21:51,,,,,,,,,,,,,,,,,,,,0,mesosphere,storage,,,,,,,"If an agent crashed abnormally and the runtime directory is lost, then a standalone container previously launched by SLRP will be considered orphan when the agent restarts. Since the orphan containers are cleaned up asynchronously, it is possible that the cleanup is racing with SLRP launching a new standalone container instance with the same ID. To avoid this race, we should use unique IDs for CSI plugin containers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-02-01 06:28:03.565,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 29 21:51:12 UTC 2018,,,,,,,"0|i3ojg7:",9223372036854775807,,,,,jieyu,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"01/Feb/18 06:28;greggomann;Short-term fix for tests:
{code}
commit 53de510ae846ce2eab707493d6ab833429b6cec7
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date:   Wed Jan 31 18:34:16 2018 -0800

    Used unique CSI plugin names in storage local resource provider tests.

    To provide better test isolation, the SLRP tests should generate
    random unique names in CSI plugin configs, so that the plugin
    containers use different container IDs.

    Review: https://reviews.apache.org/r/65427/
{code}","29/Oct/18 21:51;chhsia0;Decided to keep it as as.",,,,,,,,,,,,,,,,,,,,,,,,,,
SLRP NewVolumeRecovery and LaunchTaskRecovery tests CHECK failures.,MESOS-8393,13128533,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,chhsia0,chhsia0,chhsia0,04/Jan/18 16:33,14/Jun/18 20:07,29/Oct/20 16:32,11/Jan/18 22:10,,,,,,,,,1.5.0,,,,,,,,,,,0,mesosphere,storage,,,,,,,CHECK failures manifested on the two SLRP tests after resource upgrade/downgrade is introduced.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-8375,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 04 20:32:09 UTC 2018,,,,,,,"0|i3uvpj:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 72,,,,,,,,,,,2.0,,1.5.0,,,,,,,,,"04/Jan/18 20:32;chhsia0;Reason: https://github.com/apache/mesos/blob/master/src/resource_provider/storage/provider.cpp#L922
Not only {{operation.info}} but the whole {{operation}} needs to be upgraded, or it will contain statuses having pre-reservation-refinement resources in {{converted_resources}}. This would be fixed once MESOS-8375 is done. cc [~mcypark].",,,,,,,,,,,,,,,,,,,,,,,,,,,
Resource provider-capable agents not correctly synchronizing checkpointed agent resources on reregistration,MESOS-8350,13126299,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,bbannier,bbannier,bbannier,20/Dec/17 14:19,14/Jun/18 20:06,29/Oct/20 16:32,04/Jan/18 12:25,,,,,,,,,1.5.0,1.6.0,,,,,master,,,,,0,,,,,,,,,"For resource provider-capable agents the master does not re-send checkpointed resources on agent reregistration; instead the checkpointed resources sent as part of the {{ReregisterSlaveMessage}} should be used.

This is not what happens in reality. If e.g., checkpointing of an offer operation fails and the agent fails over the checkpointed resources would, as expected, not be reflected in the agent, but would still be assumed in the master.

A workaround is to fail over the master which would lead to the newly elected master bootstrapping agent state from {{ReregisterSlaveMessage}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-12-22 22:41:26.203,,,false,MESOS-8374,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 04 12:28:03 UTC 2018,,,,,,,"0|i3uvnz:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 71,,,,,,,,,,,2.0,,1.5.1,,,,,,,,,"22/Dec/17 22:41;jieyu;Re-target this for 1.5.1 given the likelihood for this to happen is pretty rare and we do have a workaround for this.","02/Jan/18 15:39;bbannier;Reviews:

https://reviews.apache.org/r/64888/
https://reviews.apache.org/r/64889/","04/Jan/18 12:25;bbannier;{noformat}
commit a1a7c6fb07898d22642ed76ce4068681ec05943e
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Thu Jan 4 11:12:27 2018 +0100

    Fixed handling of checkpointed resources for RP-capable agents.
    
    The master will not resend checkpointed resources when a resource
    provider-capable agent reregisters. Instead the checkpointed resources
    sent as part of the agent reregistration should be evaluated by the
    master and be used to update its state.
    
    This patch fixes the handling of checkpointed resources sent as part
    of the agent reregistration so that the resources are used to update
    the master state.
    
    Review: https://reviews.apache.org/r/64889/

commit 838850f92e30115947343db8f34966c0d5f4d43a
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Thu Jan 4 11:12:17 2018 +0100

    Added test of handling of checkpointed resources in reregistration.
    
    This patch adds a test that confirms that the master resends
    checkpointed resources to the agent on reregistration.
    
    Review: https://reviews.apache.org/r/64888/

commit a09530eab31682f09a4db349038b9d5607fdfe1a
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Thu Jan 4 11:12:01 2018 +0100

    Future-proofed use of agent capabilities in tests.
    
    Even though currently the resource provider capability is the only
    capability which can be toggled by users, when examining agent flags
    we expect either no capabilities at all or a full set of capabilities
    (including both toggleable and required capabilities).
    
    This patch cleans up test code agent capabilities by delegating the
    bulk of the work to a helper function providing a set of
    default-enabled capabilities and only adds a single capability to that
    set. This not only makes it clearer which exact capability a test
    cares about, but also future-proofs the code for the case where we
    extend the set of required capabilities in the future.
    
    Review: https://reviews.apache.org/r/64891/
{noformat}","04/Jan/18 12:28;bbannier;{noformat}
commit c29b7cd7b5627964ca75001dd3195656816f870c
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Thu Jan 4 11:12:27 2018 +0100

    Fixed handling of checkpointed resources for RP-capable agents.
    
    The master will not resend checkpointed resources when a resource
    provider-capable agent reregisters. Instead the checkpointed resources
    sent as part of the agent reregistration should be evaluated by the
    master and be used to update its state.
    
    This patch fixes the handling of checkpointed resources sent as part
    of the agent reregistration so that the resources are used to update
    the master state.
    
    Review: https://reviews.apache.org/r/64889/

commit 6f134c93e52120ad6f29ef9057e2045ad8f24c7c
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Thu Jan 4 11:12:17 2018 +0100

    Added test of handling of checkpointed resources in reregistration.
    
    This patch adds a test that confirms that the master resends
    checkpointed resources to the agent on reregistration.
    
    Review: https://reviews.apache.org/r/64888/

commit 4e5b8cfe86061451747899ef516aa0c4bea12bca
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Thu Jan 4 11:12:01 2018 +0100

    Future-proofed use of agent capabilities in tests.
    
    Even though currently the resource provider capability is the only
    capability which can be toggled by users, when examining agent flags
    we expect either no capabilities at all or a full set of capabilities
    (including both toggleable and required capabilities).
    
    This patch cleans up test code agent capabilities by delegating the
    bulk of the work to a helper function providing a set of
    default-enabled capabilities and only adds a single capability to that
    set. This not only makes it clearer which exact capability a test
    cares about, but also future-proofs the code for the case where we
    extend the set of required capabilities in the future.
    
    Review: https://reviews.apache.org/r/64891/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,
"When a resource provider driver is disconnected, it fails to reconnect.",MESOS-8349,13126291,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,nfnt,nfnt,20/Dec/17 13:36,27/Dec/17 21:44,29/Oct/20 16:32,27/Dec/17 21:44,1.5.0,,,,,,,,1.5.0,,,,,,,,,,,0,mesosphere,,,,,,,,"If the resource provider manager closes the HTTP connection of a resource provider, the resource provider should reconnect itself. For that, the resource provider driver will change its state to ""DISCONNECTED"", call a {{disconnected}} callback and use its endpoint detector to reconnect.
This doesn't work in a testing environment where a {{ConstantEndpointDetector}} is used. While the resource provider is notified of the closed HTTP connection (and logs {{End-Of-File received}}), it never disconnects itself and calls the {{disconnected}} callback. Discarding {{HttpConnectionProcess::detection}} in {{HttpConnectionProcess::disconnected}} doesn't trigger the {{onAny}} callback of that future. This might not be a problem in {{HttpConnectionProcess}} but could be related to the test case using a {{ConstantEndpointDetector}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-12-22 13:36:25.394,,,false,MESOS-7235,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 27 21:44:13 UTC 2017,,,,,,,"0|i3o4rz:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 71,,,,,,,,,,,2.0,,1.5.0,,,,,,,,,"20/Dec/17 14:12;nfnt;Discarding a {{Future}} (instead of discarding its {{Promise}}) won't call {{onAny}} callbacks, only a {{onDiscarded}} callback that we haven't set up here.","22/Dec/17 13:36;bbannier;Review: https://reviews.apache.org/r/64806/","27/Dec/17 21:44;jieyu;commit 41cdab963121cead0c0937ac821c8d7cdbeddacf (HEAD -> master, origin/master, origin/HEAD, connection_discard)
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Fri Dec 22 13:24:53 2017 -0800

    Fixed resource provider driver disconnection handling.

    The expectation for disconnection of the resource provider driver is
    that disconnection handlers of the resource provider would be invoked
    and a new connection would be detected.

    This patch fixed the issue by transition the future returned by
    ConstantEndpointDetector into DISCARDED if a ""discard"" is initiated by
    the caller. This will properly trigger `detected` callback to be called.

    This patch is based on: https://reviews.apache.org/r/64806/

    Review: https://reviews.apache.org/r/64856",,,,,,,,,,,,,,,,,,,,,,,,,
Resubscription of a resource provider will crash the agent if its HTTP connection isn't closed,MESOS-8346,13125911,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,nfnt,nfnt,nfnt,19/Dec/17 12:46,21/Dec/17 17:24,29/Oct/20 16:32,21/Dec/17 17:24,1.5.0,,,,,,,,1.5.0,,,,,,,,,,,0,mesosphere,,,,,,,,"A resource provider might resubscribe while its old HTTP connection wasn't properly closed. In that case an agent will crashm with, e.g., the following log:
{noformat}
I1219 13:33:51.937295 128610304 manager.cpp:570] Subscribing resource provider {""id"":{""value"":""8e71beef-796e-4bde-9257-952ed0f230a5""},""name"":""test"",""type"":""org.apache.mesos.rp.test""}
I1219 13:33:51.937443 128610304 manager.cpp:134] Terminating resource provider 8e71beef-796e-4bde-9257-952ed0f230a5
I1219 13:33:51.937760 128610304 manager.cpp:134] Terminating resource provider 8e71beef-796e-4bde-9257-952ed0f230a5
E1219 13:33:51.937851 129683456 http_connection.hpp:445] End-Of-File received
I1219 13:33:51.937865 131293184 slave.cpp:7105] Handling resource provider message 'DISCONNECT: resource provider 8e71beef-796e-4bde-9257-952ed0f230a5'
I1219 13:33:51.937968 131293184 slave.cpp:7347] Forwarding new total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]
F1219 13:33:51.938052 132366336 manager.cpp:606] Check failed: resourceProviders.subscribed.contains(resourceProviderId) 
*** Check failure stack trace: ***
E1219 13:33:51.938583 130756608 http_connection.hpp:445] End-Of-File received
I1219 13:33:51.938987 129683456 hierarchical.cpp:669] Agent 0019c3fa-28c5-43a9-88d0-709eee271c62-S0 (172.18.8.13) updated with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]
    @        0x1125380ef  google::LogMessageFatal::~LogMessageFatal()
    @        0x112534ae9  google::LogMessageFatal::~LogMessageFatal()
I1219 13:33:51.939131 129683456 hierarchical.cpp:1517] Performed allocation for 1 agents in 61830ns
I1219 13:33:51.945793 2646795072 slave.cpp:927] Agent terminating
I1219 13:33:51.945955 129146880 master.cpp:1305] Agent 0019c3fa-28c5-43a9-88d0-709eee271c62-S0 at slave(1)@172.18.8.13:64430 (172.18.8.13) disconnected
I1219 13:33:51.945979 129146880 master.cpp:3364] Disconnecting agent 0019c3fa-28c5-43a9-88d0-709eee271c62-S0 at slave(1)@172.18.8.13:64430 (172.18.8.13)
I1219 13:33:51.946022 129146880 master.cpp:3383] Deactivating agent 0019c3fa-28c5-43a9-88d0-709eee271c62-S0 at slave(1)@172.18.8.13:64430 (172.18.8.13)
I1219 13:33:51.946081 131293184 hierarchical.cpp:766] Agent 0019c3fa-28c5-43a9-88d0-709eee271c62-S0 deactivated
    @        0x115f2761d  mesos::internal::ResourceProviderManagerProcess::subscribe()::$_2::operator()()
    @        0x115f2977d  _ZN5cpp176invokeIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS2_14HttpConnectionERKNS1_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEEEDTclclsr3stdE7forwardIT_Efp_Espclsr3stdE7forwardIT0_Efp0_EEEOSG_DpOSH_
    @        0x115f29740  _ZN6lambda8internal7PartialIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS3_14HttpConnectionERKNS2_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEE13invoke_expandISC_NSt3__15tupleIJSG_EEENSK_IJEEEJLm0EEEEDTclsr5cpp17E6invokeclsr3stdE7forwardIT_Efp_Espcl6expandclsr3stdE3getIXT2_EEclsr3stdE7forwardIT0_Efp0_EEclsr3stdE7forwardIT1_Efp2_EEEEOSN_OSO_N5cpp1416integer_sequenceImJXspT2_EEEEOSP_
    @        0x115f296bb  _ZNO6lambda8internal7PartialIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS3_14HttpConnectionERKNS2_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEEclIJEEEDTcl13invoke_expandclL_ZNSt3__14moveIRSC_EEONSJ_16remove_referenceIT_E4typeEOSN_EdtdefpT1fEclL_ZNSK_IRNSJ_5tupleIJSG_EEEEESQ_SR_EdtdefpT10bound_argsEcvN5cpp1416integer_sequenceImJLm0EEEE_Eclsr3stdE16forward_as_tuplespclsr3stdE7forwardIT_Efp_EEEEDpOSY_
    @        0x115f2965d  _ZN5cpp176invokeIN6lambda8internal7PartialIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS5_14HttpConnectionERKNS4_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEEEJEEEDTclclsr3stdE7forwardIT_Efp_Espclsr3stdE7forwardIT0_Efp0_EEEOSK_DpOSL_
    @        0x115f29631  _ZN6lambda8internal6InvokeIvEclINS0_7PartialIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS6_14HttpConnectionERKNS5_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEEEJEEEvOT_DpOT0_
    @        0x115f29526  _ZNO6lambda12CallableOnceIFvvEE10CallableFnINS_8internal7PartialIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS7_14HttpConnectionERKNS6_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEEEEclEv
    @        0x10b6ca690  _ZNO6lambda12CallableOnceIFvvEEclEv
    @        0x10be09295  _ZZN7process8internal8DispatchIvEclIN6lambda12CallableOnceIFvvEEEEEvRKNS_4UPIDEOT_ENKUlOS7_PNS_11ProcessBaseEE_clESD_SF_
    @        0x10be09180  _ZN5cpp176invokeIZN7process8internal8DispatchIvEclIN6lambda12CallableOnceIFvvEEEEEvRKNS1_4UPIDEOT_EUlOS9_PNS1_11ProcessBaseEE_JS9_SH_EEEDTclclsr3stdE7forwardISD_Efp_Espclsr3stdE7forwardIT0_Efp0_EEESE_DpOSJ_
    @        0x10be0912b  _ZN6lambda8internal7PartialIZN7process8internal8DispatchIvEclINS_12CallableOnceIFvvEEEEEvRKNS2_4UPIDEOT_EUlOS9_PNS2_11ProcessBaseEE_JS9_NSt3__112placeholders4__phILi1EEEEE13invoke_expandISI_NSJ_5tupleIJS9_SM_EEENSP_IJOSH_EEEJLm0ELm1EEEEDTclsr5cpp17E6invokeclsr3stdE7forwardISD_Efp_Espcl6expandclsr3stdE3getIXT2_EEclsr3stdE7forwardIT0_Efp0_EEclsr3stdE7forwardIT1_Efp2_EEEESE_OST_N5cpp1416integer_sequenceImJXspT2_EEEEOSU_
    @        0x10be0905f  _ZNO6lambda8internal7PartialIZN7process8internal8DispatchIvEclINS_12CallableOnceIFvvEEEEEvRKNS2_4UPIDEOT_EUlOS9_PNS2_11ProcessBaseEE_JS9_NSt3__112placeholders4__phILi1EEEEEclIJSH_EEEDTcl13invoke_expandclL_ZNSJ_4moveIRSI_EEONSJ_16remove_referenceISD_E4typeESE_EdtdefpT1fEclL_ZNSP_IRNSJ_5tupleIJS9_SM_EEEEESU_SE_EdtdefpT10bound_argsEcvN5cpp1416integer_sequenceImJLm0ELm1EEEE_Eclsr3stdE16forward_as_tuplespclsr3stdE7forwardIT_Efp_EEEEDpOS11_
    @        0x10be08f4d  _ZN5cpp176invokeIN6lambda8internal7PartialIZN7process8internal8DispatchIvEclINS1_12CallableOnceIFvvEEEEEvRKNS4_4UPIDEOT_EUlOSB_PNS4_11ProcessBaseEE_JSB_NSt3__112placeholders4__phILi1EEEEEEJSJ_EEEDTclclsr3stdE7forwardISF_Efp_Espclsr3stdE7forwardIT0_Efp0_EEESG_DpOSQ_
    @        0x10be08f11  _ZN6lambda8internal6InvokeIvEclINS0_7PartialIZN7process8internal8DispatchIvEclINS_12CallableOnceIFvvEEEEEvRKNS5_4UPIDEOT_EUlOSC_PNS5_11ProcessBaseEE_JSC_NSt3__112placeholders4__phILi1EEEEEEJSK_EEEvSH_DpOT0_
    @        0x10be08d36  _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8internal8DispatchIvEclINS0_IFvvEEEEEvRKNS1_4UPIDEOT_EUlOSE_S3_E_JSE_NSt3__112placeholders4__phILi1EEEEEEEclEOS3_
    @        0x11fd64bc9  _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEEclES3_
    @        0x11fd64a69  process::ProcessBase::consume()
    @        0x11fe20ac4  _ZNO7process13DispatchEvent7consumeEPNS_13EventConsumerE
    @        0x113c77819  process::ProcessBase::serve()
    @        0x11fd5b8c9  process::ProcessManager::resume()
    @        0x11fe8260b  process::ProcessManager::init_threads()::$_1::operator()()
    @        0x11fe82190  _ZNSt3__114__thread_proxyINS_5tupleIJNS_10unique_ptrINS_15__thread_structENS_14default_deleteIS3_EEEEZN7process14ProcessManager12init_threadsEvE3$_1EEEEEPvSB_
    @     0x7fff64da56c1  _pthread_body
    @     0x7fff64da556d  _pthread_start
    @     0x7fff64da4c5d  thread_start
Abort trap: 6
{noformat}

This is due to a race condition in {{resource_provider/manager.cpp}} when handling closed HTTP connections of resource providers. If a resource provider resubscribes and its old HTTP connection is still open, the resource provider manager will close it. This is unexpected and will trigger closing the new HTTP connection which results in a failed {{CHECK}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-12-20 08:54:06.771,,,false,MESOS-7235,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 21 17:24:43 UTC 2017,,,,,,,"0|i3o2g7:",9223372036854775807,,,,,bbannier,,,,,,Mesosphere Sprint 70,,,,,,,,,,,2.0,,1.5.0,,,,,,,,,"19/Dec/17 15:14;nfnt;https://reviews.apache.org/r/64713/","20/Dec/17 08:54;gilbert;[~nfnt], seems like this is targeted for 1.5.0. Do we have an estimate when it will land?","20/Dec/17 09:03;nfnt;It will land today, the patch seems to be good, just needs a small update.","21/Dec/17 17:24;bbannier;{noformat}
commit 7db362c9e3428d72bc20b549d1988d7eb40c8fca
Author: Jan Schlicht <jan@mesosphere.io>
Date:   Thu Dec 21 09:57:59 2017 +0100

    Fixed a crash when resubscribing resource providers.
    
    If a resource provider resubscribed while its old HTTP connection was
    still open, the agent would crash, as a continuation would be called
    erroneously. This continuation is now only called when a HTTP connection
    is closed by a remote side (i.e. the resource provider) and not when
    the resource provider manager closes the connection.
    
    Review: https://reviews.apache.org/r/64713/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,
Mesos containerizer does not properly handle old running containers,MESOS-8325,13124530,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,zhitao,greggomann,greggomann,12/Dec/17 23:50,13/Dec/17 07:11,29/Oct/20 16:32,13/Dec/17 07:11,,,,,,,,,1.5.0,,,,,,agent,containerization,,,,0,containerizer,mesosphere,recovery,upgrade,,,,,"We were testing an upgrade scenario recently and encountered the following assertion failure:
{code}
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.693977 20810 http.cpp:3116] Processing LAUNCH_NESTED_CONTAINER_SESSION call for container 'a89b211a-4549-462d-9cc7-0ea2bac2f729.1c262420-7525-4fee-99c1-aff4f66996bd.check-a41362ae-13c6-4750-990e-a1a0b2792b5f'
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.695179 20807 containerizer.cpp:1169] Trying to chown '/var/lib/mesos/slave/slaves/aaf0a62f-a6eb-4c1d-80db-5fdd26fe8008-S12/frameworks/dcf5f8b5-86a8-44df-ac03-b39404239ad8-0377/executors/kafka__68baefd4-aa8c-4b97-a23e-eb6a73fa91f6/runs/a89b211a-4549-462d-9cc7-0ea2bac2f729/containers/1c262420-7525-4fee-99c1-aff4f66996bd/containers/check-a41362ae-13c6-4750-990e-a1a0b2792b5f' to user 'nobody'
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: W1212 16:45:42.695309 20807 containerizer.cpp:1198] Cannot determine executor_info for root container 'a89b211a-4549-462d-9cc7-0ea2bac2f729' which has no config recovered.
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.695327 20807 containerizer.cpp:1203] Starting container a89b211a-4549-462d-9cc7-0ea2bac2f729.1c262420-7525-4fee-99c1-aff4f66996bd.check-a41362ae-13c6-4750-990e-a1a0b2792b5f
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.695829 20807 containerizer.cpp:2932] Transitioning the state of container a89b211a-4549-462d-9cc7-0ea2bac2f729.1c262420-7525-4fee-99c1-aff4f66996bd.check-a41362ae-13c6-4750-990e-a1a0b2792b5f from PROVISIONING to PREPARING
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.700569 20811 systemd.cpp:98] Assigned child process '20941' to 'mesos_executors.slice'
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.702945 20811 systemd.cpp:98] Assigned child process '20942' to 'mesos_executors.slice'
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.706069 20806 switchboard.cpp:575] Created I/O switchboard server (pid: 20943) listening on socket file '/tmp/mesos-io-switchboard-74af71bb-2385-4dde-9762-94d0196124d3' for container a89b211a-4549-462d-9cc7-0ea2bac2f729.1c262420-7525-4fee-99c1-aff4f66996bd.check-a41362ae-13c6-4750-990e-a1a0b2792b5f
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: mesos-agent: /pkg/src/mesos/3rdparty/stout/include/stout/option.hpp:115: T& Option<T>::get() & [with T = mesos::slave::ContainerConfig]: Assertion `isSome()' failed.
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: *** Aborted at 1513097142 (unix time) try ""date -d @1513097142"" if you are using GNU date ***
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: PC: @     0x7f472f2851f7 __GI_raise
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: *** SIGABRT (@0x5134) received by PID 20788 (TID 0x7f472a2bf700) from PID 20788; stack trace: ***
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f6225e0 (unknown)
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f2851f7 __GI_raise
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f2868e8 __GI_abort
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f27e266 __assert_fail_base
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f27e312 __GI___assert_fail
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f4731c481e3 _ZNR6OptionIN5mesos5slave15ContainerConfigEE3getEv.part.170
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f4731c61c2d mesos::internal::slave::MesosContainerizerProcess::_launch()
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f4731c7f403 _ZN5cpp176invokeIZN7process8dispatchIN5mesos8internal5slave13Containerizer12LaunchResultENS5_25MesosContainerizerProcessERKNS3_11ContainerIDERK6OptionINS3_5slave11ContainerIOEERKSt3mapISsSsSt4lessISsESaISt4pairIKSsSsEEERKSC_ISsESB_SH_SR_SU_EENS1_6FutureIT_EERKNS1_3PIDIT0_EEMSZ_FSX_T1_T2_T3_T4_EOT5_OT6_OT7_OT8_EUlSt10unique_ptrINS1_7PromiseIS7_EESt14default_deleteIS1J_EEOS9_OSF_OSP_OSS_PNS1_11ProcessBaseEE_IS1M_S9_SF_SP_SS_S1S_EEEDTclcl7forwardISW_Efp_Espcl7forwardIT0_Efp0_EEEOSW_DpOS1U_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f4731c7f4f1 _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8dispatchIN5mesos8internal5slave13Containerizer12LaunchResultENSC_25MesosContainerizerProcessERKNSA_11ContainerIDERK6OptionINSA_5slave11ContainerIOEERKSt3mapISsSsSt4lessISsESaISt4pairIKSsSsEEERKSJ_ISsESI_SO_SY_S11_EENS1_6FutureIT_EERKNS1_3PIDIT0_EEMS16_FS14_T1_T2_T3_T4_EOT5_OT6_OT7_OT8_EUlSt10unique_ptrINS1_7PromiseISE_EESt14default_deleteIS1Q_EEOSG_OSM_OSW_OSZ_S3_E_IS1T_SG_SM_SW_SZ_St12_PlaceholderILi1EEEEEEclEOS3_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f47325dbb31 process::ProcessBase::consume()
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f47325ea882 process::ProcessManager::resume()
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f47325efcf6 _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472fafa230 (unknown)
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f61ae25 start_thread
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f34834d __clone
Dec 12 16:45:42 agent.hostname systemd[1]: dcos-mesos-slave.service: main process exited, code=killed, status=6/ABRT
Dec 12 16:45:42 agent.hostname systemd[1]: Unit dcos-mesos-slave.service entered failed state.
Dec 12 16:45:42 agent.hostname systemd[1]: dcos-mesos-slave.service failed.
{code}

Looking into {{Slave::_launch}}, indeed we find an unguarded access to the parent container's {{ContainerConfig}} [here|https://github.com/apache/mesos/blob/c320ab3b2dc4a16de7e060b9e15e9865a73389b0/src/slave/containerizer/mesos/containerizer.cpp#L1716].

We recently [added checkpointing|https://github.com/apache/mesos/commit/03a2a4dfa47b1d47c5eb23e81f5ef8213e46d545] of {{ContainerConfig}} to the Mesos containerizer. It seems that we are not appropriately handling upgrades, when there may be old containers running for which we do not expect to recover a {{ContainerConfig}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-12-13 07:11:02.382,,,false,MESOS-4945,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 13 07:11:35 UTC 2017,,,,,,,"0|i3ntzj:",9223372036854775807,,,,,gilbert,,,,,,Mesosphere Sprint 70,,,,,,,,,,,2.0,,1.5.0,,,,,,,,,"13/Dec/17 07:11;gilbert;https://reviews.apache.org/r/64558/","13/Dec/17 07:11;gilbert;commit 4695ac0c23b732e18e794f7fe28834123e6586df
Author: Zhitao Li <zhitaoli.cs@gmail.com>
Date:   Wed Dec 13 14:13:00 2017 +0800

    Added a missing check for parent container has config recovered.
    
    This is necessary because before MESOS-6894, containers do not
    have ContainerConfig checkpointed. For the upgrade scenario,
    if any nested container is launched under an existing legacy
    container, the agent would fail due to an unguarded access to
    the parent legacy container's ContainerConfig. We need to add
    this check. Please see MESOS-8325 for details.
    
    Review: https://reviews.apache.org/r/64558/",,,,,,,,,,,,,,,,,,,,,,,,,,
Pass resource provider information to master as part of UpdateSlaveMessage,MESOS-8312,13123537,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,bbannier,bbannier,07/Dec/17 20:11,08/Dec/17 16:04,29/Oct/20 16:32,08/Dec/17 16:04,,,,,,,,,1.5.0,,,,,,,,,,,0,,,,,,,,,"We extended {{UpdateSlaveMessage}} so updates to an agent's total resources from resource providers are possible. We realized that will need to explicitly pass resource provider details (here for now: {{ResourceProviderInfo}}) to the master so it can be queried for the providers present on certain agents. This should happen as part of {{UpdateSlaveMessage}} so a single synchronization channel is used for this kind of information.

We need to adjust {{UpdateSlaveMessage}} for these requirements. This should happen before 1.5.0 gets released so we do not need to deprecate a never really used message format.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-7235,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 08 16:04:49 UTC 2017,,,,,,,"0|i3nnvb:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 69,,,,,,,,,,,5.0,,1.5.0,,,,,,,,,"08/Dec/17 00:18;bbannier;Reviews:

https://reviews.apache.org/r/64430
https://reviews.apache.org/r/64422
https://reviews.apache.org/r/64423
https://reviews.apache.org/r/64424","08/Dec/17 16:04;bbannier;{noformat}
commit a345c2a656598656d6172231e96cf3d5e7be0800
Author: Benjamin Bannier <bbannier@apache.org>
Date:   Thu Dec 7 23:03:01 2017 +0100

    Only passed agent's resource version in top-level 'UpdateSlaveMessage'.
    
    Since we have moved all resource provider-related information like
    e.g., resource versions to an explicit field in 'UpdateSlaveMessage',
    we now only pass agent information in top-level fields.
    
    To model that we changed the top-level resource versions there from a
    repeated field to a single optional field. This will allow us to in
    the future pass agent information in the resource provider data
    structure as well.
    
    Review: https://reviews.apache.org/r/64430

commit bd55be1066b8f2553e10e9d5bd35a3582f659b00
Author: Benjamin Bannier <bbannier@apache.org>
Date:   Wed Dec 6 14:27:34 2017 +0100

    Removed 'total' from 'UpdateSlaveMessage'.
    
    This field was added during the development leading up to version 1.5
    in order to allow updates to agent-total resources in the context of
    resource providers. It was never used in a released Mesos version.
    
    Review: https://reviews.apache.org/r/64424

commit 7cfa0e9206647ddd6217cd69090f5eb328a73529
Author: Benjamin Bannier <bbannier@apache.org>
Date:   Thu Dec 7 11:05:05 2017 +0100

    Explicitly passed resource-provider information in 'UpdateSlaveMessage'.
    
    This patch changes the way resource-provider related information is
    passed. Instead of aggregating all information from both the agent and
    resource providers into global per-agent lists in
    'UpdateSlaveMessage', with this patch we pass resource-provider
    related information explicitly. We can in a subsequent patch surface
    this information in e.g., the operator API.
    
    Review: https://reviews.apache.org/r/64423

commit b5d64414e0def423aa1beb5552391ea7b975fcc6
Author: Benjamin Bannier <bbannier@apache.org>
Date:   Tue Dec 5 13:39:07 2017 +0100

    Added explicit resource provider information to 'UpdateSlaveMessage'.
    
    The added fields will allow us to explicitly surface resource
    provider-related information in the master.
    
    Review: https://reviews.apache.org/r/64422
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos Containerizer GC should set 'layers' after checkpointing layer ids in provisioner.,MESOS-8280,13121783,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,zhitao,gilbert,gilbert,30/Nov/17 00:04,07/Dec/17 16:56,29/Oct/20 16:32,07/Dec/17 16:56,,,,,,,,,1.5.0,,,,,,image-gc,provisioner,,,,0,containerizer,image-gc,mesosphere,provisioner,uber,,,,"{noformat}
11111
222222
333333
444444
11111
222222
333333
444444
I1129 23:24:45.469543  6592 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/MVgVC7/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/MVgVC7/38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e/rootfs.overlay'
I1129 23:24:45.473287  6592 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/MVgVC7/sha256:b56ae66c29370df48e7377c8f9baa744a3958058a766793f821dadcb144a4647 to rootfs '/tmp/mesos/store/docker/staging/MVgVC7/b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3/rootfs.overlay'
I1129 23:24:45.582002  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs.overlay'
I1129 23:24:45.589404  6595 metadata_manager.cpp:167] Successfully cached image 'alpine'
I1129 23:24:45.590204  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e/rootfs.overlay'
I1129 23:24:45.595190  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f/rootfs.overlay'
I1129 23:24:45.599500  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721/rootfs.overlay'
I1129 23:24:45.602047  6597 provisioner.cpp:506] Provisioning image rootfs '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/rootfses/b5d48445-848d-4274-a4f8-e909351ebc35' for container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 using overlay backend
I1129 23:24:45.602751  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16/rootfs.overlay'
I1129 23:24:45.603054  6596 overlay.cpp:168] Created symlink '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/scratch/b5d48445-848d-4274-a4f8-e909351ebc35/links' -> '/tmp/xAWQ8y'
I1129 23:24:45.604398  6596 overlay.cpp:196] Provisioning image rootfs with overlayfs: 'lowerdir=/tmp/xAWQ8y/1:/tmp/xAWQ8y/0,upperdir=/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/scratch/b5d48445-848d-4274-a4f8-e909351ebc35/upperdir,workdir=/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/scratch/b5d48445-848d-4274-a4f8-e909351ebc35/workdir'
I1129 23:24:45.607802  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229/rootfs.overlay'
I1129 23:24:45.612139  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/rootfs.overlay'
I1129 23:24:45.612253  6593 containerizer.cpp:1369] Checkpointed ContainerConfig at '/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/config'
I1129 23:24:45.612298  6593 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 from PROVISIONING to PREPARING
I1129 23:24:45.625658  6596 containerizer.cpp:1838] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""clone_namespaces"":[131072],""command"":{""shell"":true,""value"":""sleep 1""},""environment"":{""variables"":[{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""\/mnt\/mesos\/sandbox""},{""name"":""PATH"",""type"":""VALUE"",""value"":""\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin""},{""name"":""MESOS_CONTAINER_IP"",""type"":""VALUE"",""value"":""10.0.2.15""}]},""pre_exec_commands"":[{""arguments"":[""mesos-containerizer"",""mount"",""--help=false"",""--operation=make-rslave"",""--path=\/""],""shell"":false,""value"":""\/vagrant\/mesos\/build\/src\/mesos-containerizer""},{""arguments"":[""mount"",""-n"",""--rbind"",""\/tmp\/slaves\/65fa58c5-48c6-4998-b336-ceb9bcd2ec43-S0\/frameworks\/55b825d4-c922-46bb-ab8e-4e01abf1a756-0000\/executors\/default-executor\/runs\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/01de09c5-d8e9-412e-8825-a592d2c875e5"",""\/tmp\/provisioner\/containers\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/01de09c5-d8e9-412e-8825-a592d2c875e5\/backends\/overlay\/rootfses\/b5d48445-848d-4274-a4f8-e909351ebc35\/mnt\/mesos\/sandbox""],""shell"":false,""value"":""mount""}],""rootfs"":""\/tmp\/provisioner\/containers\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/01de09c5-d8e9-412e-8825-a592d2c875e5\/backends\/overlay\/rootfses\/b5d48445-848d-4274-a4f8-e909351ebc35"",""task_environment"":{},""user"":""root"",""working_directory"":""\/mnt\/mesos\/sandbox""}"" --pipe_read=""12"" --pipe_write=""15"" --runtime_directory=""/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5"" --unshare_namespace_mnt=""false""'
I1129 23:24:45.626317  6598 linux_launcher.cpp:438] Launching nested container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 and cloning with namespaces CLONE_NEWNS
I1129 23:24:45.633211  6598 systemd.cpp:96] Assigned child process '6745' to 'mesos_executors.slice'
I1129 23:24:45.636270  6596 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 from PREPARING to ISOLATING
I1129 23:24:45.691830  6597 metadata_manager.cpp:167] Successfully cached image 'mesosphere/inky'
I1129 23:24:45.694399  6594 provisioner.cpp:506] Provisioning image rootfs '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/backends/overlay/rootfses/1187cc83-a23a-4390-9c28-092a7b7690b5' for container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 using overlay backend
I1129 23:24:45.694919  6596 overlay.cpp:168] Created symlink '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/backends/overlay/scratch/1187cc83-a23a-4390-9c28-092a7b7690b5/links' -> '/tmp/GXhXiT'
I1129 23:24:45.695103  6596 overlay.cpp:196] Provisioning image rootfs with overlayfs: 'lowerdir=/tmp/GXhXiT/6:/tmp/GXhXiT/5:/tmp/GXhXiT/4:/tmp/GXhXiT/3:/tmp/GXhXiT/2:/tmp/GXhXiT/1:/tmp/GXhXiT/0,upperdir=/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/backends/overlay/scratch/1187cc83-a23a-4390-9c28-092a7b7690b5/upperdir,workdir=/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/backends/overlay/scratch/1187cc83-a23a-4390-9c28-092a7b7690b5/workdir'
I1129 23:24:45.696255  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:45.696349  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers
I1129 23:24:45.696449  6593 containerizer.cpp:1369] Checkpointed ContainerConfig at '/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/config'
I1129 23:24:45.696506  6593 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 from PROVISIONING to PREPARING
I1129 23:24:45.697865  6595 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:45.697918  6595 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:45.697968  6595 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:45.697999  6595 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:45.698025  6595 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:45.698050  6595 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache
I1129 23:24:45.698076  6595 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:45.698104  6595 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
I1129 23:24:45.698129  6595 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache
I1129 23:24:45.698894  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:45.698966  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers
I1129 23:24:45.700333  6596 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:45.700394  6596 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:45.700412  6596 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:45.700428  6596 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:45.700441  6596 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:45.700454  6596 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache
I1129 23:24:45.700467  6596 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:45.700480  6596 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
I1129 23:24:45.700495  6596 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache
I1129 23:24:45.702491  6594 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:45.702554  6594 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers
I1129 23:24:45.703707  6592 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:45.703783  6592 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:45.703812  6592 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:45.703816  6592 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:45.703816  6592 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:45.704164  6592 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache
I1129 23:24:45.704208  6592 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:45.704255  6592 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
I1129 23:24:45.704285  6592 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache
I1129 23:24:45.704814  6595 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:45.704861  6595 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers
I1129 23:24:45.708112  6592 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:45.708204  6592 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:45.708238  6592 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:45.708264  6592 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:45.708375  6592 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:45.708407  6592 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache
I1129 23:24:45.708472  6592 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:45.708514  6592 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
I1129 23:24:45.708545  6592 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache
I1129 23:24:45.709048  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:45.709161  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers
I1129 23:24:45.710321  6594 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:45.710382  6594 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:45.710412  6594 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:45.710436  6594 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:45.710458  6594 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:45.710480  6594 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache
I1129 23:24:45.710522  6594 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:45.710551  6594 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
I1129 23:24:45.710590  6594 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache
I1129 23:24:45.713176  6594 containerizer.cpp:1838] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""clone_namespaces"":[131072],""command"":{""shell"":true,""value"":""sleep 100000""},""environment"":{""variables"":[{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""\/mnt\/mesos\/sandbox""},{""name"":""HOME"",""type"":""VALUE"",""value"":""\/""},{""name"":""PATH"",""type"":""VALUE"",""value"":""\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin""},{""name"":""MESOS_CONTAINER_IP"",""type"":""VALUE"",""value"":""10.0.2.15""}]},""pre_exec_commands"":[{""arguments"":[""mesos-containerizer"",""mount"",""--help=false"",""--operation=make-rslave"",""--path=\/""],""shell"":false,""value"":""\/vagrant\/mesos\/build\/src\/mesos-containerizer""},{""arguments"":[""mount"",""-n"",""--rbind"",""\/tmp\/slaves\/65fa58c5-48c6-4998-b336-ceb9bcd2ec43-S0\/frameworks\/55b825d4-c922-46bb-ab8e-4e01abf1a756-0000\/executors\/default-executor\/runs\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/7be37efd-7ec4-459a-a51a-97f6be99c3c3"",""\/tmp\/provisioner\/containers\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/7be37efd-7ec4-459a-a51a-97f6be99c3c3\/backends\/overlay\/rootfses\/1187cc83-a23a-4390-9c28-092a7b7690b5\/mnt\/mesos\/sandbox""],""shell"":false,""value"":""mount""}],""rootfs"":""\/tmp\/provisioner\/containers\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/7be37efd-7ec4-459a-a51a-97f6be99c3c3\/backends\/overlay\/rootfses\/1187cc83-a23a-4390-9c28-092a7b7690b5"",""task_environment"":{},""user"":""root"",""working_directory"":""\/mnt\/mesos\/sandbox""}"" --pipe_read=""13"" --pipe_write=""14"" --runtime_directory=""/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3"" --unshare_namespace_mnt=""false""'
I1129 23:24:45.713954  6597 linux_launcher.cpp:438] Launching nested container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 and cloning with namespaces CLONE_NEWNS
I1129 23:24:45.721781  6597 systemd.cpp:96] Assigned child process '6775' to 'mesos_executors.slice'
I1129 23:24:45.725494  6594 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 from PREPARING to ISOLATING
I1129 23:24:45.791635  6595 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 from ISOLATING to FETCHING
I1129 23:24:45.791880  6591 fetcher.cpp:379] Starting to fetch URIs for container: 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5, directory: /tmp/slaves/65fa58c5-48c6-4998-b336-ceb9bcd2ec43-S0/frameworks/55b825d4-c922-46bb-ab8e-4e01abf1a756-0000/executors/default-executor/runs/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5
I1129 23:24:45.792626  6591 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 from FETCHING to RUNNING
11111
222222
333333
444444
I1129 23:24:45.807262  6592 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:45.807375  6592 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers
I1129 23:24:45.808658  6591 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:45.808843  6591 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:45.808869  6591 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:45.808897  6591 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:45.808962  6591 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:45.808990  6591 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache
I1129 23:24:45.809012  6591 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:45.809036  6591 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
I1129 23:24:45.809057  6591 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache
I1129 23:24:45.893280  6596 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 from ISOLATING to FETCHING
I1129 23:24:45.893523  6596 fetcher.cpp:379] Starting to fetch URIs for container: 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3, directory: /tmp/slaves/65fa58c5-48c6-4998-b336-ceb9bcd2ec43-S0/frameworks/55b825d4-c922-46bb-ab8e-4e01abf1a756-0000/executors/default-executor/runs/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3
I1129 23:24:45.894335  6594 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 from FETCHING to RUNNING
I1129 23:24:45.902606  6598 process.cpp:3503] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1/executor'
I1129 23:24:45.903908  6597 process.cpp:3503] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1'
I1129 23:24:45.904618  6597 process.cpp:3503] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1'
I1129 23:24:45.908681  6597 http.cpp:1185] HTTP POST for /slave(1)/api/v1 from 10.0.2.15:57620
I1129 23:24:45.909113  6597 http.cpp:1185] HTTP POST for /slave(1)/api/v1 from 10.0.2.15:57622
I1129 23:24:45.909708  6597 http.cpp:2589] Processing WAIT_NESTED_CONTAINER call for container '3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5'
I1129 23:24:45.910148  6597 http.cpp:2589] Processing WAIT_NESTED_CONTAINER call for container '3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3'
I1129 23:24:45.938350  6596 process.cpp:3503] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1/executor'
I1129 23:24:45.938781  6596 http.cpp:1185] HTTP POST for /slave(1)/api/v1/executor from 10.0.2.15:57564
I1129 23:24:45.939141  6596 slave.cpp:4584] Handling status update TASK_RUNNING (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.940809  6591 http.cpp:1185] HTTP POST for /slave(1)/api/v1/executor from 10.0.2.15:57564
I1129 23:24:45.941130  6591 slave.cpp:4584] Handling status update TASK_RUNNING (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.942906  6591 task_status_update_manager.cpp:328] Received task status update TASK_RUNNING (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.943076  6591 task_status_update_manager.cpp:383] Forwarding task status update TASK_RUNNING (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to the agent
I1129 23:24:45.943322  6595 slave.cpp:5067] Forwarding the update TASK_RUNNING (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to master@127.0.0.1:5050
I1129 23:24:45.943332  6591 task_status_update_manager.cpp:328] Received task status update TASK_RUNNING (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.943506  6591 task_status_update_manager.cpp:383] Forwarding task status update TASK_RUNNING (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to the agent
I1129 23:24:45.943717  6595 slave.cpp:4960] Task status update manager successfully handled status update TASK_RUNNING (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.943905  6595 slave.cpp:5067] Forwarding the update TASK_RUNNING (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to master@127.0.0.1:5050
I1129 23:24:45.943905  6595 slave.cpp:4960] Task status update manager successfully handled status update TASK_RUNNING (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.992866  6591 task_status_update_manager.cpp:401] Received task status update acknowledgement (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.993261  6595 slave.cpp:3868] Task status update manager successfully handled status update acknowledgement (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.993968  6593 task_status_update_manager.cpp:401] Received task status update acknowledgement (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:45.994295  6598 slave.cpp:3868] Task status update manager successfully handled status update acknowledgement (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
11111
222222
333333
444444
I1129 23:24:46.808684  6592 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:46.808876  6592 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers
I1129 23:24:46.810683  6593 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:46.810751  6593 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:46.810781  6593 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:46.810808  6593 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:46.810834  6593 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:46.810860  6593 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache
I1129 23:24:46.810885  6593 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:46.810911  6593 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
I1129 23:24:46.810937  6593 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache
I1129 23:24:47.057590  6596 containerizer.cpp:2775] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has exited
I1129 23:24:47.057667  6596 containerizer.cpp:2324] Destroying container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 in RUNNING state
I1129 23:24:47.057695  6596 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 from RUNNING to DESTROYING
I1129 23:24:47.058082  6596 linux_launcher.cpp:514] Asked to destroy container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5
I1129 23:24:47.059027  6596 linux_launcher.cpp:560] Using freezer to destroy cgroup mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5
I1129 23:24:47.060667  6596 cgroups.cpp:3058] Freezing cgroup /sys/fs/cgroup/freezer/mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5
I1129 23:24:47.062700  6597 cgroups.cpp:1413] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5 after 1.838336ms
I1129 23:24:47.064627  6592 cgroups.cpp:3076] Thawing cgroup /sys/fs/cgroup/freezer/mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5
I1129 23:24:47.066498  6598 cgroups.cpp:1442] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5 after 1.642752ms
I1129 23:24:47.071521  6592 provisioner.cpp:648] Destroying container rootfs at '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/rootfses/b5d48445-848d-4274-a4f8-e909351ebc35' for container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5
I1129 23:24:47.098203  6596 overlay.cpp:296] Removed temporary directory '/tmp/xAWQ8y' pointed by '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/scratch/b5d48445-848d-4274-a4f8-e909351ebc35/links'
I1129 23:24:47.100265  6591 containerizer.cpp:2613] Checkpointing termination state to nested container's runtime directory '/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/termination'
I1129 23:24:47.107206  6594 process.cpp:3503] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1/executor'
I1129 23:24:47.151911  6594 http.cpp:1185] HTTP POST for /slave(1)/api/v1/executor from 10.0.2.15:57564
I1129 23:24:47.152243  6594 slave.cpp:4584] Handling status update TASK_FINISHED (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:47.154391  6594 task_status_update_manager.cpp:328] Received task status update TASK_FINISHED (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:47.154578  6594 task_status_update_manager.cpp:383] Forwarding task status update TASK_FINISHED (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to the agent
I1129 23:24:47.154810  6593 slave.cpp:5067] Forwarding the update TASK_FINISHED (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to master@127.0.0.1:5050
I1129 23:24:47.157249  6593 slave.cpp:4960] Task status update manager successfully handled status update TASK_FINISHED (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:47.385977  6592 task_status_update_manager.cpp:401] Received task status update acknowledgement (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:47.386334  6592 task_status_update_manager.cpp:538] Cleaning up status update stream for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:47.387459  6592 slave.cpp:3868] Task status update manager successfully handled status update acknowledgement (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000
I1129 23:24:47.387568  6592 slave.cpp:8457] Completing task 1
11111
222222
333333
444444
I1129 23:24:47.818768  6595 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:47.824591  6598 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:47.824724  6598 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:47.824753  6598 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:47.824767  6598 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:47.824782  6598 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:47.824861  6598 store.cpp:550] Marking layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' to gc by renaming '/tmp/mesos/store/docker/layers/38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' to '/tmp/mesos/store/docker/gc/38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e.1511997887824822784'
I1129 23:24:47.824918  6598 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:47.824960  6598 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
I1129 23:24:47.825088  6598 store.cpp:550] Marking layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' to gc by renaming '/tmp/mesos/store/docker/layers/b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' to '/tmp/mesos/store/docker/gc/b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3.1511997887825015040'
I1129 23:24:47.825389  6598 store.cpp:577] Deleting path '/tmp/mesos/store/docker/gc/b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3.1511997887825015040'
I1129 23:24:47.829903  6598 store.cpp:584] Deleted '/tmp/mesos/store/docker/gc/b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3.1511997887825015040'
I1129 23:24:47.829980  6598 store.cpp:577] Deleting path '/tmp/mesos/store/docker/gc/38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e.1511997887824822784'
I1129 23:24:47.830047  6598 store.cpp:584] Deleted '/tmp/mesos/store/docker/gc/38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e.1511997887824822784'
11111
222222
333333
444444
I1129 23:24:48.829519  6595 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:48.831161  6598 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:48.831225  6598 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:48.831248  6598 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:48.831266  6598 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:48.831284  6598 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:48.831302  6598 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:48.831321  6598 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
11111
222222
333333
444444
I1129 23:24:49.830904  6597 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:49.832487  6597 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache
I1129 23:24:49.832584  6597 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache
I1129 23:24:49.833329  6597 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache
I1129 23:24:49.833367  6597 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache
I1129 23:24:49.833387  6597 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache
I1129 23:24:49.833406  6597 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache
I1129 23:24:49.833425  6597 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache
{noformat}

Please neglect the debugging logs like '111111'. To reproduce this issue, just continuously trigger image gc. The log above was from a scenario that we launch two nested containers. One sleeps 1 second, another sleep forever.

This is related to this patch: https://github.com/apache/mesos/commit/e273efe6976434858edb85bbcf367a02e963a467#diff-a3593ed0ebd2b205775f7f04d9b5afe7

The root cause is that we did not set the 'layers' after we checkpoint the layer ids in provisioner. The log below is the prove:
{noformat}
I1129 23:24:45.698894  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers
I1129 23:24:45.698966  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-11-30 06:33:16.615,,,false,MESOS-4945,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 07 16:56:46 UTC 2017,,,,,,,"0|i3nd5b:",9223372036854775807,,,,,gilbert,,,,,,Mesosphere Sprint 69,,,,,,,,,,,3.0,,1.5.0,,,,,,,,,"30/Nov/17 06:33;zhitao;https://reviews.apache.org/r/64197/","06/Dec/17 20:10;gilbert;commit 4934eb77c0fdd20ee8f19f6d77ec8c2f708768ac
Author: Zhitao Li <zhitaoli.cs@gmail.com>
Date:   Wed Dec 6 11:42:29 2017 -0800

    Tracked layers in memory for provisioned containers.
    
    While adding layer checkpointing in previous patch,
    this case was missed, thus provisioner could incorrectly
    think a container is missing layers.
    
    Review: https://reviews.apache.org/r/64197/","07/Dec/17 16:56;gilbert;commit 4934eb77c0fdd20ee8f19f6d77ec8c2f708768ac
Author: Zhitao Li zhitaoli.cs@gmail.com
Date:   Wed Dec 6 11:42:29 2017 -0800

Tracked layers in memory for provisioned containers.

While adding layer checkpointing in previous patch,
this case was missed, thus provisioner could incorrectly
think a container is missing layers.

Review: https://reviews.apache.org/r/64197/",,,,,,,,,,,,,,,,,,,,,,,,,
Support image prune in mesos containerizer and provisioner.,MESOS-8249,13119765,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Done,zhitao,gilbert,gilbert,20/Nov/17 20:38,20/Nov/17 21:59,29/Oct/20 16:32,20/Nov/17 20:40,,,,,,,,,1.5.0,,,,,,containerization,provisioner,,,,0,containerizer,gc,mesosphere,provisioner,,,,,"Implement image prune in containerizer and the provisioner, by using mark and sweep to garbage collect unused layers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-4945,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 20 20:40:00 UTC 2017,,,,,,,"0|i3n0pr:",9223372036854775807,,,,,gilbert,,,,,,Mesosphere Sprint 68,,,,,,,,,,,13.0,,,,,,,,,,,"20/Nov/17 20:40;gilbert;commit bdb604a9dc29ab7bc4b9398cf4c1a2bd8b6061c4
Author: Zhitao Li <zhitaoli.cs@gmail.com>
Date:   Fri Nov 17 16:36:31 2017 -0800

    Implemented pruneImages with a mark and sweep in docker store.
    
    This includes the following changes:
    - add a `pruneImages()` function on the chain of relevant classes;
    - implement prune in docker store;
    - fix mock interface to keep existing tests pass.
    
    Review: https://reviews.apache.org/r/56721/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Unified Containerizer Auto backend should check xfs ftype for overlayfs backend.,MESOS-8121,13111097,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,mzhu,gilbert,gilbert,20/Oct/17 21:20,30/Nov/17 00:44,29/Oct/20 16:32,30/Nov/17 00:44,,,,,,,,,1.5.0,,,,,,containerization,,,,,0,backend,provisioner,,,,,,,"when using xfs as the backing filesystem in unified containerizer, the `ftype` has to be equal to 1 if we are using the overlay fs backend. we should add the detection in auto backend logic because some OS (like centos 7.2) has xfs ftype=0 by default.

https://docs.docker.com/engine/userguide/storagedriver/overlayfs-driver/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-10-20 21:37:44.365,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 30 00:44:39 UTC 2017,,,,,,,"0|i3ljd3:",9223372036854775807,,,,,gilbert,,,,,,Mesosphere Sprint 67,,,,,,,,,,,3.0,,,,,,,,,,,"20/Oct/17 21:37;avinash.mesos;[~gilbert] [~jieyu] we should return a failure if we see an incompatibility on the ftype instead of selecting another backend by default. I am just concerned that this is something that needs to be boiled up to the operator instead of silently failing and selecting a less performant backend?

","08/Nov/17 00:19;mzhu;https://reviews.apache.org/r/63652/","30/Nov/17 00:44;jamespeach;{noformat}
commit 9d27e429cbe63ed882df8cb2162776ec33bc3c81
Author: Meng Zhu <mzhu@mesosphere.io>
Date:   Wed Nov 29 15:11:03 2017 -0800

    Added a d_type check in containerizer backend validation.
    
    In the unified containerizer, the overlay fs backend depends
    on the underlying filesystem populating the `d_type` field in
    `struct dirent`. Raise an error if the user specifies overlayfs
    as backend on an unsupported filesystem.  Fallback to other
    backends in the default case and raise a warning.
    
    Review: https://reviews.apache.org/r/63652/
{noformat}

{noformat}
commit 212bb114f7e0bbb4c22654147d430b13ae9539b8 (HEAD -> master, origin/master, origin/HEAD)
Author: Meng Zhu <mzhu@mesosphere.io>
Date:   Wed Nov 29 15:13:54 2017 -0800

    Added tests for the d_type support validation.
    
    Review: https://reviews.apache.org/r/64018/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,
v1 role-related endpoints need to reflect hierarchical accounting.,MESOS-8069,13108424,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,tillt,bmahler,bmahler,10/Oct/17 22:20,25/Sep/19 16:40,29/Oct/20 16:32,,,,,,,,,,,,,,,,agent,HTTP API,master,,,0,mesosphere,multitenancy,resource-management,,,,,,"With the introduction of hierarchical roles, the role-related endpoints need to be updated to provide aggregated accounting information.

For example, information about how many resources are allocated to ""/eng"" should include the resources allocated to ""/eng/frontend"" and ""/eng/backend"", since quota guarantees and limits are also applied on the aggregation.

This also affects the UI display, for example the 'Roles' tab.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/18 00:38;tillt;Screen Shot 2018-03-06 at 15.06.04.png;https://issues.apache.org/jira/secure/attachment/12913297/Screen+Shot+2018-03-06+at+15.06.04.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,2018-03-07 00:23:43.302,,,false,MESOS-6375,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 31 16:27:05 UTC 2019,,,,,,,"0|hzzxib:i",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 75,Mesosphere Sprint 76,,,,,,,,,,8.0,,,,,,,,,,,"07/Mar/18 00:23;tillt;https://reviews.apache.org/r/65939/
https://reviews.apache.org/r/65940/","07/Mar/18 00:44;tillt; !Screen Shot 2018-03-06 at 15.06.04.png! 

In the second row, we see a framework is registered with role ""a/b"" and has gotten some resources allocated for that role. The first row, role ""a"" shows those resources aggregated for ""a"" and ""a/b"". ","10/Apr/18 03:32;tillt;After some discussions, the approach has changed significantly and hence the above screenshot is not valid anymore. We may want to display the additional set of resources ""Total Allocated"" and the additional set of frameworks ""Total Frameworls"" - both of which are aggregated from leaf nodes towards their parents. These now are reported by   {{GetRoles}} and {{/roles}}.","31/Jul/19 16:27;bmahler;This was done for the v0 /roles endpoint but still needs to be done for v1 GET_ROLES.",,,,,,,,,,,,,,,,,,,,,,,,
Agent and master can race when updating agent state.,MESOS-8058,13107542,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,bbannier,bbannier,bbannier,06/Oct/17 15:45,05/Dec/17 13:48,29/Oct/20 16:32,10/Oct/17 21:16,1.5.0,,,,,,,,1.5.0,,,,,,agent,,,,,0,mesosphere,,,,,,,,"In {{2af9a5b07dc80151154264e974d03f56a1c25838}} we introduce the use of {{UpdateSlaveMessage}} for the agent to inform the master about its current total resources. Currently we trigger this message only on agent registration and reregistration.

This can race with operations applied in the master and communicated via {{CheckpointResourcesMessage}}.

Example:

1. Agent ({{cpus:4(\*)}} registers.
2. Master is triggered to apply an operation to the agent's resources, e.g., a reservation: {{cpus:4(\*) -> cpus:4(A)}}. The master applies the operation to its current view of the agent's resources and sends the agent a {{CheckpointResourcesMessage}} so the agent can persist the result.
3. The agent sends the master an {{UpdateSlaveMessage}}, e.g., {{cpus:4(\*)}} since it hasn't received the {{CheckpointResourcesMessage}} yet.
4. The master processes the {{UpdateSlaveMessage}} and updates its view of the agent's resources to be {{cpus:4(\*)}}.
5. The agent processes the {{CheckpointResourcesMessage}} and updates its view of its resources to be {{cpus:4(A)}}.
6. The agent and the master have an inconsistent view of the agent's resources.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-7997,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-10-29 15:16:09.097,,,false,MESOS-7235,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 29 15:17:09 UTC 2017,,,,,,,"0|i3kyw7:",9223372036854775807,,,,,alexr,,,,,,Mesosphere Sprint 65,,,,,,,,,,,2.0,,,,,,,,,,,"09/Oct/17 12:15;bbannier;Review: https://reviews.apache.org/r/62868/","10/Oct/17 21:16;bbannier;{noformat}
Commit: a3faf6cf854cd64271bff7246ace13acfa37e731 [a3faf6c]
Author: Benjamin Bannier bbannier@apache.org
Date: 10 October 2017 at 11:36:48 GMT-7
Committer: Alexander Rukletsov alexr@apache.org

Reverted 2af9a5b07dc80151154264e974d03f56a1c25838.

The changes from 2af9a5b07dc80151154264e974d03f56a1c25838 introduced a
race between 'CheckpointResourcesMessage' and changes to agent total
resources via 'UpdateSlaveMessage' which needs to be addressed before
this change can go in.

Review: https://reviews.apache.org/r/62868/
{noformat}","29/Oct/17 15:16;jieyu;commit e9ac9f8252a1aa01d72fb7b918bf723df8e7dd7b
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Tue Oct 10 17:43:28 2017 -0700

    Sent CheckpointResourcesMessage only when reregister with an old master.

    No need for sending checkpoint resources message to the agent if the
    master does not have state about the agent.

    Review: https://reviews.apache.org/r/62878","29/Oct/17 15:16;jieyu;commit 33d1ff1798f8cbf83b4e5f7bc79dbf8e231dff1f
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Tue Oct 10 20:17:53 2017 -0700

    Stopped sending checkpoint resources message on agent re-registration.

    Given that resource provider capable agents will send update slave
    message to the master during re-registration, no need for the master
    to send checkpoint resources message to the agent anymore.

    This also makes the code more consistent because agent should be the
    source of truth. This also eliminates the possible retry incurred by
    this message, which is never the intention.

    Review: https://reviews.apache.org/r/62879","29/Oct/17 15:17;jieyu;commit 75cad1213e218a7f114c46c3d7c92047dae80345 (HEAD -> master, origin/master, origin/HEAD)
Author: Benjamin Bannier <bbannier@apache.org>
Date:   Fri Oct 13 21:05:27 2017 -0700

    Disallowed combining resource providers and CheckpointResourcesMessage.

    Offer operations on resource provider resources can require
    asynchronous handling since they can in principal take a long time to
    complete. Additionally, they can fail even after passing validation in
    the master, e.g., due to outside changes to the affected resources.
    For these reasons, resource provider resources require an offer
    operation protocol allowing failures outside of the master and
    communicating these failures to the master.

    Since this feedback can only be provided asynchronously, resource
    provider resources are incompatible with `CheckpointResourcesMessage`
    which by design updates the agent with the master's view of the
    agent's resources, and does not account for asynchronous changes to
    the agent's resources (leading e.g., to incompatible state between
    master and agents).

    This patch makes sure that agents with resource providers do not use
    the 'CheckpointResourcesMessage' protocol. This prevents users from
    running resource provider agents against legacy masters.

    Review: https://reviews.apache.org/r/62974/",,,,,,,,,,,,,,,,,,,,,,,
OOM due to LibeventSSLSocket send incorrectly returning 0 after shutdown.,MESOS-7934,13099350,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,bmahler,bmahler,bmahler,01/Sep/17 23:53,26/Mar/18 20:10,29/Oct/20 16:32,02/Sep/17 03:32,,,,,,,,,1.2.3,1.3.2,1.4.0,,,,libprocess,,,,,0,,,,,,,,,"LibeventSSLSocket can return 0 from send incorrectly, which leads the caller to send the data twice!

See here: https://github.com/apache/mesos/blob/1.3.1/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L396-L398

In some particular cases, it's possible that the caller keeps getting back 0 and loops infinitely, blowing up the memory and OOMing the process.

One example is when a send occurs after a shutdown:

{code}
TEST_F(SSLTest, ShutdownThenSend)
{
  Clock::pause();

  Try<Socket> server = setup_server({
      {""LIBPROCESS_SSL_ENABLED"", ""true""},
      {""LIBPROCESS_SSL_KEY_FILE"", key_path().string()},
      {""LIBPROCESS_SSL_CERT_FILE"", certificate_path().string()}});

  ASSERT_SOME(server);
  ASSERT_SOME(server.get().address());
  ASSERT_SOME(server.get().address().get().hostname());

  Future<Socket> socket = server.get().accept();

  Clock::settle();
  EXPECT_TRUE(socket.isPending());

  Try<Socket> client = Socket::create(SocketImpl::Kind::SSL);
  ASSERT_SOME(client);
  AWAIT_ASSERT_READY(client->connect(server->address().get()));

  AWAIT_ASSERT_READY(socket);

  EXPECT_SOME(Socket(socket.get()).shutdown());

  // This loops forever!
  AWAIT_FAILED(Socket(socket.get()).send(""Hello World""));
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-7930,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 02 03:32:21 UTC 2017,,,,,,,"0|i3jkmn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,5.0,,1.2.3,1.3.2,1.4.0,,,,,,,"02/Sep/17 00:55;bmahler;https://reviews.apache.org/r/62049/","02/Sep/17 03:32;bmahler;Fix:
{noformat}
commit 9d1b812d92592f5d0ff8e000d022837de89ac9ba
Author: Benjamin Mahler <bmahler@apache.org>
Date:   Fri Sep 1 17:17:59 2017 -0700

    Fixed an OOM due to a send loop for SSL sockets.

    Per MESOS-7934, the LibeventSSLSocket incorrectly returns 0 to the
    sender when an EOF, or ""dirty"" SSL shutdown (i.e. TCP close before
    SSL close), or a send is performed on a socket after it has been
    shut down. Not only is this incorrect due to the caller re-sending
    the same data again, in the case that the socket has been shut down,
    the caller of send will enter an infinite loop of retrying the send
    which will rapidly lead to an OOM in libprocess.

    The fix here is to fail the send instead. Note that with libevent
    2.0.x the 'events' will not contain BEV_EVENT_READING or
    BEV_EVENT_WRITING for SSL buffevents. With libevent 2.1.x, we can
    update our logic to deal with the read and write side events
    separately.

    https://github.com/libevent/libevent/commit/f7eb69ace

    Comments are added in a follow up change to explain this for
    posterity, and MESOS-7930 tracks the additional tech debt that
    needs to be addressed for SSL socket support.

    Review: https://reviews.apache.org/r/62049
{noformat}

Test:
{noformat}
commit eb8c2c7e418dc19bde456517ab44ed78b4406719
Author: Benjamin Mahler <bmahler@apache.org>
Date:   Fri Sep 1 17:26:18 2017 -0700

    Added a test to reproduce the OOM issue in MESOS-7934.

    Review: https://reviews.apache.org/r/62050
{noformat}

Cleanup:
{noformat}
commit 7f0c48f759832c5fed1f4ea4cce667881cad9a49
Author: Benjamin Mahler <bmahler@apache.org>
Date:   Fri Sep 1 17:27:19 2017 -0700

    Clarified some issues in the LibeventSSLSocket event_callback logic.

    This updates the comments to include my findings after having looked
    into using BEV_EVENT_READING / BEV_EVENT_WRITING to handle read and
    write path errors separately. Unfortunately, we cannot do this using
    libevent 2.0.x (see the comments).

    This also clarifies the ""dirty"" SSL shutdown case and the case where
    futher sends are performed on a shut down socket.

    Review: https://reviews.apache.org/r/62051
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix communication between old masters and new agents.,MESOS-7922,13098050,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,mcypark,mcypark,mcypark,28/Aug/17 18:30,28/Aug/17 23:42,29/Oct/20 16:32,28/Aug/17 23:42,,,,,,,,,1.4.0,,,,,,agent,master,,,,0,,,,,,,,,"For re-registration, agents currently send the resources in tasks
and executors to the master in the ""post-reservation-refinement"" format,
which is incompatible for pre-1.4 masters. We should change the agent
such that it always downgrades the resources to
the ""pre-reservation-refinement"" format, and the master unconditionally
upgrade the resources to ""post-reservation-refinement"" format.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-7575,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 28 23:42:00 UTC 2017,,,,,,,"0|i3jd5j:",9223372036854775807,,,,,greggomann,,,,,,Mesosphere Sprint 62,,,,,,,,,,,2.0,,1.4.0,,,,,,,,,"28/Aug/17 18:31;mcypark;https://reviews.apache.org/r/61952","28/Aug/17 23:42;mcypark;{noformat}
commit 30e2b2ad818e4e90c8df03b9802a4b1a431605c7
Author: Michael Park <mpark@apache.org>
Date:   Mon Aug 28 15:19:31 2017 -0700

    Fixed the communication between old masters and new agents.

    For re-registration, 1.4 agents used to send the resources in tasks
    and executors to the master in the ""post-reservation-refinement"" format,
    which is incompatible for pre-1.4 masters. This patch changes the agent
    such that it always downgrades the resources to
    the ""pre-reservation-refinement"" format, and the master unconditionally
    upgrades the resources to ""post-reservation-refinement"" format.

    Review: https://reviews.apache.org/r/61952/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Non-checkpointing framework's tasks should not be marked LOST when agent disconnects.,MESOS-7911,13097124,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,,,bmahler,bmahler,23/Aug/17 19:50,03/Apr/19 17:29,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,foundations,reliability,,,,,,,"Currently, when framework with checkpointing disabled has tasks running on an agent and that agent disconnects from the master, the master will mark those tasks LOST and remove them from its memory. The assumption is that the agent is disconnecting because it terminated.

However, it's possible that this disconnection occurred due to a transient loss of connectivity and the agent re-connects while never having terminated. This case violates our assumption of there being no unknown tasks to the master:

```
 void Master::reconcileKnownSlave(
 Slave* slave,
 const vector<ExecutorInfo>& executors,
 const vector<Task>& tasks)
 {
 ...

// TODO(bmahler): There's an implicit assumption here the slave
 // cannot have tasks unknown to the master. This _should_ be the
 // case since the causal relationship is:
 // slave removes task -> master removes task
 // Add error logging for any violations of this assumption!
 ```

As a result, the tasks would remain on the agent but the master would not know about them!

A more appropriate action here would be:

# When an agent disconnects, mark the tasks as unreachable.
## If the framework is not partition aware, only show it the last known task state.
## If the framework is partition aware, let it know that it's now unreachable.
# If the agent re-connects:
## And the agent had restarted, let the non-checkpointing framework know its tasks are GONE/LOST.
## If the agent still holds the tasks, the tasks are restored as reachable.
# If the agent gets removed:
## For partition aware non-checkpointing frameworks, let them know the tasks are unreachable.
## For non partition aware non-checkpointing frameworks, let them know the tasks are lost and kill them if the agent comes back.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-6394,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-08-23 19:50:34.0,,,,,,,"0|hzzxsj:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 74,Mesosphere Sprint 75,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos master rescinds all the in-flight offers from all the registered agents when a new maintenance schedule is posted for a subset of slaves,MESOS-7882,13094026,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,,kaysoky,sagar8192,sagar8192,11/Aug/17 06:47,03/Apr/19 16:47,29/Oct/20 16:32,,1.3.0,,,,,,,,,,,,,,master,,,,,1,maintenance,mesosphere,,,,,,,"We are running mesos 1.1.0 in production. We use a custom autoscaler for scaling our mesos  cluster up and down. While scaling down the cluster, autoscaler makes a POST request to mesos master /maintenance/schedule endpoint with a set of slaves to move to maintenance mode. This forces mesos master to rescind all the in-flight offers from *all the slaves* in the cluster. If our scheduler accepts one of these offers, then we get a TASK_LOST status update back for that task. We also see such (https://gist.github.com/sagar8192/8858e7cb59a23e8e1762a27571824118) log lines in mesos master logs.

After reading the code(refs: https://github.com/apache/mesos/blob/master/src/master/master.cpp#L6772), it appears that offers are getting rescinded for all the slaves. I am not sure what is the expected behavior here, but it makes more sense if only resources from slaves marked for maintenance are reclaimed.

*Experiment:*
To verify that it is actually happening, I checked out the master branch(sha: a31dd52ab71d2a529b55cd9111ec54acf7550ded ) and added some log lines(https://gist.github.com/sagar8192/42ca055720549c5ff3067b1e6c7c68b3). Built the binary and started a mesos master and 2 agent processes. Used a basic python framework that launches docker containers on these slaves. Verified that there is no existing schedule for any slaves using `curl 10.40.19.239:5050/maintenance/status`. Posted maintenance schedule for one of the slaves(https://gist.github.com/sagar8192/fb65170240dd32a53f27e6985c549df0) after starting the mesos framework.

*Logs:*
mesos-master: https://gist.github.com/sagar8192/91888419fdf8284e33ebd58351131203
mesos-slave1: https://gist.github.com/sagar8192/3a83364b1f5ffc63902a80c728647f31
mesos-slave2: https://gist.github.com/sagar8192/1b341ef2271dde11d276974a27109426
Mesos framework: https://gist.github.com/sagar8192/bcd4b37dba03bde0a942b5b972004e8a

I think mesos should rescind offers and inverse offers only for those slaves that are marked for maintenance(draining mode).","Ubuntu 14:04(trusty)
Mesos master branch.
SHA: a31dd52ab71d2a529b55cd9111ec54acf7550ded",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-08-14 19:53:21.259,,,false,MESOS-7201,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 16 00:03:18 UTC 2017,,,,,,,"0|i3iokn:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 73,Mesosphere Sprint 74,Mesosphere Sprint 75,,,,,,,,,3.0,,1.9.0,,,,,,,,,"14/Aug/17 19:53;huadongliu;I was able to repro the problem. The test setup has two mesos agents
{noformat}
af584a07-7b1c-4955-861e-63585af8bb5d-S0: 10.255.55.153
af584a07-7b1c-4955-861e-63585af8bb5d-S1: 10.255.52.14
{noformat}

The modified example framework is going to hold received offers for 30 seconds and it only launches tasks on S0.
{noformat}
diff --git a/src/examples/python/test_framework.py b/src/examples/python/test_framework.py
     def resourceOffers(self, driver, offers):
+        time.sleep(30)
         for offer in offers:
+            if 'af584a07-7b1c-4955-861e-63585af8bb5d-S1' == offer.slave_id.value:
+                print(""ignore offers from af584a07-7b1c-4955-861e-63585af8bb5d-S1"")
+                continue
             tasks = []
{noformat} 

Start test-framework, and post a maintenance schedule of S1 on another terminal while test-framework is in sleep.
{noformat}
~/mesos/build$ ./src/examples/python/test-framework 10.255.52.14:5050
I0814 11:48:21.296404  4182 sched.cpp:232] Version: 1.3.0
I0814 11:48:21.301652  4222 sched.cpp:336] New master detected at master@10.255.52.14:5050
I0814 11:48:21.302145  4222 sched.cpp:352] No credentials provided. Attempting to register without authentication
I0814 11:48:21.306299  4224 sched.cpp:759] Framework registered with af584a07-7b1c-4955-861e-63585af8bb5d-0014
Registered with framework ID af584a07-7b1c-4955-861e-63585af8bb5d-0014

---------------------
$ cat schedule.json
{
  ""windows"" : [
    {
      ""machine_ids"" : [
        { ""ip"" : ""10.255.52.14"" }
      ],
      ""unavailability"" : {
        ""start"" : { ""nanoseconds"" : 1502734375000000000 },
        ""duration"" : { ""nanoseconds"" : 3600000000000 }
      }
    }
  ]
}
$ curl http://10.255.52.14:5050/maintenance/schedule -H ""Content-type: application/json"" -X POST -d @schedule.json
----------------

Received offer af584a07-7b1c-4955-861e-63585af8bb5d-O153 with cpus: 3.0 and mem: 2927.0
Launching task 0 using offer af584a07-7b1c-4955-861e-63585af8bb5d-O153
Launching task 1 using offer af584a07-7b1c-4955-861e-63585af8bb5d-O153
Launching task 2 using offer af584a07-7b1c-4955-861e-63585af8bb5d-O153
ignore offers from af584a07-7b1c-4955-861e-63585af8bb5d-S1
ignore offers from af584a07-7b1c-4955-861e-63585af8bb5d-S1
Received offer af584a07-7b1c-4955-861e-63585af8bb5d-O156 with cpus: 3.0 and mem: 2927.0
Launching task 3 using offer af584a07-7b1c-4955-861e-63585af8bb5d-O156
Launching task 4 using offer af584a07-7b1c-4955-861e-63585af8bb5d-O156
W0814 11:49:51.406801  4218 sched.cpp:1371] Attempting to accept an unknown offer af584a07-7b1c-4955-861e-63585af8bb5d-O153
Task 0 is in state TASK_LOST
{noformat}

Mesos master log while this is happening is captured below:
{noformat}
I0814 11:48:21.302987  1530 master.cpp:2596] Received SUBSCRIBE call for framework 'Test Framework (Python)' at scheduler-6d672749-4414-4266-adfc-2b7ff5694d5b@10.255.52.14:45893
I0814 11:48:21.303450  1530 master.cpp:2672] Subscribing framework Test Framework (Python) with checkpointing enabled and capabilities [  ]
I0814 11:48:21.304566  1529 hierarchical.cpp:275] Added framework af584a07-7b1c-4955-861e-63585af8bb5d-0014
I0814 11:48:21.306139  1530 master.cpp:6517] Sending 2 offers to framework af584a07-7b1c-4955-861e-63585af8bb5d-0014 (Test Framework (Python)) at scheduler-6d672749-4414-4266-adfc-2b7ff5694d5b@10.255.52.14:45893
I0814 11:48:25.076035  1533 http.cpp:391] HTTP POST for /master/maintenance/schedule from 10.255.55.153:37186 with User-Agent='curl/7.47.0'
I0814 11:48:25.077271  1533 registrar.cpp:461] Applied 1 operations in 272915ns; attempting to update the registry
I0814 11:48:25.078277  1533 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 39
I0814 11:48:25.079033  1533 replica.cpp:537] Replica received write request for position 39 from __req_res__(44)@10.255.52.14:5050
I0814 11:48:25.082299  1531 replica.cpp:691] Replica received learned notice for position 39 from @0.0.0.0:0
I0814 11:48:25.085546  1531 registrar.cpp:506] Successfully updated the registry in 8.176128ms
I0814 11:48:25.085726  1535 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 40
I0814 11:48:25.086496  1528 master.cpp:5645] Removing unavailability of agent af584a07-7b1c-4955-861e-63585af8bb5d-S1 at slave(1)@10.255.52.14:5051 (10.255.52.14)
I0814 11:48:25.086550  1530 replica.cpp:537] Replica received write request for position 40 from __req_res__(45)@10.255.52.14:5050
I0814 11:48:25.087936  1530 replica.cpp:691] Replica received learned notice for position 40 from @0.0.0.0:0
I0814 11:48:25.088673  1528 master.cpp:5645] Removing unavailability of agent af584a07-7b1c-4955-861e-63585af8bb5d-S0 at slave(1)@10.255.55.153:5051 (10.255.55.153)
I0814 11:48:25.089725  1528 master.cpp:6517] Sending 1 offers to framework af584a07-7b1c-4955-861e-63585af8bb5d-0014 (Test Framework (Python)) at scheduler-6d672749-4414-4266-adfc-2b7ff5694d5b@10.255.52.14:45893
I0814 11:48:25.090461  1529 master.cpp:6517] Sending 1 offers to framework af584a07-7b1c-4955-861e-63585af8bb5d-0014 (Test Framework (Python)) at scheduler-6d672749-4414-4266-adfc-2b7ff5694d5b@10.255.52.14:45893
W0814 11:49:51.408465  1534 master.cpp:3494] Ignoring accept of offer af584a07-7b1c-4955-861e-63585af8bb5d-O153 since it is no longer valid
W0814 11:49:51.408888  1534 master.cpp:3505] ACCEPT call used invalid offers '[ af584a07-7b1c-4955-861e-63585af8bb5d-O153 ]': Offer af584a07-7b1c-4955-861e-63585af8bb5d-O153 is no longer valid
I0814 11:49:51.409276  1534 master.cpp:5772] Sending status update TASK_LOST for task 0 of framework af584a07-7b1c-4955-861e-63585af8bb5d-0014 'Task launched with invalid offers: Offer af584a07-7b1c-4955-861e-63585af8bb5d-O153 is no longer valid'
I0814 11:49:51.409920  1534 master.cpp:5772] Sending status update TASK_LOST for task 1 of framework af584a07-7b1c-4955-861e-63585af8bb5d-0014 'Task launched with invalid offers: Offer af584a07-7b1c-4955-861e-63585af8bb5d-O153 is no longer valid'
I0814 11:49:51.410332  1534 master.cpp:5772] Sending status update TASK_LOST for task 2 of framework af584a07-7b1c-4955-861e-63585af8bb5d-0014 'Task launched with invalid offers: Offer af584a07-7b1c-4955-861e-63585af8bb5d-O153 is no longer valid'
{noformat}","15/Nov/17 23:05;vinodkone;[~kaysoky] Is this a bug?","16/Nov/17 00:03;kaysoky;Yeah, this could be improved.  The first iteration of the {{/maintenance/schedule}} API precisely updates every machine.  And we never got any requests to make this more efficient (until now I suppose :) ).

Note that in the repro by [~huadongliu], the {{MachineID}} is missing a hostname, so it does not match either of the two agents.  Both get their unavailability ""removed"" (even though there was no unavailability to remove, technically).",,,,,,,,,,,,,,,,,,,,,,,,,
Master stores old resource format in the registry,MESOS-7851,13091946,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,mcypark,greggomann,greggomann,02/Aug/17 17:12,06/Nov/17 03:44,29/Oct/20 16:32,06/Nov/17 03:44,,,,,,,,,1.5.0,,,,,,master,,,,,0,master,mesosphere,reservation,,,,,,"We intend for the master to store all internal resource representations in the new, post-reservation-refinement format. However, [when persisting registered agents to the registrar|https://github.com/apache/mesos/blob/498a000ac1bb8f51dc871f22aea265424a407a17/src/master/master.cpp#L5861-L5876], the master does not convert the resources; agents provide resources in the pre-reservation-refinement format, and these resources are stored as-is. This means that after recovery, any agents in the master's {{slaves.recovered}} map will have {{SlaveInfo.resources}} in the pre-reservation-refinement format.

We should update the master to convert these resources before persisting them to the registry.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-08-25 23:23:26.043,,,false,MESOS-7575,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 06 03:44:52 UTC 2017,,,,,,,"0|hzzyaj:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 61,Mesosphere Sprint 62,Mesosphere Sprint 63,Mesosphere Sprint 64,Mesosphere Sprint 65,Mesosphere Sprint 66,Mesosphere Sprint 67,,,,,3.0,,1.5.0,,,,,,,,,"02/Aug/17 20:24;greggomann;Note that when this issue is resolved, the {{authorizeResource()}} helper introduced in [this patch|https://reviews.apache.org/r/61171/] should be updated.","25/Aug/17 23:23;mcypark;Just writing down what should be done here.

The master has 2 things that contain resources that go into the registry:
{{SlaveInfo}}, and {{QuotaInfo}}. In order to support master downgrades
(e.g., 1.4.0 => 1.3.1), we must store the resources in
the ""pre-reservation-refinement"" format. This happens for {{SlaveInfo}} today
(albeit incidentally), but not for {{QuotaInfo}}.

Resources inside {{QuotaInfo}} should probably be downgraded for the {{Registry}}
and upgraded on their way out. However, with the current requirement that
{{QuotaInfo}} can only hold unreserved resources, we don't need to do anything
for this. (tested manually by setting a quota with 1.4.0 master, downgrading to
1.3.1 and hitting the quota endpoint).

{{Master::_recover}} should upgrade the resources inside {{SlaveInfo}} before
inserting it into the {{slaves.recovered}} map. {{authorizeResources}} can be
updated after this.","24/Oct/17 00:54;mcypark;https://reviews.apache.org/r/63232/","06/Nov/17 03:44;mcypark;{noformat}
commit 2dfd753f126fd467eaf3607b712e63581bc88b4c
Author: Michael Park <mpark@apache.org>
Date:   Mon Oct 23 16:51:59 2017 -0700

    Converted SlaveInfo resources from the registry during master recovery.

    The intention is for us to work with resources in the `post-reservation-
    refinement` format in the master memory, and store them in the master
    registry or agent checkpoint in the `pre` format in order to support
    downgrades. In this case, we correctly store the `pre` format resources
    in the master registry, but we don't convert them back when we read it
    out of the registry. This patch converts the resources back to `post`
    when we read from the registry.

    This simply addresses a tech-debt. The `authorizeResource` function
    for example currently handles both formats because it has to account
    for the old format that is stored in the recovered agents.

    Review: https://reviews.apache.org/r/63232
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,
Sandbox_path volume does not have ownership set correctly.,MESOS-7830,13090007,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,25/Jul/17 19:15,06/Jul/18 21:30,29/Oct/20 16:32,28/Jul/17 20:05,,,,,,,,,1.1.3,1.2.2,1.3.1,1.4.0,,,containerization,,,,,0,containerizer,mesosphere,volumes,,,,,,"This issue was exposed when using sandbox_path volume to support shared volume for nested containers under one task group. Here is a scenario:

The agent process runs as 'root' user, while the framework user is set as 'nobody'. No matter the commandinfo user is set or not, any non-root user cannot access the sandbox_path volume (e.g., a PARENT sandbox_path volume is not writable from a nested container). This is because the source path at the parent sandbox level is created by the agent process (aka root in this case). 

While the operator is responsible for guaranteeing a nested container should have permission to write to its sandbox path volume at its parent's sandbox, we should guarantee the source path created at parent's sandbox should be set as the same ownership as this sandbox's ownership.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-5187,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 28 20:05:44 UTC 2017,,,,,,,"0|i3i03z:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 60,,,,,,,,,,,3.0,,1.4.0,,,,,,,,,"25/Jul/17 23:06;gilbert;https://reviews.apache.org/r/61120/
https://reviews.apache.org/r/61121/","28/Jul/17 20:05;gilbert;commit f99a7170716bba52b05732833fb26df1d01e2b42
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Fri Jul 28 12:27:52 2017 -0700

    Added regression test for sandbox_path volume ownership issue.
    
    Added regression test for sandbox_path volume ownership issue.
    
    Review: https://reviews.apache.org/r/61121/

commit 63fd94ca8836be32ba7ec6e770df4b1b411ab726
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Fri Jul 28 12:27:49 2017 -0700

    Fixed the sandbox_path volume source path ownership.
    
    This bugfix addresses the issue from MESOS-7830. Basically, the
    sandbox path volume ownership was not set correctly. This issue
    can be exposed if a framework user is non-root while the agent
    process runs as root. Then, the non-root user does not have
    permissions to write to this volume.
    
    The correct solution should be giving permissions to corresponding
    users by leveraging supplementary groups. But we can still
    introduce a workaround in this patch by changing the ownership
    of the sandbox path volume to its sandbox's ownership.
    
    Review: https://reviews.apache.org/r/61120/",,,,,,,,,,,,,,,,,,,,,,,,,,
Current approach to parse protobuf enum from JSON does not support upgrades,MESOS-7828,13089901,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,qianzhang,qianzhang,qianzhang,25/Jul/17 14:58,03/Oct/17 01:41,29/Oct/20 16:32,03/Oct/17 01:41,,,,,,,,,1.5.0,,,,,,,,,,,0,,,,,,,,,"To use protobuf enum in a backwards compatible way, [the suggestion on the protobuf mailing list|https://groups.google.com/forum/#!msg/protobuf/NhUjBfDyGmY/pf294zMi2bIJ] is to use optional enum fields and include an UNKNOWN value as the first entry in the enum list (and/or explicitly specifying it as the default). This can handle the case of parsing protobuf message from a serialized string, but it can not handle the case of parsing protobuf message from JSON.

E.g., when I access master endpoint with an inexistent enum {{xxx}}, I will get an error:
{code}
$ curl -X POST -H ""Content-Type: application/json"" -d '{""type"": ""xxx""}' 127.0.0.1:5050/api/v1
Failed to convert JSON into Call protobuf: Failed to find enum for 'xxx'% 
{code}

In the {{Call}} protobuf message, the enum {{Type}} already has a default value {{UNKNOWN}} (see [here|https://github.com/apache/mesos/blob/1.3.0/include/mesos/v1/master/master.proto#L45] for details) and the field {{Call.type}} is optional, but the above curl command will still fail. The root cause is, in the code [here|https://github.com/apache/mesos/blob/1.3.0/3rdparty/stout/include/stout/protobuf.hpp#L449:L454] when we try to get the enum value for the string ""xxx"", it will fail since there is no any enum value corresponding to ""xxx"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-07-31 19:46:49.732,,,false,MESOS-4997,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 03 01:40:53 UTC 2017,,,,,,,"0|hzzydo:zzzr",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 64,Mesosphere Sprint 65,,,,,,,,,,3.0,,1.5.0,,,,,,,,,"25/Jul/17 15:03;qianzhang;My proposal is, when we fail to get an enum for a string, check if it is an optional enum field, if yes, call FieldDescriptor::default_value_enum() to get the default value, and use that default value.","25/Jul/17 15:17;qianzhang;RR: https://reviews.apache.org/r/61109/","31/Jul/17 19:46;bmahler;[~qianzhang] do you know how the protobuf supplied json libraries address this issue? Seems like they don't? Or does the 'ignore unknown fields' parameter they provide also apply to unknown enum values? I would be curious to know what they did in proto3, since they wrote this in the release notes:

{quote}
iv. Fix semantics for unknown enum values.
{quote}

https://github.com/google/protobuf/releases/tag/v3.0.0-alpha-1
","02/Aug/17 09:28;qianzhang;[~bmahler] Yes, the protobuf supplied json libraries also have this issue, I confirmed it with a small program:
{code}
// test_protobuf.proto 
syntax = ""proto2"";

message Foo {
  enum Type {
    UNKNOWN = 0;
    CREATE = 1;
    DELETE = 2;

  }

  optional Type type = 1;
}


// main.cc
#include <iostream>
#include <string>
#include <google/protobuf/util/json_util.h>

#include ""test_protobuf.pb.h""

using namespace google::protobuf::util;

int main()
{
  Foo msg;
  std::string json =
      ""{""
      ""  \""type\"": \""xxx\""""
      ""}"";

  JsonParseOptions options;
  Status status = JsonStringToMessage(json, &msg, options);
  if (status.ok()) {
    std::cout << msg.type() << std::endl;
  } else {
    std::cout << status.error_message() << std::endl;
  }

  return 0;
}
{code}

In the above program, the utility function {{JsonStringToMessage()}} provided by protobuf is called to convert a JSON string to a protobuf message. I built and ran this program with protobuf-3.3.0 bundled in Mesos code, and its output is:
{code}
$ ./main
type: invalid value ""xxx"" for type TYPE_ENUM
{code}
","03/Aug/17 06:42;qianzhang;And I get the same result when setting {{JsonParseOptions.ignore_unknown_fields}} to true. And based on my test, {{JsonParseOptions.ignore_unknown_fields}} will take effect only if there is an unknown field (rather than a known field with unknown enum value) in the JSON.","21/Sep/17 01:26;qianzhang;RR:
https://reviews.apache.org/r/61109/","03/Oct/17 01:40;qianzhang;commit 2f3ceb45106e79586f2c32bfd26db0318d608075
Author: Qian Zhang zhq527725@gmail.com
Date:   Thu Jul 27 16:15:44 2017 +0800

Added a test `ProtobufTest.ParseJSONUnrecognizedEnum`.

Review: https://reviews.apache.org/r/61174

3rdparty/stout/tests/protobuf_tests.cpp   | 33 +++++++++++++++++++++++++++++++++
 3rdparty/stout/tests/protobuf_tests.proto |  9 +++++++++
 2 files changed, 42 insertions(+)

commit b10a4ea59231d134662d49417add2ccd7779cde7
Author: Qian Zhang <zhq527725@gmail.com>
Date:   Tue Jul 25 23:03:43 2017 +0800

    Fixed JSON protobuf deserialization to ignore unrecognized enum values.
    
    Protobuf deserialization will discard any unrecognized enum values.
    This patch fixes our custom JSON -> protobuf conversion code to be
    consistent with this behavior.
    
    See MESOS-4997 for why this matters when dealing with upgrades.
    
    Fixes MESOS-7828.
    
    Review: https://reviews.apache.org/r/61109

 3rdparty/stout/include/stout/protobuf.hpp | 14 +++++++++++++-
 1 file changed, 13 insertions(+), 1 deletion(-)",,,,,,,,,,,,,,,,,,,,,
mesos-execute has incorrect example TaskInfo in help string,MESOS-7805,13088435,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,bbannier,bbannier,19/Jul/17 15:36,26/Mar/18 08:03,29/Oct/20 16:32,08/Aug/17 23:55,1.4.0,,,,,,,,1.4.0,,,,,,documentation,,,,,0,mesosphere,,,,,,,,"{{mesos-execute}} documents that a task can be defined via JSON as
{noformat}
{
  ""name"": ""Name of the task"",
  ""task_id"": {""value"" : ""Id of the task""},
  ""agent_id"": {""value"" : """"},
  ""resources"": [
    {
      ""name"": ""cpus"",
      ""type"": ""SCALAR"",
      ""scalar"": {
        ""value"": 0.1
      },
      ""role"": ""*""
    },
    {
      ""name"": ""mem"",
      ""type"": ""SCALAR"",
      ""scalar"": {
        ""value"": 32
      },
      ""role"": ""*""
    }
  ],
  ""command"": {
    ""value"": ""sleep 1000""
  }
}
{noformat}

If one actually uses that example task definition one gets
{noformat}
% ./build/src/mesos-execute --master=127.0.0.1:5050 --task=task.json
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0719 17:08:17.909696 3291313088 parse.hpp:114] Specifying an absolute filename to read a command line option out of without using 'file:// is deprecated and will be removed in a future release. Simply adding 'file://' to the beginning of the path should eliminate this warning.
[warn] kq_init: detected broken kqueue; not using.: Undefined error: 0
I0719 17:08:17.919190 119246848 scheduler.cpp:184] Version: 1.4.0
I0719 17:08:17.923991 119783424 scheduler.cpp:470] New master detected at master@127.0.0.1:5050
Subscribed with ID bb0d36b4-fee0-4412-9cd9-1fa4e330355c-0000
F0719 17:08:18.137984 119783424 resources.cpp:1081] Check failed: !resource.has_role()
*** Check failure stack trace: ***
    @        0x101d65f5f  google::LogMessageFatal::~LogMessageFatal()
    @        0x101d62609  google::LogMessageFatal::~LogMessageFatal()
    @        0x1016ef3a3  mesos::v1::Resources::isEmpty()
    @        0x1016ed267  mesos::v1::Resources::add()
    @        0x1016f05af  mesos::v1::Resources::operator+=()
    @        0x1016f08fb  mesos::v1::Resources::Resources()
    @        0x100c0d89f  CommandScheduler::offers()
    @        0x100c085e4  CommandScheduler::received()
    @        0x100c0ae06  _ZZN7process8dispatchI16CommandSchedulerNSt3__15queueIN5mesos2v19scheduler5EventENS2_5dequeIS7_NS2_9allocatorIS7_EEEEEESC_EEvRKNS_3PIDIT_EEMSE_FvT0_ET1_ENKUlPNS_11ProcessBaseEE_clESN_
    @        0x101ce5a21  process::ProcessBase::visit()
    @        0x101ce3747  process::ProcessManager::resume()
    @        0x101d0e243  _ZNSt3__114__thread_proxyINS_5tupleIJNS_10unique_ptrINS_15__thread_structENS_14default_deleteIS3_EEEEZN7process14ProcessManager12init_threadsEvE3$_0EEEEEPvSB_
    @     0x7fffbb5d693b  _pthread_body
    @     0x7fffbb5d6887  _pthread_start
    @     0x7fffbb5d608d  thread_start
[1]    73521 abort      ./build/src/mesos-execute --master=127.0.0.1:5050 --task=task.json
{noformat}

Removing the resource role field allows the task to execute.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-08-08 23:55:13.783,,,false,MESOS-7575,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 08 23:55:13 UTC 2017,,,,,,,"0|i3hqt3:",9223372036854775807,,,,,mcypark,,,,,,Mesosphere Sprint 61,,,,,,,,,,,1.0,,1.4.0,,,,,,,,,"31/Jul/17 12:49;bbannier;Review: https://reviews.apache.org/r/61253/diff/1#index_header","08/Aug/17 23:55;mcypark;{noformat}
commit a31dd52ab71d2a529b55cd9111ec54acf7550ded
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Tue Aug 8 16:00:31 2017 -0700

    Fixed `mesos-execute` help text related to task resources.

    With reservation refinement, `mesos-execute` requires input resources
    to be in the post-reservation-refinement format. This patch updates
    the help string to reflect this correctly.

    Review: https://reviews.apache.org/r/61253/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
fs::list drops path components on Windows,MESOS-7803,13087773,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,andschwa,andschwa,andschwa,17/Jul/17 23:24,06/Feb/18 22:50,29/Oct/20 16:32,06/Feb/18 22:50,,,,,,,,,1.6.0,,,,,,stout,,,,,0,mesosphere,windows,,,,,,,"fs::list(/foo/bar/*.txt) returns a.txt, b.txt, not /foo/bar/a.txt, /foo/bar/b.txt

This breaks a ZooKeeper test on Windows.",Windows 10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6713,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 17 22:05:38 UTC 2018,,,,,,,"0|i3hmpz:",9223372036854775807,,,,,kaysoky,,,,,,Mesosphere Sprint 74,,,,,,,,,,,2.0,,,,,,,,,,,"26/Sep/17 17:03;andschwa;{noformat}
73-    foreach (const string& jar, jars.get()) {
74-#ifdef __WINDOWS__
75-      // TODO(andschwa): `fs::list` on Windows only returns the file names, not
76:      // the full paths. Remove this work-around when MESOS-7803 is resolved.
77-      classpath += "";"" + path::join(zkHome, ""lib"", jar);
78-#else
79-      classpath += "":"" + jar;
80-#endif
81-    }
{noformat}","17/Jan/18 22:05;andschwa;https://reviews.apache.org/r/65397/
https://reviews.apache.org/r/65398/",,,,,,,,,,,,,,,,,,,,,,,,,,
Copy-n-paste error in slave/main.cpp,MESOS-7772,13085626,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,avinash.mesos,bbannier,bbannier,07/Jul/17 23:01,19/Jul/17 17:47,29/Oct/20 16:32,19/Jul/17 17:47,1.4.0,,,,,,,,1.4.0,,,,,,agent,,,,,0,coverity,mesosphere,,,,,,,"Coverity diagnosed a copy-n-paste error in {{slave/main.cpp}} (https://scan5.coverity.com/reports.htm#v10074/p10429/fileInstanceId=120155401&defectInstanceId=33592186&mergedDefectId=1414687+1+Comment),

{noformat}
323  } else if (flags.ip6.isSome()) {
CID 1414687 (#1 of 1): Copy-paste error (COPY_PASTE_ERROR)
copy_paste_error: ip in flags.ip looks like a copy-paste error.
   	Should it say ip6 instead?
324    os::setenv(""LIBPROCESS_IP6"", flags.ip.get());
325  }
{noformat}

We check the incorrect IP for some value here (check on {{ip6}}, but use of {{ip}}), and it seems extremely likely we intended to use {{flags.ip6}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-07-08 04:46:14.821,,,false,MESOS-1027,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 19 17:47:45 UTC 2017,,,,,,,"0|i3h9j3:",9223372036854775807,,,,,benjaminhindman,,,,,,Mesosphere Sprint 59,,,,,,,,,,,1.0,,1.4.0,,,,,,,,,"08/Jul/17 04:46;avinash.mesos;https://reviews.apache.org/r/60724/","19/Jul/17 17:47;avinash.mesos;commit edd8453c74441099348535a42397a694bd3f78a2
Author: Avinash sridharan avinash@mesosphere.io
Date:   Mon Jul 10 10:56:07 2017 -0700

Fixed initialization of `LIBPROCESS_IP6` on agent.

Review: https://reviews.apache.org/r/60724/",,,,,,,,,,,,,,,,,,,,,,,,,,
Persistent volume might not be mounted if there is a sandbox volume whose source is the same as the target of the persistent volume.,MESOS-7770,13085606,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,gilbert,jieyu,jieyu,07/Jul/17 21:40,11/Jul/17 22:06,29/Oct/20 16:32,11/Jul/17 22:06,1.1.2,1.2.1,1.3.0,,,,,,1.1.3,1.2.2,1.3.1,1.4.0,,,containerization,,,,,0,mesosphere,persistent-volumes,,,,,,,"This issue is only for Mesos Containerizer.

If the source of a sandbox volume is a relative path, we'll create the directory in the sandbox in Isolator::prepare method:
https://github.com/apache/mesos/blob/1.3.x/src/slave/containerizer/mesos/isolators/filesystem/linux.cpp#L480-L485

And then, we'll try to mount persistent volumes. However, because of this TODO in the code:
https://github.com/apache/mesos/blob/1.3.x/src/slave/containerizer/mesos/isolators/filesystem/linux.cpp#L726-L739

We'll skip mounting the persistent volume. That will cause a silent failure.

This is important because the workaround we suggest folks to solve MESOS-4016 is to use an additional sandbox volume.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-07-10 18:01:18.177,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 11 21:53:16 UTC 2017,,,,,,,"0|i3h9en:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 59,,,,,,,,,,,3.0,,1.1.3,1.2.2,1.3.1,1.4.0,,,,,,"10/Jul/17 18:01;gilbert;https://reviews.apache.org/r/60729/
https://reviews.apache.org/r/60750/","11/Jul/17 21:53;jieyu;commit c3632f67df4435f4dc3d9cb2d4d50db63aa8bcf8
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Jul 11 14:14:31 2017 -0700

    Added unit tests for persistent volume and host volume conflict issue.

    Review: https://reviews.apache.org/r/60750/

commit 2106fc7a2fd5f54e4a6454ba3cf7de023f732561
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Jul 11 14:14:28 2017 -0700

    Fixed persistent volume and host volume conflict issue.

    This is the fix for MESOS-7770. Basically, if a persistent volume
    and a host volume are both specified and the source path of the
    host volume is the same as the container path of the persistent
    volume, then the persistent volume will be skipped and is not
    mounted correctly. We should precisely check the mount table
    to determine whether the persistent volume is mounted or not.
    If not mounted, make sure we do mount the persistent volume.

    Review: https://reviews.apache.org/r/60729/",,,,,,,,,,,,,,,,,,,,,,,,,,
libprocess initializes to bind to random port if --ip is not specified,MESOS-7769,13085600,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,avinash.mesos,xujyan,xujyan,07/Jul/17 21:16,19/Jul/17 17:49,29/Oct/20 16:32,19/Jul/17 17:49,1.4.0,,,,,,,,1.4.0,,,,,,libprocess,,,,,0,mesosphere,,,,,,,,"When running current [HEAD|https://github.com/apache/mesos/commit/c90bea80486c089e933bef64aca341e4cfaaef25],

{noformat:title=without --ip}
./mesos-master.sh --work_dir=/tmp/mesos-test1
...
I0707 14:14:05.927870  5820 master.cpp:438] Master db2a2d26-a9a9-4e6f-9909-b9eca47a2862 (<host>) started on <addr>:36839
{noformat}

{noformat:title=with --ip}
./mesos-master.sh --ip=<addr> --work_dir=/tmp/mesos-test1
I0707 14:09:56.851483  5729 master.cpp:438] Master 963e0f42-9767-4629-8e3d-02c6ab6ad225 (<host>) started on <addr>:5050
{noformat}

It would be great this is caught by tests/CI.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-07-08 00:51:08.204,,,false,MESOS-1027,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 19 17:49:08 UTC 2017,,,,,,,"0|i3h9db:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 59,,,,,,,,,,,1.0,,1.4.0,,,,,,,,,"08/Jul/17 00:51;avinash.mesos;https://reviews.apache.org/r/60720/
https://reviews.apache.org/r/60723/","19/Jul/17 17:49;avinash.mesos;commit 78671375662f286128cd80d3b1f6586d0ec35cf8
Author: Avinash sridharan avinash@mesosphere.io
Date:   Fri Jul 7 16:34:23 2017 -0700

Fixed initialization of __address__ in the abscense of --ip flag.

When we introduced the __address6__ optional IPv6 storage into
libprocess we also introduced a regression because of which the port
doesn't get initialized to whatever is specified in the --port flag
until the --ip flag is specified.

This change fixes the initialization of the __address__ in the
absence of the --ip flag.

Review: https://reviews.apache.org/r/60720/



","19/Jul/17 17:49;avinash.mesos;commit 78671375662f286128cd80d3b1f6586d0ec35cf8
Author: Avinash sridharan avinash@mesosphere.io
Date:   Fri Jul 7 16:34:23 2017 -0700

Fixed initialization of __address__ in the abscense of --ip flag.

When we introduced the __address6__ optional IPv6 storage into
libprocess we also introduced a regression because of which the port
doesn't get initialized to whatever is specified in the --port flag
until the --ip flag is specified.

This change fixes the initialization of the __address__ in the
absence of the --ip flag.

Review: https://reviews.apache.org/r/60720/",,,,,,,,,,,,,,,,,,,,,,,,,
MasterTest.KillUnknownTask is failling due to a bug in `net::IPv4::ANY()`,MESOS-7765,13085333,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,avinash.mesos,avinash.mesos,avinash.mesos,06/Jul/17 21:46,07/Jul/17 04:49,29/Oct/20 16:32,07/Jul/17 04:48,,,,,,,,,1.4.0,,,,,,,,,,,0,mesosphere,,,,,,,,"Seeing the following failure when running `MasterTest.KillUnknownTask`:
```
I0706 14:08:20.724071 25596 sched.cpp:1041] Scheduler::statusUpdate took 19411ns
[libprotobuf FATAL google/protobuf/message_lite.cc:294] CHECK failed: IsInitialized(): Can't serialize message of type ""mesos.scheduler.Call"" because it is missing required fields: acknowledge.slave_id.value
libprocess: scheduler-5cca230e-e4c9-466e-b2cd-bde7b7d7ed71@127.0.0.1:44650 terminating due to CHECK failed: IsInitialized(): Can't serialize message of type ""mesos.scheduler.Call"" because it is missing required fields: acknowledge.slave_id.valueI0706 14:08:20.724196 25570 sched.cpp:2021] Asked to stop the driver
```

Looks we introduced a bug when we create the `net::IPv4` class. The `ANY` method of this class returns `INADDR_LOOPBACK` instead of `INADDR_ANY`. This ends up causing weird issues in terms of connectivity. We need to fix `net::IPv4::ANY` to return `INADDR_ANY`.",Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-1027,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 07 04:48:52 UTC 2017,,,,,,,"0|i3h7qv:",9223372036854775807,,,,,benjaminhindman,,,,,,Mesosphere Sprint 58,,,,,,,,,,,1.0,,1.4.0,,,,,,,,,"07/Jul/17 04:48;avinash.mesos;commit 952a7422c4f97289ccf633c746d12b54a36aea45
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Thu Jul 6 16:50:36 2017 -0700

    Fixed `net::IPv4` to return `INADDR_ANY` for `ANY()`.

    Review: https://reviews.apache.org/r/60692/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Master's agent removal rate limit also applies to agent unreachability.,MESOS-7721,13082411,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,,,bmahler,bmahler,26/Jun/17 05:17,25/Mar/19 16:29,29/Oct/20 16:32,,,,,,,,,,,,,,,,master,,,,,0,foundations,,,,,,,,"Currently, the implementation of partition awareness re-uses the {{--agent_removal_rate_limit}} when marking agents as unreachable. This means that partition aware frameworks are exposed to the agent removal rate limit, when they rather would like to see the information immediately and impose their own rate limiting.

Rather than waiting for non-partition-aware support to be removed (that may not occur for a long time) per MESOS-5948, we should instead fix the implementation so that unreachability does not get gated behind the agent removal rate limiting.

Marking this as a bug since from the user's perspective it doesn't behave as expected, there should be a separate flag for rate limiting unreachability marking, but likely unreachability marking does not need rate limiting, since the intention was for frameworks to impose their own rate limiting for replacing tasks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-5948,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-6394,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-06-26 05:17:50.0,,,,,,,"0|hzzxxc:00au",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test that master rejects requests to create refined reservations on a non-capable agent.,MESOS-7715,13081936,Bug,Open,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,mcypark,mcypark,22/Jun/17 22:09,29/Mar/19 06:39,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,mesosphere,resource-management,,,,,,,"This test was done manually for now, but we should write a test for it. Similar to {{CreateOperationValidationTest.AgentHierarchicalRoleCapability}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-7575,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-06-22 22:09:16.0,,,,,,,"0|i3gmu7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix agent downgrade for reservation refinement,MESOS-7714,13081933,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,mcypark,mcypark,mcypark,22/Jun/17 22:01,24/Aug/17 23:31,29/Oct/20 16:32,24/Aug/17 23:31,,,,,,,,,1.4.0,,,,,,,,,,,0,,,,,,,,,"The agent code only partially supports downgrading of an agent correctly.
The checkpointed resources are done correctly, but the resources within
the {{SlaveInfo}} message as well as tasks and executors also need to be downgraded
correctly and converted back on recovery.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-07-27 17:05:36.135,,,false,MESOS-7575,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 24 23:31:35 UTC 2017,,,,,,,"0|hzzyic:zz",9223372036854775807,,,,,xujyan,,,,,,Mesosphere Sprint 61,Mesosphere Sprint 62,,,,,,,,,,8.0,,1.4.0,,,,,,,,,"27/Jul/17 17:05;xujyan;[~mcypark] [~bmahler] Just to be sure. This ticket is not for supporting downgrade of an agent from 1.4 to <= 1.3.x right? Could you clarify?","27/Jul/17 17:13;mcypark;[~xujyan]: This *is* for the downgrade of 1.4 to <= 1.3.x.","27/Jul/17 18:08;xujyan;Great. I guess I am just not clear on the mechanism to achieve that.

Downgrading currently fails [the CHECK here|https://github.com/apache/mesos/blob/1.3.0/src/slave/paths.cpp#L478] for me (from a 1.4 agent with persistent volumes). It looks like the proposal is to commit some downgrading logic to 1.3.x branch? Sorry it's not clear to me if the case I am mentioning is covered.","28/Jul/17 23:40;mcypark;Ah okay. First, the support is for downgrading a 1.4 agent to <= 1.3.x agent as long as refined reservations have not been made yet.
The way we achieve this is to ""downgrade"" all the resources that get checkpointed in the ""pre-reservation-refinement"" format
as long as none of them have refined reservations. The reason why that {{CHECK}} would fail would be either (1) there are refined
reservations made on the 1.4 agent, or (2) there are resources that we didn't checkpoint in the ""pre-reservation-refinement"" when
we should have. The goal of this ticket is to fix (2).","28/Jul/17 23:56;xujyan;I see, but 1) is a real operational concern for upgrading to 1.4 right? I wouldn't want to upgrade my agents to 1.4 knowing I won't be able to roll them back once refined reservations are made (i.e., after they are used for a while)...

I think to support 1) we have to support 'pre-reservation-refinement' for a while (across 1.x versions?)

https://github.com/apache/mesos/blob/master/docs/versioning.md#upgrades mentions upgrades but not downgrades but I don't see how it would work if downgrades are not implicitly covered by the same guarantee...

Thoughts?","29/Jul/17 00:14;mcypark;In order to support (1) I think we'd have to checkpoint resources with refined reservations in a different location.
You're saying you wouldn't want to upgrade to 1.4 because you can't downgrade once people start using new features?
Just for comparison, we have the same limitations for multi-role support. That is, once you upgrade to 1.3 and
and frameworks start using multi-role, you can't downgrade.","29/Jul/17 01:16;xujyan;I mean when we are not using new features, so this appears to be 2). I didn't know the details until I just read the design doc and saw that you mentioned about the agent ""On disk (checkpointing), it will also generally use the new Resources format, except for resources with a single dynamic reservation it will continue to checkpoint in the old Resource format.""","29/Jul/17 01:21;mcypark;Ah, yes, and this ticket is for (2). Seems like we're on the same page now?","31/Jul/17 17:49;xujyan;Yes we are. Thanks! Hope we can prioritize this one (possibly over other 1.4 blockers) so we can promote dev versions of 1.4 further for more thorough testing.","09/Aug/17 19:00;xujyan;[~mcypark] did you get a chance to work on this?","17/Aug/17 01:21;adam-mesos;Downgrading from blocker because [~mcypark] says ""I think we’ll have to ship without it"".
Please retarget to 1.4.1 and/or 1.5.0 so [~karya] and [~anandmazumdar] can cut 1.4.0-rc1","24/Aug/17 08:07;mcypark;https://reviews.apache.org/r/61880/","24/Aug/17 23:31;mcypark;{noformat}
commit a955458dfec77967df8437805f15f5ed81c16b5c
Author: Michael Park <mpark@apache.org>
Date:   Thu Aug 24 15:25:49 2017 -0700

    Fixed agent downgrades for reservation refinement.

    Previously, `checkpoint(path, resources)` was overloaded such that it
    would automatically downgrade the resources before being checkpointed
    on the agent. However, `checkpoint(path, protobuf_containing_resources)`
    did not work correctly since we didn't recursively look within
    the messages to downgrade the resources. Ideally, we would use
    protobuf reflection to ensure that these are handled automatically.
    For now, we attempt to get all of the places where resources are
    present within a message.

    Review: https://reviews.apache.org/r/61880/
{noformat}",,,,,,,,,,,,,,,
Prevent non-RESERVATION_REFINEMENT frameworks from refining reservations.,MESOS-7705,13081610,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,mcypark,mcypark,22/Jun/17 00:52,29/Mar/19 06:38,29/Oct/20 16:32,,,,,,,,,,,,,,,,master,,,,,0,mesosphere,resource-management,,,,,,,"We output the ""endpoint"" format through the endpoints
for backward compatibility of external tooling. A framework should be
able to use the result of an endpoint and pass it back to Mesos,
since the result was produced by Mesos. This is especially applicable
to the V1 API. We also allow the ""pre-reservation-refinement"" format
because existing ""resources files"" are written in that format, and
they should still be usable without modification.

This is probably too flexible however, since a framework without
a RESERVATION_REFINEMENT capability could make refined reservations
using the ""post-reservation-refinement"" format, although they wouldn't be
offered such resources. It still seems undesirable if anyone were to
run into it, and we should consider adding sensible restrictions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-07-01 06:32:36.022,,,false,MESOS-7575,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 08 19:03:15 UTC 2018,,,,,,,"0|i3gktr:",9223372036854775807,,,,,bmahler,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"01/Jul/17 06:32;bmahler;Marking as a blocker for 1.4.","22/Dec/17 22:59;jieyu;[~bmahler], [~mcypark], is this a blocker for 1.5.0? If not, can you retarget?","22/Dec/17 23:11;bmahler;[~mcypark] re-targeting for 1.5.1.","17/Apr/18 22:39;gilbert;[~mcypark][~bmahler], could we re-target this to 1.5.2?","19/Apr/18 22:50;mcypark;[~gilbert] 1.6.0 is probably more appropriate.","08/Aug/18 19:03;chhsia0;Untargeting this ticket because it seems not urgent anymore.",,,,,,,,,,,,,,,,,,,,,,
Docker image with universal containerizer does not work if WORKDIR is missing in the rootfs.,MESOS-7652,13078820,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,gilbert,realmbgl,realmbgl,10/Jun/17 02:39,17/Aug/17 08:28,29/Oct/20 16:32,17/Aug/17 08:28,1.2.1,,,,,,,,1.2.3,1.3.2,1.4.0,,,,containerization,,,,,0,mesosphere,,,,,,,,"hello,
used the following docker image recently

quay.io/spinnaker/front50:master
https://quay.io/repository/spinnaker/front50

Here the link to the Dockerfile
https://github.com/spinnaker/front50/blob/master/Dockerfile

and here the source
{color:blue}FROM java:8

MAINTAINER delivery-engineering@netflix.com

COPY . workdir/

WORKDIR workdir

RUN GRADLE_USER_HOME=cache ./gradlew buildDeb -x test && \
  dpkg -i ./front50-web/build/distributions/*.deb && \
  cd .. && \
  rm -rf workdir

CMD [""/opt/front50/bin/front50""]{color}


The image works fine with the docker containerizer, but the universal containerizer shows the following in stderr.

""Failed to chdir into current working directory '/workdir': No such file or directory""

The problem comes from the fact that the Dockerfile creates a workdir but then later removes the created dir as part of a RUN. The docker containerizer has no problem with it if you do

docker run -ti --rm quay.io/spinnaker/front50:master bash

you get into the working dir, but the universal containerizer fails with the error.

thanks for your help,
Michael",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-08-11 16:01:52.201,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 17 08:28:30 UTC 2017,,,,,,,"0|hzzyj0:zzi",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 60,Mesosphere Sprint 61,,,,,,,,,,3.0,,1.2.3,1.3.2,1.4.0,,,,,,,"12/Jul/17 17:26;realmbgl;Researched this some more, found the following spot in the mesos code.

https://github.com/apache/mesos/blob/35dd2b600b8af0204d03c4ee5348a1a6672b136c/src/slave/containerizer/mesos/launch.cpp
line 567

The Dockerfile reference says for WORKDIR the following.

""
...
If the WORKDIR doesn’t exist, it will be created even if it’s not used in any subsequent Dockerfile instruction.
""

Looks like the create is missing in the code if it does not exist.

","11/Aug/17 16:01;karya;[~gilbert]: Retargeting it to 1.5.0. Please revert if you see fit.","11/Aug/17 16:03;jieyu;[~karya] Re-targeted this to 1.4. IMO, this is a blocker for 1.4. cc [~gilbert]","11/Aug/17 16:05;karya;Thanks [~jieyu]!","12/Aug/17 01:05;gilbert;https://reviews.apache.org/r/61602/","17/Aug/17 08:28;gilbert;commit 917b229219338846427efd1f1f2c5906c7eb238f
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Fri Aug 11 17:52:18 2017 -0700

    Fixed mesos containerizer to support docker image WORKDIR missing.
    
    Some docker image may have 'WORKDIR' set in its manifest but that
    'WORKDIR' does not exist in the image rootfs (e.g., the workdir
    is removed in the following dockerfile).
    
    From the reference of dockerfile, ""If the WORKDIR doesn’t exist,
    it will be created even if it’s not used in any subsequent
    Dockerfile instruction"". So we should create the working directory
    if it does not exist in the image's rootfs.
    
    Review: https://reviews.apache.org/r/61602",,,,,,,,,,,,,,,,,,,,,,
Introduce a heartbeat mechanism for v1 HTTP executor <-> agent communication.,MESOS-7564,13074700,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Done,kaysoky,anandmazumdar,anandmazumdar,25/May/17 00:24,11/Apr/19 00:09,29/Oct/20 16:32,14/Dec/18 21:38,,,,,,,,,1.8.0,,,,,,agent,executor,,,,0,api,foundations,mesosphere,v1_api,,,,,"Currently, we do not have heartbeats for executor <-> agent communication. This is especially problematic in scenarios when IPFilters are enabled since the default conntrack keep alive timeout is 5 days. When that timeout elapses, the executor doesn't get notified via a socket disconnection when the agent process restarts. The executor would then get killed if it doesn't re-register when the agent recovery process is completed.

Enabling application level heartbeats or TCP KeepAlive's can be a possible way for fixing this issue.

We should also update executor API documentation to explain the new behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-540,MESOS-8366,MESOS-9258,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-10-26 01:31:28.037,,,false,MESOS-7563,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 14 21:35:09 UTC 2018,,,,,,,"0|i00a3k:",9223372036854775807,,,,,,,,,,,Mesos Foundation R7 Sprint 33,Mesos Foundations R8 Sprint 34,Mesos Foundations R8 Sprint 35,,,,,,,,,5.0,,,,,,,,,,,"26/Oct/18 01:31;greggomann;I think that luckily the backward-compatibility story of this change should be fairly simple; executors can opt-in to heartbeats in their SUBSCRIBE call, either with a special field or by specifying the correct Content-Type.

We've had some users with CNI deployments that close executor <=> agent connections after extended periods of inactivity, so this fix will be prioritized soon.","15/Nov/18 02:05;kaysoky;Historically, we've considered the agent<->executor connection to be reliable.  This is evident when you look at the agent's lack of handling for executor disconnections.  Currently, if an HTTP executor successfully registers, and then closes its connection, the agent will consider the executor ""RUNNING"".  The agent will then merrily send all sorts of messages over the broken connection (and onto the floor), including LaunchTask messages.  The agent might log warnings, but it does not attempt to reconnect (it can't).  (The PID executor does not have this problem, because libprocess will make transient connections to send messages if the persistent connection breaks.)

If we are considering the agent<->executor connection to be unreliable, we first need to add/test logic to handle executor disconnections.  I believe it may be sufficient to detect (even belatedly) disconnections on the agent, and transition the agent's view of the executor from RUNNING to REGISTERING and start the registration timeout.  This would only be necessary for HTTP executors.

-----

Next to handle cases where the connection is ""connected"" but dropping packets...   We will probably want to add heartbeats in both directions.

Just on the HTTP executor library, we have two connections to consider:
1) The SUBSCRIBE Call is one persistent connection where the executor sends one Call, and receives a stream of Events.  There is currently no Executor->Agent traffic except the first request.  This connection could probably use heartbeating in both directions.  Agent->Executor heartbeats may come in the form of Events.  Executor->Agent heartbeats will need to be something else (like the heartbeating suggested here: https://reviews.apache.org/r/69183/ ).

2) Other calls go through a secondary connection.  This persistent connection is used to send any number of Calls and their subsequent responses (202 Accepted) back.  When the executor discovers a disconnection here, it remakes both connections.  This connection does not need heartbeating or monitoring.
","27/Nov/18 19:37;vinodkone;{quote}

1) The SUBSCRIBE Call is one persistent connection where the executor sends one Call, and receives a stream of Events. There is currently no Executor->Agent traffic except the first request. This connection could probably use heartbeating in both directions. Agent->Executor heartbeats may come in the form of Events. Executor->Agent heartbeats will need to be something else (like the heartbeating suggested here: [https://reviews.apache.org/r/69183/] ).

{quote}

Do we really need heartbeats in both directions given it is a single connection? I would imagine agent -> executor heartbeat events should be enough like we did with v1 scheduler API?

 ","27/Nov/18 21:10;kaysoky;I guess I'll summarize a bit of the discussion that happened in the API WG.

The current plan is to add some regular traffic to any persistent connections between agent and executor, so that the connection does not get marked ""stale"".  We want to make a minimal change first, to maintain backwards compatibility between new/old agents and new/old executors.  Since there are two persistent connections, we want to add Heartbeat Events from Agent to Executor, and Heartbeat Calls from Executor to Agent.  Neither agent nor executor will expect heartbeats (i.e. they won't disconnect if heartbeats don't appear).  Unfortunately, in the case of old agents/executors, when they receive an unknown Call/Event, they will log a warning.","27/Nov/18 22:11;kaysoky;Here are the proposed protobuf changes: https://reviews.apache.org/r/69463/","14/Dec/18 21:35;greggomann;{code}
commit d82075bef6bb52e135427ff4916f684af4f9226b
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Thu Dec 13 16:34:21 2018 -0800

    Added HEARTBEAT events and calls for the executor HTTP API.

    These new messages are meant to be backwards compatible, in that
    they won't cause crashes when new executors send heartbeats to old
    agents, or new agents send heartbeats to old executors.  All recipients
    of these heartbeats are currently expected to ignore them, as their
    only purpose is to keep certain connections from being marked ""stale""
    by network intermediaries.

    Review: https://reviews.apache.org/r/69463/
{code}

{code}
commit ba46deb2ba31bd7f3d9bff3db979a1e850eedf0c
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Thu Dec 13 16:34:24 2018 -0800

    Refactored master and agent streaming connections.

    This moves the very similar `HttpConnection` classes inside the
    master and agent into a common header.  The refactored
    `StreamingHttpConnection<Event>` is more explicitly named to avoid
    potentially clashing with the libprocess HTTP helpers.

    This also moves the master's heartbeater helper into a new header
    and transforms it into an RAII libprocess actor wrapper.  The
    heartbeater depends on this `StreamingHttpConnection` and is currently
    used by the master for heartbeating the operator event stream
    and HTTP framework connection.  A later patch will use this heartbeater
    for agent->executor heartbeats.

    Review: https://reviews.apache.org/r/69472/
{code}

{code}
commit a47c7dea6ccb3558464219f3c6edf376b2f55086
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Thu Dec 13 16:34:36 2018 -0800

    Added heartbeaters for agent and HTTP executors.

    This implements two separate heartbeaters for Executor Events (agent
    to executor) and Executor Calls (executor to agent).  Both are set to
    non-configurable intervals of 30 minutes, which should be sufficient
    to keep the connections alive while not flooding logs with warnings
    if the executor/agent does not have this patch.

    Review: https://reviews.apache.org/r/69473/
{code}

{code}
commit 828a28cec699e11d16006a6596b9f88ff75c55c0
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Thu Dec 13 16:34:43 2018 -0800

    Added tests for agent/executor heartbeating.

    This adds two separate tests which check if the Agent sends heartbeats
    to HTTP executors, and if the HTTP executor driver sends heartbeats
    to the agent.

    Review: https://reviews.apache.org/r/69474/
{code}",,,,,,,,,,,,,,,,,,,,,,
Command checks via agent lead to flaky tests.,MESOS-7500,13071124,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,abudnik,alexr,alexr,11/May/17 10:34,24/May/18 09:18,29/Oct/20 16:32,04/Oct/17 12:03,,,,,,,,,1.5.0,,,,,,,,,,,0,check,flaky-test,health-check,mesosphere,,,,,Tests that rely on command checks via agent are flaky on Apache CI. Here is an example from one of the failed run: https://pastebin.com/g2mPgYzu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-7578,MESOS-8949,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-05-26 21:12:55.541,,,false,MESOS-5916,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 04 12:03:26 UTC 2017,,,,,,,"0|hzzypx:",9223372036854775807,,,,,alexr,,,,,,Mesosphere Sprint 56,Mesosphere Sprint 65,,,,,,,,,,8.0,,,,,,,,,,,"26/May/17 21:12;gkleiman;The failures seem to be related to the agent not being able to attach to the DEBUG container launched by the health checker.

This is however not really necessary for checks, so I created a [design document|https://docs.google.com/document/d/1YCMtH8i2-ovTVtKDsCTrXdygS7ieaSrJLVnFbR66qfA/] with two proposals that'd make it possible to start DEBUG containers without an I/O switchboard.","21/Sep/17 13:52;abudnik;Another example from the failed run, including debug output (https://reviews.apache.org/r/59107):
https://pastebin.com/iKA1WaZB","25/Sep/17 13:05;abudnik;The issue is caused by recompilation/relinking of an executable by libtool wrapper script. E.g. when we launch `mesos-io-switchboard` for the first time, executable might be missing, so wrapper script starts to compile/link corresponding executable. On slow machines compilation takes quite a while, hence these tests become flaky.

One possible solution is to pass [\-\-enable-fast-install=no (--disable-fast-install)|http://mdcc.cx/pub/autobook/autobook-latest/html/autobook_85.html] as $CONFIGURATION environment variable into docker helper script.","28/Sep/17 11:38;abudnik;https://reviews.apache.org/r/62661/","28/Sep/17 12:22;abudnik;Command health checks are executed via `LAUNCH_NESTED_CONTAINER_SESSION` call and launched inside DEBUG container.
DEBUG container is always launched in pair with `mesos-io-switcboard` process. After spawning `mesos-io-switcboard` agent tries to connect to it via unix domain socket. If DEBUG container exits before `mesos-io-switcboard` exits, agent sends SIGTERM to switchboard process after 5 second delay. If `mesos-switchboard-process` exits after being killed by signal, then `LAUNCH_NESTED_CONTAINER_SESSION` call is considered to be failed as well as corresponding health check.
It turned out that `mesos-io-switchboard` is not an executable, but a special wrapper script generated by libtool. First time this script is executed, relinking of an executable triggered. Relinking takes quite a while on slow machines (e.g. in Apache CI): I've seen 8 seconds and more. It turned out, that when DEBUG container exits, agent sends SIGTERM (as described above) to a process which is still being relinking. This happens each time health check is launched and as the result we see a bunch of failed tests in Apache CI.
To fix this issue we need to force libtool/autotools to generate binary instead of wrapper script, see:
1. https://autotools.io/libtool/wrappers.html
2. `info libtool`","28/Sep/17 12:51;abudnik;Example of related failing tests:
[ FAILED ] CommandExecutorCheckTest.CommandCheckDeliveredAndReconciled
[ FAILED ] CommandExecutorCheckTest.CommandCheckStatusChange
[ FAILED ] DefaultExecutorCheckTest.CommandCheckDeliveredAndReconciled
[ FAILED ] DefaultExecutorCheckTest.CommandCheckStatusChange
[ FAILED ] DefaultExecutorCheckTest.CommandCheckSeesParentsEnv
[ FAILED ] DefaultExecutorCheckTest.CommandCheckSharesWorkDirWithTask","04/Oct/17 12:03;alexr;{noformat}
Commit: d863620e5cb82b7f22cade0da0a0d18afbdf9136 [d863620]
Author: Andrei Budnik abudnik@mesosphere.com
Date: 4 October 2017 at 08:00:09 GMT-4
Committer: Alexander Rukletsov alexr@apache.org

Added --disable-libtool-wrapper configuration to Mesos.

This flag is used to force libtool to generate executables instead of
wrapper scripts. A wrapper script might trigger relinking, which takes
quite a while on slow machines, thus causing failure of tests.

Review: https://reviews.apache.org/r/62661/
{noformat}",,,,,,,,,,,,,,,,,,,,,
Provisioner recover should not always assume 'rootfses' dir exists.,MESOS-7471,13070132,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,gilbert,gilbert,gilbert,08/May/17 19:40,17/Aug/17 22:54,29/Oct/20 16:32,09/May/17 16:50,,,,,,,,,1.1.2,1.2.1,1.3.0,1.4.0,,,provisioner,,,,,0,provisioner,,,,,,,,"The mesos agent would restart due to many reasons (e.g., disk full). Always assume the provisioner 'rootfses' dir exists would block the agent to recover.

{noformat}
Failed to perform recovery: Collect failed: Unable to list rootfses belonged to container a30b74d5-53ac-4fbf-b8f3-5cfba58ea847: Unable to list the backend directory: Failed to opendir '/var/lib/mesos/slave/provisioner/containers/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847/backends/overlay/rootfses': No such file or directory
{noformat}

This issue may occur due to the race between removing the provisioner container dir and the agent restarts:
{noformat}
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.058349 11441 linux_launcher.cpp:429] Launching container a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 and cloning with namespaces CLONE_NEWNS | CLONE_NEWPID
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.072191 11441 systemd.cpp:96] Assigned child process '11577' to 'mesos_executors.slice'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.075932 11439 containerizer.cpp:1592] Checkpointing container's forked pid 11577 to '/var/lib/mesos/slave/meta/slaves/36a25adb-4ea2-49d3-a195-448cff1dc146-S34/frameworks/6dd898d6-7f3a-406c-8ead-24b4d55ed262-0008/executors/node__fc5e0825-f10e-465c-a2e2-938b9dc3fe05/runs/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847/pids/forked.pid'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.081516 11438 linux_launcher.cpp:429] Launching container 03a57a37-eede-46ec-8420-dda3cc54e2e0 and cloning with namespaces CLONE_NEWNS | CLONE_NEWPID
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.083516 11438 systemd.cpp:96] Assigned child process '11579' to 'mesos_executors.slice'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.087345 11444 containerizer.cpp:1592] Checkpointing container's forked pid 11579 to '/var/lib/mesos/slave/meta/slaves/36a25adb-4ea2-49d3-a195-448cff1dc146-S34/frameworks/36a25adb-4ea2-49d3-a195-448cff1dc146-0002/executors/66897/runs/03a57a37-eede-46ec-8420-dda3cc54e2e0/pids/forked.pid'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: W0505 02:14:32.213049 11440 fetcher.cpp:896] Begin fetcher log (stderr in sandbox) for container 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac from running command: /opt/mesosphere/packages/mesos--aaedd03eee0d57f5c0d49c74ff1e5721862cad98/libexec/mesos/mesos-fetcher
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.006201 11561 fetcher.cpp:531] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/slaves\/36a25adb-4ea2-49d3-a195-448cff1dc146-S34\/root"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""https:\/\/downloads.mesosphere.com\/libmesos-bundle\/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz""}},{""action"":""BYPASS_CACHE"",
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.009678 11561 fetcher.cpp:442] Fetching URI 'https://downloads.mesosphere.com/libmesos-bundle/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.009693 11561 fetcher.cpp:283] Fetching directly into the sandbox directory
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.009711 11561 fetcher.cpp:220] Fetching URI 'https://downloads.mesosphere.com/libmesos-bundle/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz'
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.009723 11561 fetcher.cpp:163] Downloading resource from 'https://downloads.mesosphere.com/libmesos-bundle/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz' to '/var/lib/mesos/slave/slaves/36a25adb-4ea2-49d3-a195-448cff1dc146-S34/frameworks/6dd898d6-7f3a-406c-8ead-24b4d55ed262-0011/executors/hello__91922a16-889e-4e94-9dab-9f6754f091de/
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: Failed to fetch 'https://downloads.mesosphere.com/libmesos-bundle/libmesos-bundle-1.9.0-rc2-1.2.0-rc2-1.tar.gz': Error downloading resource: Failed writing received data to disk/application
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: End fetcher log for container 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: E0505 02:14:32.213114 11440 fetcher.cpp:558] Failed to run mesos-fetcher: Failed to fetch all URIs for container '6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac' with exit status: 256
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: E0505 02:14:32.213351 11444 slave.cpp:4642] Container '6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac' for executor 'hello__91922a16-889e-4e94-9dab-9f6754f091de' of framework 6dd898d6-7f3a-406c-8ead-24b4d55ed262-0011 failed to start: Failed to fetch all URIs for container '6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac' with exit status: 256
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.213614 11443 containerizer.cpp:2071] Destroying container 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac in FETCHING state
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.213977 11443 linux_launcher.cpp:505] Asked to destroy container 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.214757 11443 linux_launcher.cpp:548] Using freezer to destroy cgroup mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.216047 11444 cgroups.cpp:2692] Freezing cgroup /sys/fs/cgroup/freezer/mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.218407 11443 cgroups.cpp:1405] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac after 2.326016ms
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.220391 11445 cgroups.cpp:2710] Thawing cgroup /sys/fs/cgroup/freezer/mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.222124 11445 cgroups.cpp:1434] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac after 1.693952ms
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: E0505 02:14:32.239018 11441 fetcher.cpp:558] Failed to run mesos-fetcher: Failed to create 'stdout' file: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: E0505 02:14:32.239162 11442 slave.cpp:4642] Container 'a30b74d5-53ac-4fbf-b8f3-5cfba58ea847' for executor 'node__fc5e0825-f10e-465c-a2e2-938b9dc3fe05' of framework 6dd898d6-7f3a-406c-8ead-24b4d55ed262-0008 failed to start: Failed to create 'stdout' file: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.239284 11445 containerizer.cpp:2071] Destroying container a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 in FETCHING state
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.239390 11444 linux_launcher.cpp:505] Asked to destroy container a30b74d5-53ac-4fbf-b8f3-5cfba58ea847
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.240103 11444 linux_launcher.cpp:548] Using freezer to destroy cgroup mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.241353 11440 cgroups.cpp:2692] Freezing cgroup /sys/fs/cgroup/freezer/mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.243120 11444 cgroups.cpp:1405] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 after 1.726976ms
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.245045 11440 cgroups.cpp:2710] Thawing cgroup /sys/fs/cgroup/freezer/mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.246800 11440 cgroups.cpp:1434] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 after 1.715968ms
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.285477 11438 slave.cpp:1625] Got assigned task 'dse-1-agent__720d6f09-9d60-4667-b224-abcd495e0e58' for framework 6dd898d6-7f3a-406c-8ead-24b4d55ed262-0009
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: F0505 02:14:32.296481 11438 slave.cpp:6381] CHECK_SOME(state::checkpoint(path, info)): Failed to create temporary file: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: *** Check failure stack trace: ***
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: @     0x7f5856be857d  google::LogMessage::Fail()
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: @     0x7f5856bea3ad  google::LogMessage::SendToLog()
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: @     0x7f5856be816c  google::LogMessage::Flush()
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: @     0x7f5856beaca9  google::LogMessageFatal::~LogMessageFatal()
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: @     0x7f5855e4b5e9  _CheckFatal::~_CheckFatal()
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.314082 11445 containerizer.cpp:2434] Container 6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac has exited
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.314826 11440 containerizer.cpp:2434] Container a30b74d5-53ac-4fbf-b8f3-5cfba58ea847 has exited
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[17142]: Failed to write: No space left on device
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316660 11439 container_assigner.cpp:101] Unregistering container_id[value: ""6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac""].
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316761 11474 container_assigner_strategy.cpp:202] Closing ephemeral-port reader for container[value: ""6aebb9e0-fd2c-4a42-b8f4-bd6ba11e9eac""] at endpoint[198.51.100.1:34273].
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316804 11474 container_reader_impl.cpp:38] Triggering ContainerReader shutdown
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316833 11474 sync_util.hpp:39] Dispatching and waiting <=5s for ticket 7: ~ContainerReaderImpl:shutdown
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316769 11439 container_assigner.cpp:101] Unregistering container_id[value: ""a30b74d5-53ac-4fbf-b8f3-5cfba58ea847""].
May 05 02:14:32 ip-172-31-7-83.us-west-2.compute.internal mesos-agent[11432]: I0505 02:14:32.316864 11474 container_reader
{noformat}

In provisioner recover, when listing the container rootfses, it is possible that the 'rootfses' dir does not exist. Because a possible race between the provisioner destroy and the agent restart. For instance, while the provisioner is destroying the container dir the agent restarts. Due to os::rmdir() is recursive by traversing the FTS tree, it is possible that 'rootfses' dir is removed but the others (e.g., scratch dir) are not.

Currently, we are returning an error if the 'rootfses' dir does not exist, which blocks the agent from recovery. We should skip it if 'rootfses' does not exist.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-05-09 07:58:05.979,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 09 07:58:05 UTC 2017,,,,,,,"0|i3enq7:",9223372036854775807,,,,,jieyu,,,,,,,,,,,,,,,,,2.0,,1.1.2,1.2.1,1.3.0,1.4.0,,,,,,"08/May/17 19:51;gilbert;https://reviews.apache.org/r/59061/","09/May/17 07:58;jieyu;commit 64cb24fc75ef41e0760649891ea1bb62d6fe9b11
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue May 9 09:56:13 2017 +0200

    Fixed provisioner recover blockage by non-existing rootfses dir.

    In provisioner recover, when listing the container rootfses, it is
    possible that the 'rootfses' dir does not exist. Because a possible
    race between the provisioner destroy and the agent restart. For
    instance, while the provisioner is destroying the container dir the
    agent restarts. Due to os::rmdir() is recursive by traversing the
    FTS tree, it is possible that 'rootfses' dir is removed but the
    others (e.g., scratch dir) are not.

    Currently, we are returning an error if the 'rootfses' dir does not
    exist, which blocks the agent from recovery. We should skip it if
    'rootfses' does not exist.

    Review: https://reviews.apache.org/r/59061/",,,,,,,,,,,,,,,,,,,,,,,,,,
Double free or corruption when using parallel test runner,MESOS-7438,13067839,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Cannot Reproduce,,bmahler,bmahler,28/Apr/17 22:42,29/Apr/19 09:25,29/Oct/20 16:32,20/Feb/19 14:16,,,,,,,,,,,,,,,test,,,,,0,integration,mesosphere,tech-debt,,,,,,"I observed the following when using the parallel test runner:

{noformat}
/home/bmahler/git/mesos/build/../support/mesos-gtest-runner.py --sequential=*ROOT_* ./mesos-tests
..
*** Error in `/home/bmahler/git/mesos/build/src/.libs/mesos-tests': double free or corruption (out): 0x00007fa818001310 ***
======= Backtrace: =========
/usr/lib64/libc.so.6(+0x7c503)[0x7fa87f27e503]
/usr/lib64/libsasl2.so.3(+0x866d)[0x7fa880f0d66d]
/usr/lib64/libsasl2.so.3(sasl_dispose+0x3b)[0x7fa880f1075b]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN5mesos8internal8cram_md527CRAMMD5AuthenticateeProcessD1Ev+0x5d)[0x7fa88708f67d]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN5mesos8internal8cram_md527CRAMMD5AuthenticateeProcessD0Ev+0x18)[0x7fa88708f734]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN5mesos8internal8cram_md520CRAMMD5AuthenticateeD1Ev+0xfb)[0x7fa88708a065]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN5mesos8internal8cram_md520CRAMMD5AuthenticateeD0Ev+0x18)[0x7fa88708a0b4]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN5mesos8internal5slave5Slave13_authenticateEv+0x67)[0x7fa8879ff579]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZZN7process8dispatchIN5mesos8internal5slave5SlaveEEEvRKNS_3PIDIT_EEMS6_FvvEENKUlPNS_11ProcessBaseEE_clESD_+0xe2)[0x7fa887a60b7a]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos8internal5slave5SlaveEEEvRKNS0_3PIDIT_EEMSA_FvvEEUlS2_E_E9_M_invokeERKSt9_Any_dataS2_+0x37)[0x7fa887aa0efe]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZNKSt8functionIFvPN7process11ProcessBaseEEEclES2_+0x49)[0x7fa8888d1177]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN7process11ProcessBase5visitERKNS_13DispatchEventE+0x2f)[0x7fa8888b5063]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZNK7process13DispatchEvent5visitEPNS_12EventVisitorE+0x2e)[0x7fa8888c0422]
/home/bmahler/git/mesos/build/src/.libs/mesos-tests(_ZN7process11ProcessBase5serveERKNS_5EventE+0x2e)[0xb088c8]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN7process14ProcessManager6resumeEPNS_11ProcessBaseE+0x525)[0x7fa8888b10d5]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(+0x5f1a880)[0x7fa8888ad880]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(+0x5f2ca8a)[0x7fa8888bfa8a]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(+0x5f2c9ce)[0x7fa8888bf9ce]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(+0x5f2c958)[0x7fa8888bf958]
/usr/lib64/libstdc++.so.6(+0xb5230)[0x7fa87fb90230]
/usr/lib64/libpthread.so.0(+0x7dc5)[0x7fa88040ddc5]
/usr/lib64/libc.so.6(clone+0x6d)[0x7fa87f2f973d]
{noformat}

Not sure how reproducible this is, appears to occur in the authentication path of the agent.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-02-20 14:16:49.739,,,false,MESOS-4809,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 20 14:16:49 UTC 2019,,,,,,,"0|hzzyak:i0dr",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,"20/Feb/19 14:16;tillt;[~bmahler] do we still see this? If so please re-open.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Registry puller cannot fetch manifests from Google GCR: 403 Forbidden.,MESOS-7431,13067228,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,chhsia0,chhsia0,chhsia0,27/Apr/17 02:55,16/May/17 17:52,29/Oct/20 16:32,16/May/17 03:33,,,,,,,,,1.3.0,,,,,,containerization,,,,,0,,,,,,,,,"When the registry puller is pulling a repository from Google's GCE Container Registry, a '403 Forbidden' error occurs instead of 401 when fetching manifests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-7481,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-04-27 03:58:20.521,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 16 03:29:09 UTC 2017,,,,,,,"0|i3e5t3:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 57,,,,,,,,,,,5.0,,1.3.0,,,,,,,,,"27/Apr/17 03:58;mcypark;[~chhsiao], [~jieyu]: Re-targeting this for 1.4.0. Please let me know if this is a blocker for 1.3.0.","01/May/17 19:25;chhsia0;https://reviews.apache.org/r/58778/","09/May/17 18:24;gilbert;Changed this issue as a blocker for 1.3.0. We will land it asap.

[~mcypark]^^","16/May/17 03:29;jieyu;commit 43cd04e1472f285d79041090b66c9625c6b25ae8
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date:   Mon May 15 20:10:50 2017 -0700

    Supported GCE container registry.

    Certain registries, such as GCE registry, reply 403 instead of 401 for
    unauthorized requests. When fetching image manifests and blobs, instead
    of sending out unauthorized requests first and waiting for a possible
    401, we should always look up the docker config and send requests with
    basic authorization when possible.

    Review: https://reviews.apache.org/r/58778/",,,,,,,,,,,,,,,,,,,,,,,,
Running DOCKER images in Mesos Container Runtime without `linux/filesystem` isolation enabled renders host unusable,MESOS-7374,13062765,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,chhsia0,timcharper,timcharper,10/Apr/17 02:57,02/Aug/17 23:41,29/Oct/20 16:32,02/Aug/17 23:41,1.2.0,,,,,,,,1.4.0,,,,,,containerization,,,,,0,containerizer,mesosphere,,,,,,,"If I run the pod below (using Marathon 1.4.2) against a mesos agent that has the flags (also below), then the overlay filesystem replaces the system root mount, effectively rendering the host unusable until reboot.

flags:

- {{--containerizers mesos,docker}}
- {{--image_providers APPC,DOCKER}}
- {{--isolation cgroups/cpu,cgroups/mem,docker/runtime}}

pod definition for Marathon:
{code:java}
{
  ""id"": ""/simplepod"",
  ""scaling"": { ""kind"": ""fixed"", ""instances"": 1 },
  ""containers"": [
    {
      ""name"": ""sleep1"",
      ""exec"": { ""command"": { ""shell"": ""sleep 1000"" } },
      ""resources"": { ""cpus"": 0.1, ""mem"": 32 },
      ""image"": {
        ""id"": ""alpine"",
        ""kind"": ""DOCKER""
      }
    }
  ],
  ""networks"": [ {""mode"": ""host""} ]
}
{code}

Mesos should probably check for this and avoid replacing the system root mount point at startup or launch time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-04-12 21:29:52.529,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 02 23:41:31 UTC 2017,,,,,,,"0|hzzyks:i",9223372036854775807,,,,,gilbert,,,,,,Mesosphere Sprint 57,Mesosphere Sprint 58,Mesosphere Sprint 59,Mesosphere Sprint 60,,,,,,,,3.0,,1.4.0,,,,,,,,,"12/Apr/17 21:29;anandmazumdar;[~gilbert] Seems severe enough to fix for 1.3 given that it renders the host unusable?","13/Apr/17 00:08;gilbert;[~anandmazumdar], yes, I added the targeted versions.

[~timcharper], thanks for reporting. I think it might be worth to add a check in provisioner for filesystem/linux isolator if there is an image specified, because we already document fs linux isolator is a MUST:
https://github.com/apache/mesos/blob/master/docs/container-image.md#configure-the-agent

I will reproduce the host mount issue first and then add a check once I have a chance.","18/Apr/17 01:04;adam-mesos;[~gilbert] Who's going to work on this issue and when? We're hoping to cut 1.3.0 and 1.2.1 this week, and it'd be great to include this.","02/May/17 22:53;mcypark;[~chhsia0]: Pushing this off to target 1.4.0. Please let me know if this is a blocker for 1.3.0.","03/May/17 00:02;chhsia0;https://reviews.apache.org/r/58939/","03/May/17 09:24;xujyan;This feels like a 1.3 blocker?","02/Aug/17 23:41;gilbert;commit 1793f8f2a6e98757dba06d9d70d7bd3c03830cf0
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date:   Wed Aug 2 12:29:44 2017 -0700

    Filesystem isolation check for Mesos image provisioner.
    
    Checked if the 'filesystem/linux' isolator is enabled and the 'linux'
    launcher is used when launching a mesos containerizer with an image
    under Linux. This prevents the executor from messing up with the host
    filesystem. The check is in `MesosContainerizerProcess::prepare()`
    after provisioning and before launching, since provisioning itself
    does not depend on the filesystem isolator.
    
    Also checked that the 'filesystem/linux' is enabled and the 'linux'
    launcher is used when enabling the 'docker/runtime' isolator.
    
    Review: https://reviews.apache.org/r/58939/",,,,,,,,,,,,,,,,,,,,,
Failed to pull image from Nexus Registry due to signature missing.,MESOS-7350,13061850,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,gilbert,newf,newf,05/Apr/17 19:21,17/Aug/17 22:54,29/Oct/20 16:32,18/Apr/17 03:24,1.2.0,,,,,,,,1.1.2,1.2.1,1.3.0,,,,,,,,,0,,,,,,,,,"I’m trying to launch docker container with universal containerizer, mesos 1.2.0. But getting error “Failed to parse the image manifest: Docker v2 image manifest validation failed: ‘signatures’ field size must be at least one”. And if I switch to docker containerizer, app is starting normally. 

We are working with private docker registry v2 backed by nexus repository manager  3.1.0
{code}
cat /etc/mesos-slave/docker_registry 
https://docker.company.ru

cat /etc/mesos-slave/docker_config 
{
	""auths"": {
		""docker.company.ru"": {
			""auth"": ""........""
		}
	}
}
{code}

Here agent's log:

{code}
I0405 22:00:49.860234 44856 slave.cpp:4346] Received ping from slave-observer(7)@10.34.1.31:5050
I0405 22:00:50.327030 44865 slave.cpp:1625] Got assigned task 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' for framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.327785 44865 slave.cpp:1785] Launching task 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' for framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.329324 44865 paths.cpp:547] Trying to chown '/export/intssd/mesos-slave/workdir/slaves/5ad97c04-d982-49d3-ac4f-53c468993190-S1/frameworks/5ad97c04-d982-49d3-ac4f-53c468993190-0000/executors/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/runs/f82f5f69-87a3-4586-b4cc-b91d285dcaff' to user 'dockdata'
I0405 22:00:50.329607 44865 slave.cpp:6896] Checkpointing ExecutorInfo to '/export/intssd/mesos-slave/workdir/meta/slaves/5ad97c04-d982-49d3-ac4f-53c468993190-S1/frameworks/5ad97c04-d982-49d3-ac4f-53c468993190-0000/executors/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/executor.info'
I0405 22:00:50.330531 44865 slave.cpp:6472] Launching executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 with resources cpus(*)(allocated: general_marathon_service_role):0.1; mem(*)(allocated: general_marathon_service_role):32 in work directory '/export/intssd/mesos-slave/workdir/slaves/5ad97c04-d982-49d3-ac4f-53c468993190-S1/frameworks/5ad97c04-d982-49d3-ac4f-53c468993190-0000/executors/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/runs/f82f5f69-87a3-4586-b4cc-b91d285dcaff'
I0405 22:00:50.331244 44865 slave.cpp:6919] Checkpointing TaskInfo to '/export/intssd/mesos-slave/workdir/meta/slaves/5ad97c04-d982-49d3-ac4f-53c468993190-S1/frameworks/5ad97c04-d982-49d3-ac4f-53c468993190-0000/executors/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/runs/f82f5f69-87a3-4586-b4cc-b91d285dcaff/tasks/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/task.info'
I0405 22:00:50.331568 44862 docker.cpp:1106] Skipping non-docker container
I0405 22:00:50.331822 44865 slave.cpp:2118] Queued task 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' for executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.331966 44865 slave.cpp:884] Successfully attached file '/export/intssd/mesos-slave/workdir/slaves/5ad97c04-d982-49d3-ac4f-53c468993190-S1/frameworks/5ad97c04-d982-49d3-ac4f-53c468993190-0000/executors/md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14/runs/f82f5f69-87a3-4586-b4cc-b91d285dcaff'
I0405 22:00:50.332582 44861 containerizer.cpp:993] Starting container f82f5f69-87a3-4586-b4cc-b91d285dcaff for executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.333286 44862 metadata_manager.cpp:168] Looking for image 'docker.company.ru/company-infra/kafka:0.10.2.0-16'
I0405 22:00:50.333627 44879 registry_puller.cpp:247] Pulling image 'docker.company.ru/company-infra/kafka:0.10.2.0-16' from 'docker-manifest://docker.company.rucompany-infra/kafka?0.10.2.0-16#https' to '/export/intssd/mesos-slave/docker-store/staging/aV2yko'
E0405 22:00:50.834630 44872 slave.cpp:4642] Container 'f82f5f69-87a3-4586-b4cc-b91d285dcaff' for executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 failed to start: Failed to parse the image manifest: Docker v2 image manifest validation failed: 'signatures' field size must be at least one
I0405 22:00:50.835008 44853 containerizer.cpp:2069] Destroying container f82f5f69-87a3-4586-b4cc-b91d285dcaff in PROVISIONING state
I0405 22:00:50.835127 44853 containerizer.cpp:2124] Waiting for the provisioner to complete provisioning before destroying container f82f5f69-87a3-4586-b4cc-b91d285dcaff
I0405 22:00:50.835273 44844 provisioner.cpp:484] Ignoring destroy request for unknown container f82f5f69-87a3-4586-b4cc-b91d285dcaff
I0405 22:00:50.836199 44837 slave.cpp:4754] Executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 has terminated with unknown status
I0405 22:00:50.837193 44837 slave.cpp:3816] Handling status update TASK_FAILED (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 from @0.0.0.0:0
E0405 22:00:50.837766 44846 slave.cpp:4097] Failed to update resources for container f82f5f69-87a3-4586-b4cc-b91d285dcaff of executor 'md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14' running task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 on status update for terminal task, destroying container: Container not found
W0405 22:00:50.837962 44865 composing.cpp:630] Attempted to destroy unknown container f82f5f69-87a3-4586-b4cc-b91d285dcaff
I0405 22:00:50.838018 44877 status_update_manager.cpp:323] Received status update TASK_FAILED (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.838081 44877 status_update_manager.cpp:500] Creating StatusUpdate stream for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.838560 44877 status_update_manager.cpp:832] Checkpointing UPDATE for status update TASK_FAILED (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.838708 44877 status_update_manager.cpp:377] Forwarding update TASK_FAILED (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 to the agent
I0405 22:00:50.838860 44878 slave.cpp:4256] Forwarding the update TASK_FAILED (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000 to master@10.34.1.31:5050
I0405 22:00:50.839059 44878 slave.cpp:4150] Status update manager successfully handled status update TASK_FAILED (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.848595 44866 status_update_manager.cpp:395] Received status update acknowledgement (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.848696 44866 status_update_manager.cpp:832] Checkpointing ACK for status update TASK_FAILED (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.848801 44866 status_update_manager.cpp:531] Cleaning up status update stream for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.849365 44850 slave.cpp:3105] Status update manager successfully handled status update acknowledgement (UUID: efd419db-5350-48bf-b612-8e5b5685b9a0) for task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14 of framework 5ad97c04-d982-49d3-ac4f-53c468993190-0000
I0405 22:00:50.849431 44850 slave.cpp:6875] Completing task md_kafka_broker.2f58917d-1a32-11e7-ad66-02424dd04a14
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-04-18 01:03:16.125,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 18 01:21:00 UTC 2017,,,,,,,"0|i3d993:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 55,,,,,,,,,,,2.0,,1.1.2,1.2.1,1.3.0,,,,,,,"18/Apr/17 01:03;adam-mesos;[~gilbert] When do you think this issue can be resolved? Any chance it'll actually make it in this week for 1.3.0 or 1.2.1?","18/Apr/17 01:19;gilbert;[~adam-mesos], ah, it was resolved one hour ago.","18/Apr/17 01:20;gilbert;commit 643dafdec76bb176270fe686ec2400242ed0fe36
Author: Gilbert Song songzihao1990@gmail.com
Date:   Tue Apr 18 07:57:30 2017 +0800

Fixed the image signature check for Nexus Registry.

Currently, the signature field of the docker v2 image manifest is
not used yet. The check of at least one image signature is too
strict because some registry (e.g., Nexus Registry) does not sign
the image manifest. We should release the signature check for now.

Review: https://reviews.apache.org/r/58479/","18/Apr/17 01:21;gilbert;I will close this JIRA once the patch is backported.",,,,,,,,,,,,,,,,,,,,,,,,
Update Resource proto for storage resource providers.,MESOS-7312,13059312,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,bbannier,bbannier,27/Mar/17 07:54,12/Oct/17 21:53,29/Oct/20 16:32,12/Oct/17 21:53,,,,,,,,,1.5.0,,,,,,,,,,,0,storage,,,,,,,,"Storage resource provider support requires a number of changes to the {{Resource}} proto:

* support for {{RAW}} and {{BLOCK}} type {{Resource::DiskInfo::Source}}
* {{ResourceProviderID}} in Resource
* {{Resource::DiskInfo::Source::Path}} should be {{optional}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-29 21:54:00.982,,,false,MESOS-7235,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 21 19:22:12 UTC 2017,,,,,,,"0|hzzypp:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 53,Mesosphere Sprint 54,Mesosphere Sprint 55,Mesosphere Sprint 56,,,,,,,,3.0,,,,,,,,,,,"29/Mar/17 21:54;jieyu;commit 75c6dfaa9b816c61eb7a3e155f990d96276dcaa3
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Wed Mar 29 14:28:38 2017 -0700

    Added UNKNOWN DiskInfo.Source type.

    We introduce an explicit UNKNOWN enum kind to allow explicit handling
    of unknown enum values (e.g., when the sending and receiving end use
    different versions of a message using the enum).

    This commit also migrates pattern matching of values of this enum from
    if statements to switch statements so that compiler diagnostics can be
    used to identify unhandled cases when other types are added in the
    future.

    Review: https://reviews.apache.org/r/57911/","29/Mar/17 22:19;jieyu;commit 50dc133cdb39221d1800c92127a8de284c5ecd13
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Wed Mar 29 15:04:13 2017 -0700

    Added ResourceProviderID.

    This patch introduces a new type 'ResourceProviderID'. We will use
    this id in later patches to mark resources with their abstract
    providers (not necessarily an agent).

    Review: https://reviews.apache.org/r/57997/","04/Apr/17 08:36;bbannier;Reviews:
https://reviews.apache.org/r/57998/
https://reviews.apache.org/r/57999/
https://reviews.apache.org/r/58048/
https://reviews.apache.org/r/58047/","07/Apr/17 00:10;jieyu;commit b92a46f0633f5ffac528a57c99af80c519e9ae80
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Thu Apr 6 17:02:35 2017 -0700

    Added ResourceProviderID to Resource protos.

    This patch adds an optional resource provider id to resources. In
    future changes we will introduce abstract providers of resources.
    While currently agents are implicit resource providers, later on an
    agent might use multiple resource providers. By having a provider id
    in the resource we can unambigously detect which provider contributed
    which resource.

    Review: https://reviews.apache.org/r/57998/","14/Apr/17 00:01;jieyu;commit 5660d7633280bf592b12eeed88c4ebadb33f44c5
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Thu Apr 13 14:35:34 2017 -0700

    Made Path and Mount root optional in Resource.DiskInfo.Source.

    This commit is preparation for future changes where path or mount
    roots might be calculated lazily, i.e., not known early enough
    anymore.

    Review: https://reviews.apache.org/r/57999/","03/Aug/17 15:33;bbannier;Still remaining reviews:

https://reviews.apache.org/r/58048/
https://reviews.apache.org/r/58047/","06/Sep/17 19:51;jieyu;commit 48e39a4c0da049534f9f90bc66d38b1368b1807a
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Wed Sep 6 11:17:43 2017 -0700

    Introduce BLOCK and RAW disk types.

    BLOCK and RAW disk types are low-level disk resources which will need
    to be transformed into e.g., volumes by dedicated, still to implement
    offer operations.

    Review: https://reviews.apache.org/r/58047/","21/Sep/17 19:22;bbannier;{noformat}
commit 91e279ad1855ac7f1ae628778731173aa603d5e3
Author: Benjamin Bannier <bbannier@apache.org>
Date:   Thu Sep 21 15:03:22 2017 +0200

    Added 'id' and 'metadata' fields to 'Resource.DiskInfo.Source'.

    IDs will allow to create distinguishable resources, e.g., of RAW or
    BLOCK type. We also add a metadata field which can be used to expose
    additional disk information.

    Review: https://reviews.apache.org/r/58048/
{noformat}",,,,,,,,,,,,,,,,,,,,
Unified containerizer provisions docker image error with COPY backend,MESOS-7280,13058072,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,chhsia0,depay,depay,22/Mar/17 02:12,26/Apr/17 19:15,29/Oct/20 16:32,26/Apr/17 19:09,1.0.2,1.2.0,,,,,,,1.1.2,1.2.1,1.3.0,,,,containerization,docker,,,,0,copy-backend,,,,,,,,"Error occurs on some specific docker images with COPY backend, both 1.0.2 and 1.2.0. It works well with OVERLAY backend on 1.2.0.

{quote}
I0321 09:36:07.308830 27613 paths.cpp:528] Trying to chown '/data/mesos/slaves/55f6df5e-2812-40a0-baf5-ce96f20677d3-S102/frameworks/20151223-150303-2677017098-5050-30032-0000/executors/ct:Transcoding_Test_114489497_1490060156172:3/runs/7e518538-7b56-4b14-a3c9-bee43c669bd7' to user 'root'
I0321 09:36:07.319628 27613 slave.cpp:5703] Launching executor ct:Transcoding_Test_114489497_1490060156172:3 of framework 20151223-150303-2677017098-5050-30032-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/data/mesos/slaves/55f6df5e-2812-40a0-baf5-ce96f20677d3-S102/frameworks/20151223-150303-2677017098-5050-30032-0000/executors/ct:Transcoding_Test_114489497_1490060156172:3/runs/7e518538-7b56-4b14-a3c9-bee43c669bd7'
I0321 09:36:07.321436 27615 containerizer.cpp:781] Starting container '7e518538-7b56-4b14-a3c9-bee43c669bd7' for executor 'ct:Transcoding_Test_114489497_1490060156172:3' of framework '20151223-150303-2677017098-5050-30032-0000'
I0321 09:36:37.902195 27600 provisioner.cpp:294] Provisioning image rootfs '/data/mesos/provisioner/containers/7e518538-7b56-4b14-a3c9-bee43c669bd7/backends/copy/rootfses/8d2f7fe8-71ff-4317-a33c-a436241a93d9' for container 7e518538-7b56-4b14-a3c9-bee43c669bd7
*E0321 09:36:58.707718 27606 slave.cpp:4000] Container '7e518538-7b56-4b14-a3c9-bee43c669bd7' for executor 'ct:Transcoding_Test_114489497_1490060156172:3' of framework 20151223-150303-2677017098-5050-30032-0000 failed to start: Collect failed: Failed to copy layer: cp: cannot create regular file ‘/data/mesos/provisioner/containers/7e518538-7b56-4b14-a3c9-bee43c669bd7/backends/copy/rootfses/8d2f7fe8-71ff-4317-a33c-a436241a93d9/usr/bin/python’: Text file busy*
I0321 09:36:58.707991 27608 containerizer.cpp:1622] Destroying container '7e518538-7b56-4b14-a3c9-bee43c669bd7'
I0321 09:36:58.708468 27607 provisioner.cpp:434] Destroying container rootfs at '/data/mesos/provisioner/containers/7e518538-7b56-4b14-a3c9-bee43c669bd7/backends/copy/rootfses/8d2f7fe8-71ff-4317-a33c-a436241a93d9' for container 7e518538-7b56-4b14-a3c9-bee43c669bd7
{quote}

Docker image is a private one, so that i have to try to reproduce this bug with some sample Dockerfile as possible.","CentOS 7.2，ext4, COPY",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-24 04:21:33.049,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 26 19:09:41 UTC 2017,,,,,,,"0|i3clxz:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 55,,,,,,,,,,,2.0,,,,,,,,,,,"24/Mar/17 04:21;avinash.mesos;[~gilbert] is this an issue with duplicate image layers?","04/Apr/17 23:34;anandmazumdar;^^ [~gilbert] [~jieyu] Can you folks take a look at this?","04/Apr/17 23:50;gilbert;[~avinash.mesos], should be copy backend specific. The duplicate layers issue is already fixed.

[~depay], are you able to provide a public image or point us to an open image? so that we can reproduce and triage on this issue. Thanks!
","05/Apr/17 11:08;depay;[~gilbert] I've tried to build a sample image but failed. I'll update here the issue once successfully doing that.","14/Apr/17 22:40;chhsia0;[~depay] Is any of /usr, /usr/bin, or /usr/bin/python in any layer of your image a symbolic link or linked by another file?","17/Apr/17 01:35;depay;[~chhsia0] yes, there's a python link changed from 2.6 to 2.7 with complex replacements. I've tried to rebuild a image with those command, but failed to reproduce.","17/Apr/17 14:15;chhsia0;Can you describe more about how the files are linked? Is it like that /usr/bin/python links to some 2.6 binary in a lower layer and then it is changed to link to some 2.7 binary in an upper layer? I'm suspecting that this bug might be related to how symbolic links are handled in the copy backend.","18/Apr/17 01:49;depay;the major part about python in Dockerfile is something like this

{code}
from centos6

run set -eu && yum install -y python27 python27-devel python27-setuptools python-setuptools && mv /usr/bin/python /usr/bin/python.bak && ln -s /usr/bin/python2.7 /usr/bin/python && for f in /usr/bin/yum /usr/bin/yumdownloader;do sed -i s/python/python2.6/ $f;done

run rm -f /usr/bin/python && ln -s /usr/bin/python2.7 /usr/bin/python

run python2.7 -c ""import xxxxxx"" # just import something
{code}
","18/Apr/17 06:18;chhsia0;Thanks for your information! This confirms my suspicion. Hopefully we will have a fix soon.","18/Apr/17 06:37;jieyu;[~chhsia0] Can we close this ticket. Does your fix in MESOS-5028 fix this issue as well?","18/Apr/17 06:48;chhsia0;The fix to MESOS-6327 should work. Should we also design a unit test to verify that the host filesystem should not be modified before closing it? Or do you think the same unit test for MESOS-6327 is sufficient?","18/Apr/17 07:01;jieyu;[~chhsia0] I think a unit test will be great!","25/Apr/17 18:59;chhsia0;Fixed by Commit https://github.com/apache/mesos/commit/3c8deedc9a1bce617965c3442713ebdc6691d1ae.
Unit test: https://reviews.apache.org/r/58640/","26/Apr/17 19:09;jieyu;commit 73c962831c5ce8ee47742d9951fec7c38e691c2f
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date:   Wed Apr 26 12:06:33 2017 -0700

    Updated the Overwrite docker provisioner test for MESOS-7280.

    Created a non-empty file 'abc' under the agent's work directory, which
    is linked by the symlink '/xyz' in the 2nd layer of the testing image.
    This file should not be overwritten by the empty file '/xyz' in the 3rd
    layer. Please see the following link for more details:
      https://hub.docker.com/r/chhsiao/overwrite/

    The testing image is updated in a way that it is compatible to the
    previous Overwrite test.

    Review: https://reviews.apache.org/r/58640/",,,,,,,,,,,,,,
Unified containerizer does not support docker registry version < 2.3.,MESOS-7272,13057788,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,gilbert,depay,depay,21/Mar/17 05:34,27/Apr/17 01:38,29/Oct/20 16:32,27/Apr/17 01:38,1.2.0,,,,,,,,1.2.1,1.3.0,,,,,containerization,docker,,,,0,easyfix,,,,,,,,"in file `src/uri/fetchers/docker.cpp`

```
    Option<string> contentType = response.headers.get(""Content-Type"");  
        if (contentType.isSome() &&  
            !strings::startsWith(  
                contentType.get(),  
                ""application/vnd.docker.distribution.manifest.v1"")) {  
          return Failure(  
              ""Unsupported manifest MIME type: "" + contentType.get());  
        }  
```

Docker fetcher check the contentType strictly, while docker registry with version < 2.3 returns manifests with contentType `application/json`, that leading failure like `E0321 13:27:27.572402 40370 slave.cpp:4650] Container 'xxx' for executor 'xxx' of framework xxx failed to start: Unsupported manifest MIME type: application/json; charset=utf-8`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-21 05:58:50.958,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 27 01:30:14 UTC 2017,,,,,,,"0|hzzyrw:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 53,Mesosphere Sprint 54,Mesosphere Sprint 55,,,,,,,,,2.0,,1.2.1,1.3.0,,,,,,,,"21/Mar/17 05:58;gilbert;[~depay], thanks for reporting. I will fix it asap!","24/Mar/17 03:56;avinash.mesos;[~jieyu] assuming you are the shepherd here?","18/Apr/17 01:12;adam-mesos;Any progress here [~gilbert], [~jieyu]? Looks like it's marked as a Blocker for 1.3.0/1.2.1/1.1.2, so we'd like to land it this week (I see it's in the current sprint).","27/Apr/17 00:13;gilbert;https://reviews.apache.org/r/58747/","27/Apr/17 01:30;jieyu;commit dce71bf3776ac648e6a1b5492093f5003ad3b694
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Apr 26 18:25:08 2017 -0700

    Fixed docker uri fetcher strict v2 schema 1 check.

    This check was introduced from this patch:
    https://reviews.apache.org/r/53848/

    The check on registry v2 schema 1 is incorrect. It does not work
    for registries that are older version < 2.3, because the ContentType
    header may be something like this ""application/json; charset=utf-8"".
    The check missed a note from docker registry spec that
    ""application/json"" will also be accepted for schema 1.

    Depending on the docker registry spec doc, docker support the
    following three media type for V2 schema 1 manifest:
      1. application/vnd.docker.distribution.manifest.v1+json
      2. application/vnd.docker.distribution.manifest.v1+prettyjws
      3. application/json
    For more details, see:
      https://docs.docker.com/registry/spec/manifest-v2-1/

    Review: https://reviews.apache.org/r/58747/",,,,,,,,,,,,,,,,,,,,,,,
Support pulling images from AliCloud private registry.,MESOS-7251,13056535,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Duplicate,gilbert,kaiyu.shi,kaiyu.shi,16/Mar/17 03:32,17/Apr/17 23:38,29/Oct/20 16:32,11/Apr/17 00:03,1.1.0,1.2.0,,,,,,,,,,,,,containerization,,,,,0,docker,fetcher,provisioner,,,,,,"The image puller via curl doesn't work when I'm specifying the image name as:
registry.cn-hangzhou.aliyuncs.com/kaiyu/pytorch-cuda75
400 BAD REQUEST

But the docker pulls it successfully 
bq. docker pull registry.cn-hangzhou.aliyuncs.com/kaiyu/pytorch-cuda75",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-5172,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-16 20:36:50.801,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 16 20:36:50 UTC 2017,,,,,,,"0|hzzysk:x",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 53,Mesosphere Sprint 54,,,,,,,,,,2.0,,,,,,,,,,,"16/Mar/17 20:36;gilbert;I can reproduce this using an image hosted from the ali cloud registry:
{noformat}
<?xml version=""1.0"" encoding=""UTF-8""?>
<Error>
  <Code>InvalidArgument</Code>
  <Message>Either the Signature query string parameter or the Authorization header should be specified, not both.</Message>
  <RequestId>58CAF48657BB7445E375F545</RequestId>
  <HostId>aliregistry.oss-cn-hangzhou.aliyuncs.com</HostId>
  <Authorization>Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsImtpZCI6IjRSSU06SEhMNDpHU1MyOjdaQ0w6QkNMRDpKN0ZIOlVPNzM6Q1FETzpNUUg1OjdNQ1E6T0lQUTpYQlk1In0.eyJpc3MiOiJkb2NrZXJhdXRoLmFsaXl1bmNzLmNvbSIsImF1ZCI6InJlZ2lzdHJ5LmFsaXl1bmNzLmNvbTpjbi1oYW5nemhvdSIsInN1YiI6IiIsImlhdCI6MTQ4OTY5NTg0MywianRpIjoiT1NEYlFqM3NESXFDLXcxU0xHRl9iUSIsIm5iZiI6MTQ4OTY5NTU0MywiZXhwIjoxNDg5Njk2NDQzLCJhY2Nlc3MiOlt7Im5hbWUiOiJrYWl5dS9weXRvcmNoLWN1ZGE3NSIsInR5cGUiOiJyZXBvc2l0b3J5IiwiYWN0aW9ucyI6WyJwdWxsIl19XX0.VuILoCFXvzIh9r8DLH-_Glpb5dWCP1SA2ZhuxIrIx-8fwo0I1aITVJNNsUKSp6DIkyzb2tHOV5EXErGWH5zjdPTjO-vDLTA86IuLx8JYDiOR5iTLbiV2w21_kUo8n97tFt2COvy7Yo6nbG7_V0YF_6HcqaKqHiOyfUmgBbGxV4qqhQcjwTzG-5tRFIeFIScq7o_-_ol6-MASQ1ONJryCc9ReCACr6amrCjqecs3I1czToXhmOoNXTZJh3eH0jBOqcIT7IIDEi-BubzRYN26ixf1s1VmuRCBUEEIUBLsIp5RoEzYDcO86u89xiHGnGH7sgTys4--oBOIKabBR3gGx1w</Authorization>
</Error>
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,
Tasks launched via the default executor cannot access disk resource volumes.,MESOS-7225,13049674,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,anandmazumdar,anandmazumdar,09/Mar/17 17:37,23/Mar/17 20:03,29/Oct/20 16:32,23/Mar/17 20:03,,,,,,,,,1.3.0,,,,,,,,,,,0,mesosphere,,,,,,,,"Currently, when a task in a task group tries to access a volume specified in disk resources (e.g., persistent volumes), it doesn't have access to them since they are mounted in the root container (executor). This happens due to there being no mechanism to specify resources for child containers yet. Hence, by default any resources (e.g., disk) are added to the root container.

A possible solution can be to set up the mapping manually by the default executor using the {{SANDBOX_PATH}} volume source type giving child containers access to the volume mounted in the parent container. This is at best a workaround and the ideal solution would be tackled as part of MESOS-7207.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-6355,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 23 20:03:30 UTC 2017,,,,,,,"0|hzzyt0:zr",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 52,Mesosphere Sprint 53,,,,,,,,,,3.0,,1.3.0,,,,,,,,,"09/Mar/17 17:48;anandmazumdar;https://reviews.apache.org/r/57469/","23/Mar/17 20:03;anandmazumdar;{noformat}
commit 28876a442da780156d9376c8e7a1671628857bb2
Author: Anand Mazumdar anand@apache.org
Date:   Wed Mar 8 17:51:03 2017 -0800

Made the default executor populate volume mappings for disk resources.

A task can now access any volumes specified in disk resources from
its own sandbox owing to the default executor populating the mapping
from the child container sandbox to the parent. Previously, without
such a mapping it was not possible for the child container to access
those volumes mounted on the parent container.

Review: https://reviews.apache.org/r/57469
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
HTTP health check doesn't work when mesos runs with --docker_mesos_image,MESOS-7210,13048456,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,xds2000,sielaq,sielaq,06/Mar/17 08:04,18/Apr/17 02:31,29/Oct/20 16:32,16/Apr/17 18:25,1.1.0,1.1.1,1.2.0,,,,,,1.1.2,1.2.1,1.3.0,,,,docker,,,,,1,,,,,,,,,"When running mesos-slave with option ""docker_mesos_image"" like:
{code}
--master=zk://standalone:2181/mesos  --containerizers=docker,mesos  --executor_registration_timeout=5mins  --hostname=standalone  --ip=0.0.0.0  --docker_stop_timeout=5secs  --gc_delay=1days  --docker_socket=/var/run/docker.sock  --no-systemd_enable_support  --work_dir=/tmp/mesos  --docker_mesos_image=panteras/paas-in-a-box:0.4.0
{code}

from the container that was started with option ""pid: host"" like:
{code}
  net:        host
  privileged: true
  pid:        host
{code}

and example marathon job, that use MESOS_HTTP checks like:
{code}
{
 ""id"": ""python-example-stable"",
 ""cmd"": ""python3 -m http.server 8080"",
 ""mem"": 16,
 ""cpus"": 0.1,
 ""instances"": 2,
 ""container"": {
   ""type"": ""DOCKER"",
   ""docker"": {
     ""image"": ""python:alpine"",
     ""network"": ""BRIDGE"",
     ""portMappings"": [
        { ""containerPort"": 8080, ""hostPort"": 0, ""protocol"": ""tcp"" }
     ]
   }
 },
 ""env"": {
   ""SERVICE_NAME"" : ""python""
 },
 ""healthChecks"": [
   {
     ""path"": ""/"",
     ""portIndex"": 0,
     ""protocol"": ""MESOS_HTTP"",
     ""gracePeriodSeconds"": 30,
     ""intervalSeconds"": 10,
     ""timeoutSeconds"": 30,
     ""maxConsecutiveFailures"": 3
   }
 ]
}
{code}

I see the errors like:
{code}
F0306 07:41:58.844293    35 health_checker.cpp:94] Failed to enter the net namespace of task (pid: '13527'): Pid 13527 does not exist
*** Check failure stack trace: ***
    @     0x7f51770b0c1d  google::LogMessage::Fail()
    @     0x7f51770b29d0  google::LogMessage::SendToLog()
    @     0x7f51770b0803  google::LogMessage::Flush()
    @     0x7f51770b33f9  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f517647ce46  _ZNSt17_Function_handlerIFivEZN5mesos8internal6health14cloneWithSetnsERKSt8functionIS0_E6OptionIiERKSt6vectorINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEESaISG_EEEUlvE_E9_M_invokeERKSt9_Any_data
    @     0x7f517647bf2b  mesos::internal::health::cloneWithSetns()
    @     0x7f517648374b  std::_Function_handler<>::_M_invoke()
    @     0x7f5177068167  process::internal::cloneChild()
    @     0x7f5177065c32  process::subprocess()
    @     0x7f5176481a9d  mesos::internal::health::HealthCheckerProcess::_httpHealthCheck()
    @     0x7f51764831f7  mesos::internal::health::HealthCheckerProcess::_healthCheck()
    @     0x7f517701f38c  process::ProcessBase::visit()
    @     0x7f517702c8b3  process::ProcessManager::resume()
    @     0x7f517702fb77  _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUt_vEEE6_M_runEv
    @     0x7f51754ddc80  (unknown)
    @     0x7f5174cf06ba  start_thread
    @     0x7f5174a2682d  (unknown)
I0306 07:41:59.077986     9 health_checker.cpp:199] Ignoring failure as health check still in grace period
{code}

Looks like option docker_mesos_image makes, that newly started mesos job is not using ""pid host"" option same as mother container was started, but has his own PID namespace (so it doesn't matter if mother container was started with ""pid host"" or not it will never be able to find PID)","Ubuntu 16.04.02
Docker version 1.13.1
mesos 1.1.0, runs from container
docker containers  spawned by marathon 1.4.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-07 04:12:34.07,,,false,MESOS-5916,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 18 02:31:19 UTC 2017,,,,,,,"0|i3azgv:",9223372036854775807,,,,,haosdent@gmail.com,,,,,,,,,,,,,,,,,3.0,,1.1.2,1.2.1,1.3.0,,,,,,,"07/Mar/17 04:12;avinash.mesos;[~alexr] ^^ [~gkleiman]","20/Mar/17 19:04;alexr;Hey [~sielaq], thanks a lot for the report and sorry for a tardy reply. This indeed looks like a bug we need to fix. I'll add it to our backlog and will get back to you once we have a fix or a time plan for getting the fix out.","21/Mar/17 02:42;haosdent@gmail.com;Thanks a lot  [~sielaq] [~alexr]'s help. Let me try to fix this.","31/Mar/17 03:11;xds2000;add me ","31/Mar/17 10:05;xds2000;sorry, this is my misunderstand. if add mesos_docker_image, the docker executor will spawn new container, the container is docker executor, it should be add pid=host to mapping host pid pool.

============
--it difficult to fix due to the mesos agent is wrap into container, we only manually add --pid=host to the mesos-agent container, then the pid can find same pid with container inside process pid. this is not mesos fault, we prefer suggest user can use systemd to running the mesos agent instead of mesos agent container, it will benefit with developers and users each other.--","03/Apr/17 07:17;sielaq;[~xds2000] exactly, the mesos-slave (container) and the docker executor (container) need to runs in the same pid pool (host).","03/Apr/17 10:25;alexr;[~xds2000], [~haosdent@gmail.com] Let's fix it and backport.","04/Apr/17 20:12;xds2000;Hi [~alexr] [~haosdent@gmail.com]

i can't found any useful reference for HOWTO use docker parameters fields in mesos protobuf

```
    ContainerInfo::DockerInfo dockerInfo;
    dockerInfo.set_image(flags.docker_mesos_image.get());
```
i need a example to add --pid=host to this dockerInfo.parameters. could you please give a help. thanks a lot.","04/Apr/17 20:30;xds2000;found https://issues.apache.org/jira/browse/MESOS-6589 , but not found any reference to this docker parameter usage. ","05/Apr/17 06:12;xds2000;thanks [~haosdent@gmail.com] it works.","05/Apr/17 06:59;xds2000;patch: https://reviews.apache.org/r/58200/

let me testing it asap.","05/Apr/17 23:35;xds2000;first testing:
https://gist.github.com/xiaods/c5a11e3ab51e89a9609edc2c477f7ea8
","08/Apr/17 11:56;xds2000;in second try. i have subimt new patch to 58200. let me testing it again.","13/Apr/17 06:54;xds2000;for avoid of abuse priviledges, just use --cap-add SYS_ADMIN to resolve the net operation issue.

```
Failed to enter the net namespace of task (pid: '78851'): Operation not permitted
```","16/Apr/17 18:25;haosdent@gmail.com;{code}
commit d1c549e8c8c788d5a7bcf4017c107a25ff02f80a
Author: Deshi Xiao xiaods@gmail.com
Date:   Mon Apr 17 02:00:47 2017 +0800

Fixed health check bug when running agents with `docker_mesos_image`.

When running Mesos agents in docker with the `docker_mesos_image` flag,
HTTP health check would fail because the `mesos-docker-executor` could
not find the pid of the task and don't have permissions to enter the
namespaces of the task. This patch updated the options used to run
`mesos-docker-executor` in a separate docker container and ensure
`mesos-docker-executor` got the appropriate permissions to enter the
namespaces of the tasks.

Review: https://reviews.apache.org/r/58200/
{code}","18/Apr/17 01:14;adam-mesos;[~haosdent@gmail.com], could you please backport this to the 1.2.x and 1.1.x branches so we can include it in the next patch releases (1.2.1 and 1.1.2)? Hoping to cut those this week.","18/Apr/17 02:31;haosdent@gmail.com;Hi, [~adam-mesos] thanks a lot, have backported to 1.2.x and 1.1.x.",,,,,,,,,,,
Persistent volume ownership is set to root when task is running with non-root user,MESOS-7208,13048405,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,gilbert,newf,newf,06/Mar/17 01:57,09/Mar/17 22:32,29/Oct/20 16:32,09/Mar/17 22:32,1.0.2,1.1.0,1.2.0,,,,,,1.2.1,1.3.0,,,,,containerization,,,,,0,user,,,,,,,,"I’m running docker container in universal containerizer, mesos 1.1.0. switch_user=true, isolator=filesystem/linux,docker/runtime.  Container is launched with marathon, “user”:”someappuser”. I’d want to use persistent volume, but it’s exposed to container with root user permissions even if root folder is created with someppuser ownership (looks like mesos do chown to this folder). 

here logs for my container:
{code}
I0305 22:51:36.414655 10175 slave.cpp:1701] Launching task 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a' for framework e9d0e39e-b67d-4142-b95d-b0987998eb92-0000
I0305 22:51:36.415118 10175 paths.cpp:536] Trying to chown '/export/intssd/mesos-slave/workdir/slaves/85150805-a201-4b23-ab21-b332a458fc97-S10/frameworks/e9d0e39e-b67d-4142-b95d-b0987998eb92-0000/executors/md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a/runs/e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a' to user 'root'
I0305 22:51:36.422992 10175 slave.cpp:6179] Launching executor 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a' of framework e9d0e39e-b67d-4142-b95d-b0987998eb92-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/export/intssd/mesos-slave/workdir/slaves/85150805-a201-4b23-ab21-b332a458fc97-S10/frameworks/e9d0e39e-b67d-4142-b95d-b0987998eb92-0000/executors/md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a/runs/e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a'
I0305 22:51:36.424278 10175 slave.cpp:1987] Queued task 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a' for executor 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a' of framework e9d0e39e-b67d-4142-b95d-b0987998eb92-0000
I0305 22:51:36.424347 10158 docker.cpp:1000] Skipping non-docker container
I0305 22:51:36.425639 10142 containerizer.cpp:938] Starting container e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a for executor 'md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a' of framework e9d0e39e-b67d-4142-b95d-b0987998eb92-0000
I0305 22:51:36.428725 10166 provisioner.cpp:294] Provisioning image rootfs '/export/intssd/mesos-slave/workdir/provisioner/containers/e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a/backends/copy/rootfses/0e2181e9-1bf2-42d4-8cb0-ee70e466c3ae' for container e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a
I0305 22:51:42.981240 10149 linux.cpp:695] Changing the ownership of the persistent volume at '/export/intssd/mesos-slave/data/volumes/roles/general_marathon_service_role/md_hdfs_journal#data#23f813aa-01dd-11e7-a012-0242ce94d92a' with uid 0 and gid 0
I0305 22:51:42.986593 10136 linux_launcher.cpp:421] Launching container e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a and cloning with namespaces CLONE_NEWNS
{code}

{code}
ls -la /export/intssd/mesos-slave/workdir/slaves/85150805-a201-4b23-ab21-b332a458fc97-S10/frameworks/e9d0e39e-b67d-4142-b95d-b0987998eb92-0000/executors/md_hdfs_journal.23f813ab-01dd-11e7-a012-0242ce94d92a/runs/e978d4eb-5ec1-44ad-b50a-9ae6bfe1065a/
drwxr-xr-x 3 someappuser someappgroup   4096 22:51 .
drwxr-xr-x 3 root     root            4096 22:51 ..
drwxr-xr-x 2 root     root            4096 22:51 data
-rw-r--r-- 1 root     root             169 22:51 stderr
-rw-r--r-- 1 root     root          183012 23:00 stdout
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-08 01:32:02.328,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 09 22:32:31 UTC 2017,,,,,,,"0|i3az5j:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 53,,,,,,,,,,,3.0,,1.2.1,1.3.0,,,,,,,,"08/Mar/17 01:32;gilbert;https://reviews.apache.org/r/57401/
https://reviews.apache.org/r/57402/
https://reviews.apache.org/r/57403/","09/Mar/17 22:32;jieyu;commit 4f50c575f17150ddadb24e3092dcc4453c20bff8
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Thu Mar 9 12:42:48 2017 -0800

    Added unit test for verifying user in command task with image specified.

    Review: https://reviews.apache.org/r/57403/

commit f32ca0173a35f530bfbfc317346e487e25a5b8ce
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Thu Mar 9 12:42:44 2017 -0800

    Fixed command task with container image 'root' user issue.

    This issue is command task with container image provided specific.
    We used to set user as 'root' explicitly for command task with
    container image. However, this would break operators who set 'user'
    for FrameworkInfo/CommandInfo to any user other than 'root' because
    the task cannot access all other contents owned by 'root', e.g.,
    persistent volumes, stdout/stderr or any other directories/files
    written by modules.

    Instead of relying on each isolator/module to explicitly chown,
    Mesos should set user to 'root' right before launching the command
    executor, because the root privilege is only necessary for 'chroot'
    in command executor launch, which should not impact on other
    components.

    Review: https://reviews.apache.org/r/57402/

commit e2f46f1c5e0fda4c2bf152a5ccb904f6d1af4b09
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Thu Mar 9 12:42:40 2017 -0800

    Removed redundant 'root' user set in containerizer::launch().

    The 'user' in launch command is ignored, so it is not necessary to
    explicitly set 'root' user to 'CommandInfo' in the case of command
    task.

    Review: https://reviews.apache.org/r/57401/",,,,,,,,,,,,,,,,,,,,,,,,,,
Add documentation for Debug APIs to Operator API doc,MESOS-7188,13046832,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,klueska,klueska,klueska,28/Feb/17 01:31,28/Feb/17 21:35,29/Oct/20 16:32,28/Feb/17 21:32,,,,,,,,,1.2.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-7103,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 28 21:32:53 UTC 2017,,,,,,,"0|i3api7:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 52,,,,,,,,,,,1.0,,1.2.0,,,,,,,,,"28/Feb/17 01:36;klueska;https://reviews.apache.org/r/57118/","28/Feb/17 21:32;klueska;{noformat}
commit 86f32a4bee0ee981f8dfe1d46a3c86652bc1dd4b
Author: Kevin Klues <klueska@gmail.com>
Date:   Sun Feb 26 22:49:49 2017 -0800

    Added debugging APIs to agent API docs.
    
    Review: https://reviews.apache.org/r/57118
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Agent should validate that the nested container ID does not exceed certain length.,MESOS-7168,13046157,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,xiaowei-cuc,jieyu,jieyu,24/Feb/17 21:24,21/Jun/18 20:16,29/Oct/20 16:32,21/Jun/18 20:16,1.1.0,1.2.0,,,,,,,1.7.0,,,,,,containerization,,,,,0,newbie,,,,,,,,"This is related to MESOS-691.

Since nested container ID is generated by the executor, the agent should verify that the length of it does not exceed certain length.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-06-12 01:15:24.946,,,false,MESOS-6355,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 21 20:16:57 UTC 2018,,,,,,,"0|i3alqn:",9223372036854775807,,,,,gilbert,,,,,,Mesosphere Sprint 2018-23,,,,,,,,,,,3.0,,,,,,,,,,,"12/Jun/18 01:15;xiaowei-cuc;the generated id is uuid, include dash, total 36.  Normal we can set the limit to 255, did we should consider windows os? [~gilbert]  If not only windows 10, the file path will be limit to 260.  In Windows 10, the whole path limits to [32767|[https://msdn.microsoft.com/en-us/library/windows/desktop/aa365247(v=vs.85).aspx#maxpath].]  And I think it does not need to be reconfigured by mesos administrator.","13/Jun/18 23:24;andschwa;I fixed long path issues on Windows. That is, all Windows APIs used by Mesos use the introduced long path support automatically (that is, we use the Unicode versions of the APIs and prepend the long path marker \\?\ when calling these APIs), so you should be good to go for Windows, and thanks for checking :)

Also note that on Linux, 255 is too long. If you're running in docker, you'll often be on AUFS, and it has a per-path-component limit of 242 (see http://aufs.sourceforge.net/aufs3/man.html).","21/Jun/18 20:16;gilbert;commit c545bbfa584d303956f3d8b4075680d3d4c9995e
Author: wei xiao <xw927@outlook.com>
Date:   Thu Jun 21 11:58:14 2018 -0700

    Added a length validation for container IDs.
    
    Review: https://reviews.apache.org/r/67616/",,,,,,,,,,,,,,,,,,,,,,,,,
The agent may be flapping after the machine reboots due to provisioner recover.,MESOS-7152,13044988,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,gilbert,gilbert,gilbert,21/Feb/17 22:22,26/Apr/17 12:48,29/Oct/20 16:32,23/Feb/17 02:49,,,,,,,,,1.1.2,1.2.0,,,,,,,,,,0,nested,provisioner,,,,,,,"After the agent machine reboots, if the agent work dir survives (e.g., /var/lib/mesos) and the container runtime directory is gone (an empty SlaveState as well), the provisioner recover() would get into segfault because that case break the semantic that a child container should always be cleaned up before it parent container.

This is a particular case which only happens if the machine reboots and the provisioner directory survives.

{noformat}
F0217 01:10:18.423238 30099 provisioner.cpp:504] Check failed: entry.parent() != containerId Failed to destroy container 1 since its nested container 1.2 has not been destroyed yet
*** Check failure stack trace: ***
    @     0x7fceb444121d  google::LogMessage::Fail()
    @     0x7fceb44405ee  google::LogMessage::SendToLog()
    @     0x7fceb4440eed  google::LogMessage::Flush()
    @     0x7fceb4444368  google::LogMessageFatal::~LogMessageFatal()
    @     0x7fceb36137f9  mesos::internal::slave::ProvisionerProcess::destroy()
    @     0x7fceb36126f0  mesos::internal::slave::ProvisionerProcess::recover()
    @     0x7fceb3637fc6  _ZZN7process8dispatchI7NothingN5mesos8internal5slave18ProvisionerProcessERK7hashsetINS2_11ContainerIDESt4hashIS7_ESt8equal_toIS7_EESC_EENS_6FutureIT_EERKNS_3PIDIT0_EEMSJ_FSH_T1_ET2_ENKUlPNS_11ProcessBaseEE_clESS_
    @     0x7fceb3637bc2  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchI7NothingN5mesos8internal5slave18ProvisionerProcessERK7hashsetINS6_11ContainerIDESt4hashISB_ESt8equal_toISB_EESG_EENS0_6FutureIT_EERKNS0_3PIDIT0_EEMSN_FSL_T1_ET2_EUlS2_E_E9_M_invokeERKSt9_Any_dataOS2_
    @     0x7fceb43848e4  std::function<>::operator()()
    @     0x7fceb436baf4  process::ProcessBase::visit()
    @     0x7fceb43e5fde  process::DispatchEvent::visit()
    @           0x9e4101  process::ProcessBase::serve()
    @     0x7fceb4369007  process::ProcessManager::resume()
    @     0x7fceb4377a8c  process::ProcessManager::init_threads()::$_2::operator()()
    @     0x7fceb4377995  _ZNSt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvE3$_2vEE9_M_invokeIJEEEvSt12_Index_tupleIJXspT_EEE
    @     0x7fceb4377965  std::_Bind_simple<>::operator()()
    @     0x7fceb437793c  std::thread::_Impl<>::_M_run()
    @     0x7fceadefa030  (unknown)
    @     0x7fcead70b6aa  start_thread
    @     0x7fcead440e9d  (unknown)
{noformat}

The provisioner directory is supposed to be under the container runtime directory. However, this is not backward compatible. We can only change it after a deprecation cycle.

For now, we have to three options:
1. make provisioner::destroy() recursive.
2. sort the container during recovery to guarantee `child before parent` semantic.
3. remove the check-failure since the while provisioner dir will be removed eventually at the end (not recommended).

Recommend (1).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-02-22 20:12:50.63,,,false,MESOS-6355,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 27 14:03:17 UTC 2017,,,,,,,"0|i3aejz:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 52,,,,,,,,,,,5.0,,1.1.2,1.2.0,,,,,,,,"21/Feb/17 22:26;gilbert;https://reviews.apache.org/r/56808/
https://reviews.apache.org/r/56809/","22/Feb/17 20:12;jieyu;commit 020b37ee9c44007ecd0016fbbf6012054953dd5b
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Feb 22 09:10:58 2017 -0800

    Fixed nested container agent flapping issue after reboot.

    When recovering containers in provisioner, there is a particular case
    that after the machine reboots the container runtime directory and
    slave state is gone but the provisioner directory still exists since
    it is under the agent work_dir(e.g., agent work_dir is /var/lib/mesos).
    Then, all checkpointed containers will be cleaned up as unknown
    containers in provisioner during recovery. However, the semantic that
    a child container is always cleaned up before its parent container
    cannot be guaranteed for this particular case. Ideally, we should
    put the provisioner directory under the container runtime dir but this
    is not backward compactible. It is an unfortunate that we have to
    make the provisioner::destroy() to be recursive.

    Review: https://reviews.apache.org/r/56808/","23/Feb/17 02:49;jieyu;commit b5ba2b8dd65518a45e847b79b24f48cf5a779353
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Feb 22 18:48:59 2017 -0800

    Added test for nested container agent reboot case.

    Review: https://reviews.apache.org/r/56809/","27/Feb/17 14:03;alexr;[~gilbert], [~jieyu] If this has to be ported to `1.1.x` please cherry-pick it into the respective branch. For now I'm retargeting this for 1.1.2.",,,,,,,,,,,,,,,,,,,,,,,,
Wrap IOSwitchboard.connect() in a dispatch,MESOS-7144,13044225,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,klueska,klueska,klueska,18/Feb/17 18:36,24/Feb/17 04:00,29/Oct/20 16:32,24/Feb/17 04:00,,,,,,,,,1.2.0,,,,,,,,,,,0,debugging,,,,,,,,"Since the IOSwitchboard is implemented as a MesosIsolatorProcess, most
of its API calls are automatically dispatched onto its underlying
process by an Isolator wrapper. However, the IOSwitchboard also
includes an additional connect() call which is not accessed through
the Isolator wrapper. As such, we need to wrap it in a dispatch call
manually.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-02-21 09:37:40.528,,,false,MESOS-7103,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 24 04:00:10 UTC 2017,,,,,,,"0|hzzyyc:y",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 51,Mesosphere Sprint 52,,,,,,,,,,1.0,,1.2.0,,,,,,,,,"18/Feb/17 18:38;klueska;https://reviews.apache.org/r/56814/","21/Feb/17 09:37;adam-mesos;Please backport to the 1.2.x branch once it's landed in master.","24/Feb/17 04:00;adam-mesos;commit b2ad1ae29514e67e3f29cb666e70060e2951ec42
Author: Kevin Klues <klueska@gmail.com>
Date:   Thu Feb 23 18:43:27 2017 -0800

    Wrapped IOSwitchboard.connect() in a dispatch.
    
    Since the IOSwitchboard is implemented as a MesosIsolatorProcess, most
    of its API calls are automatically dispatched onto its underlying
    process by an Isolator wrapper. However, the IOSwitchboard also
    includes an additional connect() call which is not accessed through
    the Isolator wrapper. As such, we need to wrap it in a dispatch call
    manually.
    
    Review: https://reviews.apache.org/r/56814/

Backported to 1.2.x

commit 4b2f5033d10ef347ab1f51851425b07f9e4249b3
Author: Kevin Klues <klueska@gmail.com>
Date:   Thu Feb 23 18:43:27 2017 -0800

    Wrapped IOSwitchboard.connect() in a dispatch.
    
    Since the IOSwitchboard is implemented as a MesosIsolatorProcess, most
    of its API calls are automatically dispatched onto its underlying
    process by an Isolator wrapper. However, the IOSwitchboard also
    includes an additional connect() call which is not accessed through
    the Isolator wrapper. As such, we need to wrap it in a dispatch call
    manually.
    
    Review: https://reviews.apache.org/r/56814/",,,,,,,,,,,,,,,,,,,,,,,,,
Quota can be exceeded due to coarse-grained offer technique.,MESOS-7099,13041883,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,mzhu,bmahler,bmahler,09/Feb/17 23:31,03/May/18 00:01,29/Oct/20 16:32,21/Dec/17 02:54,,,,,,,,,1.4.2,1.5.0,,,,,allocation,,,,,0,multitenancy,,,,,,,,"The current implementation of quota allocation allocates the entire available resources on an agent when trying to satisfy the quota. What this means is that quota can be exceeded by the size of an agent.

This is especially problematic for large machines, consider a 48 core, 512 GB memory server where a role is given 4 cores and 4GB of memory. Given our current approach, we will send an offer for the entire 48 cores and 512 GB of memory!

This ticket is to perform fine grained offers when the allocation will exceed the quota.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-12-13 22:21:02.908,,,false,MESOS-6514,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 21 02:54:12 UTC 2017,,,,,,,"0|i39vm7:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 70,,,,,,,,,,,5.0,,1.4.2,,,,,,,,,"13/Dec/17 22:21;mzhu;https://reviews.apache.org/r/64003/","21/Dec/17 02:54;bmahler;{noformat}
commit 9aeef9ce3a90829b2166dac931e64d5bbf18e230
Author: Meng Zhu <mzhu@mesosphere.io>
Date:   Wed Dec 20 18:32:02 2017 -0800

    Made quota resource allocation fine-grained.

    The allocator now does fine-grained resource allocation up
    to the role's quota limit. And a role's quota is only
    considered to be satisfied if all of its quota resources
    are satisfied. Previously, a role's quota is considered
    to be met if any one of its quota resources reaches the limit.

    Also fixed a few affected allocator tests.

    Review: https://reviews.apache.org/r/64003/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
libprocess tests fail when using libevent 2.1.8,MESOS-7076,13041008,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,tillt,nfnt,nfnt,07/Feb/17 15:30,29/Apr/19 09:26,29/Oct/20 16:32,26/Sep/18 13:35,,,,,,,,,1.8.0,,,,,,build,libprocess,test,,,1,ci,integration,,,,,,,"Running {{libprocess-tests}} on Mesos compiled with {{--enable-libevent --enable-ssl}} on an operating system using libevent 2.1.8, SSL related tests fail like
{noformat}
[ RUN      ] SSLTest.SSLSocket
I0207 15:20:46.017881 2528580544 openssl.cpp:419] CA file path is unspecified! NOTE: Set CA file path with LIBPROCESS_SSL_CA_FILE=<filepath>
I0207 15:20:46.017904 2528580544 openssl.cpp:424] CA directory path unspecified! NOTE: Set CA directory path with LIBPROCESS_SSL_CA_DIR=<dirpath>
I0207 15:20:46.017918 2528580544 openssl.cpp:429] Will not verify peer certificate!
NOTE: Set LIBPROCESS_SSL_VERIFY_CERT=1 to enable peer certificate verification
I0207 15:20:46.017923 2528580544 openssl.cpp:435] Will only verify peer certificate if presented!
NOTE: Set LIBPROCESS_SSL_REQUIRE_CERT=1 to require peer certificate verification
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0207 15:20:46.033001 2528580544 openssl.cpp:419] CA file path is unspecified! NOTE: Set CA file path with LIBPROCESS_SSL_CA_FILE=<filepath>
I0207 15:20:46.033179 2528580544 openssl.cpp:424] CA directory path unspecified! NOTE: Set CA directory path with LIBPROCESS_SSL_CA_DIR=<dirpath>
I0207 15:20:46.033196 2528580544 openssl.cpp:429] Will not verify peer certificate!
NOTE: Set LIBPROCESS_SSL_VERIFY_CERT=1 to enable peer certificate verification
I0207 15:20:46.033201 2528580544 openssl.cpp:435] Will only verify peer certificate if presented!
NOTE: Set LIBPROCESS_SSL_REQUIRE_CERT=1 to require peer certificate verification
../../../3rdparty/libprocess/src/tests/ssl_tests.cpp:257: Failure
Failed to wait 15secs for Socket(socket.get()).recv()
[  FAILED  ] SSLTest.SSLSocket (15196 ms)
{noformat}

Tests failing are

{noformat}
SSLTest.SSLSocket
SSLTest.NoVerifyBadCA
SSLTest.VerifyCertificate
SSLTest.ProtocolMismatch
SSLTest.ECDHESupport
SSLTest.PeerAddress
SSLTest.HTTPSGet
SSLTest.HTTPSPost
SSLTest.SilentSocket
SSLTest.ShutdownThenSend
SSLVerifyIPAdd/SSLTest.BasicSameProcess/0, where GetParam() = ""false""
SSLVerifyIPAdd/SSLTest.BasicSameProcess/1, where GetParam() = ""true""
SSLVerifyIPAdd/SSLTest.BasicSameProcessUnix/0, where GetParam() = ""false""
SSLVerifyIPAdd/SSLTest.BasicSameProcessUnix/1, where GetParam() = ""true""
SSLVerifyIPAdd/SSLTest.RequireCertificate/0, where GetParam() = ""false""
SSLVerifyIPAdd/SSLTest.RequireCertificate/1, where GetParam() = ""true""
{noformat}","macOS 10.12.3, libevent 2.1.8 (installed via Homebrew)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-8019,,,,,,,,"31/Aug/18 00:57;tillt;libevent-openssl11.patch;https://issues.apache.org/jira/secure/attachment/12937841/libevent-openssl11.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,2018-01-11 14:31:49.187,,,false,MESOS-9176,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 26 13:24:14 UTC 2018,,,,,,,"0|i39q7r:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 2018-27,Mesosphere Sprint 2018-28,Mesosphere Sprint 2018-29,,,,,,,,,8.0,,1.8.0,,,,,,,,,"07/Feb/17 15:48;nfnt;Downgrading libevent to 2.0.22 makes all libprocess tests pass again.","10/Feb/17 13:13;nfnt;Seems to be related to {{send}} and {{recv}}. The tests that are failing all send or receive on a socket.","11/Jan/18 14:31;arojas;Note that this issue also affects newer versions of linux, see MESOS-8271 for details.","11/Jan/18 14:33;arojas;Initial research shows that in the libevent callback the [event bit error is marked as BEV_EVENT_ERROR|https://github.com/apache/mesos/blob/f06e6184b0d6cc4a3245a714e5b56f26eb454233/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L1176], however neither {{[EVUTIL_SOCKET_ERROR()|https://github.com/apache/mesos/blob/f06e6184b0d6cc4a3245a714e5b56f26eb454233/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L1178]}} nor {{[bufferevent_get_openssl_error()|https://github.com/apache/mesos/blob/f06e6184b0d6cc4a3245a714e5b56f26eb454233/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L1182]}} report any failure.","19/Jan/18 11:18;arojas;After lots of debugging I found that the issues in both linux and macos are different.

In linux, the server breaks while [accepting a connection|https://github.com/apache/mesos/blob/19cf3068b81355818a9c4cdb797883f2b73cdbce/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L1176], while the client breaks during the {{connect()}} step. The issues are similar, the variable {{events}} is marked with both {{BEV_EVENT_ERROR}} and {{BEV_EVENT_READ}} (which is curious for the client) as mentioned before, neither libevent nor openssl mention any error having happened, however the socket never properly transitions from a listening state to an accepted state.

In macos the connection succeeds and the client is able to send data to the server, however the [{{recv_callback()}}|https://github.com/apache/mesos/blob/19cf3068b81355818a9c4cdb797883f2b73cdbce/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L236] is never called by the server, indicating that the data is never received.

As a sidenote, the output of the client, which runs as subprocess is always discarded by the tests, which could cause some confusion for anyone trying to debug this issue. To get any output, one needs to change the std out from [{{process::Subprocess::PIPE()}}|https://github.com/apache/mesos/blob/19cf3068b81355818a9c4cdb797883f2b73cdbce/3rdparty/libprocess/include/process/ssl/gtest.hpp#L372] to {{process::subprocess::FD(STDOUT_FILENO)}}.","05/Feb/18 14:14;arojas;I've started a conversation in the [libevent users mailing list|http://archives.seul.org/libevent/users/], the messages thread are [here|http://archives.seul.org/libevent/users/Jan-2018/msg00002.html] and continuation if February [here|http://archives.seul.org/libevent/users/Feb-2018/msg00000.html]","17/May/18 23:32;karya;Now that Ubuntu 18.04 LTS is out, maybe it's time to reprioritize this ticket.","28/Aug/18 14:53;tillt;Interesting findings;
- libevent 2.1.5 fails on macOS 10.13 but works for 10.14.
- libevent 2.1.8 fails on any macOS tested
","29/Aug/18 07:09;arojas;[~tillt] if you check the mailing list of the conversation I had with the guys from libevent, they are willing to help us debug the issue, what they request is a container with a debug version of libevent and a compiled mesos test that he can use to debug. I was timeboxed when I worked on this, so I had to stop but perhaps we can try to get their help again?","29/Aug/18 19:26;tillt;[~arojas] yes I saw that. My current plan is as follows;

1st; integrate version checks into our build systems that make sure we don't run into known incompatible version combinations
2nd; bundle non problematic libevent-2.0.22 and libssl-1.0.2p with Mesos (libprocess) to make sure ""it just works™"" while (1) still secures unbundled builds
3rd; dive deep again into actual debugging while totally involving the libevent mailing list

Generally speaking, (1) and (2) are commonly a good idea in my opinion as long as unbundled builds are an option.","30/Aug/18 20:59;chhsia0;[~tillt] I'm not sure if bundling libssl is a good idea.

Debian 9's patched libevent 2.0.22 works with OpenSSL 1.1.x and Mesos.
If it also works with OpenSSL 1.0.x (which I believe so),
we could try to bundle that version of libevent and avoid bundling SSL.
If you want to go with this route, I might be able to help after my on-call rotation.","31/Aug/18 00:57;tillt;Ah, that patch is gold! - I did not see it before. Thanks [~chhsia0] -- have now attached that baby to this ticket.

I absolutely agree, bundling libssl feels icky given how much it is platform dependent even in its build configuration.","31/Aug/18 00:59;tillt;Meanwhile I also sent a docker-container to the libevent-ML - hoping they can shed some light into this issue.

For your entertainment - reproducing the problem with an extremely verbose libevent 2.1.8 on Ubuntu 18;
{noformat}
docker run -it docker.io/tillt/mesos-debug-libevent-ubuntu18:version1 /root/mesos/build/3rdparty/libprocess/libprocess-tests --gtest_filter=""SSLTest.SSLSocket"" --verbose
{noformat}","31/Aug/18 08:41;alexr;Original libevent-ML thread: http://archives.seul.org/libevent/users/Feb-2018/msg00003.html
Follow-up from Till: http://archives.seul.org/libevent/users/Aug-2018/msg00009.html","06/Sep/18 00:33;tillt;Autotools specific patches:
https://reviews.apache.org/r/68640/
https://reviews.apache.org/r/68641/","10/Sep/18 01:03;tillt;Offending commit in libevent (after that one, the tests break on macOS 10.14):
{noformat}
f4b6284b8393dbabf389ddce734a30f4cdeffa17 is the first bad commit
commit f4b6284b8393dbabf389ddce734a30f4cdeffa17
Author: Azat Khuzhin <a3at.mail@gmail.com>
Date:   Thu Nov 5 17:40:25 2015 +0300

   be_openssl: don't add events during bev creation (like be_sock)

   Using the following examples you can get changes between be_openssl and
   be_sock:
   $ function diff_addr()
   {
       eval diff -u $(printf ""<(strip_addr %s) "" ""$@"")
   }
   $ function strip_addr()
   {
       sed 's/0x[a-zA-Z0-9]*/0xFFFF/g' ""$@""
   }
   $ EVENT_DEBUG_LOGGING_ALL= regress --verbose --no-fork +http/https_connection_retry 2> /tmp/https-retry.log >&2
   $ EVENT_DEBUG_LOGGING_ALL= regress --verbose --no-fork +http/connection_retry 2> /tmp/http-retry.log >&2
   $ diff_addr /tmp/http-retry.log /tmp/https-retry.log

:100644 100644 4afdde27b2ab63cb92de31922688e77d97fc037b 7d469bfad00806fb4f124e55619bfa59813b09f6 M	bufferevent_openssl.c
bisect run success
{noformat}

Will look closer into this....","13/Sep/18 00:18;tillt;By now, we know the problem shows on pretty much any linux distribution using Libevent 2.1.8, not just Ubuntu 17, 18 and macOS.
Recent experiments also show it failing on Fedora 28 as well as a rolling Suse release -- which means that testing for a distro is mostly futile. Only on macOS we saw differences between 10.13 and 10.14.
Overall that means the proposed workaround patch can be simplified a lot.","13/Sep/18 00:38;tillt;I have now also double-checked on the offending commit - it is the very same on Ubuntu 18.04; {{commit f4b6284b8393dbabf389ddce734a30f4cdeffa17}}","13/Sep/18 00:48;tillt;Reverting that one however makes things worse, not better -- would have been too easy.

After a revert, it is not a timeout on the receive of that test but instead;
{noformat}
../../../3rdparty/libprocess/src/tests/ssl_tests.cpp:254: Failure
(socket).failure(): Failed accept: connection error: error:00000000:lib(0):func(0):reason(0)
{noformat}","26/Sep/18 13:24;tillt;{noformat}
commit 88af8fa693047a49ccd866b901520248775c7ec5
Author: Till Toenshoff <toenshoff@me.com>
Date:   Wed Sep 26 15:21:04 2018 +0200

    Added version check and bundling of libevent to autotools.

    Bundles libevent 2.0.22 to ensure functional SSL builds accross all
    supported platforms. Namely macOS and Ubuntu have shown problems when
    using more recent libevent versions. The underlying problem has been
    under investigation for a longer period of time - so far without a
    solution. The bundled libevent includes a patch to make it libssl
    version > 1.0.x compatible. That patch has been extracted from the
    Fedora source package libevent-2.0.22-6.fc27. The resulting libevent
    builds against both, libssl 1.0.x as well as libssl 1.1.x.
    For unbundled builds a version detection of known problematic
    distributions vs. provided libevent is included.

    Review: https://reviews.apache.org/r/68640/
{noformat}
{noformat}
commit 0f9448497ddb259063911811ab3ce5784747ca2d (HEAD -> master, origin/master, origin/HEAD)
Author: Till Toenshoff <toenshoff@me.com>
Date:   Wed Sep 26 15:22:34 2018 +0200

    Added version check and bundling of libevent within libprocess.

    Review: https://reviews.apache.org/r/68641/
{noformat}",,,,,,,,
The linux filesystem isolator should set mode and ownership for host volumes.,MESOS-7069,13040758,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,ipronin,gilbert,gilbert,06/Feb/17 20:14,26/Mar/18 23:25,29/Oct/20 16:32,08/Feb/18 17:59,,,,,,,,,1.4.0,,,,,,containerization,,,,,0,filesystem,linux,volumes,,,,,,"If the host path is a relative path, the linux filesystem isolator should set the mode and ownership for this host volume since it allows non-root user to write to the volume. Note that this is the case of sharing the host fileysystem (without rootfs).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6563,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-02-08 14:39:35.3,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 08 17:58:49 UTC 2018,,,,,,,"0|i39oo7:",9223372036854775807,,,,,gilbert,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,"06/Feb/17 20:17;gilbert;/cc [~ipronin] [~jieyu]","08/Feb/17 14:39;ipronin;Internally we tried adding the same functionality that {{filesystem/shared}} isolator had (described in [my comment in MESOS-6563|https://issues.apache.org/jira/browse/MESOS-6563?focusedCommentId=15683941&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15683941]). This can be the first step.

Also {{Volume}} protobuf has the {{mode}} field. It can be used for setting permissions on the mounted host directory.","08/Feb/17 14:53;pierrecdn;Hi,
Not sure if I do a mistake, but doing the same with only {{filesystem/linux}} rather than {{filesystem/linux,filesystem/shared}} I have different behavior in 1.1...
Basically, my volume configured via `--default_container_info` is now a bind mount and not a pure mount (meaning that the /tmp use-case described is broken in my case).
I'm using both containerized and not containerized tasks AND always set a user to my tasks.","22/Feb/17 07:52;StephanErb;Relevant patch: https://reviews.apache.org/r/56889/","21/Mar/17 17:14;pierrecdn;Sorry to be late testing that. About MESOS-7007, I observe the same behavior when using this patch.","03/Nov/17 12:46;jpepy;Hi, what is the status on this ticket? The review has been stalled for 6 months, and it looks to me that MESOS-5187 has fixed the issue.","03/Nov/17 15:26;naelyn;Did 5187 make it into 1.2.2? If so then it's still broken and this ticket
is relevant.


","08/Feb/18 17:35;jieyu;[~ipronin] is this still an issue? Or we can close this one?","08/Feb/18 17:58;ipronin;[~jieyu] I believe this was fixed in https://reviews.apache.org/r/61122/. Closing this issue.",,,,,,,,,,,,,,,,,,,
IOSwitchboard FDs leaked when containerizer launch fails -- leads to deadlock,MESOS-7050,13039648,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,klueska,klueska,klueska,02/Feb/17 00:00,01/Mar/17 18:17,29/Oct/20 16:32,01/Mar/17 18:15,,,,,,,,,1.2.0,,,,,,,,,,,0,debugging,mesosphere,,,,,,,"If the containizer launch path fails before actually
launching the container, the FDs allocated to the container by the
IOSwitchboard isolator are leaked. This leads to deadlock in
the destroy path because the IOSwitchboard does not shutdown until the
FDs it allocates to the container have been closed. Since the
switchboard doesn't shutdown, the future returned by its 'cleanup()'
function is never satisfied. 

We need a general purpose method for closing the IOSwitchboard FDs when failing in the launch path.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6280,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-02-02 22:45:56.237,,,false,MESOS-7103,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 01 18:15:46 UTC 2017,,,,,,,"0|hzzyyc:r",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 51,Mesosphere Sprint 52,,,,,,,,,,2.0,,1.2.0,,,,,,,,,"02/Feb/17 00:42;klueska;This bug came up during the development of a test for the new health checks beging developed by [~gkleiman] and [~alexr].  The test can be found here: https://reviews.apache.org/r/55901/

The new health checks rely on periodically launching nested container sessions with the agent, which increases the chances of them being destroyed at arbitrary times when the container they are execing into gets destroyed.

In the test we observed here, the setup was:
1) Start the scheduler driver
2) Launch a task group with the default executor that includes a single long running task
3) Wait for the task to return a status of HEALTHY one time
4) Stop the scheduler driver without explicitly waiting for any of the tasks to complete
5) Exit the test

With this setup, all of the ASSERTS in the test itself passed, but the test failed because there were remaining processes once the test exited (after a timeout of 15 seconds):
{noformat}
../../src/tests/cluster.cpp:570: Failure
Failed to wait 15secs for wait
...
[  FAILED  ] HealthCheckTest.DefaultExecutorCmdHealthCheck (15341 ms)
[----------] 1 test from HealthCheckTest (15347 ms total)
[----------] Global test environment tear-down
../../src/tests/environment.cpp:836: Failure
Failed
Tests completed with child processes remaining:
-+- 33680 /Users/klueska/projects/mesos/build/src/.libs/mesos-tests --verbose
 |-+- 33728 /Users/klueska/projects/mesos/build/src/.libs/mesos-containerizer launch
 | \--- 33770 sleep 120
 \--- 33772 /Users/klueska/projects/mesos/build/src/.libs/mesos-io-switchboard --heartbeat_interval=30secs --help=false --socket_address=/tmp/mesos-io-switch
[==========] 1 test from 1 test case ran. (15370 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] HealthCheckTest.DefaultExecutorCmdHealthCheck
 1 FAILED TEST
{noformat}

Exiting the test should cause all containers to be destroyed and cleaned up, so I started digging through the logs to figure out why this wasn't happening. The most interesting section of the log is:
{noformat}I0131 15:48:58.234776 528384 switchboard.cpp:601] Created I/O switchboard server (pid: 33772) listening on socket file '/tmp/mesos-io-switchboard-3491c69e-dbcf-4d13-b0f0-080f7516b2d6' for container 416f480c-fd4f-496b-939d-7a85b42362d6.96251fd8-5c2a-4016-9308-3b7f15e24ba7.96251fd8-5c2a-4016-9308-3b7f15e24ba7-health-check
W0131 15:48:58.236217 4284416 http.cpp:2278] Failed to launch nested container 416f480c-fd4f-496b-939d-7a85b42362d6.96251fd8-5c2a-4016-9308-3b7f15e24ba7.96251fd8-5c2a-4016-9308-3b7f15e24ba7-health-check: Container is being destroyed during preparing
I0131 15:49:03.227895 4284416 slave.cpp:5078] Killing executor 'default' of framework 65f6131d-5602-4f69-875b-fb84e8ad91c5-0000 (via HTTP)
E0131 15:49:03.231164 4820992 process.cpp:2419] Failed to shutdown socket with fd 12: Socket is not connected
E0131 15:49:03.231312 4820992 process.cpp:2419] Failed to shutdown socket with fd 19: Socket is not connected
E0131 15:49:03.231590 4820992 process.cpp:2419] Failed to shutdown socket with fd 9: Socket is not connected
E0131 15:49:03.231788 4820992 process.cpp:2419] Failed to shutdown socket with fd 7: Socket is not connected
I0131 15:49:03.237408 4820992 switchboard.cpp:782] Sending SIGTERM to I/O switchboard server (pid: 33772) since container 416f480c-fd4f-496b-939d-7a85b42362d6.96251fd8-5c2a-4016-9308-3b7f15e24ba7.96251fd8-5c2a-4016-9308-3b7f15e24ba7-health-check is being destroyed
{noformat}

This shows that we are attempting to kill the {{default}} executor framework (which in turn will kill all of its children). However, the timing of this results in that an IOSwitchboard for container {{416f480c-fd4f-496b-939d-7a85b42362d6.96251fd8-5c2a-4016-9308-3b7f15e24ba7.96251fd8-5c2a-4016-9308-3b7f15e24ba7-health-check}} has already been created, but the container itself is being killed during the {{PREPARE}} phase. The IOSwitchboard then get's sent the {{SIGTERM}} signal, but we never see it actually exit.

To debug this further, I added some output to the IOSwitchboard main function to print timestamps when it started, received the SIGTERM, and when it exited. What I observed was that the timestamps for starting the switchboard and sending the SIGTERM signal from the agent logs matched up nicely with the timestamps generated from the switchboard itself. However, the exit timestamp wasn't until 15 seconds later once the entire test was torn down.

This lead me to believe that the IOSwitchboard was being blocked from exiting because of the condition it relies on to exit gracefully. Namely, it requires the FDs it allocates to a container to be closed before it will terminate. It requires this so that it can flush any remaining output the container might be generating before exiting cleanly. We don't yet have a ""failsafe"" in place to force kill the switchboard if it waits in this state for too long. Even if we did, the timeout would likely be on the order or minutes, because something really bad would have to be going on if it got stuck in this state (or if there was a bug as there appears to be now).

Going back to the containerizer code, I noticed that Jie had a comment that we were leaking the IOSwitchboard FDs in cases where the container launch path failed (which is exactly the case being considered here -- remember the line from the agent log for ""{{Container is being destroyed during preparing}}"").

Now I knew why the IOSwitchboard wasn't exiting, but why was the blocking the container itself from being destroyed and causing deadlock?

Well, that has to do with the way the way the destroy path works. When destroy is called, it will loop through all of its isolators calling {{cleanup()}} on each of them and waiting for all of their futures to be satisfied before fully destroying the container. When it hits the IOSwitchboard (which is an isolator), its cleanup function was blocked on the the switchboard itself exiting. Since its FDs were leaked and the switchboard could never exit, the destroy path was blocked, resulting in the observed deadlock.

The proposed fix then is to make sure that the IOSwitchboard FDs are not leaked on all code paths where the container->launch() call might fail.","02/Feb/17 00:42;klueska;https://reviews.apache.org/r/56195","02/Feb/17 22:45;jieyu;Wondering why command health check needs io switchboard?","03/Feb/17 20:11;vinodkone;Because in the future check output needs to be passed to the scheduler. ","03/Feb/17 20:20;jieyu;OK, sounds to me that you need to get the output, but you definitely don't need the `tee` functionality from io switchboard. Can we think about how to get the output in a way that does not require io switchboard. I think it should be doable. Just need to make the logger configurable for that container, and probably introduce an ""in-memory"" logger.

fork-exec io switchboard for each command health check sounds really unfortunate and expensive.","03/Feb/17 20:26;klueska;We talked about having a way to launch a switchboard in the agent instead
of as a separate process. This seems like a use case for that combined with
an in memory logger.


","03/Feb/17 20:43;klueska;Right now though, the logger is an agent flag, so you can't pick the logger
used on a per container basis.

It sounds like what we really want is a way to launch a nested container
with DEBUG semantics that doesn't log at all and just makes its output
available to external connections.

I'm fact these are the semantics we usually want for any launch nested
container session. In all cases it could be handled in the agent without
requiring an external process (because the lifetime of the container is
tied to the lifetime of the connection).

We originally implemented the switchboard as an external process to handle
""attach"" semantics for containers that continue to run across agent
restarts.

For any session based containers though, there is no real need for an
external process (or logging for that matter).

We decided to keep the external process for sessions just to keep things
simple, since they only would only be in itnitiated very infrequently by a
user for debugging purposes.

Maybe it's worth revising this now that we will have health checks
launching nested containers very frequently.


","03/Feb/17 21:22;vinodkone;Yea, if we don't have to launch IO switchboard for every health checks, that's probably better. We opted for this approach for the MVP because the long term solution seemed quite involved?

Couple more questions though

1) I would imagine `tee` functionality being useful for debugging because someone can look in the stdout/stderr files to know what happened when an health check went wrong. Alternatively, the executor can dump the information it gets from HTTP response in the sandox.

2) Is IO switchboard per health check really expensive? We are already launching one process per command health check (the actual command), what's so worse about launching 2 (command process and IO switchboard) instead ? Just curious. ","10/Feb/17 08:04;adam-mesos;I removed the targetVersion. It didn't make it into 1.2.0-rc1, and it sounds like the real fix would be non-trivial.
Feel free to retarget to 1.3.0 if/when you like.","22/Feb/17 19:59;klueska;Updated review: https://reviews.apache.org/r/56195/","22/Feb/17 20:00;klueska;This should be backported to 1.2.x assuming it makes it in before the next RC is cut.","28/Feb/17 15:12;gkleiman;I applied the chain (https://reviews.apache.org/r/56195/) to make sure that it fixes the test I introduce in https://reviews.apache.org/r/55901/.

It improves things a bit. It used to fail consistently on macOS and now it is flaky.

What I observed is that for some reason the agent sometimes waits ~15 seconds before sending SIGTERM to the I/O Switchboard

Here are the logs of one failed run:

{noformat}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from HealthCheckTest
[ RUN      ] HealthCheckTest.DefaultExecutorCommandHealthCheck
I0228 14:29:19.078368 3475919808 cluster.cpp:160] Creating default 'local' authorizer
I0228 14:29:19.084883 238907392 master.cpp:383] Master 98c48dab-fd2b-404e-85dc-4ec5dd0d635c (172.18.8.139) started on 172.18.8.139:55836
I0228 14:29:19.084915 238907392 master.cpp:385] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/private/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/xZZCGr/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/Users/gaston/mesos/master/share/mesos/webui"" --work_dir=""/private/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/xZZCGr/master"" --zk_session_timeout=""10secs""
I0228 14:29:19.086030 238907392 master.cpp:435] Master only allowing authenticated frameworks to register
I0228 14:29:19.086041 238907392 master.cpp:449] Master only allowing authenticated agents to register
I0228 14:29:19.086046 238907392 master.cpp:462] Master only allowing authenticated HTTP frameworks to register
I0228 14:29:19.086050 238907392 credentials.hpp:37] Loading credentials for authentication from '/private/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/xZZCGr/credentials'
I0228 14:29:19.086334 238907392 master.cpp:507] Using default 'crammd5' authenticator
I0228 14:29:19.086369 238907392 authenticator.cpp:519] Initializing server SASL
I0228 14:29:19.100981 238907392 auxprop.cpp:73] Initialized in-memory auxiliary property plugin
I0228 14:29:19.101080 238907392 http.cpp:933] Using default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
I0228 14:29:19.101274 238907392 http.cpp:933] Using default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
I0228 14:29:19.101414 238907392 http.cpp:933] Using default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
I0228 14:29:19.101528 238907392 master.cpp:587] Authorization enabled
I0228 14:29:19.101702 240517120 hierarchical.cpp:161] Initialized hierarchical allocator process
I0228 14:29:19.101740 239443968 whitelist_watcher.cpp:77] No whitelist given
I0228 14:29:19.105717 240517120 master.cpp:2122] Elected as the leading master!
I0228 14:29:19.105738 240517120 master.cpp:1646] Recovering from registrar
I0228 14:29:19.105811 239443968 registrar.cpp:329] Recovering registrar
I0228 14:29:19.111151 239443968 registrar.cpp:362] Successfully fetched the registry (0B) in 5.309952ms
I0228 14:29:19.111287 239443968 registrar.cpp:461] Applied 1 operations in 35us; attempting to update the registry
I0228 14:29:19.111824 239443968 registrar.cpp:506] Successfully updated the registry in 500992ns
I0228 14:29:19.111928 239443968 registrar.cpp:392] Successfully recovered registrar
I0228 14:29:19.112283 237834240 master.cpp:1760] Recovered 0 agents from the registry (136B); allowing 10mins for agents to re-register
I0228 14:29:19.112303 239980544 hierarchical.cpp:188] Skipping recovery of hierarchical allocator: nothing to recover
I0228 14:29:19.114262 3475919808 containerizer.cpp:220] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0228 14:29:19.114516 3475919808 provisioner.cpp:249] Using default backend 'copy'
I0228 14:29:19.116535 3475919808 cluster.cpp:446] Creating default 'local' authorizer
I0228 14:29:19.117291 238370816 slave.cpp:211] Mesos agent started on (1)@172.18.8.139:55836
I0228 14:29:19.117319 238370816 slave.cpp:212] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/mesos/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_3qqMz4/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_3qqMz4/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_3qqMz4/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/Users/gaston/mesosphere/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --runtime_dir=""/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_3qqMz4"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF""
I0228 14:29:19.117663 238370816 credentials.hpp:86] Loading credential for authentication from '/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_3qqMz4/credential'
I0228 14:29:19.117790 238370816 slave.cpp:354] Agent using credential for: test-principal
I0228 14:29:19.117808 238370816 credentials.hpp:37] Loading credentials for authentication from '/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_3qqMz4/http_credentials'
I0228 14:29:19.117972 238370816 http.cpp:933] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I0228 14:29:19.118873 3475919808 sched.cpp:232] Version: 1.3.0
I0228 14:29:19.118849 238370816 slave.cpp:541] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0228 14:29:19.118916 238370816 slave.cpp:549] Agent attributes: [  ]
I0228 14:29:19.118928 238370816 slave.cpp:554] Agent hostname: 172.18.8.139
I0228 14:29:19.119047 238907392 status_update_manager.cpp:177] Pausing sending status updates
I0228 14:29:19.119207 238907392 sched.cpp:336] New master detected at master@172.18.8.139:55836
I0228 14:29:19.119254 238907392 sched.cpp:407] Authenticating with master master@172.18.8.139:55836
I0228 14:29:19.119282 238907392 sched.cpp:414] Using default CRAM-MD5 authenticatee
I0228 14:29:19.119418 239443968 authenticatee.cpp:97] Initializing client SASL
I0228 14:29:19.119933 238907392 state.cpp:62] Recovering state from '/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF/meta'
I0228 14:29:19.120177 241590272 status_update_manager.cpp:203] Recovering status update manager
I0228 14:29:19.120388 239980544 containerizer.cpp:599] Recovering containerizer
I0228 14:29:19.121381 238907392 provisioner.cpp:410] Provisioner recovery complete
I0228 14:29:19.121618 238370816 slave.cpp:5559] Finished recovery
I0228 14:29:19.122174 238370816 slave.cpp:5733] Querying resource estimator for oversubscribable resources
I0228 14:29:19.122339 239980544 status_update_manager.cpp:177] Pausing sending status updates
I0228 14:29:19.122339 238370816 slave.cpp:931] New master detected at master@172.18.8.139:55836
I0228 14:29:19.122413 238370816 slave.cpp:966] Detecting new master
I0228 14:29:19.122500 238370816 slave.cpp:5747] Received oversubscribable resources {} from the resource estimator
I0228 14:29:19.122725 239443968 authenticatee.cpp:121] Creating new client SASL connection
I0228 14:29:19.122872 241053696 master.cpp:7210] Authenticating scheduler-5711276a-2d47-49db-a175-98964e58f90c@172.18.8.139:55836
I0228 14:29:19.122949 237834240 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1)@172.18.8.139:55836
I0228 14:29:19.123051 240517120 authenticator.cpp:98] Creating new server SASL connection
I0228 14:29:19.123159 238907392 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0228 14:29:19.123199 238907392 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0228 14:29:19.123262 238907392 authenticator.cpp:204] Received SASL authentication start
I0228 14:29:19.123308 238907392 authenticator.cpp:326] Authentication requires more steps
I0228 14:29:19.123378 238907392 authenticatee.cpp:259] Received SASL authentication step
I0228 14:29:19.123486 238370816 authenticator.cpp:232] Received SASL authentication step
I0228 14:29:19.123517 238370816 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'Gastons-MacBook-Pro-2.local' server FQDN: 'Gastons-MacBook-Pro-2.local' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
I0228 14:29:19.123544 238370816 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0228 14:29:19.123574 238370816 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0228 14:29:19.123589 238370816 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'Gastons-MacBook-Pro-2.local' server FQDN: 'Gastons-MacBook-Pro-2.local' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
I0228 14:29:19.123601 238370816 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0228 14:29:19.123610 238370816 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0228 14:29:19.123622 238370816 authenticator.cpp:318] Authentication success
I0228 14:29:19.123672 238907392 authenticatee.cpp:299] Authentication success
I0228 14:29:19.123710 239443968 master.cpp:7240] Successfully authenticated principal 'test-principal' at scheduler-5711276a-2d47-49db-a175-98964e58f90c@172.18.8.139:55836
I0228 14:29:19.123755 241590272 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1)@172.18.8.139:55836
I0228 14:29:19.123926 241053696 sched.cpp:513] Successfully authenticated with master master@172.18.8.139:55836
I0228 14:29:19.123946 241053696 sched.cpp:836] Sending SUBSCRIBE call to master@172.18.8.139:55836
I0228 14:29:19.124032 241053696 sched.cpp:869] Will retry registration in 788.765853ms if necessary
I0228 14:29:19.124119 238370816 master.cpp:2789] Received SUBSCRIBE call for framework 'default' at scheduler-5711276a-2d47-49db-a175-98964e58f90c@172.18.8.139:55836
I0228 14:29:19.124137 238370816 master.cpp:2158] Authorizing framework principal 'test-principal' to receive offers for roles '{ * }'
I0228 14:29:19.124435 238907392 master.cpp:2866] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0228 14:29:19.124768 238370816 sched.cpp:759] Framework registered with 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.124790 241590272 hierarchical.cpp:286] Added framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.124850 238370816 sched.cpp:773] Scheduler::registered took 66us
I0228 14:29:19.124866 241590272 hierarchical.cpp:1838] No allocations performed
I0228 14:29:19.124884 241590272 hierarchical.cpp:1928] No inverse offers to send out!
I0228 14:29:19.124902 241590272 hierarchical.cpp:1422] Performed allocation for 0 agents in 72us
I0228 14:29:19.132097 239443968 slave.cpp:993] Authenticating with master master@172.18.8.139:55836
I0228 14:29:19.132141 239443968 slave.cpp:1004] Using default CRAM-MD5 authenticatee
I0228 14:29:19.132225 241053696 authenticatee.cpp:121] Creating new client SASL connection
I0228 14:29:19.132381 241590272 master.cpp:7210] Authenticating slave(1)@172.18.8.139:55836
I0228 14:29:19.132449 238907392 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(2)@172.18.8.139:55836
I0228 14:29:19.132587 238370816 authenticator.cpp:98] Creating new server SASL connection
I0228 14:29:19.132699 239980544 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0228 14:29:19.132719 239980544 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0228 14:29:19.132807 237834240 authenticator.cpp:204] Received SASL authentication start
I0228 14:29:19.132843 237834240 authenticator.cpp:326] Authentication requires more steps
I0228 14:29:19.132930 241053696 authenticatee.cpp:259] Received SASL authentication step
I0228 14:29:19.133054 241590272 authenticator.cpp:232] Received SASL authentication step
I0228 14:29:19.133078 241590272 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'Gastons-MacBook-Pro-2.local' server FQDN: 'Gastons-MacBook-Pro-2.local' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
I0228 14:29:19.133087 241590272 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0228 14:29:19.133097 241590272 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0228 14:29:19.133107 241590272 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'Gastons-MacBook-Pro-2.local' server FQDN: 'Gastons-MacBook-Pro-2.local' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
I0228 14:29:19.133116 241590272 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0228 14:29:19.133123 241590272 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0228 14:29:19.133134 241590272 authenticator.cpp:318] Authentication success
I0228 14:29:19.133209 239443968 authenticatee.cpp:299] Authentication success
I0228 14:29:19.133249 241053696 master.cpp:7240] Successfully authenticated principal 'test-principal' at slave(1)@172.18.8.139:55836
I0228 14:29:19.133347 237834240 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(2)@172.18.8.139:55836
I0228 14:29:19.133471 238370816 slave.cpp:1088] Successfully authenticated with master master@172.18.8.139:55836
I0228 14:29:19.133594 238370816 slave.cpp:1516] Will retry registration in 15.661984ms if necessary
I0228 14:29:19.133688 240517120 master.cpp:5423] Registering agent at slave(1)@172.18.8.139:55836 (172.18.8.139) with id 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0
I0228 14:29:19.133873 239443968 registrar.cpp:461] Applied 1 operations in 37us; attempting to update the registry
I0228 14:29:19.134423 239443968 registrar.cpp:506] Successfully updated the registry in 525824ns
I0228 14:29:19.134778 241590272 slave.cpp:4347] Received ping from slave-observer(1)@172.18.8.139:55836
I0228 14:29:19.134827 239980544 master.cpp:5497] Registered agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0 at slave(1)@172.18.8.139:55836 (172.18.8.139) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0228 14:29:19.134873 241590272 slave.cpp:1134] Registered with master master@172.18.8.139:55836; given agent ID 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0
I0228 14:29:19.134896 241590272 fetcher.cpp:94] Clearing fetcher cache
I0228 14:29:19.135002 240517120 status_update_manager.cpp:184] Resuming sending status updates
I0228 14:29:19.135016 238907392 hierarchical.cpp:518] Added agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0 (172.18.8.139) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: {})
I0228 14:29:19.135195 241590272 slave.cpp:1162] Checkpointing SlaveInfo to '/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF/meta/slaves/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0/slave.info'
I0228 14:29:19.135504 241590272 slave.cpp:1200] Forwarding total oversubscribed resources {}
I0228 14:29:19.135565 238907392 hierarchical.cpp:1928] No inverse offers to send out!
I0228 14:29:19.135587 238907392 hierarchical.cpp:1422] Performed allocation for 1 agents in 512us
I0228 14:29:19.135793 239443968 master.cpp:7039] Sending 1 offers to framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 (default) at scheduler-5711276a-2d47-49db-a175-98964e58f90c@172.18.8.139:55836
I0228 14:29:19.135916 239443968 master.cpp:6056] Received update of agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0 at slave(1)@172.18.8.139:55836 (172.18.8.139) with total oversubscribed resources {}
I0228 14:29:19.136096 241053696 sched.cpp:933] Scheduler::resourceOffers took 109us
I0228 14:29:19.141821 237834240 master.cpp:3825] Processing ACCEPT call for offers: [ 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-O0 ] on agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0 at slave(1)@172.18.8.139:55836 (172.18.8.139) for framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 (default) at scheduler-5711276a-2d47-49db-a175-98964e58f90c@172.18.8.139:55836
I0228 14:29:19.141887 237834240 master.cpp:3401] Authorizing framework principal 'test-principal' to launch task d36c01f3-5d2f-4772-883b-db620421324e
I0228 14:29:19.143775 237834240 master.cpp:9101] Adding task d36c01f3-5d2f-4772-883b-db620421324e with resources cpus(*)(allocated: *):1.9; mem(*)(allocated: *):992; disk(*)(allocated: *):992; ports(*)(allocated: *):[31000-32000] on agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0 at slave(1)@172.18.8.139:55836 (172.18.8.139)
I0228 14:29:19.143901 237834240 master.cpp:4680] Launching task group { d36c01f3-5d2f-4772-883b-db620421324e } of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 (default) at scheduler-5711276a-2d47-49db-a175-98964e58f90c@172.18.8.139:55836 with resources cpus(*)(allocated: *):1.9; mem(*)(allocated: *):992; disk(*)(allocated: *):992; ports(*)(allocated: *):[31000-32000] on agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0 at slave(1)@172.18.8.139:55836 (172.18.8.139)
I0228 14:29:19.144158 239443968 slave.cpp:1626] Got assigned task group containing tasks [ d36c01f3-5d2f-4772-883b-db620421324e ] for framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.144616 239443968 slave.cpp:1786] Launching task group containing tasks [ d36c01f3-5d2f-4772-883b-db620421324e ] for framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.145138 239443968 paths.cpp:547] Trying to chown '/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF/slaves/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0/frameworks/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000/executors/default/runs/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452' to user 'gaston'
I0228 14:29:19.145843 239443968 slave.cpp:6481] Launching executor 'default' of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 with resources cpus(*)(allocated: *):0.1; mem(*)(allocated: *):32; disk(*)(allocated: *):32 in work directory '/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF/slaves/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0/frameworks/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000/executors/default/runs/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452'
I0228 14:29:19.146229 240517120 containerizer.cpp:992] Starting container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452 for executor 'default' of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.146332 239443968 slave.cpp:2119] Queued task group containing tasks [ d36c01f3-5d2f-4772-883b-db620421324e ] for executor 'default' of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.146389 239443968 slave.cpp:884] Successfully attached file '/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF/slaves/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0/frameworks/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000/executors/default/runs/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452'
I0228 14:29:19.148890 241590272 containerizer.cpp:1489] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""command"":{""arguments"":[""mesos-default-executor"",""--launcher_dir=\/Users\/gaston\/mesosphere\/mesos\/build\/src""],""shell"":false,""value"":""\/Users\/gaston\/mesosphere\/mesos\/build\/src\/mesos-default-executor""},""environment"":{""variables"":[{""name"":""LIBPROCESS_PORT"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_AGENT_ENDPOINT"",""type"":""VALUE"",""value"":""172.18.8.139:55836""},{""name"":""MESOS_CHECKPOINT"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_DIRECTORY"",""type"":""VALUE"",""value"":""\/var\/folders\/jr\/17y2w4ld019bsn9vhx0c13f80000gn\/T\/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF\/slaves\/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0\/frameworks\/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000\/executors\/default\/runs\/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452""},{""name"":""MESOS_EXECUTOR_ID"",""type"":""VALUE"",""value"":""default""},{""name"":""MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD"",""type"":""VALUE"",""value"":""5secs""},{""name"":""MESOS_FRAMEWORK_ID"",""type"":""VALUE"",""value"":""98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000""},{""name"":""MESOS_HTTP_COMMAND_EXECUTOR"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_SLAVE_ID"",""type"":""VALUE"",""value"":""98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0""},{""name"":""MESOS_SLAVE_PID"",""type"":""VALUE"",""value"":""slave(1)@172.18.8.139:55836""},{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""\/var\/folders\/jr\/17y2w4ld019bsn9vhx0c13f80000gn\/T\/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF\/slaves\/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0\/frameworks\/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000\/executors\/default\/runs\/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452""}]},""user"":""gaston"",""working_directory"":""\/var\/folders\/jr\/17y2w4ld019bsn9vhx0c13f80000gn\/T\/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF\/slaves\/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0\/frameworks\/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000\/executors\/default\/runs\/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452""}"" --pipe_read=""6"" --pipe_write=""8"" --runtime_directory=""/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_3qqMz4/containers/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452""'
I0228 14:29:19.153650 241590272 launcher.cpp:135] Forked child with pid '16006' for container 'a49b6fdc-56e6-4ccf-b400-83aaf3ad8452'
I0228 14:29:19.156491 238907392 fetcher.cpp:353] Starting to fetch URIs for container: a49b6fdc-56e6-4ccf-b400-83aaf3ad8452, directory: /var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF/slaves/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0/frameworks/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000/executors/default/runs/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452
I0228 14:29:19.243399 240517120 process.cpp:3704] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1/executor'
I0228 14:29:19.244144 239443968 http.cpp:327] HTTP POST for /slave(1)/api/v1/executor from 172.18.8.139:55843
I0228 14:29:19.244298 239443968 slave.cpp:3170] Received Subscribe request for HTTP executor 'default' of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.245337 238370816 slave.cpp:2361] Sending queued task group task group containing tasks [ d36c01f3-5d2f-4772-883b-db620421324e ] to executor 'default' of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 (via HTTP)
I0228 14:29:19.258466 238370816 process.cpp:3704] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1'
I0228 14:29:19.258891 241053696 http.cpp:327] HTTP POST for /slave(1)/api/v1 from 172.18.8.139:55845
I0228 14:29:19.259102 241053696 http.cpp:541] Processing call LAUNCH_NESTED_CONTAINER
I0228 14:29:19.260445 239443968 containerizer.cpp:1756] Starting nested container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6
I0228 14:29:19.260642 239443968 containerizer.cpp:1780] Trying to chown '/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF/slaves/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0/frameworks/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000/executors/default/runs/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452/containers/f80bfc66-462a-421a-9ea2-3c46aa812ae6' to user 'gaston'
I0228 14:29:19.262100 238907392 containerizer.cpp:1489] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""command"":{""shell"":true,""value"":""sleep 120""},""environment"":{""variables"":[{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""\/var\/folders\/jr\/17y2w4ld019bsn9vhx0c13f80000gn\/T\/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF\/slaves\/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0\/frameworks\/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000\/executors\/default\/runs\/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452\/containers\/f80bfc66-462a-421a-9ea2-3c46aa812ae6""}]},""user"":""gaston"",""working_directory"":""\/var\/folders\/jr\/17y2w4ld019bsn9vhx0c13f80000gn\/T\/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF\/slaves\/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0\/frameworks\/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000\/executors\/default\/runs\/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452\/containers\/f80bfc66-462a-421a-9ea2-3c46aa812ae6""}"" --pipe_read=""10"" --pipe_write=""11"" --runtime_directory=""/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_3qqMz4/containers/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452/containers/f80bfc66-462a-421a-9ea2-3c46aa812ae6""'
I0228 14:29:19.263371 238907392 launcher.cpp:135] Forked child with pid '16034' for container 'a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6'
I0228 14:29:19.265130 240517120 fetcher.cpp:353] Starting to fetch URIs for container: a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6, directory: /var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF/slaves/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0/frameworks/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000/executors/default/runs/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452/containers/f80bfc66-462a-421a-9ea2-3c46aa812ae6
I0228 14:29:19.272331 241590272 process.cpp:3704] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1'
I0228 14:29:19.272691 238907392 http.cpp:327] HTTP POST for /slave(1)/api/v1 from 172.18.8.139:55846
I0228 14:29:19.273295 241590272 http.cpp:541] Processing call LAUNCH_NESTED_CONTAINER_SESSION
I0228 14:29:19.273628 241053696 containerizer.cpp:1756] Starting nested container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check
I0228 14:29:19.273794 241053696 containerizer.cpp:1780] Trying to chown '/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF/slaves/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0/frameworks/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000/executors/default/runs/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452/containers/f80bfc66-462a-421a-9ea2-3c46aa812ae6/containers/f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check' to user 'gaston'
I0228 14:29:19.277046 239443968 process.cpp:3704] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1/executor'
E0228 14:29:19.279794 242126848 process.cpp:2426] Failed to shutdown socket with fd 9: Socket is not connected
I0228 14:29:19.280057 238907392 http.cpp:327] HTTP POST for /slave(1)/api/v1/executor from 172.18.8.139:55844
I0228 14:29:19.280192 238907392 slave.cpp:3817] Handling status update TASK_RUNNING (UUID: a6dba5ec-a683-454f-8f74-ff551c831ba9) for task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.280318 240517120 switchboard.cpp:542] Launching 'mesos-io-switchboard' with flags '--heartbeat_interval=""30secs"" --help=""false"" --socket_address=""/tmp/mesos-io-switchboard-60f3f4d2-5ebf-4e7d-bf28-d563d309b88a"" --stderr_from_fd=""15"" --stderr_to_fd=""2"" --stdin_to_fd=""12"" --stdout_from_fd=""13"" --stdout_to_fd=""1"" --tty=""false"" --wait_for_connection=""true""' for container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check
I0228 14:29:19.280520 238907392 process.cpp:3704] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1'
I0228 14:29:19.281404 240517120 switchboard.cpp:572] Created I/O switchboard server (pid: 16044) listening on socket file '/tmp/mesos-io-switchboard-60f3f4d2-5ebf-4e7d-bf28-d563d309b88a' for container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check
I0228 14:29:19.281841 237834240 http.cpp:327] HTTP POST for /slave(1)/api/v1 from 172.18.8.139:55847
I0228 14:29:19.282076 237834240 http.cpp:541] Processing call WAIT_NESTED_CONTAINER
I0228 14:29:19.283356 239443968 containerizer.cpp:1489] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""command"":{""shell"":true,""value"":""exit $STATUS""},""environment"":{""variables"":[{""name"":""STATUS"",""type"":""VALUE"",""value"":""0""}]},""user"":""gaston""}"" --pipe_read=""9"" --pipe_write=""12"" --runtime_directory=""/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_3qqMz4/containers/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452/containers/f80bfc66-462a-421a-9ea2-3c46aa812ae6/containers/f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check""'
I0228 14:29:19.283644 238370816 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: a6dba5ec-a683-454f-8f74-ff551c831ba9) for task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.283686 238370816 status_update_manager.cpp:500] Creating StatusUpdate stream for task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.284668 239443968 launcher.cpp:135] Forked child with pid '16045' for container 'a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check'
I0228 14:29:19.284781 238370816 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: a6dba5ec-a683-454f-8f74-ff551c831ba9) for task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 to the agent
I0228 14:29:19.285346 240517120 slave.cpp:4257] Forwarding the update TASK_RUNNING (UUID: a6dba5ec-a683-454f-8f74-ff551c831ba9) for task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 to master@172.18.8.139:55836
I0228 14:29:19.285527 239980544 slave.cpp:4151] Status update manager successfully handled status update TASK_RUNNING (UUID: a6dba5ec-a683-454f-8f74-ff551c831ba9) for task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.285584 240517120 master.cpp:6201] Status update TASK_RUNNING (UUID: a6dba5ec-a683-454f-8f74-ff551c831ba9) for task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 from agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0 at slave(1)@172.18.8.139:55836 (172.18.8.139)
I0228 14:29:19.285619 240517120 master.cpp:6269] Forwarding status update TASK_RUNNING (UUID: a6dba5ec-a683-454f-8f74-ff551c831ba9) for task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.285775 240517120 master.cpp:8350] Updating the state of task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0228 14:29:19.285989 241053696 sched.cpp:1041] Scheduler::statusUpdate took 130us
I0228 14:29:19.286181 239980544 master.cpp:5137] Processing ACKNOWLEDGE call a6dba5ec-a683-454f-8f74-ff551c831ba9 for task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 (default) at scheduler-5711276a-2d47-49db-a175-98964e58f90c@172.18.8.139:55836 on agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0
I0228 14:29:19.286715 239443968 status_update_manager.cpp:395] Received status update acknowledgement (UUID: a6dba5ec-a683-454f-8f74-ff551c831ba9) for task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.286967 241053696 slave.cpp:3106] Status update manager successfully handled status update acknowledgement (UUID: a6dba5ec-a683-454f-8f74-ff551c831ba9) for task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.287169 240517120 fetcher.cpp:353] Starting to fetch URIs for container: a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check, directory: /var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF/slaves/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0/frameworks/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000/executors/default/runs/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452/containers/f80bfc66-462a-421a-9ea2-3c46aa812ae6/containers/f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check
I0228 14:29:19.333760 238907392 http.cpp:2724] Received EOF attach response for a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check
I0228 14:29:19.334024 239980544 containerizer.cpp:2067] Destroying container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check in RUNNING state
W0228 14:29:19.334038 238907392 http.cpp:2739] Launch nested container session connection for container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check closed
I0228 14:29:19.334241 239980544 launcher.cpp:151] Asked to destroy container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check
I0228 14:29:19.335008 241053696 process.cpp:3704] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1'
I0228 14:29:19.335394 238907392 http.cpp:327] HTTP POST for /slave(1)/api/v1 from 172.18.8.139:55849
I0228 14:29:19.335556 238907392 http.cpp:541] Processing call WAIT_NESTED_CONTAINER
I0228 14:29:19.347013 241053696 switchboard.cpp:885] I/O switchboard server process for container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check has terminated (status=0)
I0228 14:29:19.356932 239980544 containerizer.cpp:2430] Container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check has exited
I0228 14:29:19.357700 240517120 provisioner.cpp:484] Ignoring destroy request for unknown container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check
I0228 14:29:19.357811 239443968 containerizer.cpp:2346] Checkpointing termination state to nested container's runtime directory '/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_3qqMz4/containers/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452/containers/f80bfc66-462a-421a-9ea2-3c46aa812ae6/containers/f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check/termination'
I0228 14:29:19.363263 238907392 process.cpp:3704] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1'
I0228 14:29:19.363633 241590272 http.cpp:327] HTTP POST for /slave(1)/api/v1 from 172.18.8.139:55850
I0228 14:29:19.363831 241590272 http.cpp:541] Processing call LAUNCH_NESTED_CONTAINER_SESSION
I0228 14:29:19.364068 240517120 containerizer.cpp:1756] Starting nested container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check
I0228 14:29:19.364181 240517120 containerizer.cpp:1780] Trying to chown '/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF/slaves/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0/frameworks/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000/executors/default/runs/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452/containers/f80bfc66-462a-421a-9ea2-3c46aa812ae6/containers/f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check' to user 'gaston'
I0228 14:29:19.364517 238370816 process.cpp:3704] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1/executor'
I0228 14:29:19.364897 241590272 http.cpp:327] HTTP POST for /slave(1)/api/v1/executor from 172.18.8.139:55844
I0228 14:29:19.364995 241590272 slave.cpp:3817] Handling status update TASK_RUNNING (UUID: aef04c6c-c29b-45d0-b4bd-c8557cd330ef) for task d36c01f3-5d2f-4772-883b-db620421324e in health state healthy of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.365909 239980544 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: aef04c6c-c29b-45d0-b4bd-c8557cd330ef) for task d36c01f3-5d2f-4772-883b-db620421324e in health state healthy of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.366019 239980544 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: aef04c6c-c29b-45d0-b4bd-c8557cd330ef) for task d36c01f3-5d2f-4772-883b-db620421324e in health state healthy of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 to the agent
I0228 14:29:19.366130 237834240 slave.cpp:4257] Forwarding the update TASK_RUNNING (UUID: aef04c6c-c29b-45d0-b4bd-c8557cd330ef) for task d36c01f3-5d2f-4772-883b-db620421324e in health state healthy of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 to master@172.18.8.139:55836
I0228 14:29:19.366253 237834240 slave.cpp:4151] Status update manager successfully handled status update TASK_RUNNING (UUID: aef04c6c-c29b-45d0-b4bd-c8557cd330ef) for task d36c01f3-5d2f-4772-883b-db620421324e in health state healthy of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.366312 240517120 master.cpp:6201] Status update TASK_RUNNING (UUID: aef04c6c-c29b-45d0-b4bd-c8557cd330ef) for task d36c01f3-5d2f-4772-883b-db620421324e in health state healthy of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 from agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0 at slave(1)@172.18.8.139:55836 (172.18.8.139)
I0228 14:29:19.366348 240517120 master.cpp:6269] Forwarding status update TASK_RUNNING (UUID: aef04c6c-c29b-45d0-b4bd-c8557cd330ef) for task d36c01f3-5d2f-4772-883b-db620421324e in health state healthy of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.366454 240517120 master.cpp:8350] Updating the state of task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0228 14:29:19.366621 238907392 sched.cpp:1041] Scheduler::statusUpdate took 118us
I0228 14:29:19.366901 240517120 master.cpp:5137] Processing ACKNOWLEDGE call aef04c6c-c29b-45d0-b4bd-c8557cd330ef for task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 (default) at scheduler-5711276a-2d47-49db-a175-98964e58f90c@172.18.8.139:55836 on agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0
I0228 14:29:19.367105 241053696 status_update_manager.cpp:395] Received status update acknowledgement (UUID: aef04c6c-c29b-45d0-b4bd-c8557cd330ef) for task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.367364 237834240 slave.cpp:3106] Status update manager successfully handled status update acknowledgement (UUID: aef04c6c-c29b-45d0-b4bd-c8557cd330ef) for task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.367382 3475919808 sched.cpp:2021] Asked to stop the driver
I0228 14:29:19.367460 239980544 sched.cpp:1203] Stopping framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.367565 241590272 master.cpp:7752] Processing TEARDOWN call for framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 (default) at scheduler-5711276a-2d47-49db-a175-98964e58f90c@172.18.8.139:55836
I0228 14:29:19.367588 241590272 master.cpp:7764] Removing framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 (default) at scheduler-5711276a-2d47-49db-a175-98964e58f90c@172.18.8.139:55836
I0228 14:29:19.367600 241590272 master.cpp:3134] Deactivating framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 (default) at scheduler-5711276a-2d47-49db-a175-98964e58f90c@172.18.8.139:55836
I0228 14:29:19.367761 239980544 hierarchical.cpp:415] Deactivated framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.367810 240517120 slave.cpp:2659] Asked to shut down framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 by master@172.18.8.139:55836
I0228 14:29:19.367827 240517120 slave.cpp:2684] Shutting down framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.367828 241590272 master.cpp:8350] Updating the state of task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I0228 14:29:19.367848 240517120 slave.cpp:5084] Shutting down executor 'default' of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 (via HTTP)
I0228 14:29:19.368228 241590272 master.cpp:8444] Removing task d36c01f3-5d2f-4772-883b-db620421324e with resources cpus(*)(allocated: *):1.9; mem(*)(allocated: *):992; disk(*)(allocated: *):992; ports(*)(allocated: *):[31000-32000] of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 on agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0 at slave(1)@172.18.8.139:55836 (172.18.8.139)
I0228 14:29:19.368283 239443968 hierarchical.cpp:1092] Recovered cpus(*)(allocated: *):1.9; mem(*)(allocated: *):992; disk(*)(allocated: *):992; ports(*)(allocated: *):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*)(allocated: *):0.1; mem(*)(allocated: *):32; disk(*)(allocated: *):32) on agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0 from framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.368439 241590272 master.cpp:8473] Removing executor 'default' with resources cpus(*)(allocated: *):0.1; mem(*)(allocated: *):32; disk(*)(allocated: *):32 of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 on agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0 at slave(1)@172.18.8.139:55836 (172.18.8.139)
I0228 14:29:19.368811 239980544 hierarchical.cpp:1092] Recovered cpus(*)(allocated: *):0.1; mem(*)(allocated: *):32; disk(*)(allocated: *):32 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: {}) on agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0 from framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.368932 239980544 hierarchical.cpp:362] Removed framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:19.372098 237834240 process.cpp:3704] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1'
I0228 14:29:19.372248 238370816 switchboard.cpp:542] Launching 'mesos-io-switchboard' with flags '--heartbeat_interval=""30secs"" --help=""false"" --socket_address=""/tmp/mesos-io-switchboard-3ca09e4e-04c7-424a-b413-414479a93b4e"" --stderr_from_fd=""14"" --stderr_to_fd=""2"" --stdin_to_fd=""11"" --stdout_from_fd=""12"" --stdout_to_fd=""1"" --tty=""false"" --wait_for_connection=""true""' for container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check
I0228 14:29:19.373545 238370816 switchboard.cpp:572] Created I/O switchboard server (pid: 16078) listening on socket file '/tmp/mesos-io-switchboard-3ca09e4e-04c7-424a-b413-414479a93b4e' for container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check
I0228 14:29:19.373714 239443968 http.cpp:327] HTTP POST for /slave(1)/api/v1 from 172.18.8.139:55851
I0228 14:29:19.373986 239443968 http.cpp:541] Processing call KILL_NESTED_CONTAINER
I0228 14:29:19.374460 240517120 containerizer.cpp:2067] Destroying container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6 in RUNNING state
I0228 14:29:19.374486 240517120 containerizer.cpp:2067] Destroying container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check in PREPARING state
I0228 14:29:19.374856 240517120 containerizer.cpp:2138] Waiting for the isolators to complete preparing before destroying container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check
W0228 14:29:19.375854 241590272 http.cpp:2303] Failed to launch nested container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check: Container is being destroyed during preparing
I0228 14:29:19.375948 240517120 process.cpp:3761] Failed to process request for '/slave(1)/api/v1': Container is being destroyed during preparing
I0228 14:29:19.375988 239980544 process.cpp:1456] Returning '500 Internal Server Error' for '/slave(1)/api/v1' (Container is being destroyed during preparing)
I0228 14:29:20.104522 239443968 hierarchical.cpp:1838] No allocations performed
I0228 14:29:20.104562 239443968 hierarchical.cpp:1422] Performed allocation for 1 agents in 105us
I0228 14:29:21.105509 237834240 hierarchical.cpp:1838] No allocations performed
I0228 14:29:21.105546 237834240 hierarchical.cpp:1422] Performed allocation for 1 agents in 104us
I0228 14:29:22.108916 238907392 hierarchical.cpp:1838] No allocations performed
I0228 14:29:22.108978 238907392 hierarchical.cpp:1422] Performed allocation for 1 agents in 166us
I0228 14:29:23.114136 241590272 hierarchical.cpp:1838] No allocations performed
I0228 14:29:23.114208 241590272 hierarchical.cpp:1422] Performed allocation for 1 agents in 136us
I0228 14:29:24.118439 239443968 hierarchical.cpp:1838] No allocations performed
I0228 14:29:24.118491 239443968 hierarchical.cpp:1422] Performed allocation for 1 agents in 121us
I0228 14:29:24.368199 238370816 slave.cpp:5157] Killing executor 'default' of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 (via HTTP)
I0228 14:29:24.368348 240517120 containerizer.cpp:2067] Destroying container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452 in RUNNING state
E0228 14:29:24.370486 242126848 process.cpp:2426] Failed to shutdown socket with fd 17: Socket is not connected
E0228 14:29:24.370611 242126848 process.cpp:2426] Failed to shutdown socket with fd 18: Socket is not connected
E0228 14:29:24.370775 242126848 process.cpp:2426] Failed to shutdown socket with fd 6: Socket is not connected
E0228 14:29:24.371027 242126848 process.cpp:2426] Failed to shutdown socket with fd 8: Socket is not connected
I0228 14:29:24.376883 242126848 switchboard.cpp:786] Sending SIGTERM to I/O switchboard server (pid: 16078) since container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check is being destroyed
I0228 14:29:24.402745 239443968 switchboard.cpp:885] I/O switchboard server process for container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check has terminated (status=0)
I0228 14:29:24.402770 240517120 containerizer.cpp:2430] Container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452 has exited
I0228 14:29:24.403672 237834240 provisioner.cpp:484] Ignoring destroy request for unknown container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6.f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check
I0228 14:29:24.403811 241053696 containerizer.cpp:2346] Checkpointing termination state to nested container's runtime directory '/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_3qqMz4/containers/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452/containers/f80bfc66-462a-421a-9ea2-3c46aa812ae6/containers/f80bfc66-462a-421a-9ea2-3c46aa812ae6-health-check/termination'
I0228 14:29:24.404547 241053696 launcher.cpp:151] Asked to destroy container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6
I0228 14:29:24.506726 240517120 containerizer.cpp:2430] Container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6 has exited
I0228 14:29:24.507402 239443968 provisioner.cpp:484] Ignoring destroy request for unknown container a49b6fdc-56e6-4ccf-b400-83aaf3ad8452.f80bfc66-462a-421a-9ea2-3c46aa812ae6
I0228 14:29:24.507544 237834240 containerizer.cpp:2346] Checkpointing termination state to nested container's runtime directory '/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_3qqMz4/containers/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452/containers/f80bfc66-462a-421a-9ea2-3c46aa812ae6/termination'
I0228 14:29:24.508003 237834240 process.cpp:3761] Failed to process request for '/slave(1)/api/v1': discarded
I0228 14:29:24.508090 237834240 process.cpp:3761] Failed to process request for '/slave(1)/api/v1': discarded
E0228 14:29:24.508436 238907392 slave.cpp:4748] Termination of executor 'default' of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 failed: Failed to destroy nested containers: discarded
I0228 14:29:24.508592 238907392 slave.cpp:4870] Cleaning up executor 'default' of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 (via HTTP)
W0228 14:29:24.508638 241590272 master.cpp:6325] Ignoring unknown exited executor 'default' of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000 on agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0 at slave(1)@172.18.8.139:55836 (172.18.8.139)
I0228 14:29:24.508813 238370816 gc.cpp:55] Scheduling '/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF/slaves/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0/frameworks/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000/executors/default/runs/a49b6fdc-56e6-4ccf-b400-83aaf3ad8452' for gc 6.99999411208296days in the future
../../src/tests/health_check_tests.cpp:2239: Failure
(containerizer->wait(containerId)).failure(): Failed to destroy nested containers: discarded
I0228 14:29:24.508894 238907392 slave.cpp:4958] Cleaning up framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:24.508911 238370816 gc.cpp:55] Scheduling '/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF/slaves/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0/frameworks/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000/executors/default' for gc 6.99999411031407days in the future
I0228 14:29:24.508985 238370816 status_update_manager.cpp:285] Closing status update streams for framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:24.509016 238370816 status_update_manager.cpp:531] Cleaning up status update stream for task d36c01f3-5d2f-4772-883b-db620421324e of framework 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000
I0228 14:29:24.509093 237834240 gc.cpp:55] Scheduling '/var/folders/jr/17y2w4ld019bsn9vhx0c13f80000gn/T/HealthCheckTest_DefaultExecutorCommandHealthCheck_tLRhsF/slaves/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0/frameworks/98c48dab-fd2b-404e-85dc-4ec5dd0d635c-0000' for gc 6.99999410862222days in the future
../../src/tests/cluster.cpp:576: Failure
Value of: containers.get().empty()
  Actual: false
Expected: true
Failed to destroy containers: { a49b6fdc-56e6-4ccf-b400-83aaf3ad8452 }
I0228 14:29:24.510141 238370816 slave.cpp:803] Agent terminating
I0228 14:29:24.510295 237834240 master.cpp:1264] Agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0 at slave(1)@172.18.8.139:55836 (172.18.8.139) disconnected
I0228 14:29:24.510311 237834240 master.cpp:3171] Disconnecting agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0 at slave(1)@172.18.8.139:55836 (172.18.8.139)
I0228 14:29:24.510367 237834240 master.cpp:3190] Deactivating agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0 at slave(1)@172.18.8.139:55836 (172.18.8.139)
I0228 14:29:24.510488 239980544 hierarchical.cpp:646] Agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0 deactivated
I0228 14:29:24.513273 239443968 master.cpp:1103] Master terminating
I0228 14:29:24.513537 237834240 hierarchical.cpp:551] Removed agent 98c48dab-fd2b-404e-85dc-4ec5dd0d635c-S0
[  FAILED  ] HealthCheckTest.DefaultExecutorCommandHealthCheck (5442 ms)
[----------] 1 test from HealthCheckTest (5447 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (5467 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] HealthCheckTest.DefaultExecutorCommandHealthCheck

 1 FAILED TEST
{noformat}","01/Mar/17 18:15;klueska;{noformat}
commit cbb99ce0a60c08eb0156461a8c3c3ef5ce8646ca
Author: Kevin Klues <klueska@gmail.com>
Date:   Wed Mar 1 09:55:13 2017 -0800

    Renamed 'SubprocessInfo' to 'ContainerIO' in 'ContainerLogger'.
    
    Renamed 'SubprocessInfo' to 'ContainerIO' in 'ContainerLogger'.
    
    Review: https://reviews.apache.org/r/57121/
{noformat}
{noformat}
commit 3aa7e1b67074eb67794bf2e4c18ff24d2ecf37f3
Author: Kevin Klues <klueska@gmail.com>
Date:   Wed Mar 1 09:55:18 2017 -0800

    Added STDIN IO to 'ContainerLogger::ContainerIO' class.
    
    Added STDIN IO to 'ContainerLogger::ContainerIO' class.
    
    Review: https://reviews.apache.org/r/57122/
{noformat}
{noformat}
commit 48c376a5fdbee4569e4af9144139d638c7924548
Author: Kevin Klues <klueska@gmail.com>
Date:   Wed Mar 1 09:55:21 2017 -0800

    Fixed ContainerLogger / IOSwitchboard FD leaks.
    
    Previously, the containizer launch path would leak FDs if the
    containerizer launch path failed between successfully calling
    prepare() on either the ContainerLogger (in the case of the Docker
    containerizer) or the IOSwitchboard (in the case of the mesos
    containerizer) and forking the actual container.
    
    These components relied on the Subprocess call inside launcher->fork()
    to close these FDS on their behalf. If the containerizer launch path
    failed somewhere between calling prepare() and making this fork()
    call, these FDs would never be closed.
    
    In the case of the IOSwitchboard, this would lead to deadlock in the
    destroy path because the future returned by the IOSwitchboard's
    cleanup function would never be satisfied. The IOSwitchboard doesn't
    shutdown until the FDs it allocates to the container have been closed.
    
    This commit fixes this problem by updating the
    ContainerLogger::ContainerIO::FD abstraction to change the way it
    manages FDS. Instead of tagging each FD with the Subprocess::IO::OWNED
    label and forcing the launcher->fork() call to deal with closing the
    FDs once it's forked a new subprocess, we now do things slightly
    differently now.
    
    We now keep the default DUP label on each FD (instead fo changing it
    to OWNED) to cause launcher->fork() to dup it before mapping it onto
    the stdin/stdout/stderr of the subprocess. It then only closes the
    duped FD, leaving the original one open.
    
    In doing so, it's now the containerizer's responsibility to ensure
    that these FDs are closed properly (whether that's between a
    successful prepare() call and launcher->fork()) or after
    launcher->fork() has completed successfully). While this has the
    potential to complicate things slightly on the SUCCESS path,
    at least it is now the containerizers's responsibility to close these
    FDS in *all* cases, rather than splitting that responsibility across
    components.
    
    In order to simplify this, we've also modified the
    ContainerLogger::ContainerIO::FD abstraction to hold a Shared
    pointer to its underlying file descriptor and (optionally) close it on
    destruction. With this, we can ensure that all file descriptors
    created through this abstraction will be automatically closed onced
    their final reference goes out of scope (even if its been copied
    around several times).
    
    In essence, this releases the containerizer from the burden of manually
    closing these FDS itself. So long as it holds the final reference to
    these FDs (which it does), they will be automatically closed along
    *any* path out of containerizer->launch(). These are exactly the
    semantics we want to achieve.
    
    In the case of the the ContainerLogger, ownership of these FDs (and
    thus their final reference) is passed to the containerizer in the
    ContainerIO struct returned by prepare(). In the case of the
    IOSwitchboard, we had to add a new API call to transfer ownership
    (since it is an isolator and prepare() can only return a protobuf),
    but the end result is the same.
    
    Review: https://reviews.apache.org/r/56195/
{noformat}
{noformat}
commit 0d717e08e90a38d9d17864b9d51c45ea0b35b3a5
Author: Kevin Klues <klueska@gmail.com>
Date:   Wed Mar 1 09:55:25 2017 -0800

    Pulled 'ContainerIO' out of the 'ContainerLogger'.
    
    Pulled 'ContainerIO' out of the 'ContainerLogger'.
    
    Review: https://reviews.apache.org/r/57176/
{noformat}",,,,,,,,,,,,,,,
Send SIGKILL after SIGTERM to IOSwitchboard after container termination.,MESOS-7042,13039283,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,qianzhang,vinodkone,vinodkone,31/Jan/17 20:04,16/Jan/19 00:41,29/Oct/20 16:32,08/Jan/19 00:20,,,,,,,,,1.5.2,1.6.2,1.7.1,1.8.0,,,containerization,,,,,0,containerizer,,,,,,,,This is follow up for MESOS-6664,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-01-04 08:05:24.514,,,false,MESOS-7103,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 08 00:20:48 UTC 2019,,,,,,,"0|i39fn3:",9223372036854775807,,,,,gilbert,,,,,,Containerization R9 Sprint 37,,,,,,,,,,,2.0,,,,,,,,,,,"04/Jan/19 08:05;qianzhang;RR: https://reviews.apache.org/r/69667/","08/Jan/19 00:20;gilbert;commit 3478e344fb77d931f6122980c6e94cd3913c441d
Author: Qian Zhang <zhq527725@gmail.com>
Date:   Mon Jan 7 16:16:12 2019 -0800

    Sent SIGKILL to I/O switchboard server as a safeguard.
    
    Review: https://reviews.apache.org/r/69667/",,,,,,,,,,,,,,,,,,,,,,,,,,
Update framework authorization to support multiple roles,MESOS-7022,13038527,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,bbannier,bbannier,27/Jan/17 20:50,26/Mar/18 08:04,29/Oct/20 16:32,15/Feb/17 09:51,,,,,,,,,1.3.0,,,,,,master,,,,,0,security,,,,,,,,"Currently the master assumes that a framework is only in a single role, see {{Master::authorizeFramework}}. This code should be updated to support frameworks with multiple roles. In particular it should get authorization of the framework's principal to register in each of the framework's roles.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-02-15 09:51:23.028,,,false,MESOS-1763,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 15 09:51:23 UTC 2017,,,,,,,"0|hzzz2k:z",9223372036854775807,,,,,adam-mesos,,,,,,Mesosphere Sprint 51,,,,,,,,,,,3.0,,,,,,,,,,,"01/Feb/17 16:49;bbannier;https://reviews.apache.org/r/56178/","15/Feb/17 09:51;adam-mesos;commit 69ba4ac8b1d375ea708b9b5a120f873e3bb00980
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Wed Feb 15 00:43:05 2017 -0800

    Enabled the authorizer to work with MULTI_ROLE frameworks.
    
    This updates the local authorizer so that MULTI_ROLE frameworks can be
    authorized.
    
    For non-MULTI_ROLE frameworks we continue to support use of the
    deprecated 'value' field in the authorization request's 'Object';
    however for MULTI_ROLE frameworks the 'value' field will not be set,
    and authorizers still relying on it should be updated to instead use
    the object's 'framework_info' field to extract roles to authorize
    against from.
    
    Review: https://reviews.apache.org/r/56178/",,,,,,,,,,,,,,,,,,,,,,,,,,
Change `Environment.Variable.Value` from required to optional,MESOS-6991,13037825,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,greggomann,greggomann,greggomann,25/Jan/17 20:07,01/Feb/17 22:18,29/Oct/20 16:32,01/Feb/17 22:18,,,,,,,,,1.2.0,,,,,,,,,,,0,,,,,,,,,"To prepare for future work which will enable the modular fetching of secrets, we should change the {{Environment.Variable.Value}} field from {{required}} to {{optional}}. This way, the field can be left empty and filled in by a secret fetching module.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-02-01 22:18:53.637,,,false,MESOS-6365,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 01 22:18:53 UTC 2017,,,,,,,"0|hzzz30:v",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 49,Mesosphere Sprint 50,,,,,,,,,,2.0,,,,,,,,,,,"25/Jan/17 20:13;greggomann;Reviews here:
https://reviews.apache.org/r/55954/
https://reviews.apache.org/r/55955/","01/Feb/17 22:18;vinodkone;commit 289830368675afb3a46278202687e30aafc6cb03
Author: Greg Mann <greg@mesosphere.io>
Date:   Wed Feb 1 14:18:02 2017 -0800

    Added validation tests to ensure environment variable value is set.
    
    The `value` field within `Environment::Variable` is being
    changed to `optional`, but for the time being we will
    enforce that it must be set for backward compatibility.
    This patch adds tests to ensure that environment variables
    with unset values are correctly rejected.
    
    Review: https://reviews.apache.org/r/55955/

commit 063ecdee574f7d1d491c490bda2dfb956d8c515e
Author: Greg Mann <greg@mesosphere.io>
Date:   Wed Feb 1 14:17:57 2017 -0800

    Changed 'Environment.Variable.Value' from required to optional.
    
    To prepare for future work which will enable the
    modular fetching of secrets, we should change the
    Environment.Variable.Value field from required to
    optional. This way, the field can be left empty
    and filled in by a secret fetching module.
    
    Review: https://reviews.apache.org/r/55954/
",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix BOOST random generator initialization on Windows,MESOS-6973,13036966,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,kaysoky,dpravat,dpravat,23/Jan/17 07:21,23/Mar/18 23:50,29/Oct/20 16:32,08/Feb/18 21:20,,,,,,,,,1.6.0,,,,,,,,,,,0,microsoft,Windows,,,,,,,"seed_rng::seed_rng does not produced the expected result in Windows since is using `/dev/urandom` file.  

0:005> k
 # Child-SP          RetAddr           Call Site
00 00000049`22dfc108 00007ff6`5193822f kernel32!CreateFileW
...
0e 00000049`22dfc660 00007ff6`502228fd mesos_agent!boost::uuids::detail::seed_rng::seed_rng+0x3d [d:\repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\seed_rng.hpp @ 80]
0f 00000049`22dfc690 00007ff6`502591e3 mesos_agent!boost::uuids::detail::seed<boost::random::mersenne_twister_engine<unsigned int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253> >+0x4d [d:\repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\seed_rng.hpp @ 246]
10 00000049`22dfc790 00007ff6`50395518 mesos_agent!boost::uuids::basic_random_generator<boost::random::mersenne_twister_engine<unsigned int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253> >::basic_random_generator<boost::random::mersenne_twister_engine<unsigned int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253> >+0xd3 [d:\repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\random_generator.hpp @ 50]
11 00000049`22dfc800 00007ff6`500ad140 mesos_agent!id::UUID::random+0x78 [d:\repositories\mesoswin\3rdparty\stout\include\stout\uuid.hpp @ 49]
12 00000049`22dfc870 00007ff6`5007ff55 mesos_agent!mesos::internal::slave::Framework::launchExecutor+0x70 [d:\repositories\mesoswin\src\slave\slave.cpp @ 6301]
13 00000049`22dfd520 00007ff6`502a0a35 mesos_agent!mesos::internal::slave::Slave::_run+0x2455 [d:\repositories\mesoswin\src\slave\slave.cpp @ 1990]
...
0:005> du @rcx
000001d7`cc55fb60  ""/dev/urandom""
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-23 13:09:17.046,,,false,MESOS-3094,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Thu Feb 08 21:20:36 UTC 2018,,,,,,,"0|i3928n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,"23/Jan/17 13:09;ipronin;Boost.Uuid should by default use Mersenne Twister generator from Boost.Random. Until Boost 1.59 it was seeding it's internal generators using a combination of {{/dev/urandom}} contents (if it can be opened), times, and something else. Starting from 1.59 it is using {{CryptAcquireContextW()}} instead of {{/dev/urandom}} on Windows.

With Boost 1.53 to work around this we can pass {{boost::uuids::basic_random_generator}} our own instance of a generator modeling the {{UniformRandomNumberGenerator}} concept properly seeded by ourselves.","25/Jan/17 23:18;klueska;[~hausdorff] Do you have any insight on this?","26/Jan/17 00:00;hausdorff;[~klueska] Thanks for the heads up, I don't know about this issue offhand, but I will assign it to me and tag it `microsoft` so we remember to close it before March 1.","06/Apr/17 22:09;andschwa;Quick comment: we may not even want to use Boost's random number library any more, as C++11 came with a better <random> library.","08/Feb/18 21:20;kaysoky;Boost was updated to 1.65, so this should no longer be a problem.  Changing to {{<random>}} can be a later discussion. 
{code}
commit 469363d4322c7acda7fd10acbe8822f610af5a43
Author: Benno Evers <bevers@mesosphere.com>
Date:   Tue Jan 23 14:47:31 2018 +0100

    Updated boost version.
    
    Review: https://reviews.apache.org/r/62161/
{code}",,,,,,,,,,,,,,,,,,,,,,,
Launching two tasks with the same Docker image simultaneously may cause a staging dir never cleaned up,MESOS-6950,13036035,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,qianzhang,qianzhang,qianzhang,19/Jan/17 00:13,17/Aug/17 22:53,29/Oct/20 16:32,11/Aug/17 23:30,,,,,,,,,1.1.3,1.2.3,1.3.2,1.4.0,,,containerization,,,,,0,containerizer,mesosphere,,,,,,,"If user launches two tasks with the same Docker image simultaneously (e.g., run {{mesos-executor}} twice with the same Docker image), there will be a staging directory which is for the second task never cleaned up, like this:
{code}
└── store
    └── docker
        ├── layers
        │    ...
        ├── staging
        │   └── a6rXWC
        └── storedImages
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-19 02:45:56.29,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 11 23:30:34 UTC 2017,,,,,,,"0|hzzyj0:zx",9223372036854775807,,,,,gilbert,,,,,,Mesosphere Sprint 60,Mesosphere Sprint 61,,,,,,,,,,2.0,,,,,,,,,,,"19/Jan/17 00:19;qianzhang;The root cause of this issue is, in the {{_get()}} method of Docker store, we always create the staging directory at the beginning, but we do not remove that staging directory if there is an ongoing pulling for the same image, see the following code for details.
https://github.com/apache/mesos/blob/1.1.0/src/slave/containerizer/mesos/provisioner/docker/store.cpp#L217:L244","19/Jan/17 02:45;klueska;[~jieyu] [~gilbert] Could you please take a look at this?","19/Jan/17 03:00;gilbert;Thanks [~klueska].

[~qianzhang], thanks for finding the root cause. Correct, we should create the staging dir inside of the pulling condition. The second staging dir you saw should be an empty dir though. I will fix it, should be quick.","19/Jan/17 03:11;qianzhang;Yes, it is an empty dir.
Should be a straightforward fix, thanks [~gilbert] :-)","10/Aug/17 09:29;qianzhang;RR: https://reviews.apache.org/r/61546/","11/Aug/17 23:30;gilbert;commit d2e4f02dee21511661c1d273a66e32f211794fda
Author: Qian Zhang <zhq527725@gmail.com>
Date:   Fri Aug 11 15:53:55 2017 -0700

    Created staging dir only when needed.
    
    Created staging dir only when needed.
    
    Review: https://reviews.apache.org/r/61546/",,,,,,,,,,,,,,,,,,,,,,
Improve health checks validation.,MESOS-6916,13034626,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gkleiman,gkleiman,gkleiman,13/Jan/17 12:30,09/Jun/17 14:20,29/Oct/20 16:32,09/Jun/17 14:20,,,,,,,,,1.4.0,,,,,,,,,,,0,health-check,mesosphere,,,,,,,"The ""general""  fields should also be validated (i.e., `timeout_seconds`), similar to what's done in https://reviews.apache.org/r/55458/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-06-09 14:20:24.126,,,false,MESOS-5916,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 09 14:20:24 UTC 2017,,,,,,,"0|i38ntb:",9223372036854775807,,,,,alexr,,,,,,Mesosphere Sprint 57,,,,,,,,,,,2.0,,,,,,,,,,,"07/Jun/17 23:57;gkleiman;https://reviews.apache.org/r/59902/","09/Jun/17 14:20;alexr;{noformat}
Commit: ec40f359ba70e354580865ee3934461047c0fdac [ec40f35]
Author: Gastón Kleiman gaston@mesosphere.io
Date: 9 June 2017 at 15:00:40 GMT+2
Committer: Alexander Rukletsov alexr@apache.org
Commit Date: 9 June 2017 at 15:47:50 GMT+2

Improved health checks validations.

Add validation for the ""general"" fields, such as timeout_seconds.

Review: https://reviews.apache.org/r/59902/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Zero health check timeout is interpreted literally.,MESOS-6908,13033737,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,Fixed,alexr,alexr,alexr,11/Jan/17 14:33,22/Jan/17 15:11,29/Oct/20 16:32,22/Jan/17 15:11,1.0.2,1.1.0,,,,,,,1.2.0,,,,,,,,,,,0,health-check,mesosphere,,,,,,,"Currently zero health check timeout is interpreted literally, which is not very helpful since a health check does not even get a chance to finish. We suggest to fixe this behaviour by interpreting zero as {{Duration::max()}} effectively rendering the timeout infinite.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-5916,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 22 15:11:18 UTC 2017,,,,,,,"0|i38k3r:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 50,,,,,,,,,,,1.0,,,,,,,,,,,"12/Jan/17 13:21;alexr;https://reviews.apache.org/r/55454/","22/Jan/17 15:11;alexr;{noformat}
Commit: 2c7f14f423acae0f44798d0c68bd52d611943ee2 [2c7f14f]
Author: Alexander Rukletsov <rukletsov@gmail.com>
Date: 22 January 2017 at 15:00:43 GMT+1
Committer: Alexander Rukletsov <alexr@apache.org>

Ensured zero health check timeout means infinite timeout.

Prior to this patch, zero health check timeout was interpreted
literally, which is not very helpful since a health check did not
even get a chance to finish. This patch fixes this behaviour by
interpreting zero as `Duration::max()` effectively rendering the
timeout infinite.

Review: https://reviews.apache.org/r/55454/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Add test for framework upgrading to multi-role capability.,MESOS-6900,13033405,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,bbannier,bbannier,10/Jan/17 14:41,26/Mar/18 08:04,29/Oct/20 16:32,30/Jan/17 21:57,,,,,,,,,1.2.0,,,,,,test,,,,,0,,,,,,,,,"Frameworks can upgrade to multi-role capability as long as the framework's role remains the same.

We consider the framework roles unchanged if 
* a framework previously didn't specify a {{role}} now has {{roles=()}}, or
* a framework which previously had {{role=A}} and now has {{roles=(A)}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-7035,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-19 22:52:43.985,,,false,MESOS-1763,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 19 22:52:43 UTC 2017,,,,,,,"0|hzzz30:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 49,Mesosphere Sprint 50,,,,,,,,,,2.0,,,,,,,,,,,"10/Jan/17 14:50;bbannier;Review: https://reviews.apache.org/r/55381/","19/Jan/17 22:52;bmahler;{noformat}
commit 052fb4414e2cce2b550ce0644f039b6d4a1876fa
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Thu Jan 19 14:25:48 2017 -0800

    Added a test for framework upgrading to MULTI_ROLE capability.

    Review: https://reviews.apache.org/r/55381/
{noformat}

[~bbannier] do you want to add another test that ensures that frameworks can upgrade even when tasks are running, and that new tasks can be launched? We can do this in a separate ticket as we get closer to having a working implementation.",,,,,,,,,,,,,,,,,,,,,,,,,,
Reconsider process creation primitives on Windows,MESOS-6892,13032840,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,andschwa,hausdorff,hausdorff,07/Jan/17 23:27,21/Aug/17 21:44,29/Oct/20 16:32,04/Apr/17 23:58,,,,,,,,,1.3.0,,,,,,stout,,,,,0,microsoft,,,,,,,,"Windows does not have the same notions of process hierarchies as Unix, and so killing groups of processes requires us to make sure all processes are contained in a job object, which acts something like a cgroup. This is particularly important when we decide to kill a task, as there is no way to reliably do this unless all the processes you'd like to kill are in the job object.

This causes us a number of issues; it is a big reason we needed to fork the command executor, and it is the reason tasks are currently unkillable in the default executor.

As we clean this issue up, we need to think carefully about the process governance semantics of Mesos, and how we can map them to a reliable, simple Windows implementation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6807,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-02-03 23:02:55.882,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 04 23:58:55 UTC 2017,,,,,,,"0|hzzysd:",9223372036854775807,,,,,kaysoky,,,,,,Mesosphere Sprint 52,Mesosphere Sprint 53,Mesosphere Sprint 54,,,,,,,,,5.0,,,,,,,,,,,"03/Feb/17 23:02;andschwa;The use of `os::pids`, `os::processes`, and `os::pstree` may no longer be necessary. We'll probably end up removing them for Windows, but I deem it as separate work.","29/Mar/17 01:02;kaysoky;Review chain starts here: https://reviews.apache.org/r/56364/","04/Apr/17 23:58;kaysoky;{code}
commit c94041b7cb643d701dfcf6d67c492b53f02f75bd
Author: Andrew Schwartzmeyer <andrew@schwartzmeyer.com>
Date:   Mon Apr 3 12:15:51 2017 -0700

    Windows: Added `JobObjectManager` global actor.
    
    This commit adds a Windows-specific actor for managing job objects.
    
    A subprocess launched with the `ParentHook::CREATE_JOB()` is created
    within the context of a named Windows job object. The `JobObjectManager`
    takes ownership of the handle to the job object.
    
    It is necessary to tie the lifetime of the job object to the actor by
    ownership of the open handle so that the job object can be queried for
    usage information even after the processes that were running within the
    job object have ended. These semantics were not changed; previously the
    same was achieved by leaking the handle and tying it to the lifetime of
    the actual Mesos agent process, and implicitly depending on the
    operating system to close the open handle at the death of the process.
    
    We ensure the proper death of the job object process group by defering a
    call to `cleanup()` to the process reaper for the given PID. This
    function uses the Windows system call to terminate the job object via
    `os::kill_job()`.
    
    Review: https://reviews.apache.org/r/57973/
{code}
{code}
commit cb23605674027cd4c7ee0a877b8af31ca01c85e9
Author: Andrew Schwartzmeyer <andrew@schwartzmeyer.com>
Date:   Mon Apr 3 13:38:17 2017 -0700

    Windows: Combined Posix/Windows-Launcher into SubprocessLauncher.
    
    This commit renames the `PosixLauncher` into the SubprocessLauncher`
    and deletes the trivially derived class `WindowsLauncher`.  With the
    improved job object support in stout/libprocess, the same launcher
    is now suitable for both POSIX systems and Windows.  Thus, the previous
    name became a misnomer (PosixLauncher).
    
    Review: https://reviews.apache.org/r/57974/
{code}
{code}
commit c51ebf508d9aaed2d1c75c46e72148df8aa6bb7b
Author: Andrew Schwartzmeyer <andrew@schwartzmeyer.com>
Date:   Mon Apr 3 14:19:10 2017 -0700

    Windows: Changed command executor to use Subprocess.
    
    By encapsulating the job object logic inside a (Windows-only) libprocess
    actor, we're able to reuse `Subprocess` for launching tasks on Windows.
    This allows us to remove the entirety of `launchTaskWindows` and instead
    reuse `launchTaskPosix`, which just uses `subprocess`. This also fixes
    the `CommitSuicideOnTaskFailure` test, which is now enabled.
    
    Much of the code in this commit will be moved in a subsequent commit,
    as there is no longer any need to separate the Posix/Windows launch
    paths in the command executor.
    
    Review: https://reviews.apache.org/r/57975/
{code}
{code}
commit 0f812d6816db80e2e1fe0a38f73e7d1d45d19fe2
Author: Andrew Schwartzmeyer <andrew@schwartzmeyer.com>
Date:   Mon Apr 3 14:20:52 2017 -0700

    Refactored command executor to unify launch paths.
    
    This commit reverses the file split done in e821978.
    Since `launchTaskPosix` and `launchTaskWindows` were reconciled
    using `Subprocess`, the files were pulled back into just
    `src/launcher/executor.cpp` with `launchTaskSubprocess`.
    
    Review: https://reviews.apache.org/r/57976/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,
Transition Windows away from `os::killtree`.,MESOS-6868,13032410,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,andschwa,hausdorff,hausdorff,05/Jan/17 23:13,04/Apr/17 23:59,29/Oct/20 16:32,04/Apr/17 23:59,,,,,,,,,1.3.0,,,,,,agent,,,,,0,mesosphere,microsoft,,,,,,,"Windows does not have as robust a notion of a process hierarchy as Unix, and thus functions like `os::killtree` will always have critical limitations and semantic mismatches between Unix and Windows.

We should transition away from this function when we can, and replace it with something similar to how we kill a cgroup.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-26 00:01:33.533,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 04 23:59:42 UTC 2017,,,,,,,"0|hzzysk:r",9223372036854775807,,,,,kaysoky,,,,,,Mesosphere Sprint 53,Mesosphere Sprint 54,,,,,,,,,,3.0,,,,,,,,,,,"26/Jan/17 00:01;andschwa;I've started the transition away from `os::killtree` for the `WindowsLauncher` and default executor, but there are still many instances in use elsewhere in the code base.","29/Mar/17 01:02;kaysoky;Part of this review chain: https://reviews.apache.org/r/56367/","04/Apr/17 23:59;kaysoky;{code}
commit db0f5697ed61ba44f0f50fedd55848eaaf50348a
Author: Andrew Schwartzmeyer <andrew@schwartzmeyer.com>
Date:   Mon Apr 3 15:07:10 2017 -0700

    Windows: Stout: Rewrote job object wrappers.
    
    `os::create_job` now returns a `Try<SharedHandle>` instead of a raw
    `HANDLE`, forcing ownership of the job object handle onto the caller
    of the function. `create_job` requires a `std::string name` for the
    job object, which is mapped from a PID using `os::name_job`.
    
    The assignment of a process to the job object is now done via
    `Try<Nothing> os::assign_job(SharedHandle, pid_t)`.
    
    The equivalent of killing a process tree with job object semantics
    is simply to terminate the job object. This is done via
    `os::kill_job(SharedHandle)`.
    
    Review: https://reviews.apache.org/r/56364/
{code}
{code}
commit 176c09d8debb64e98fed14ef499f9205132b9180
Author: Andrew Schwartzmeyer <andrew@schwartzmeyer.com>
Date:   Mon Apr 3 11:57:32 2017 -0700

    Windows: Stout: Adapted `os::killtree` to terminate job objects.
    
    On Windows, a ""process tree"" is in fact a job object. Thus, the
    implementation of `os::killtree` on Windows is an adapter which
    terminates the job object corresponding to the process group
    represented by the given PID.
    
    Review: https://reviews.apache.org/r/56367/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,
Container Exec should be possible with tasks belonging to a task group,MESOS-6864,13032337,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,gkleiman,gkleiman,gkleiman,05/Jan/17 18:40,20/Jan/17 00:55,29/Oct/20 16:32,20/Jan/17 00:55,,,,,,,,,1.2.0,,,,,,,,,,,0,debugging,mesosphere,,,,,,,"{{LaunchNestedContainerSession}} currently requires the parent container to be an Executor (https://github.com/apache/mesos/blob/f89f28724f5837ff414dc6cc84e1afb63f3306e5/src/slave/http.cpp#L2189-L2211).

This works for command tasks, because the task container id is the same as the executor container id.

But it won't work for pod tasks whose container id is different from executor’s container id.

In order to resolve this ticket, we need to allow launching a child container at an arbitrary level.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6280,,,,,,MESOS-6865,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-20 00:55:30.598,,,false,MESOS-6460,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 20 00:55:30 UTC 2017,,,,,,,"0|i38bh3:",9223372036854775807,,,,,anandmazumdar,,,,,,Mesosphere Sprint 49,,,,,,,,,,,5.0,,1.2.0,,,,,,,,,"13/Jan/17 13:59;gkleiman;https://reviews.apache.org/r/55676/
https://reviews.apache.org/r/55722/
https://reviews.apache.org/r/55677/
https://reviews.apache.org/r/55678/
https://reviews.apache.org/r/55679/
https://reviews.apache.org/r/55464/","20/Jan/17 00:55;anandmazumdar;{noformat}
commit dd922d51530931fe513df80b7adfd3dd0d23ae27
Author: Gastón Kleiman <gaston@mesosphere.com>
Date:   Thu Jan 19 16:52:01 2017 -0800

    Made the Agent API able to handle containers nested at arbitrary levels.

    Review: https://reviews.apache.org/r/55464/

commit 60c59adf726ac56ae85284213a24232defc03c85
Author: Gastón Kleiman <gaston@mesosphere.com>
Date:   Thu Jan 19 16:51:56 2017 -0800

    Unified the way in which the Slave API handlers perform AuthZ.

    Review: https://reviews.apache.org/r/55679/

commit 7bef188c12ffb03325c8bf0bd2f87941cc64bef1
Author: Gastón Kleiman <gaston@mesosphere.com>
Date:   Thu Jan 19 16:51:47 2017 -0800

    Renamed `locateExecutor` to `getExecutor`.

    Rnamed `locateExecutor` to `getExecutor` in `slave.cpp` to be consistent
    with other similar functions in the same file.

    Review: https://reviews.apache.org/r/55678/

commit 373781fa50ba35b3b1026e2969cfab39651638f5
Author: Gastón Kleiman <gaston@mesosphere.com>
Date:   Thu Jan 19 16:51:22 2017 -0800

    Made `AttachContainerOutput/Input` tests use an existing container ID.

    These tests verify that the Agent returns a 500 if the containerizer
    doesn't support the `attach` call.

    This chain refactors those methods and make them return a 404 instead of
    a 500 if the container can't be found, so we need to pass the ID of an
    existing container.

    Review: https://reviews.apache.org/r/55677/

commit 2fb678d285483c01b6898b141a002b1a69eb2c42
Author: Gastón Kleiman <gaston@mesosphere.com>
Date:   Thu Jan 19 16:51:15 2017 -0800

    Removed redundant `Times(1)` statements from `api_tests.cpp`.

    Review: https://reviews.apache.org/r/55722/

commit df0dfb40c8b1ec8e05eaf35f00ee8ebf2d9aa747
Author: Gastón Kleiman <gaston@mesosphere.com>
Date:   Thu Jan 19 16:51:02 2017 -0800

    Moved `getRootContainerId` to `protobuf_utils`.

    This method is pretty generic and should not be a part of the Mesos
    containerizer code.

    Review: https://reviews.apache.org/r/55676/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
FaultToleranceTest.FrameworkReregister is flaky,MESOS-6837,13030245,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,neilc,neilc,neilc,22/Dec/16 22:30,30/Jan/17 00:54,29/Oct/20 16:32,03/Jan/17 15:57,,,,,,,,,1.2.0,,,,,,master,,,,,0,flaky,flaky-test,mesosphere,,,,,,"Observed on internal CI:

{noformat}
[21:27:38] :     [Step 11/11] /mnt/teamcity/work/4240ba9ddd0997c3/src/tests/fault_tolerance_tests.cpp:892: Failure
[21:27:38] :     [Step 11/11] Value of: framework.values[""registered_time""].as().as()
[21:27:38] :     [Step 11/11]   Actual: 1482442093
[21:27:38] :     [Step 11/11] Expected: static_cast(registerTime.secs())
[21:27:38] :     [Step 11/11] Which is: 1482442094
{noformat}

Looks like another instance of MESOS-4695.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-7029,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-03 15:57:48.74,,,false,MESOS-4018,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 03 15:57:48 UTC 2017,,,,,,,"0|i37yl3:",9223372036854775807,,,,,alexr,,,,,,Mesosphere Sprint 49,,,,,,,,,,,3.0,,,,,,,,,,,"22/Dec/16 22:55;neilc;https://reviews.apache.org/r/54999/","03/Jan/17 15:57;alexr;{noformat}
Commit: a3949830b01c4e8f6956c3182c8d0dd71880a714 [a394983]
Author: Neil Conway <neil.conway@gmail.com>
Date: 3 January 2017 at 16:56:07 GMT+1
Committer: Alexander Rukletsov <alexr@apache.org>

Fixed test flakiness due to floating point conversions.

`FaultToleranceTest.FrameworkReregister` and
`MasterTest.FailoverAgentReregisterFirst` both examined a timestamp
value returned by an HTTP endpoint. Such values are the result of
several conversions (`double` to `string` to `JSON::Value`); this might
result in the returned value being an integer one larger/smaller than we
expect. Hence, make the comparison within an epsilon of 1.

A similar issue in `SlaveTest.StateEndpoint` was fixed in
595c929f2816b713b4c36ce1bd23a7767afe8135.

Review: https://reviews.apache.org/r/54999/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
consecutive_failures 0 == 1 in HealthCheck.,MESOS-6833,13030128,Bug,Open,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,lloesche,lloesche,22/Dec/16 14:44,06/Apr/17 10:52,29/Oct/20 16:32,,0.28.0,1.0.0,1.1.0,,,,,,,,,,,,agent,,,,,0,check,health-check,mesosphere,,,,,,"When defining a HealthCheck with consecutive_failures=0 one would expect Mesos to never kill the task and only notify about the failure.

What seems to happen instead is Mesos handles consecutive_failures=0 as consecutive_failures=1 and kills the task after 1 failure.

Since 0 isn't the same as 1 this seems to be a bug and results in unexpected behaviour.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6417,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-04 14:25:34.244,,,false,MESOS-7353,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 04 14:25:34 UTC 2017,,,,,,,"0|i37xv3:",9223372036854775807,,,,,alexr,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"04/Jan/17 14:25;alexr;If we make killing on the executor optional, we should provide more data to the scheduler about the health status. Here is what I have in mind:
{code}
message TaskStatus {
 <...>

 // Describes whether the task has been determined to be healthy
 // (true) or unhealthy (false) according to the HealthCheck field in
 // the command info.
 //
 // NOTE: This field will be deprecated in favor of a more verbose
 // `health_status` starting from 2.0.
 optional bool healthy = 8;

 // Contains health status for the health check specified in corresponding
 // `TaskInfo`. If no health check has been specified, this field must be
 // absent, otherwise it must be present even if the health status is not
 // available yet.
 //
 // NOTE: A task status update must be sent if:
 // 1) The health check fails, regardless of the previous value of
 //    `HealthStatusInfo.healthy`.
 // 2) the value or presence of the `HealthStatusInfo.healthy` field changes.
 optional HealthStatusInfo health_status = 15;

 <...>
}
{code}
{code}
/**
* Describes the status of a health check. An empty message means that the
* status is currently not available, for example, due to the health check
* being in a grace period.
*/
message HealthStatusInfo {
 // Contains either command exit code, HTTP status code, or TCP handshake
 // return code.
 optional int32 state = 1;

 // Executor must decide locally whether the task is healthy or not based
 // on the health check specification in corresponding `TaskInfo`.
 optional bool healthy = 2;
 
 // This field tells how many times the health check failed consecutively.
 // It is particularly useful after scheduler failover or disconnect if the
 // task killing decision is delegated to the scheduler.
 optional uint32 consecutive_failures = 3;
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,
mesos-this-capture clang-tidy check has false positives,MESOS-6824,13029800,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,bbannier,bbannier,21/Dec/16 15:56,26/Mar/18 08:04,29/Oct/20 16:32,11/Jan/17 03:15,,,,,,,,,1.2.0,,,,,,,,,,,0,clang-tidy,,,,,,,,"The {{mesos-this-capture}} clang-tidy checks incorrectly triggers on the code here,

  https://github.com/apache/mesos/blob/d2117362349ab4c383045720f77d42b2d9fd6871/src/slave/containerizer/mesos/io/switchboard.cpp#L1487

We should tighten the matcher to avoid triggering on such constructs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-11 03:14:40.398,,,false,MESOS-4907,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 11 03:14:40 UTC 2017,,,,,,,"0|i36o3p:",9223372036854775807,,,,,mcypark,,,,,,Mesosphere Sprint 49,,,,,,,,,,,2.0,,1.2.0,,,,,,,,,"21/Dec/16 15:57;bbannier;Review: https://github.com/mesos/clang-tools-extra/pull/3","21/Dec/16 15:57;bbannier;[~mcypark], could you help with this?","11/Jan/17 03:14;mcypark;Committed here: https://github.com/mesos/clang-tools-extra/commit/d00b4b380cece7733bf6bf42935ea2a5b4888819",,,,,,,,,,,,,,,,,,,,,,,,,
Enable glog stack traces when we call things like `ABORT` on Windows,MESOS-6815,13029235,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,andschwa,hausdorff,hausdorff,19/Dec/16 19:32,14/Mar/17 20:47,29/Oct/20 16:32,14/Mar/17 18:59,,,,,,,,,1.3.0,,,,,,stout,,,,,1,mesosphere,microsoft,windows-mvp,,,,,,"Currently in the Windows builds, if we call `ABORT` (etc.) we will simply bail out, with no stack traces.

This is highly undesirable. Stack traces are important for operating clusters in production. We should work to enable this behavior, including possibly working with glog to add this support if they currently they do not natively support it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-7245,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-25 23:54:28.896,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 14 20:47:01 UTC 2017,,,,,,,"0|i37scv:",9223372036854775807,,,,,kaysoky,,,,,,Mesosphere Sprint 53,,,,,,,,,,,5.0,,,,,,,,,,,"25/Jan/17 23:54;andschwa;Work in progress is here: https://github.com/andschwa/glog/tree/test, with an open but unfinished pull request here: https://github.com/google/glog/pull/151.

This is currently paused. The status is that stack tracing works, but relying on a signal handler to report the stack trace does not provide the desired behavior. On Linux, the stack trace is reported from the thread that the signal was called; but it seems on Windows that the stack trace is reported from the thread on which the signal handler was installed, regardless of the thread from which it was called. I believe the correct approach is to slightly change how Glog reports stack tracing (at least for Windows) to not rely on signal handlers.","10/Mar/17 02:00;andschwa;Unpaused, patches available.","10/Mar/17 02:01;andschwa;Please see updated PR https://github.com/google/glog/pull/168.

I'm most unhappy with the amount of frames to skip when generating a stack trace, as I can't seem to reliably remove the Google log functions. Perhaps we should set it to skip none?","10/Mar/17 02:01;andschwa;PR updating Glog tarball: https://github.com/3rdparty/mesos-3rdparty/pull/10","10/Mar/17 02:02;andschwa;Patch series adding patches to CMake build on Windows: https://reviews.apache.org/r/57491/ and https://reviews.apache.org/r/57490/.","14/Mar/17 18:59;kaysoky;{code}
commit 5d3908a4c3312a41ad801683089ea14eae9b0360
Author: Andrew Schwartzmeyer <andrew@schwartzmeyer.com>
Date:   Mon Mar 13 18:16:23 2017 -0700

    Windows: Updated libprocess CMake setup for Glog patch.
    
    The additional Windows library `Dbghelp` must be linked for the
    in addition to glog, in order to enable stack traces.
    
    NOTE: CMake's dependency graph does not pull in the `Dbghelp` library
    automatically as the glog dependency is added into Mesos as an
    ""external"" project.  If we were to, instead, add glog's CMake files
    to the build system directly (such as, as a git submodule), glog's
    targets would be inherited.
    
    Review: https://reviews.apache.org/r/57491/

commit ef3ebf3fda475ac0d8d51d12ba5c2e10cb0607df
Author: Andrew Schwartzmeyer <andrew@schwartzmeyer.com>
Date:   Mon Mar 13 18:06:11 2017 -0700

    Windows: Added Glog patch for enabling stack trace.
    
    This relies on an updated `glog-da816ea70.tar.gz` tarball
    in the Mesos 3rdparty repository, which is itself generated from
    the `da816ea70` commit on github.com/google/glog.
    
    Additionally, this applies a patch that includes stacktrace support
    to glog on Windows.  The code in this patch is included in an upstream
    pull request:
    https://github.com/google/glog/pull/168
    
    If and when that pull request gets merged, we will update the glog
    tarball and remove the need for the extra patch file.
    
    Review: https://reviews.apache.org/r/57490/
{code}","14/Mar/17 20:47;andschwa;NOTE: The currently applied patches explicitly don't install the `SIGABRT` handler for Windows. This can be done at any time in a separate commit, if we want them. The patches enable stack tracing for uses of `LOG_FATAL` in tests.",,,,,,,,,,,,,,,,,,,,,
Check unreachable task cache for task ID collisions on launch,MESOS-6805,13028499,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,neilc,neilc,neilc,15/Dec/16 20:56,12/Jun/17 23:07,29/Oct/20 16:32,24/Jan/17 01:40,,,,,,,,,1.2.0,,,,,,master,,,,,0,mesosphere,,,,,,,,"As discussed in MESOS-6785, it is possible to crash the master by launching a task that reuses the ID of an unreachable/partitioned task. A complete solution to this problem will be quite involved, but an incremental improvement is easy: when we see a task launch operation, reject the launch attempt if the task ID collides with an ID in the per-framework {{unreachableTasks}} cache. This doesn't catch all situations in which IDs are reused, but it is better than nothing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-3070,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-6394,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 24 01:40:53 UTC 2017,,,,,,,"0|hzzz3w:zzx",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 48,Mesosphere Sprint 49,,,,,,,,,,2.0,,,,,,,,,,,"15/Dec/16 21:14;neilc;https://reviews.apache.org/r/54793/","24/Jan/17 01:40;neilc;{noformat}
commit b8562b8fc578f8d62f4a934d315516161d9e1720
Author: Neil Conway <neil.conway@gmail.com>
Date:   Mon Jan 23 17:04:22 2017 -0800

    Prevented task launches that reuse unreachable task IDs.

    The master keeps an in-memory cache of task IDs that have recently been
    marked unreachable. The master now consults this cache to reject task
    launch attempts that reuse one of these recently unreachable task IDs
    (such tasks are not terminal and may resume running in the future). This
    check is not complete (we won't detect all cases in which unreachable
    task IDs are reused), but preventing this from happening in the common
    case seems worth doing. See MESOS-6785 for details.

    Review: https://reviews.apache.org/r/54793/
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,
SSL socket can lose bytes in the case of EOF,MESOS-6802,13028453,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,greggomann,greggomann,greggomann,15/Dec/16 17:52,16/Jan/18 17:53,29/Oct/20 16:32,11/Jan/17 02:49,,,,,,,,,1.2.0,,,,,,libprocess,,,,,0,libevent,libprocess,ssl,,,,,,"During recent work on SSL-enabled tests in libprocess (MESOS-5966), we discovered a bug in {{LibeventSSLSocketImpl}}, wherein the socket can either fail to receive an EOF, or lose data when an EOF is received.

The {{LibeventSSLSocketImpl::event_callback(short events)}} method immediately sets any pending {{RecvRequest}}'s promise to zero upon receipt of an EOF. However, at the time the promise is set, there may actually be data waiting to be read by libevent. Upon receipt of an EOF, we should attempt to read the socket's bufferevent first to ensure that we aren't losing any data previously received by the socket.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6799,MESOS-7028,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-11 02:26:11.119,,,false,MESOS-7930,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 14 00:23:25 UTC 2017,,,,,,,"0|hzzz3w:9",9223372036854775807,,,,,kaysoky,,,,,,Mesosphere Sprint 49,,,,,,,,,,,3.0,,,,,,,,,,,"15/Dec/16 17:54;greggomann;Reviews here:
https://reviews.apache.org/r/53802/
https://reviews.apache.org/r/53803/","11/Jan/17 02:26;benjaminhindman;Committed https://reviews.apache.org/r/53802:

{code}
commit d4185d4a117d9cba32d80993d14fcefb566cd9f9
Author: Greg Mann <greg@mesosphere.io>
Date:   Tue Jan 10 18:23:45 2017 -0800

    Eliminated an EOF race condition in libprocess SSL socket.
    
    Previously, it was possible for an SSL socket to either:
    1) Fail to receive an EOF if the EOF event was received when
       there was no pending recv() request.
    2) Fail to receive all data sent on the sending side if an
       EOF event was received before all sent data was read.
    
    This patch eliminates these race conditions to ensure reliable
    receipt of both sent data and EOFs.
    
    Review: https://reviews.apache.org/r/53802/
{code}","14/Jan/17 00:23;kaysoky;{code}
commit 5023e004030e6018ea64f6824c353ffe4165c907
Author: Greg Mann <greg@mesosphere.io>
Date:   Fri Jan 13 15:47:57 2017 -0800

    Added new libprocess socket tests.
    
    This patch adds NetSocketTest.EOFBeforeRecv and
    NetSocketTest.EOFAfterRecv to verify that EOFs are
    reliably received whether or not there is a pending recv()
    request at the time the EOF is received.
    
    Review: https://reviews.apache.org/r/53803/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,
SSL socket's 'shutdown()' method is broken,MESOS-6789,13027940,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,greggomann,greggomann,greggomann,14/Dec/16 00:00,01/Sep/17 23:33,29/Oct/20 16:32,14/Jan/17 00:24,,,,,,,,,1.2.0,,,,,,libprocess,,,,,0,encryption,libprocess,ssl,,,,,,"We recently uncovered two issues with the {{LibeventSSLSocketImpl::shutdown}} method:
* The introduction of a shutdown method parameter with [this commit|https://reviews.apache.org/r/54113/] means that the implementation's method is no longer overriding the default implementation. In addition to fixing the implementation method's signature, we should add the {{override}} specifier to all of our socket implementations' methods to ensure that this doesn't happen in the future.
* The {{LibeventSSLSocketImpl::shutdown}} function does not actually shutdown the SSL socket. The proper function to shutdown an SSL socket is {{SSL_shutdown}}, which is called in the implementation's destructor. We should move this into {{shutdown()}} so that by the time that method returns, the socket has actually been shutdown.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-09 07:26:04.012,,,false,MESOS-7930,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 14 00:25:16 UTC 2017,,,,,,,"0|hzzz3w:4",9223372036854775807,,,,,kaysoky,,,,,,Mesosphere Sprint 49,,,,,,,,,,,2.0,,1.2.0,,,,,,,,,"09/Jan/17 07:26;adam-mesos;Seems important enough to try to get into 1.2.
[~greggomann], [~kaysoky] Can we revisit this ticket this sprint? Should we upgrade to Critical/Blocker?","14/Jan/17 00:25;kaysoky;{code}
commit c600d12a01865daad8ba7607b53eff35686f0f35
Author: Greg Mann <greg@mesosphere.io>
Date:   Fri Jan 13 15:56:34 2017 -0800

    Fixed SSL socket 'shutdown()'.
    
    Recently, a change was made to the signature of
    `Socket::shutdown`, but the corresponding override in
    `LibeventSSLSocketImpl` was not updated, so that the
    implementation-specific method is no longer being
    executed. Further, the SSL socket's `shutdown` code
    did not actually shutdown the socket; rather, the
    shutdown was performed in the destructor.
    
    This patch updates the function's signature to match
    that of the base class's method, adds the `override`
    specifier to the implemention's method declaration,
    and updates the function to properly shutdown the
    SSL socket.
    
    Review: https://reviews.apache.org/r/55343/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
IOSwitchboardTest.KillSwitchboardContainerDestroyed is flaky,MESOS-6784,13027603,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,,neilc,neilc,12/Dec/16 21:58,22/Dec/17 17:22,29/Oct/20 16:32,22/Dec/17 17:22,,,,,,,,,1.5.0,,,,,,agent,,,,,0,mesosphere,,,,,,,,"{noformat}
[ RUN      ] IOSwitchboardTest.KillSwitchboardContainerDestroyed
I1212 13:57:02.641043  2211 containerizer.cpp:220] Using isolation: posix/cpu,filesystem/posix,network/cni
W1212 13:57:02.641438  2211 backend.cpp:76] Failed to create 'overlay' backend: OverlayBackend requires root privileges, but is running as user nrc
W1212 13:57:02.641559  2211 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges
I1212 13:57:02.642822  2268 containerizer.cpp:594] Recovering containerizer
I1212 13:57:02.643975  2253 provisioner.cpp:253] Provisioner recovery complete
I1212 13:57:02.644953  2255 containerizer.cpp:986] Starting container 09e87380-00ab-4987-83c9-fa1c5d86717f for executor 'executor' of framework
I1212 13:57:02.647004  2245 switchboard.cpp:430] Allocated pseudo terminal '/dev/pts/54' for container 09e87380-00ab-4987-83c9-fa1c5d86717f
I1212 13:57:02.652305  2245 switchboard.cpp:596] Created I/O switchboard server (pid: 2705) listening on socket file '/tmp/mesos-io-switchboard-b4af1c92-6633-44f3-9d35-e0e36edaf70a' for container 09e87380-00ab-4987-83c9-fa1c5d86717f
I1212 13:57:02.655513  2267 launcher.cpp:133] Forked child with pid '2706' for container '09e87380-00ab-4987-83c9-fa1c5d86717f'
I1212 13:57:02.655732  2267 containerizer.cpp:1621] Checkpointing container's forked pid 2706 to '/tmp/IOSwitchboardTest_KillSwitchboardContainerDestroyed_Me5CRx/meta/slaves/frameworks/executors/executor/runs/09e87380-00ab-4987-83c9-fa1c5d86717f/pids/forked.pid'
I1212 13:57:02.726306  2265 containerizer.cpp:2463] Container 09e87380-00ab-4987-83c9-fa1c5d86717f has exited
I1212 13:57:02.726352  2265 containerizer.cpp:2100] Destroying container 09e87380-00ab-4987-83c9-fa1c5d86717f in RUNNING state
E1212 13:57:02.726495  2243 switchboard.cpp:861] Unexpected termination of I/O switchboard server: 'IOSwitchboard' exited with signal: Killed for container 09e87380-00ab-4987-83c9-fa1c5d86717f
I1212 13:57:02.726563  2265 launcher.cpp:149] Asked to destroy container 09e87380-00ab-4987-83c9-fa1c5d86717f
E1212 13:57:02.783607  2228 switchboard.cpp:799] Failed to remove unix domain socket file '/tmp/mesos-io-switchboard-b4af1c92-6633-44f3-9d35-e0e36edaf70a' for container '09e87380-00ab-4987-83c9-fa1c5d86717f': No such file or directory
../../mesos/src/tests/containerizer/io_switchboard_tests.cpp:661: Failure
Value of: wait.get()->reasons().size() == 1
  Actual: false
Expected: true
*** Aborted at 1481579822 (unix time) try ""date -d @1481579822"" if you are using GNU date ***
PC: @          0x1bf16d0 testing::UnitTest::AddTestPartResult()
*** SIGSEGV (@0x0) received by PID 2211 (TID 0x7faed7d078c0) from PID 0; stack trace: ***
    @     0x7faecf855100 (unknown)
    @          0x1bf16d0 testing::UnitTest::AddTestPartResult()
    @          0x1be6247 testing::internal::AssertHelper::operator=()
    @          0x19ed751 mesos::internal::tests::IOSwitchboardTest_KillSwitchboardContainerDestroyed_Test::TestBody()
    @          0x1c0ed8c testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1c09e74 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1beb505 testing::Test::Run()
    @          0x1bebc88 testing::TestInfo::Run()
    @          0x1bec2ce testing::TestCase::Run()
    @          0x1bf2ba8 testing::internal::UnitTestImpl::RunAllTests()
    @          0x1c0f9b1 testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1c0a9f2 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1bf18ee testing::UnitTest::Run()
    @          0x11bc9e3 RUN_ALL_TESTS()
    @          0x11bc599 main
    @     0x7faece663b15 __libc_start_main
    @           0xa9c219 (unknown)
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-12-13 21:01:56.391,,,false,MESOS-7103,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 22 17:22:32 UTC 2017,,,,,,,"0|i37ia7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,1.0,,1.5.0,,,,,,,,,"12/Dec/16 21:59;neilc;CC [~klueska] [~jieyu]","13/Dec/16 21:01;jieyu;Another data point with different log:
{noformat}
[20:16:40] :	 [Step 11/11] [ RUN      ] IOSwitchboardTest.KillSwitchboardContainerDestroyed
[20:16:40] :	 [Step 11/11] I1213 20:16:40.465116 26604 containerizer.cpp:220] Using isolation: posix/cpu,filesystem/posix,network/cni
[20:16:40] :	 [Step 11/11] I1213 20:16:40.465904 26624 containerizer.cpp:594] Recovering containerizer
[20:16:40] :	 [Step 11/11] I1213 20:16:40.466194 26623 provisioner.cpp:253] Provisioner recovery complete
[20:16:40] :	 [Step 11/11] I1213 20:16:40.466544 26625 containerizer.cpp:986] Starting container 8fc415c0-74cc-4c13-8bcf-1f3dfc7a993b for executor 'executor' of framework 
[20:16:40] :	 [Step 11/11] I1213 20:16:40.467177 26622 switchboard.cpp:430] Allocated pseudo terminal '/dev/pts/0' for container 8fc415c0-74cc-4c13-8bcf-1f3dfc7a993b
[20:16:40] :	 [Step 11/11] I1213 20:16:40.467337 26622 switchboard.cpp:567] Launching 'mesos-io-switchboard' with flags '--heartbeat_interval=""30secs"" --help=""false"" --socket_address=""/tmp/mesos-io-switchboard-2b923443-e3d2-4513-9442-31536611ea28"" --stderr_from_fd=""13"" --stderr_to_fd=""2"" --stdin_to_fd=""13"" --stdout_from_fd=""13"" --stdout_to_fd=""1"" --tty=""true"" --wait_for_connection=""false""' for container 8fc415c0-74cc-4c13-8bcf-1f3dfc7a993b
[20:16:40] :	 [Step 11/11] I1213 20:16:40.469470 26622 switchboard.cpp:597] Created I/O switchboard server (pid: 11965) listening on socket file '/tmp/mesos-io-switchboard-2b923443-e3d2-4513-9442-31536611ea28' for container 8fc415c0-74cc-4c13-8bcf-1f3dfc7a993b
[20:16:40] :	 [Step 11/11] I1213 20:16:40.470201 26619 containerizer.cpp:1535] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""command"":{""shell"":true,""value"":""sleep 1000""},""environment"":{""variables"":[{""name"":""MESOS_SANDBOX"",""value"":""\/mnt\/teamcity\/temp\/buildTmp\/IOSwitchboardTest_KillSwitchboardContainerDestroyed_gYP6xR""}]},""err"":{""fd"":14,""type"":""FD""},""in"":{""fd"":14,""type"":""FD""},""out"":{""fd"":14,""type"":""FD""},""tty_slave_path"":""\/dev\/pts\/0"",""working_directory"":""\/mnt\/teamcity\/temp\/buildTmp\/IOSwitchboardTest_KillSwitchboardContainerDestroyed_gYP6xR""}"" --pipe_read=""13"" --pipe_write=""15"" --runtime_directory=""/mnt/teamcity/temp/buildTmp/IOSwitchboardTest_KillSwitchboardContainerDestroyed_3d8l0O/containers/8fc415c0-74cc-4c13-8bcf-1f3dfc7a993b"" --unshare_namespace_mnt=""false""'
[20:16:40] :	 [Step 11/11] I1213 20:16:40.471458 26619 launcher.cpp:133] Forked child with pid '11966' for container '8fc415c0-74cc-4c13-8bcf-1f3dfc7a993b'
[20:16:40] :	 [Step 11/11] I1213 20:16:40.471776 26619 containerizer.cpp:1634] Checkpointing container's forked pid 11966 to '/mnt/teamcity/temp/buildTmp/IOSwitchboardTest_KillSwitchboardContainerDestroyed_fdQTsM/meta/slaves/frameworks/executors/executor/runs/8fc415c0-74cc-4c13-8bcf-1f3dfc7a993b/pids/forked.pid'
[20:16:40] :	 [Step 11/11] I1213 20:16:40.472676 26621 fetcher.cpp:349] Starting to fetch URIs for container: 8fc415c0-74cc-4c13-8bcf-1f3dfc7a993b, directory: /mnt/teamcity/temp/buildTmp/IOSwitchboardTest_KillSwitchboardContainerDestroyed_gYP6xR
[20:16:40] :	 [Step 11/11] E1213 20:16:40.547350 26625 switchboard.cpp:880] Unexpected termination of I/O switchboard server: 'IOSwitchboard' exited with signal: Killed for container 8fc415c0-74cc-4c13-8bcf-1f3dfc7a993b
[20:16:40] :	 [Step 11/11] I1213 20:16:40.547364 26620 containerizer.cpp:2493] Container 8fc415c0-74cc-4c13-8bcf-1f3dfc7a993b has reached its limit for resource {} and will be terminated
[20:16:40] :	 [Step 11/11] I1213 20:16:40.547385 26620 containerizer.cpp:2113] Destroying container 8fc415c0-74cc-4c13-8bcf-1f3dfc7a993b in RUNNING state
[20:16:40] :	 [Step 11/11] I1213 20:16:40.547490 26620 launcher.cpp:149] Asked to destroy container 8fc415c0-74cc-4c13-8bcf-1f3dfc7a993b
[20:16:40] :	 [Step 11/11] I1213 20:16:40.552752 26620 containerizer.cpp:2476] Container 8fc415c0-74cc-4c13-8bcf-1f3dfc7a993b has exited
[20:16:40] :	 [Step 11/11] E1213 20:16:40.553004 26624 switchboard.cpp:801] Failed to remove unix domain socket file '/tmp/mesos-io-switchboard-2b923443-e3d2-4513-9442-31536611ea28' for container '8fc415c0-74cc-4c13-8bcf-1f3dfc7a993b': No such file or directory
[20:16:40] :	 [Step 11/11] I1213 20:16:40.553323 26624 provisioner.cpp:324] Ignoring destroy request for unknown container 8fc415c0-74cc-4c13-8bcf-1f3dfc7a993b
[20:16:40] :	 [Step 11/11] ../../src/tests/containerizer/io_switchboard_tests.cpp:668: Failure
[20:16:40] :	 [Step 11/11] Expecting WIFSIGNALED(wait.get()->status()) but  WIFEXITED(wait.get()->status()) is true and WEXITSTATUS(wait.get()->status()) is 1
[20:16:40] :	 [Step 11/11] [  FAILED  ] IOSwitchboardTest.KillSwitchboardContainerDestroyed (100 ms)
{noformat}","16/Dec/16 18:33;anandmazumdar;Committed a fix for the second log snippet that Jie posted around the test bug. 

{noformat}
commit 28eaa8df7c95130b0c244f7613ad506be899cafd
Author: Anand Mazumdar <anand@apache.org>
Date:   Wed Dec 14 17:40:47 2016 -0800

    Fixed the 'IOSwitchboardTest.KillSwitchboardContainerDestroyed' test.

    The container was launched with TTY enabled. This meant that
    killing the switchboard would trigger the task to terminate
    on its own owing to the ""master"" end of the TTY dying. This
    would make it not go through the code path of the isolator
    failing due to resource limit issue.

    Review: https://reviews.apache.org/r/54770
{noformat}

The original log in the issue description is a separate issue in the switchboard code itself and I am working on that. This should make the CI green for now.","03/Jan/17 08:09;adam-mesos;[~anandmazumdar], is this still a Blocker for Mesos 1.2 if you've committed a fix for the CI already?","10/Feb/17 08:01;adam-mesos;[~anandmazumdar], should we resolve as fixed in 1.2.0, or continue investigating for 1.3?","02/May/17 22:55;mcypark;[~anandmazumdar]: Pushing this off to target 1.4.0. Please let me know if this is a blocker for 1.3.0.","22/Dec/17 17:22;jieyu;Haven't seen this test being flaky for months on head. Close it for now. RE-open if you see this being flaky again. cc [~alexr]",,,,,,,,,,,,,,,,,,,,,
The 'http::connect(address)' always uses the DEFAULT_KIND() of socket even if SSL is undesired.,MESOS-6775,13027208,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,klueska,klueska,klueska,10/Dec/16 18:27,12/Dec/16 21:06,29/Oct/20 16:32,12/Dec/16 21:06,,,,,,,,,1.2.0,,,,,,,,,,,0,debugging,mesosphere,,,,,,,"    The 'http::connect(address)' variant of 'http::connect()' doesn't
    currently support SSL. However, when SSL is enabled, the default for
    all 'Socket::create()' calls is to use the 'DEFAULT_KIND()' of socket
    which is set to SSL. This causes problems with 'connect()' becuuse it
    will create a socket of 'kind' SSL without a way to override it.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-12-12 21:06:56.877,,,false,MESOS-6460,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 12 21:06:56 UTC 2016,,,,,,,"0|i37fun:",9223372036854775807,,,,,anandmazumdar,,,,,,,,,,,,,,,,,1.0,,1.2.0,,,,,,,,,"10/Dec/16 18:34;klueska;https://reviews.apache.org/r/54627/","12/Dec/16 21:06;jieyu;commit 8238dbf0b5dd060d8b962f7decb810123b8bf937
Author: Kevin Klues <klueska@gmail.com>
Date:   Mon Dec 12 11:24:56 2016 -0800

    Changed 'Socket::create()' in IOSwitchboard to always be of 'kind' POLL.

    This includes updating the IOSwitchboard 'connect()' call to always
    connect to the switchboard with the HTTP scheme.

    Review: https://reviews.apache.org/r/54628/

commit e1971c8d1216648015bde8d0de65891b64720a58
Author: Kevin Klues <klueska@gmail.com>
Date:   Mon Dec 12 11:24:53 2016 -0800

    Updated address of 'http::connect()' with http scheme for SSL support.

    Review: https://reviews.apache.org/r/54627/",,,,,,,,,,,,,,,,,,,,,,,,,,
Reached unreachable statement at <path>/mesos/src/slave/containerizer/mesos/launch.cpp:766,MESOS-6767,13026938,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,jieyu,klueska,klueska,09/Dec/16 10:07,06/Feb/17 08:58,29/Oct/20 16:32,12/Dec/16 21:16,,,,,,,,,1.2.0,,,,,,,,,,,0,containerizer,mesosphere,,,,,,,"This error message can pop up in unexpected places (e.g. when running a LAUNCH_NESTED_CONTAINER_SESSION and an invalid command is passed to it).

We should likely just remove the UNREACHABLE() statement here as it's obviously reachable in cases where the command we are trying to launch is not found.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-6460,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 12 21:16:04 UTC 2016,,,,,,,"0|i37e6n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,1.0,,1.2.0,,,,,,,,,"12/Dec/16 21:16;klueska;{noformat}
commit 739e52803e937cd98ef97768ac656ac2059cc722
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Mon Dec 12 11:20:23 2016 -0800

    Fixed an unreachable statement in launch.cpp.
    
    The statement is actually reachable if exec fails. This patch fix it
    so that it is actually unreachable.
    
    Review: https://reviews.apache.org/r/54673
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,
IOSwitchboardServerTest.AttachOutput has CHECK failure if run it multiple times.,MESOS-6759,13026763,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Resolved,jieyu,jieyu,jieyu,08/Dec/16 18:49,10/Jan/17 00:07,29/Oct/20 16:32,14/Dec/16 18:10,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,"I can easily repo this issue on my dev centos7 box with the following command:
{noformat}
GLOG_v=1 bin/mesos-tests.sh --gtest_filter=IOSwitchboardServerTest.AttachOutput --verbose --gtest_repeat=2
....
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from IOSwitchboardServerTest
[ RUN      ] IOSwitchboardServerTest.AttachOutput
I1208 10:46:31.574084 41813 poll_socket.cpp:209] Socket error while sending: Broken pipe
/home/jie/workspace/mesos/src/tests/containerizer/io_switchboard_tests.cpp:265: Failure
(response).failure(): Disconnected
/home/jie/workspace/mesos/src/tests/containerizer/io_switchboard_tests.cpp:266: Failure
(response).failure(): Disconnected
F1208 10:46:31.574919 41751 future.hpp:1137] Check failed: !isFailed() Future::get() but state == FAILED: Disconnected
*** Check failure stack trace: ***
    @     0x7fc3f35a633a  google::LogMessage::Fail()
    @     0x7fc3f35a6299  google::LogMessage::SendToLog()
    @     0x7fc3f35a5caa  google::LogMessage::Flush()
    @     0x7fc3f35a89de  google::LogMessageFatal::~LogMessageFatal()
    @           0xb6a352  process::Future<>::get()
    @          0x1a050fe  mesos::internal::tests::IOSwitchboardServerTest_AttachOutput_Test::TestBody()
    @          0x1c54ce2  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1c4fe00  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1c31491  testing::Test::Run()
    @          0x1c31c14  testing::TestInfo::Run()
    @          0x1c3225a  testing::TestCase::Run()
    @          0x1c38b34  testing::internal::UnitTestImpl::RunAllTests()
    @          0x1c55907  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1c50948  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1c3787a  testing::UnitTest::Run()
    @          0x11cc653  RUN_ALL_TESTS()
    @          0x11cc209  main
    @     0x7fc3ecb61b15  __libc_start_main
    @           0xab5e89  (unknown)
Aborted (core dumped)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6795,MESOS-6796,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-12-08 19:14:25.433,,,false,MESOS-6460,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 10 00:07:10 UTC 2017,,,,,,,"0|i37d3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"08/Dec/16 19:14;anandmazumdar;An additional data point that {{AgentAPIStreamingTest.AttachContainerInput}} that tests this workflow end to end passes with {{gtest_repeat}} set. Can it be something related to this test itself?","09/Dec/16 01:31;jieyu;Current hypothesis:

1) we don't discard the socket.accept() if the io switchboard server terminates, which is a bug
2) after discard the future returned by socket.accept() in finalize, the tests passed
3) I suspect that Linux has a bug when there are multiple thread listening on different domain sockets. `accept` on one domain socket might accidentally accept a connection trying to connect to the other one. Also, io:poll on both sockets will return (which is not correct). (PS: I cannot repro this bug on OSX)","09/Dec/16 01:48;kaysoky;That sounds very similar to the reasoning behind this code in libprocess's finalize logic:
https://github.com/apache/mesos/blob/1d8d5c20709f97e6893e156be75057e34cbd97a9/3rdparty/libprocess/src/process.cpp#L1260-L1268","12/Dec/16 16:16;bbannier;[~anandmazumdar]: These tests fail reliably for me, at least on OS X, see MESOS-6780.","13/Dec/16 22:11;jieyu;OK, found more clue now. Looks like the listening socket gets closed after the first test run and got reused in the second test as the listening socket. 'accept' in the first test run is not discarded (still polling the listening socket)
{noformat}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from IOSwitchboardServerTest
[ RUN      ] IOSwitchboardServerTest.AttachOutput
[pid 45388] close(7)                    = 0
[pid 45388] close(8)                    = 0
[pid 45388] bind(9, {sa_family=AF_LOCAL, sun_path=""/tmp/9OMQri/mesos-io-switchboard""}, 110) = 0
[pid 45388] close(10)                   = 0
[pid 45388] connect(10, {sa_family=AF_LOCAL, sun_path=""/tmp/9OMQri/mesos-io-switchboard""}, 110) = 0
[pid 45453] accept(9, {sa_family=AF_LOCAL, NULL}, [2]) = 11
...
[pid 45388] close(9)                    = 0
...
[       OK ] IOSwitchboardServerTest.AttachOutput (3898 ms)
[----------] 1 test from IOSwitchboardServerTest (3898 ms total)
...
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from IOSwitchboardServerTest
[ RUN      ] IOSwitchboardServerTest.AttachOutput
[pid 45388] close(7)                    = 0
[pid 45388] close(8)                    = 0
[pid 45388] bind(9, {sa_family=AF_LOCAL, sun_path=""/tmp/3P0j2A/mesos-io-switchboard""}, 110) = 0
[pid 45388] connect(10, {sa_family=AF_LOCAL, sun_path=""/tmp/3P0j2A/mesos-io-switchboard""}, 110) = 0
[pid 45453] accept(9, {sa_family=AF_LOCAL, NULL}, [2]) = 11
[pid 45453] close(11)                   = 0
[pid 45453] accept(9, 0x7fb700cc51d0, [128]) = -1 EAGAIN (Resource temporarily unavailable)
/home/jie/workspace/mesos/src/tests/containerizer/io_switchboard_tests.cpp:271: Failure
(response).failure(): Disconnected
/home/jie/workspace/mesos/src/tests/containerizer/io_switchboard_tests.cpp:272: Failure
(response).failure(): Disconnected
F1213 14:06:02.095942 45388 future.hpp:1137] Check failed: !isFailed() Future::get() but state == FAILED: Disconnected
{noformat}","13/Dec/16 23:21;jieyu;So looks like it's possible that the listening socket go out of scope, resulting in the fd being closed, while the accept for the listening socket might still be pending. Then, the fd might get reused. In this particular case, the second test run re-use the same fd for the listening socket.","14/Dec/16 18:10;jieyu;Close this triaging ticket because the root cause has been found. See linked tickets to track the fix.","10/Jan/17 00:07;jieyu;commit 87d7230171ec7a3ad2d232fbffc5bfc94f9cd68c
Author: Greg Mann <greg@mesosphere.io>
Date:   Mon Jan 9 16:06:11 2017 -0800

    Fixed an FD leak in the IO switchboard.

    Previously, the IO switchboard could leak file descriptors
    because it held a reference to its server socket within the
    socket's accept loop. This patch explicitly discards the
    future containing this reference to eliminate the leak.

    Review: https://reviews.apache.org/r/55355/",,,,,,,,,,,,,,,,,,,,
I/O switchboard should deal with the case when reaping of the server failed.,MESOS-6756,13026532,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,jieyu,jieyu,08/Dec/16 01:45,08/Dec/16 02:42,29/Oct/20 16:32,08/Dec/16 02:42,,,,,,,,,1.2.0,,,,,,,,,,,0,,,,,,,,,"Currently, we don't deal with the reaping failure, which we should.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-6460,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 08 02:42:31 UTC 2016,,,,,,,"0|i37bof:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 47,,,,,,,,,,,3.0,,,,,,,,,,,"08/Dec/16 02:42;jieyu;commit 0701fa596d4c706b9f96bea037c6a9286538d270
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Wed Dec 7 17:42:18 2016 -0800

    Refactored the reaping logic in I/O switchboard.

    Previously, we don't handle the case where reaping failed. This patch
    refactored the reaping logic to address the issue. Here are the
    changes in this patch:

    (1) Add a reaped() function which we call whenever a process is
        reaped. From there we log why the process was reaped and
        (possibly) set a limitation on it if it died abnormally.
        We store this limitation in the switchboard's info struct.

    (2) Simplify the logic in watch() down to do nothing more than return
        the limitation future from the switchboard's info struct.

    (3) Change cleanup() to only send SIGTERM to the switchboard process
        if its status is still pending (i.e. it hasn't been reaped yet).

    (4) Change cleanup to properly remove the unix domain socket file in
        all cases of the reaped() future being satisifed.

    Review: https://reviews.apache.org/r/54518",,,,,,,,,,,,,,,,,,,,,,,,,,,
I/O switchboard should inherit agent environment variables.,MESOS-6748,13026421,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,jieyu,jieyu,07/Dec/16 18:12,15/Feb/17 11:25,29/Oct/20 16:32,07/Dec/16 22:52,,,,,,,,,1.2.0,,,,,,,,,,,0,,,,,,,,,"Since it is a libexec binary that owned by Mesos. Agent might have some environment variables (e.g., LD_LIBRARY_PATH) that are needed by the io switchboard server process.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6751,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-12-07 22:52:48.226,,,false,MESOS-6460,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 07 22:52:48 UTC 2016,,,,,,,"0|i37azr:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 47,,,,,,,,,,,1.0,,,,,,,,,,,"07/Dec/16 22:52;adam-mesos;[~jieyu] Doesn't this commit resolve this issue?
commit 57d986c6251ba06c48cbaba4c0504e5cd3aa5be1
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Wed Dec 7 10:10:06 2016 -0800

    Used agent environment variables for I/O switchboard.
    
    THe I/O switchboard server process will inheirt most of the agent
    environment variables, except for those that are prefixed with MESOS
    or LIBPROCESS since we don't rely on those environment variables.
    
    The patch also manually set a few LIBPROCESS environment variables to
    make sure the libprocess for the I/O switchboard can be initailzied
    properly without depending on external environment.
    
    Review: https://reviews.apache.org/r/54487
",,,,,,,,,,,,,,,,,,,,,,,,,,,
IOSwitchboard doesn't properly flush data on ATTACH_CONTAINER_OUTPUT,MESOS-6746,13026408,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,klueska,klueska,07/Dec/16 17:22,07/Dec/16 21:44,29/Oct/20 16:32,07/Dec/16 21:44,,,,,,,,,1.2.0,,,,,,,,,,,0,debugging,mesosphere,,,,,,,"Currently we are doing a close on the write end of all connection pipes when we exit the switchboard, but we don't wait until the read is flushed before exiting. This can cause some data to get dropped since the process may exit before the reader is flushed.  The current code is:
{noformat}
void IOSwitchboardServerProcess::finalize()                                   
{ 
  foreach (HttpConnection& connection, outputConnections) {                   
    connection.close();                                              
  }                                                                           
  
  if (failure.isSome()) {
    promise.fail(failure->message);                                           
  } else {
    promise.set(Nothing());                                                   
  }                                                                           
} 
{noformat}

We should change it to:
{noformat}
void IOSwitchboardServerProcess::finalize()                                   
{ 
  foreach (HttpConnection& connection, outputConnections) {                   
    connection.close();
    connection.closed().await();                                              
  }                                                                           
  
  if (failure.isSome()) {
    promise.fail(failure->message);                                           
  } else {
    promise.set(Nothing());                                                   
  }                                                                           
} 
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-12-07 20:16:13.573,,,false,MESOS-6460,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 07 21:44:33 UTC 2016,,,,,,,"0|i37awv:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 47,,,,,,,,,,,1.0,,1.2.0,,,,,,,,,"07/Dec/16 20:16;anandmazumdar;https://reviews.apache.org/r/54496/","07/Dec/16 21:44;anandmazumdar;{noformat}
commit a56734506b666bebb1d16c437c78b7e79311bd4f
Author: Anand Mazumdar <anand@apache.org>
Date:   Wed Dec 7 13:43:27 2016 -0800

    Made the IO Switchboard wait for the reader to signal before exiting.

    The switchboard never used to wait for the read end of the pipe
    to finish processing the data before exiting. This was problematic
    as the client connected to the output entrypoint might not receive
    the data. Note that we are relying on `socket.close()` to
    eventually ensure that the data is flushed to the client when the
    socket is closed.

    Review: https://reviews.apache.org/r/54496/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
The agent should synchronize with the IOSwitchboard to determine when it is ready to accept incoming connections.,MESOS-6737,13026216,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,klueska,klueska,klueska,07/Dec/16 02:47,06/Feb/17 08:56,29/Oct/20 16:32,07/Dec/16 05:39,,,,,,,,,1.2.0,,,,,,,,,,,0,debugging,mesosphere,,,,,,,"Currently, the agent has no way of knowing when the IOSwitchboard has started up and is ready to listen for incoming connections. We should add support to synchronize between them so the agent can figure this out.

The implementation should not block the launch path of the container, but rather incoming connections through the IOSwitchboards {{connect()}} call.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-12-07 05:39:31.951,,,false,MESOS-6460,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 07 05:39:31 UTC 2016,,,,,,,"0|i379q7:",9223372036854775807,,,,,jieyu,,,,,,,,,,,,,,,,,2.0,,1.2.0,,,,,,,,,"07/Dec/16 03:26;klueska;https://reviews.apache.org/r/54465","07/Dec/16 05:39;jieyu;Committed https://reviews.apache.org/r/54472/ which used some of the code from https://reviews.apache.org/r/54465 (see discussion in this review).

commit 9da19f5cc7a527a69a29a2d76c16cde8775c1b67
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Tue Dec 6 21:19:32 2016 -0800

    Waited for the domain socket file in IOSwitchboard::connect.

    Do not try to connect to the domain socket until the socket file has
    been created by the server.

    Review: https://reviews.apache.org/r/54472",,,,,,,,,,,,,,,,,,,,,,,,,,
"Create a test filter for stout tests that use `symlink` on Windows, as they will fail if not run as admin",MESOS-6731,13026086,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,johnkord,hausdorff,hausdorff,06/Dec/16 19:02,11/Apr/17 21:49,29/Oct/20 16:32,11/Apr/17 21:49,,,,,,,,,1.3.0,,,,,,stout,,,,,1,cmake,mesosphere,microsoft,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-23 20:47:50.885,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 11 21:49:29 UTC 2017,,,,,,,"0|hzzysk:i",9223372036854775807,,,,,kaysoky,,,,,,Mesosphere Sprint 53,Mesosphere Sprint 54,,,,,,,,,,3.0,,,,,,,,,,,"23/Mar/17 20:47;kaysoky;Review: https://reviews.apache.org/r/57824/","11/Apr/17 21:49;kaysoky;{code}
commit 113da435b12f826b14c807d7d666676e01d58d14
Author: John Kordich <johnkord@microsoft.com>
Date:   Mon Apr 10 18:31:08 2017 -0700

    Added test filters to stout tests.
    
    This commit moves the test-name-based filtering done in the Mesos
    tests into a stout test header.  The test filters search each test
    name for substrings and disables tests under certain circumstances.
    
    This move comes with a single new filter: a SymlinkFilter.
    On Windows, the creation of symlinks requires special privileges
    similar to `ROOT_` tests in Mesos.  Prior to this commit, we would
    need to manually filter out specific tests when building Mesos in
    a non-privileged CI (such as the ASF infrastructure).
    
    Review: https://reviews.apache.org/r/57824/
{code}
{code}
commit 9bf251bf6f67c4420ef2359b90706aeb1b32b524
Author: John Kordich <johnkord@microsoft.com>
Date:   Tue Apr 11 12:12:22 2017 -0700

    Added test filters to libprocess tests.
    
    This commit builds upon the introduction of test filters to stout
    tests by using the same set of filters in libprocess tests.
    
    Review: https://reviews.apache.org/r/57971/
{code}
{code}
commit a1400f79ef1c7bebd33c81a4ad6ebb64af968d8b
Author: John Kordich <johnkord@microsoft.com>
Date:   Tue Apr 11 12:19:09 2017 -0700

    Refactored Mesos tests `Environment` class on stout test changes.
    
    In order to support test filters on stout and libprocess, some of the
    logic in the Mesos test `Environment` class was moved into stout.
    This commit changes the Mesos test `Environment` to inherit the
    stout test `Environment`.
    
    Review: https://reviews.apache.org/r/57972/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Reserve operation should validate reserved resource role against resource allocationInfo role,MESOS-6730,13026009,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,gyliu,gyliu,06/Dec/16 16:19,26/Mar/18 08:08,29/Oct/20 16:32,16/Feb/17 02:07,,,,,,,,,1.3.0,,,,,,,,,,,0,,,,,,,,,"When doing dynamic reservation validation, the current logic is make sure the reserved resources role is same as the framework role (see [src/master/validation.cpp|https://github.com/apache/mesos/blob/0228fa74c25f450478a6a5a42e1ca384c26db8bd/src/master/validation.cpp#L1539-L1544]):

{code}
  if (frameworkRole.isSome() && resource.role() != frameworkRole.get()) {
      return Error(
          ""A reserve operation was attempted for a resource with role""
          "" '"" + resource.role() + ""', but the framework can only reserve""
          "" resources with role '"" + frameworkRole.get() + ""'"");
    }
{code}

With multi-role framework, we should validate reserved resource role same as resource allocation role.

Please make sure distinguish dynamic reservation with framework and http endpoint. If dynamic reservation was triggered by a framework, then we need to do such validation. If done by the http endpoint, then no need to validate the roles.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-12 14:54:30.434,,,false,MESOS-1763,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 16 02:07:37 UTC 2017,,,,,,,"0|i378g7:",9223372036854775807,,,,,gyliu,,,,,,Mesosphere Sprint 51,,,,,,,,,,,3.0,,,,,,,,,,,"12/Jan/17 14:54;bbannier;Reviews:

https://reviews.apache.org/r/55461/
https://reviews.apache.org/r/55462/ (WIP)","16/Feb/17 02:07;bmahler;{noformat}
commit 07b016f053c1c82aa177922293a9701da1783de7
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Wed Feb 15 17:15:55 2017 -0800

    Updated RESERVE operation validation for MULTI_ROLE frameworks.

    This change introduces validation of the 'AllocationInfo' of
    resources used in reservations.

    Review: https://reviews.apache.org/r/55462/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Check that `PreferredToolArchitecture` is set to `x64` on Windows before building,MESOS-6720,13025690,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hausdorff,hausdorff,hausdorff,05/Dec/16 17:26,29/Apr/19 09:28,29/Oct/20 16:32,23/Feb/17 22:30,,,,,,,,,1.3.0,,,,,,cmake,,,,,0,cmake,microsoft,,,,,,,"If this variable is not set before we build, it will cause the linker to occasionally hang forever, due to a MSVC toolchain bug in the linker.

We should make this easy on developers and check for them. If the variable is not set, we should display an error message explaining.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-02-22 23:49:45.814,,,false,MESOS-898,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 23 22:30:28 UTC 2017,,,,,,,"0|i376hb:",9223372036854775807,,,,,kaysoky,,,,,,Mesosphere Sprint 52,,,,,,,,,,,2.0,,,,,,,,,,,"22/Feb/17 23:49;kaysoky;Review: https://reviews.apache.org/r/55543/","23/Feb/17 22:30;kaysoky;{code}
commit af933d39d71f47696800a57cc48455cd7c06d0a4
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Fri Feb 17 15:43:55 2017 -0800

    Windows: Added build check for %PreferredToolArchitecture% == x64.
    
    Before building Mesos on a Windows machine, it is necessary to set
    %PreferredToolArchitecture% to the value ""x64"". This is necessary to
    work around (at least) two bugs in the MSVC backend: in particular, the
    linker can sometimes take hours or days to link `mesos-x.x.x.lib`, and
    the build system occasionally finds it self spuriously unable to find
    file `mesos-x.x.x.lib` to link against.
    
    These issues are well-known and documented (e.g., in the official Mesos
    ""getting started"" document), but it is better to simply refuse to build
    Mesos at all on Windows unless that environment variable is set.
    
    This commit will introduce such a check.
    
    Review: https://reviews.apache.org/r/55543/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Should destroy DEBUG containers on agent recovery.,MESOS-6718,13025579,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,klueska,klueska,klueska,05/Dec/16 09:39,07/Dec/16 03:29,29/Oct/20 16:32,07/Dec/16 03:28,,,,,,,,,1.2.0,,,,,,,,,,,0,debugging,mesosphere,,,,,,,We need to add support to destroy DEBUG containers on agent recovery. Right now these containers will stick around forever (or until they run to completion).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-6460,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 07 03:28:34 UTC 2016,,,,,,,"0|i375sn:",9223372036854775807,,,,,jieyu,,,,,,,,,,,,,,,,,3.0,,1.2.0,,,,,,,,,"07/Dec/16 03:28;klueska;{noformat}
commit c18fd732de72a45aca8af3c01c18794afefec15a
Author: Kevin Klues <klueska@gmail.com>
Date:   Tue Dec 6 15:21:56 2016 -0800

    Added helpers to checkpoint a 'destroy-on-recovery' file for containers.
    
    The existence of this file causes the containerizer to mark the
    container for destruction after agent restarts. This is currently
    useful for DEBUG containers launched by a
    `LAUNCH_NESTED_CONTAINER_SESSION` call.
    
    Review: https://reviews.apache.org/r/54368/
{noformat}
{noformat}
commit f54babdba179a0dfed379c243dc2f5f16c94ed81
Author: Kevin Klues <klueska@gmail.com>
Date:   Tue Dec 6 16:27:50 2016 -0800

    Added support to destroy running DEBUG containers on agent recovery.
    
    Review: https://reviews.apache.org/r/54367/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,
Port `slave_recovery_tests.cpp`,MESOS-6713,13025556,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,andschwa,hausdorff,hausdorff,05/Dec/16 07:50,09/Feb/18 21:28,29/Oct/20 16:32,09/Feb/18 21:19,,,,,,,,,1.6.0,,,,,,agent,,,,,0,mesosphere,microsoft,windows-mvp,,,,,,https://reviews.apache.org/r/65408/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-02-09 21:19:50.601,,,false,MESOS-6695,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 09 21:19:50 UTC 2018,,,,,,,"0|i375nj:",9223372036854775807,,,,,kaysoky,,,,,,Mesosphere Sprint 74,,,,,,,,,,,3.0,,,,,,,,,,,"09/Feb/18 21:19;andschwa;{noformat}
commit 0bc1c8ce3
Author: Andrew Schwartzmeyer andrew@schwartzmeyer.com
Date:   Wed Jan 10 14:41:51 2018 -0800
Fixed `SlaveRecoveryTest.ReconcileTasksMissingFromSlave`.

Because it is not possible to delete a file (or a folder recursively)
with open handles on Windows, we have to explicitly `reset()` the agent
before removing the framework meta directory. Otherwise, the task status
update manager will be destructed too late, and so an open handle for
`task.updates` will cause the `os::rmdir` to fail.

This is safe because we previously destructed the agent anyway, just
later in the test when it was reassigned.

Review: https://reviews.apache.org/r/65409

commit e4b319911
Author: Andrew Schwartzmeyer andrew@schwartzmeyer.com
Date:   Mon Jan 29 11:32:27 2018 -0800
Windows: Ported `slave_recovery_tests.cpp`.

This commit enables the unit tests found in `slave_recovery_tests.cpp`
to test agent recovery on Windows.

Review: https://reviews.apache.org/r/65408

commit bdfee0cbc
Author: Andrew Schwartzmeyer andrew@schwartzmeyer.com
Date:   Wed Nov 8 11:50:09 2017 -0800
Windows: Enabled `Flags::runtime_directory` for checkpointing.

This had previously been compiled out on Windows, but we bring it back
in order to support checkpointing.

Review: https://reviews.apache.org/r/65401

commit 42d57869b
Author: Andrew Schwartzmeyer andrew@schwartzmeyer.com
Date:   Wed Jan 17 13:45:03 2018 -0800
Windows: Tied task lifetimes to executors.

To enable recovery of checkpointed tasks, the agent must be able to die
without also killing the executors and tasks, thus we cannot set the
""job object kill on close"" limit unconditionally. However, the executors
must still be able to kill their tasks when they die, so we explicitly
enable this limit through a parent hook when launching the container for
the task. In this way, the agent can be restarted (e.g. for an upgrade)
without killing the executors, but the executors are still capable of
killing their tasks on catastrophic death.

Review: https://reviews.apache.org/r/65400

commit 65df55a40
Author: Andrew Schwartzmeyer andrew@schwartzmeyer.com
Date:   Wed Nov 8 11:47:22 2017 -0800
Windows: Moved ""kill on close"" job object flag to own function.

Instead of setting this flag unconditionally when we create a job
object, this patch abstracts it to the function
`set_job_kill_on_close_limit(pid_t)` so that users can choose how to tie
job object lifetimes together.

Review: https://reviews.apache.org/r/65399
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove of unix domain socket path in IOSwitchboard::cleanup,MESOS-6689,13025456,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,klueska,klueska,klueska,05/Dec/16 00:35,05/Dec/16 21:42,29/Oct/20 16:32,05/Dec/16 21:42,,,,,,,,,1.2.0,,,,,,,,,,,0,debugging,mesosphere,,,,,,,We currently leak all of the unix domain socket files created by the switchboard in the `/tmp` directory. We need to clean them up properly.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-12-05 21:42:05.633,,,false,MESOS-6460,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 05 21:42:05 UTC 2016,,,,,,,"0|i3751b:",9223372036854775807,,,,,jieyu,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,"05/Dec/16 00:39;klueska;https://reviews.apache.org/r/54352/","05/Dec/16 21:42;jieyu;commit 3ad06661847a8c38ee9bfe9c7f593873f4769cbe
Author: Kevin Klues <klueska@gmail.com>
Date:   Mon Dec 5 13:41:25 2016 -0800

    Added removal of unix domain socket path in IOSwitchboard::cleanup.

    Review: https://reviews.apache.org/r/54352/",,,,,,,,,,,,,,,,,,,,,,,,,,
IOSwitchboard should recover spawned server pid on agent restarts,MESOS-6688,13025455,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,klueska,klueska,klueska,05/Dec/16 00:31,07/Dec/16 06:07,29/Oct/20 16:32,07/Dec/16 06:07,,,,,,,,,1.2.0,,,,,,,,,,,0,debugging,mesosphere,,,,,,,"We need to do proper recovery of the io switchboard server pid across agent restarts. As of now, if the agent restarts there is now way to recover this pid.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-12-06 02:26:36.534,,,false,MESOS-6460,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 07 03:35:20 UTC 2016,,,,,,,"0|i37513:",9223372036854775807,,,,,jieyu,,,,,,,,,,,,,,,,,2.0,,1.2.0,,,,,,,,,"05/Dec/16 00:39;klueska;https://reviews.apache.org/r/54354/","05/Dec/16 00:39;klueska;https://reviews.apache.org/r/54355/","06/Dec/16 02:26;jieyu;commit 4c80eaec0098c9664588fdd9b21cbc53e0822611
Author: Kevin Klues <klueska@gmail.com>
Date:   Mon Dec 5 17:39:45 2016 -0800

    Reorganized location of checkpointed files for the 'IOSwitchboard'.

    Review: https://reviews.apache.org/r/54401/

commit a1db860ac5a60887d7241da552d925158362bf8b
Author: Kevin Klues <klueska@gmail.com>
Date:   Mon Dec 5 17:39:42 2016 -0800

    Added path helpers for checkpointing the io switchboard pid.

    Review: https://reviews.apache.org/r/54354/

commit 543533f09814b6ce7a7822acc3accb47379577a2
Author: Kevin Klues <klueska@gmail.com>
Date:   Mon Dec 5 17:39:39 2016 -0800

    Added short circuit for `local` mode in `IOSwitchboard::connect()'.

    Review: https://reviews.apache.org/r/54353/","07/Dec/16 03:35;klueska;{noformat}
commit 4069e1c4248302a0b72201c674b60f1bb988d276
Author: Kevin Klues <klueska@gmail.com>
Date:   Tue Dec 6 15:21:47 2016 -0800

    Added implementation of `recover()` to the IOSwitchboard isolator.
    
    Review: https://reviews.apache.org/r/54355/
{noformat}
{noformat}
commit 575e19646b30f0286a573af399cbb087c7cd44e5
Author: Kevin Klues <klueska@gmail.com>
Date:   Tue Dec 6 15:21:53 2016 -0800

    Added test for IOSwitchboard `recovery()`.
    
    Review: https://reviews.apache.org/r/54455/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,
Duplicate image layer ids may make the backend failed to mount rootfs.,MESOS-6654,13024526,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,gilbert,gilbert,gilbert,30/Nov/16 18:09,29/Jan/17 00:59,29/Oct/20 16:32,29/Jan/17 00:59,,,,,,,,,1.2.0,,,,,,containerization,,,,,0,aufs,backend,containerizer,,,,,,"Some images (e.g., 'mesosphere/inky') may contain duplicate layer ids in manifest, which may cause some backends unable to mount the rootfs (e.g., 'aufs' backend). We should make sure that each layer path returned in 'ImageInfo' is unique.

Here is an example manifest from 'mesosphere/inky':
{noformat}
[20:13:08]W:	 [Step 10/10]    ""name"": ""mesosphere/inky"",
[20:13:08]W:	 [Step 10/10]    ""tag"": ""latest"",
[20:13:08]W:	 [Step 10/10]    ""architecture"": ""amd64"",
[20:13:08]W:	 [Step 10/10]    ""fsLayers"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ],
[20:13:08]W:	 [Step 10/10]    ""history"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""parent\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""created\"":\""2014-08-15T00:31:36.247988044Z\"",\""container\"":\""ff756d99367825677c3c18cc5054bfbb3674a7f52a9f916282fb46b8feaddfb7\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) CMD [inky]\""],\""Image\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""parent\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""created\"":\""2014-08-15T00:31:36.068514721Z\"",\""container\"":\""696c3d66c8575dfff3ba71267bf194ae97f0478231042449c98aa0d9164d3c8c\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) MAINTAINER support@mesosphere.io\""],\""Image\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\""],\""Image\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""parent\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""created\"":\""2014-06-05T00:05:35.990887725Z\"",\""container\"":\""bb3475b3130b6a47104549a0291a6569d24e41fa57a7f094591f0d4611fd15bc\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) CMD [/bin/sh]\""],\""Image\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\""],\""Image\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""parent\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""created\"":\""2014-06-05T00:05:35.692528634Z\"",\""container\"":\""fc203791c4d5024b1a976223daa1cc7b1ceeb5b3abf25a2fb73034eba6398026\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ADD file:88f36b32456f849299e5df807a1e3514cf1da798af9692a0004598e500be5901 in /\""],\""Image\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":null,\""Image\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":2433303}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""parent\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""created\"":\""2014-06-05T00:05:35.589531476Z\"",\""container\"":\""f7d939e68b5afdd74637d9204c40fe00295e658923be395c761da3278b98e446\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) MAINTAINER Jrme Petazzoni \\u003cjerome@docker.com\\u003e\""],\""Image\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":null,\""Image\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""comment\"":\""Imported from -\"",\""created\"":\""2013-06-13T14:03:50.821769-07:00\"",\""container_config\"":{\""Hostname\"":\""\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":null,\""Cmd\"":null,\""Image\"":\""\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":null,\""Labels\"":null},\""docker_version\"":\""0.4.0\"",\""architecture\"":\""x86_64\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ],
[20:13:08]W:	 [Step 10/10]    ""schemaVersion"": 1,
[20:13:08]W:	 [Step 10/10]    ""signatures"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""header"": {
[20:13:08]W:	 [Step 10/10]             ""jwk"": {
[20:13:08]W:	 [Step 10/10]                ""crv"": ""P-256"",
[20:13:08]W:	 [Step 10/10]                ""kid"": ""4AYN:KH32:GJJD:I6BX:SJAZ:A3EC:P7IC:7O7C:22ZQ:3Z5O:75VQ:3QOT"",
[20:13:08]W:	 [Step 10/10]                ""kty"": ""EC"",
[20:13:08]W:	 [Step 10/10]                ""x"": ""o8bvrUwNpXKZdgoo2wQ7EHQzCVYhVuoOvjqGEXtRylU"",
[20:13:08]W:	 [Step 10/10]                ""y"": ""DCHyGr0Cbi-fZzqypQm16qKfefUMqCTk0rQME-q5GmA""
[20:13:08]W:	 [Step 10/10]             },
[20:13:08]W:	 [Step 10/10]             ""alg"": ""ES256""
[20:13:08]W:	 [Step 10/10]          },
[20:13:08]W:	 [Step 10/10]          ""signature"": ""f3fAob4XPT0pUW9TiPtxAE_zPAe0PdM2imxAeaCmJbBf6Lb-SuFPVGE4iqz1CO0VOijeYVuB1G1lv_a5Nnj5zg"",
[20:13:08]W:	 [Step 10/10]          ""protected"": ""eyJmb3JtYXRMZW5ndGgiOjEzNzA3LCJmb3JtYXRUYWlsIjoiQ24wIiwidGltZSI6IjIwMTYtMDgtMDVUMjA6MTM6MDdaIn0""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ]
[20:13:08]W:	 [Step 10/10] }'
{noformat}

These two layer ids are totally identical:
{noformat}
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
{noformat}

It would make the backend (e.g., aufs) failed to mount the rootfs due to invalid arguments.
{noformat}
[20:13:08]W:	 [Step 10/10] E0805 20:13:08.614994 23432 slave.cpp:4029] Container 'f2c1fd6d-4d11-45cd-a916-e4d73d226451' for executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 failed to start: Failed to mount rootfs '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/rootfses/427b7851-bf82-4553-80f3-da2d42cede77' with aufs: Invalid argument
{noformat}

We should make sure the vector of layer paths that is passed to the backend contains only unique layer path.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-29 00:59:05.631,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 29 00:59:05 UTC 2017,,,,,,,"0|hzzz3w:z",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 47,Mesosphere Sprint 48,Mesosphere Sprint 50,,,,,,,,,3.0,,1.2.0,,,,,,,,,"30/Nov/16 19:00;gilbert;https://reviews.apache.org/r/54215/","29/Jan/17 00:59;jieyu;commit d261076edff66c6a22a47e9f46c425917394a1e3
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Sat Jan 28 16:05:04 2017 -0800

    Fixed duplicate image layer ids returned by docker store.

    This issue is exposed by pulling the 'mesosphere/inky' docker
    image using registry puller. Due to the duplicate layer id
    from the manifest, there are duplicate layer pathes passed
    to the backend. The aufs backend cannot handle this case and
    returns 'invalid arguments' error. Ideally, we should make
    sure that layer paths that are passed to the backend are
    unique.

    Review: https://reviews.apache.org/r/54215/",,,,,,,,,,,,,,,,,,,,,,,,,,
Overlayfs backend may fail to mount the rootfs if both container image and image volume are specified.,MESOS-6653,13024523,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,gilbert,gilbert,gilbert,30/Nov/16 17:54,07/Feb/17 18:19,29/Oct/20 16:32,26/Jan/17 02:02,,,,,,,,,1.2.0,,,,,,containerization,,,,,0,backend,containerizer,overlayfs,,,,,,"Depending on MESOS-6000, we use symlink to shorten the overlayfs mounting arguments. However, if more than one image need to be provisioned (e.g., a container image is specified while image volumes are specified for the same container), the symlink .../backends/overlay/links would fail to be created since it exists already.

Here is a simple log when we hard code overlayfs as our default backend:
{noformat}
[07:02:45] :	 [Step 10/10] [ RUN      ] Nesting/VolumeImageIsolatorTest.ROOT_ImageInVolumeWithRootFilesystem/0
[07:02:46] :	 [Step 10/10] I1127 07:02:46.416021  2919 containerizer.cpp:207] Using isolation: filesystem/linux,volume/image,docker/runtime,network/cni
[07:02:46] :	 [Step 10/10] I1127 07:02:46.419312  2919 linux_launcher.cpp:150] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[07:02:46] :	 [Step 10/10] E1127 07:02:46.425336  2919 shell.hpp:107] Command 'hadoop version 2>&1' failed; this is the output:
[07:02:46] :	 [Step 10/10] sh: 1: hadoop: not found
[07:02:46] :	 [Step 10/10] I1127 07:02:46.425379  2919 fetcher.cpp:69] Skipping URI fetcher plugin 'hadoop' as it could not be created: Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
[07:02:46] :	 [Step 10/10] I1127 07:02:46.425452  2919 local_puller.cpp:94] Creating local puller with docker registry '/tmp/R6OUei/registry'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.427258  2934 containerizer.cpp:956] Starting container 9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330 for executor 'test_executor' of framework 
[07:02:46] :	 [Step 10/10] I1127 07:02:46.427592  2938 metadata_manager.cpp:167] Looking for image 'test_image_rootfs'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.427774  2936 local_puller.cpp:147] Untarring image 'test_image_rootfs' from '/tmp/R6OUei/registry/test_image_rootfs.tar' to '/tmp/R6OUei/store/staging/9krDz2'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.512070  2933 local_puller.cpp:167] The repositories JSON file for image 'test_image_rootfs' is '{""test_image_rootfs"":{""latest"":""815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346""}}'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.512279  2933 local_puller.cpp:295] Extracting layer tar ball '/tmp/R6OUei/store/staging/9krDz2/815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346/layer.tar to rootfs '/tmp/R6OUei/store/staging/9krDz2/815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346/rootfs'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617442  2937 metadata_manager.cpp:155] Successfully cached image 'test_image_rootfs'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617908  2938 provisioner.cpp:286] Image layers: 1
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617925  2938 provisioner.cpp:296] Should hit here
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617949  2938 provisioner.cpp:315] !!!!: bind
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617959  2938 provisioner.cpp:315] !!!!: overlay
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617967  2938 provisioner.cpp:315] !!!!: copy
[07:02:46] :	 [Step 10/10] I1127 07:02:46.617974  2938 provisioner.cpp:318] Provisioning image rootfs '/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/rootfses/c71e83d2-5dbe-4eb7-a2fc-b8cc826771f7' for container 9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330 using overlay backend
[07:02:46] :	 [Step 10/10] I1127 07:02:46.618408  2936 overlay.cpp:175] Created symlink '/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/links' -> '/tmp/DQ3blT'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.618472  2936 overlay.cpp:203] Provisioning image rootfs with overlayfs: 'lowerdir=/tmp/DQ3blT/0,upperdir=/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/scratch/c71e83d2-5dbe-4eb7-a2fc-b8cc826771f7/upperdir,workdir=/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/scratch/c71e83d2-5dbe-4eb7-a2fc-b8cc826771f7/workdir'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.619098  2933 linux.cpp:451] Ignored an image volume for container 9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330
[07:02:46] :	 [Step 10/10] I1127 07:02:46.619745  2938 metadata_manager.cpp:167] Looking for image 'test_image_volume'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.619925  2937 local_puller.cpp:147] Untarring image 'test_image_volume' from '/tmp/R6OUei/registry/test_image_volume.tar' to '/tmp/R6OUei/store/staging/2GNlJO'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.713526  2935 local_puller.cpp:167] The repositories JSON file for image 'test_image_volume' is '{""test_image_volume"":{""latest"":""815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346""}}'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.713726  2935 local_puller.cpp:295] Extracting layer tar ball '/tmp/R6OUei/store/staging/2GNlJO/815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346/layer.tar to rootfs '/tmp/R6OUei/store/staging/2GNlJO/815b809d588c80fd6ddf4d6ac244ad1c01ae4cbe0f91cc7480e306671ee9c346/rootfs'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.818696  2937 metadata_manager.cpp:155] Successfully cached image 'test_image_volume'
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819169  2934 provisioner.cpp:286] Image layers: 1
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819188  2934 provisioner.cpp:296] Should hit here
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819221  2934 provisioner.cpp:315] !!!!: bind
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819232  2934 provisioner.cpp:315] !!!!: overlay
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819236  2934 provisioner.cpp:315] !!!!: copy
[07:02:46] :	 [Step 10/10] I1127 07:02:46.819241  2934 provisioner.cpp:318] Provisioning image rootfs '/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/rootfses/baf632b3-29c5-45e4-9d2e-6f3a2bdd9759' for container 9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330 using overlay backend
[07:02:46] :	 [Step 10/10] ../../src/tests/containerizer/volume_image_isolator_tests.cpp:214: Failure
[07:02:46] :	 [Step 10/10] (launch).failure(): Failed to create symlink '/mnt/teamcity/temp/buildTmp/Nesting_VolumeImageIsolatorTest_ROOT_ImageInVolumeWithRootFilesystem_0_1fMo0c/provisioner/containers/9af6c98a-d9f7-4c89-a5ed-fc7ae2fa1330/backends/overlay/links' -> '/tmp/6dj9IG'
[07:02:46] :	 [Step 10/10] [  FAILED  ] Nesting/VolumeImageIsolatorTest.ROOT_ImageInVolumeWithRootFilesystem/0, where GetParam() = false (919 ms)
{noformat}

We should differenciate the links for different provisioned images.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-12-14 18:49:49.781,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 07 18:17:25 UTC 2017,,,,,,,"0|hzzz3w:y",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 47,Mesosphere Sprint 48,Mesosphere Sprint 50,,,,,,,,,3.0,,1.2.0,,,,,,,,,"30/Nov/16 19:01;gilbert;https://reviews.apache.org/r/54211/
https://reviews.apache.org/r/54212/","14/Dec/16 18:49;jieyu;commit 812e5e3d4e4d9e044a1cfe6cc7eaab10efb499b6
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Dec 14 10:49:19 2016 -0800

    Fixed overlay backend symlink error message.

    Review: https://reviews.apache.org/r/54211/","26/Jan/17 01:56;gilbert;commit 27a5016154384eb18ec5d7577c1723c9d17e87f7
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jan 25 07:51:35 2017 -0800

    Fixed overlay backend provisioning multi images symlink.
    
    Since the fix of MESOS-6000, symlinks are used in overlayfs
    backend to shorten the arguments when mounting the rootfs.
    E.g., '.../backends/overlay/links' is the symlink created
    for a provisioned image. It becomes problematic if a
    container image is specified while some image volumes are
    specified for the same container. An unique symlink is
    needed for each image to be provisioned.
    
    Please note that changing the symlinks directory would
    still be backward compatible for legacy containers, since
    the container backend directory will be removed anyway in
    provisioner::destroy().
    
    Review: https://reviews.apache.org/r/54212/","07/Feb/17 18:17;alexr;[~gilbert], [~jieyu], [~tillt] This has not been backported to 1.1.1, nor to 1.0.3, hence I'm removing 1.1.1 from target versions.",,,,,,,,,,,,,,,,,,,,,,,,
Expose container id in ContainerStatus in DockerContainerizer.,MESOS-6625,13022607,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,jieyu,jieyu,22/Nov/16 19:43,28/Nov/16 21:22,29/Oct/20 16:32,23/Nov/16 06:41,,,,,,,,,1.2.0,,,,,,,,,,,0,debugging,mesosphere,,,,,,,"Currently, the container id is only exposed for MesosContainerizer. We should make it consistent in DockerContainerizer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-6460,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 23 06:41:30 UTC 2016,,,,,,,"0|hzzz6c:zlm",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 47,,,,,,,,,,,2.0,,,,,,,,,,,"22/Nov/16 21:51;jieyu;https://reviews.apache.org/r/54001/","23/Nov/16 06:06;jieyu;https://reviews.apache.org/r/54021/","23/Nov/16 06:41;jieyu;commit 3f061f3cbf5f399af7bdad10f3fce4edca3d4de6
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Tue Nov 22 22:04:27 2016 -0800

    Updated a Docker containerizer test.

    The test is updated to check for ContainerID in ContainerStatus.

    Review: https://reviews.apache.org/r/54021

commit 21b999f04669486ca41a38524793a92fb6b3821b
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Tue Nov 22 13:46:21 2016 -0800

    Addes status method to DockerContainerizer.

    This patch added the implementation of the 'status' method to
    DockerContainerizer. Previously, this method is missing. Currently,
    the only status the DockerContainerizer reports is the ContainerID.

    Review: https://reviews.apache.org/r/54001",,,,,,,,,,,,,,,,,,,,,,,,,
SSL downgrade path will CHECK-fail when using both temporary and persistent sockets,MESOS-6621,13022371,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,kaysoky,kaysoky,kaysoky,22/Nov/16 02:30,01/Sep/17 23:34,29/Oct/20 16:32,23/Nov/16 00:33,0.28.2,1.0.2,1.1.0,,,,,,0.28.3,1.0.3,1.1.1,1.2.0,,,libprocess,,,,,0,mesosphere,,,,,,,,"The code path for downgrading sockets from SSL to non-SSL includes this code:
{code}
    // If this address is a temporary link.
    if (temps.count(addresses[to_fd]) > 0) {
      temps[addresses[to_fd]] = to_fd;
      // No need to erase as we're changing the value, not the key.
    }

    // If this address is a persistent link.
    if (persists.count(addresses[to_fd]) > 0) {
      persists[addresses[to_fd]] = to_fd;
      // No need to erase as we're changing the value, not the key.
    }
{code}
https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L2311-L2321

It is possible for libprocess to hold both temporary and persistent sockets to the same address.  This can happen when a message is first sent ({{ProcessBase::send}}), and then a link is established ({{ProcessBase::link}}).  When the target of the message/link is a non-SSL socket, both temporary and persistent sockets go through the downgrade path.

If a temporary socket is present while a persistent socket is being created, the above code will remap both temporary and persistent sockets to the same address (it should only remap the persistent socket).  This leads to some CHECK failures if those sockets are used or closed later:
* {code}
    bool persist = persists.count(address) > 0;
    bool temp = temps.count(address) > 0;
    if (persist || temp) {
      int s = persist ? persists[address] : temps[address];
      CHECK(sockets.count(s) > 0);
socket = sockets.at(s);
{code}
https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L1942
* {code}
        if (dispose.count(s) > 0) {
          // This is either a temporary socket we created or it's a
          // socket that we were receiving data from and possibly
          // sending HTTP responses back on. Clean up either way.
          if (addresses.count(s) > 0) {
            const Address& address = addresses[s];
            CHECK(temps.count(address) > 0 && temps[address] == s);
temps.erase(address);
{code}
https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L2044",SSL with downgrade enabled,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/16 02:33;kaysoky;test.patch;https://issues.apache.org/jira/secure/attachment/12839934/test.patch","22/Nov/16 02:33;kaysoky;test_linkee.cpp;https://issues.apache.org/jira/secure/attachment/12839935/test_linkee.cpp",,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-7930,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 23 01:16:43 UTC 2016,,,,,,,"0|i36lzr:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 47,,,,,,,,,,,3.0,,0.28.3,1.0.3,1.1.1,1.2.0,,,,,,"22/Nov/16 02:35;kaysoky;This turns out to be clunky (but possible) to repro in a unit test.  I've attached a patch which includes an SSL test and a test-specific replacement for the {{3rdparty/libprocess/src/tests/test_linkee.cpp}} file which exists in the code base already.

The check failure can be reproduced with relatively few iterations:
{code}
3rdparty/libprocess/libprocess-tests --gtest_filter=""SSLTest.TempDowngrade"" --gtest_break_on_failure --gtest_repeat=10
{code}","22/Nov/16 20:28;kaysoky;The fix (without the test): https://reviews.apache.org/r/53997/","23/Nov/16 00:33;kaysoky;Backports will be added shortly.
{code}
commit fa617bd5b542c607205d96a6b00beda75dc5fdca
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Tue Nov 22 13:55:17 2016 -0800

    Fix SSL downgrade pathway for temporary/persistent sockets.
    
    This fixes some potential CHECK failures when a libprocess process
    has (1) SSL downgrade enabled and (2) temporary and persistent
    connections open with the same remote address.  The second point is
    only possible if messages are to a remote address without a persistent
    connection and then a persistent connection is created.
    
    The SSL downgrade path was only checking if the address of a socket
    matched when performing the downgrade.  The code must also check to
    see if the socket itself matches.
    
    Review: https://reviews.apache.org/r/53997/
{code}","23/Nov/16 01:16;kaysoky;{code:title=0.28.3}
commit 95ee5a583eef1e60ba743900d49629b2286dbc97
Author: Joseph Wu <josephwu@apache.org>
Date:   Tue Nov 22 17:08:42 2016 -0800

    Added MESOS-6621 to CHANGELOG for 0.28.3.

commit 4c56f51f5ea8b633cee12cbc212f29a760f99583
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Tue Nov 22 17:07:40 2016 -0800

    Fix SSL downgrade pathway for temporary/persistent sockets.
{code}
{code:title=1.0.3}
commit c82a283d87af557128a96ec02f2ef9a00f00d4df
Author: Joseph Wu <josephwu@apache.org>
Date:   Tue Nov 22 17:06:59 2016 -0800

    Added MESOS-6621 to CHANGELOG for 1.0.3.

commit 3c5b5e054c0328dc500d4940d527d40bb1526b65
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Tue Nov 22 17:03:02 2016 -0800

    Fix SSL downgrade pathway for temporary/persistent sockets.
{code}
{code:title=1.1.1}
commit de3a1b58a105493e45e87299bcaf3a4279939a02
Author: Joseph Wu <josephwu@apache.org>
Date:   Tue Nov 22 16:58:56 2016 -0800

    Added MESOS-6621 to CHANGELOG for 1.1.1.

commit 1d84b0b42d0cd067c034091a4897e817fc2f86d0
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Tue Nov 22 17:00:01 2016 -0800

    Fix SSL downgrade pathway for temporary/persistent sockets.
{code}",,,,,,,,,,,,,,,,,,,,,,,,
Improve task management for unreachable tasks,MESOS-6619,13022255,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,neilc,neilc,neilc,21/Nov/16 17:47,24/Jan/17 01:37,29/Oct/20 16:32,24/Jan/17 01:37,,,,,,,,,1.2.0,,,,,,master,,,,,0,mesosphere,,,,,,,,"Scenario:

# Framework starts non-partition-aware task T on agent A
# Agent A is partitioned. Task T is marked as a ""completed task"" in the {{Framework}} struct of the master, as part of {{Framework::removeTask}}.
# Agent A re-registers with the master. The tasks running on A are re-added to their respective frameworks on the master as running tasks.
# In {{Master::\_reregisterSlave}}, the master sends a {{ShutdownFrameworkMessage}} for all non-partition-aware frameworks running on the agent. The master then does {{removeTask}} for each task managed by one of these frameworks, which results in calling {{Framework::removeTask}}, which adds _another_ task to {{completed_tasks}}. Note that {{completed_tasks}} does not attempt to detect/suppress duplicates, so this results in two elements in the {{completed_tasks}} collection.

Similar problems occur when a partition-aware task is running on a partitioned agent that re-registers: the result is a task in the {{tasks}} list _and_ a task in the {{completed_tasks}} list.

Possible fixes/changes:

* Adding a task to the {{completed_tasks}} list when an agent becomes partitioned is debatable; certainly for partition-aware tasks, the task is not ""completed"". We might consider adding an ""{{unreachable_tasks}}"" list to the HTTP endpoints.
* Regardless of whether we continue to use {{completed_tasks}} or add a new collection, we should ensure the consistency of that data structure after agent re-registration.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-6394,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 24 01:37:45 UTC 2017,,,,,,,"0|hzzz3w:zzi",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 48,Mesosphere Sprint 49,,,,,,,,,,8.0,,,,,,,,,,,"29/Nov/16 22:37;neilc;https://reviews.apache.org/r/54183/
https://reviews.apache.org/r/54182/
https://reviews.apache.org/r/54181/
https://reviews.apache.org/r/54180/
https://reviews.apache.org/r/54179/
https://reviews.apache.org/r/54178/
https://reviews.apache.org/r/54177/","24/Jan/17 01:37;neilc;{noformat}
commit 44fc29fd62bf6b21b7e21b31542f9ef9fefc948b
Author: Neil Conway <neil.conway@gmail.com>
Date:   Mon Jan 23 17:03:00 2017 -0800

    Improved management of unreachable and completed tasks in master.

    Before partition-awareness, when an agent failed health checks, the
    master removed the agent from the registry, marked all of its tasks
    TASK_LOST, and moved them to the `completedTasks` list in the master's
    memory. Although ""lost"" tasks might still be running, partitioned agents
    would only be allowed to re-register if the master failed over, in which
    case the `completedTasks` map would be emptied.

    When partition-awareness was introduced, we initially followed the same
    scheme, with the only difference that partition-aware tasks are marked
    TASK_UNREACHABLE, not TASK_LOST.

    This scheme has a few shortcomings. First, partition-aware tasks might
    resume running when the partitioned agent re-registers. Second, we
    re-added non-partition aware tasks when the agent re-registered but then
    marked them completed when the framework is shutdown, resulting in two
    entries in `completedTasks`.

    This commit introduces a separate bounded map, `unreachableTasks`. These
    tasks are reported separately via the HTTP endpoints, because they have
    different semantics (unlike completed tasks, unreachable tasks can
    resume running). The size of this map is limited by a new master flag,
    `--max_unreachable_tasks_per_framework`. This commit also changes the
    master to omit re-adding non-partition-aware tasks on re-registering
    agents (unless the master has failed over): those tasks will shortly be
    shutdown anyway.

    Finally, this commit fixes a minor bug in the previous code: the
    previous coding neglected to shutdown non-partition-aware frameworks
    running on pre-1.0 Mesos agents that re-register with the master after
    a network partition.

    Review: https://reviews.apache.org/r/54183/
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,
Some tests use hardcoded port numbers.,MESOS-6618,13022175,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,Fixed,bbannier,bbannier,bbannier,21/Nov/16 12:31,26/Apr/17 17:00,29/Oct/20 16:32,25/Nov/16 11:08,1.0.0,1.1.0,,,,,,,1.2.0,,,,,,test,,,,,0,mesosphere,parallel-tests,,,,,,,"DockerContainerizerTest.ROOT_DOCKER_NoTransitionFromKillingToRunning and many HealthCheckTests use hardcoded port numbers. This can create false failures if these tests are run in parallel on the same machine.

It appears instead we should use random port numbers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6620,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-11-25 11:08:46.209,,,false,MESOS-4809,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 25 11:08:46 UTC 2016,,,,,,,"0|i36ks7:",9223372036854775807,,,,,alexr,,,,,,Mesosphere Sprint 48,,,,,,,,,,,3.0,,1.2.0,,,,,,,,,"21/Nov/16 18:16;bbannier;Reviews:

https://reviews.apache.org/r/53948/
https://reviews.apache.org/r/53949/
https://reviews.apache.org/r/53950/","25/Nov/16 11:08;alexr;{noformat}
Commit: 9de5fb277b00e77414075e268e8709c0b6d5d0a9 [9de5fb2]
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date: 25 November 2016 at 11:58:46 GMT+1
Committer: Alexander Rukletsov <alexr@apache.org>

Added a test helper to obtain unused port.

Review: https://reviews.apache.org/r/53949/
{noformat}
{noformat}
Commit: 79cee8ad9653ba2514d0d63ee53e05bb7f78ddc9 [79cee8a]
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date: 25 November 2016 at 11:58:57 GMT+1
Committer: Alexander Rukletsov <alexr@apache.org>

Avoided hardcoding ports in some tests.

Review: https://reviews.apache.org/r/53950/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Shutdown completed frameworks when unreachable agent re-registers,MESOS-6602,13021657,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,neilc,neilc,neilc,18/Nov/16 02:49,26/Apr/17 16:52,29/Oct/20 16:32,24/Jan/17 01:35,,,,,,,,,1.2.0,,,,,,,,,,,0,mesosphere,,,,,,,,"We currently shutdown completed frameworks when an agent re-registers with a master that it is already registered with (MESOS-633). We should also shutdown completed frameworks when an unreachable agent re-registers.

This is distinct from the more difficult problem of shutting down completed frameworks after master failover (MESOS-4659).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6608,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-6394,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 24 01:35:56 UTC 2017,,,,,,,"0|hzzz3w:zz",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 48,Mesosphere Sprint 49,,,,,,,,,,5.0,,,,,,,,,,,"30/Nov/16 22:26;neilc;https://reviews.apache.org/r/54232/","24/Jan/17 01:35;neilc;{noformat}
commit 482796bb1fb3f65269b0fd40143eccb8ced2027a
Author: Neil Conway <neil.conway@gmail.com>
Date:   Mon Jan 23 17:03:53 2017 -0800

    Shutdown tasks of completed frameworks on agent re-registration.

    Previously, if a framework completed (e.g., due to a teardown operation
    or framework shutdown), any framework tasks running on partitioned
    agents would not be shutdown when the agent re-registered. For tasks
    that are not partition-aware, the task would be shutdown on agent
    re-registration anyway. But for partition-aware tasks, this could lead
    to orphan tasks.

    Fix this by changing the master to shutdown such tasks when the agent
    reregisters.

    Note that if the master fails over between the time the framework
    completes and a partitioned agent re-registers, any framework tasks
    running on the agent will NOT be shutdown. This is a known bug; fixing
    it requires persisting the framework shutdown operation to the registry
    (MESOS-1719).

    Review: https://reviews.apache.org/r/54232/

commit 6fd373b2643dd792c9141c2b4833d5945a50562e
Author: Neil Conway <neil.conway@gmail.com>
Date:   Mon Jan 23 17:03:49 2017 -0800

    Changed TASK_UNREACHABLE to be a non-terminal state.

    This task state was always conceptually non-terminal, but previously
    `isTerminalState` returned true for it. The master now distinguishes
    between ""removable"" and ""terminal"" task states, so we can correctly
    classify TASK_UNREACHABLE as a removable but non-terminal task state.

    Review: https://reviews.apache.org/r/55476/

commit d4acb10d74638b24617a5007425161fad4a6d47d
Author: Neil Conway <neil.conway@gmail.com>
Date:   Mon Jan 23 17:03:42 2017 -0800

    Added `Master::isRemovable(const TaskState&)`.

    This determines whether a task in the given state can safely be
    discarded from the master's in-memory state. When a task becomes
    removable, we move the task from the master's main task data structures
    to a fixed-size cache (either the ""unreachable"" or ""completed"" task
    list, as appropriate).

    Review: https://reviews.apache.org/r/55475/

commit 484db11a4e75696fa8e047508975f8061e3c0bb4
Author: Neil Conway <neil.conway@gmail.com>
Date:   Mon Jan 23 17:03:35 2017 -0800

    Renamed `taskTerminated` for Slave/Framework to `recoverResources`.

    The old name was misleading: these functions are invoked when a task
    becomes unreachable, which does not count as ""task termination"".

    Review: https://reviews.apache.org/r/55474/
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,
MesosContainerizer/DefaultExecutorTest.KillTask/0 failing on ASF CI,MESOS-6569,13019718,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,xujyan,xujyan,10/Nov/16 01:43,26/Mar/18 08:10,29/Oct/20 16:32,05/Dec/16 18:25,1.1.0,,,,,,,,1.2.0,,,,,,,,,,,0,flaky,mesosphere,newbie,,,,,,"{noformat:title=}
[ RUN      ] MesosContainerizer/DefaultExecutorTest.KillTask/0
I1110 01:20:11.482097 29700 cluster.cpp:158] Creating default 'local' authorizer
I1110 01:20:11.485241 29700 leveldb.cpp:174] Opened db in 2.774513ms
I1110 01:20:11.486237 29700 leveldb.cpp:181] Compacted db in 953614ns
I1110 01:20:11.486299 29700 leveldb.cpp:196] Created db iterator in 24739ns
I1110 01:20:11.486325 29700 leveldb.cpp:202] Seeked to beginning of db in 2300ns
I1110 01:20:11.486344 29700 leveldb.cpp:271] Iterated through 0 keys in the db in 378ns
I1110 01:20:11.486399 29700 replica.cpp:776] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1110 01:20:11.486933 29733 recover.cpp:451] Starting replica recovery
I1110 01:20:11.487289 29733 recover.cpp:477] Replica is in EMPTY status
I1110 01:20:11.488503 29721 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from __req_res__(7318)@172.17.0.3:52462
I1110 01:20:11.488855 29727 recover.cpp:197] Received a recover response from a replica in EMPTY status
I1110 01:20:11.489398 29729 recover.cpp:568] Updating replica status to STARTING
I1110 01:20:11.490223 29723 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 575135ns
I1110 01:20:11.490284 29732 master.cpp:380] Master d28fbae1-c3dc-45fa-8384-32ab9395a975 (3a31be8bf679) started on 172.17.0.3:52462
I1110 01:20:11.490317 29732 master.cpp:382] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/k50x7x/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-1.2.0/_inst/share/mesos/webui"" --work_dir=""/tmp/k50x7x/master"" --zk_session_timeout=""10secs""
I1110 01:20:11.490696 29732 master.cpp:432] Master only allowing authenticated frameworks to register
I1110 01:20:11.490712 29732 master.cpp:446] Master only allowing authenticated agents to register
I1110 01:20:11.490720 29732 master.cpp:459] Master only allowing authenticated HTTP frameworks to register
I1110 01:20:11.490730 29732 credentials.hpp:37] Loading credentials for authentication from '/tmp/k50x7x/credentials'
I1110 01:20:11.490281 29723 replica.cpp:320] Persisted replica status to STARTING
I1110 01:20:11.491210 29732 master.cpp:504] Using default 'crammd5' authenticator
I1110 01:20:11.491225 29720 recover.cpp:477] Replica is in STARTING status
I1110 01:20:11.491394 29732 http.cpp:895] Using default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
I1110 01:20:11.491621 29732 http.cpp:895] Using default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
I1110 01:20:11.491770 29732 http.cpp:895] Using default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
I1110 01:20:11.491937 29732 master.cpp:584] Authorization enabled
I1110 01:20:11.492276 29725 whitelist_watcher.cpp:77] No whitelist given
I1110 01:20:11.492310 29723 hierarchical.cpp:149] Initialized hierarchical allocator process
I1110 01:20:11.492569 29721 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from __req_res__(7319)@172.17.0.3:52462
I1110 01:20:11.492830 29719 recover.cpp:197] Received a recover response from a replica in STARTING status
I1110 01:20:11.493371 29720 recover.cpp:568] Updating replica status to VOTING
I1110 01:20:11.494002 29721 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 367673ns
I1110 01:20:11.494032 29721 replica.cpp:320] Persisted replica status to VOTING
I1110 01:20:11.494218 29734 recover.cpp:582] Successfully joined the Paxos group
I1110 01:20:11.494469 29734 recover.cpp:466] Recover process terminated
I1110 01:20:11.495633 29733 master.cpp:2033] Elected as the leading master!
I1110 01:20:11.495685 29733 master.cpp:1560] Recovering from registrar
I1110 01:20:11.495880 29720 registrar.cpp:329] Recovering registrar
I1110 01:20:11.496842 29730 log.cpp:553] Attempting to start the writer
I1110 01:20:11.498610 29725 replica.cpp:493] Replica received implicit promise request from __req_res__(7320)@172.17.0.3:52462 with proposal 1
I1110 01:20:11.499179 29725 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 524192ns
I1110 01:20:11.499213 29725 replica.cpp:342] Persisted promised to 1
I1110 01:20:11.500258 29726 coordinator.cpp:238] Coordinator attempting to fill missing positions
I1110 01:20:11.501874 29731 replica.cpp:388] Replica received explicit promise request from __req_res__(7321)@172.17.0.3:52462 for position 0 with proposal 2
I1110 01:20:11.502413 29731 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 484138ns
I1110 01:20:11.502457 29731 replica.cpp:708] Persisted action NOP at position 0
I1110 01:20:11.503885 29720 replica.cpp:537] Replica received write request for position 0 from __req_res__(7322)@172.17.0.3:52462
I1110 01:20:11.503985 29720 leveldb.cpp:436] Reading position from leveldb took 56800ns
I1110 01:20:11.504534 29720 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 467426ns
I1110 01:20:11.504566 29720 replica.cpp:708] Persisted action NOP at position 0
I1110 01:20:11.505470 29721 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I1110 01:20:11.505988 29721 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 479078ns
I1110 01:20:11.506021 29721 replica.cpp:708] Persisted action NOP at position 0
I1110 01:20:11.506706 29732 log.cpp:569] Writer started with ending position 0
I1110 01:20:11.508041 29734 leveldb.cpp:436] Reading position from leveldb took 50010ns
I1110 01:20:11.509210 29733 registrar.cpp:362] Successfully fetched the registry (0B) in 13.068032ms
I1110 01:20:11.509356 29733 registrar.cpp:461] Applied 1 operations in 27124ns; attempting to update the registry
I1110 01:20:11.510251 29732 log.cpp:577] Attempting to append 168 bytes to the log
I1110 01:20:11.510457 29724 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I1110 01:20:11.511355 29728 replica.cpp:537] Replica received write request for position 1 from __req_res__(7323)@172.17.0.3:52462
I1110 01:20:11.511828 29728 leveldb.cpp:341] Persisting action (187 bytes) to leveldb took 423890ns
I1110 01:20:11.511859 29728 replica.cpp:708] Persisted action APPEND at position 1
I1110 01:20:11.512572 29734 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I1110 01:20:11.513051 29734 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 368122ns
I1110 01:20:11.513087 29734 replica.cpp:708] Persisted action APPEND at position 1
I1110 01:20:11.514302 29726 registrar.cpp:506] Successfully updated the registry in 4.862976ms
I1110 01:20:11.514503 29726 registrar.cpp:392] Successfully recovered registrar
I1110 01:20:11.514593 29728 log.cpp:596] Attempting to truncate the log to 1
I1110 01:20:11.514760 29730 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I1110 01:20:11.515249 29723 master.cpp:1676] Recovered 0 agents from the registry (129B); allowing 10mins for agents to re-register
I1110 01:20:11.515534 29722 hierarchical.cpp:176] Skipping recovery of hierarchical allocator: nothing to recover
I1110 01:20:11.516068 29722 replica.cpp:537] Replica received write request for position 2 from __req_res__(7324)@172.17.0.3:52462
I1110 01:20:11.516619 29722 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 497823ns
I1110 01:20:11.516652 29722 replica.cpp:708] Persisted action TRUNCATE at position 2
I1110 01:20:11.517526 29734 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I1110 01:20:11.518040 29734 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 384129ns
I1110 01:20:11.518111 29734 leveldb.cpp:399] Deleting ~1 keys from leveldb took 39398ns
I1110 01:20:11.518138 29734 replica.cpp:708] Persisted action TRUNCATE at position 2
I1110 01:20:11.525027 29700 containerizer.cpp:201] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni
W1110 01:20:11.525806 29700 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges, but is running as user mesos
W1110 01:20:11.526018 29700 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges
I1110 01:20:11.527331 29700 cluster.cpp:435] Creating default 'local' authorizer
I1110 01:20:11.528741 29725 slave.cpp:208] Mesos agent started on (571)@172.17.0.3:52462
I1110 01:20:11.528789 29725 slave.cpp:209] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-1.2.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD""
I1110 01:20:11.529228 29725 credentials.hpp:86] Loading credential for authentication from '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/credential'
I1110 01:20:11.529305 29700 scheduler.cpp:176] Version: 1.2.0
I1110 01:20:11.529436 29725 slave.cpp:346] Agent using credential for: test-principal
I1110 01:20:11.529464 29725 credentials.hpp:37] Loading credentials for authentication from '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/http_credentials'
I1110 01:20:11.529747 29725 http.cpp:895] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I1110 01:20:11.529855 29729 scheduler.cpp:469] New master detected at master@172.17.0.3:52462
I1110 01:20:11.529884 29729 scheduler.cpp:478] Waiting for 0ns before initiating a re-(connection) attempt with the master
I1110 01:20:11.531039 29725 slave.cpp:533] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1110 01:20:11.531113 29725 slave.cpp:541] Agent attributes: [  ]
I1110 01:20:11.531126 29725 slave.cpp:546] Agent hostname: 3a31be8bf679
I1110 01:20:11.532897 29723 state.cpp:57] Recovering state from '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/meta'
I1110 01:20:11.533222 29727 status_update_manager.cpp:203] Recovering status update manager
I1110 01:20:11.533269 29721 scheduler.cpp:353] Connected with the master at http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.533627 29734 containerizer.cpp:557] Recovering containerizer
I1110 01:20:11.534519 29725 scheduler.cpp:235] Sending SUBSCRIBE call to http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.535482 29732 provisioner.cpp:253] Provisioner recovery complete
I1110 01:20:11.535652 29734 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1110 01:20:11.535815 29724 slave.cpp:5411] Finished recovery
I1110 01:20:11.536440 29724 slave.cpp:5585] Querying resource estimator for oversubscribable resources
I1110 01:20:11.536898 29721 slave.cpp:915] New master detected at master@172.17.0.3:52462
I1110 01:20:11.536906 29731 status_update_manager.cpp:177] Pausing sending status updates
I1110 01:20:11.536941 29721 slave.cpp:974] Authenticating with master master@172.17.0.3:52462
I1110 01:20:11.537076 29721 slave.cpp:985] Using default CRAM-MD5 authenticatee
I1110 01:20:11.537214 29733 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54635
I1110 01:20:11.537353 29719 authenticatee.cpp:121] Creating new client SASL connection
I1110 01:20:11.537256 29721 slave.cpp:947] Detecting new master
I1110 01:20:11.537591 29733 master.cpp:2329] Received subscription request for HTTP framework 'default'
I1110 01:20:11.537611 29721 slave.cpp:5599] Received oversubscribable resources {} from the resource estimator
I1110 01:20:11.537701 29733 master.cpp:2069] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1110 01:20:11.538077 29733 master.cpp:6745] Authenticating slave(571)@172.17.0.3:52462
I1110 01:20:11.538208 29732 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1121)@172.17.0.3:52462
I1110 01:20:11.538291 29733 master.cpp:2427] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I1110 01:20:11.538508 29731 authenticator.cpp:98] Creating new server SASL connection
I1110 01:20:11.538782 29720 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I1110 01:20:11.538823 29720 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I1110 01:20:11.539227 29730 hierarchical.cpp:275] Added framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.539317 29722 master.hpp:2161] Sending heartbeat to d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.539331 29730 hierarchical.cpp:1694] No allocations performed
I1110 01:20:11.539696 29730 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:11.539818 29730 hierarchical.cpp:1286] Performed allocation for 0 agents in 554795ns
I1110 01:20:11.540354 29720 authenticator.cpp:204] Received SASL authentication start
I1110 01:20:11.540361 29719 scheduler.cpp:675] Enqueuing event SUBSCRIBED received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.540438 29720 authenticator.cpp:326] Authentication requires more steps
I1110 01:20:11.540750 29720 authenticatee.cpp:259] Received SASL authentication step
I1110 01:20:11.541038 29721 authenticator.cpp:232] Received SASL authentication step
I1110 01:20:11.541081 29721 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '3a31be8bf679' server FQDN: '3a31be8bf679' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1110 01:20:11.541112 29721 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I1110 01:20:11.541147 29719 scheduler.cpp:675] Enqueuing event HEARTBEAT received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.541178 29721 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1110 01:20:11.541260 29721 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '3a31be8bf679' server FQDN: '3a31be8bf679' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1110 01:20:11.541285 29721 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1110 01:20:11.541307 29721 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1110 01:20:11.541342 29721 authenticator.cpp:318] Authentication success
I1110 01:20:11.541517 29733 authenticatee.cpp:299] Authentication success
I1110 01:20:11.541586 29720 master.cpp:6775] Successfully authenticated principal 'test-principal' at slave(571)@172.17.0.3:52462
I1110 01:20:11.541826 29721 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1121)@172.17.0.3:52462
I1110 01:20:11.542129 29730 slave.cpp:1069] Successfully authenticated with master master@172.17.0.3:52462
I1110 01:20:11.542362 29730 slave.cpp:1483] Will retry registration in 9.532818ms if necessary
I1110 01:20:11.542577 29733 master.cpp:5154] Registering agent at slave(571)@172.17.0.3:52462 (3a31be8bf679) with id d28fbae1-c3dc-45fa-8384-32ab9395a975-S0
I1110 01:20:11.543083 29731 registrar.cpp:461] Applied 1 operations in 60476ns; attempting to update the registry
I1110 01:20:11.543926 29729 log.cpp:577] Attempting to append 337 bytes to the log
I1110 01:20:11.544077 29723 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I1110 01:20:11.545238 29731 replica.cpp:537] Replica received write request for position 3 from __req_res__(7325)@172.17.0.3:52462
I1110 01:20:11.546116 29731 leveldb.cpp:341] Persisting action (356 bytes) to leveldb took 825474ns
I1110 01:20:11.546169 29731 replica.cpp:708] Persisted action APPEND at position 3
I1110 01:20:11.547427 29725 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I1110 01:20:11.547969 29725 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 483290ns
I1110 01:20:11.548005 29725 replica.cpp:708] Persisted action APPEND at position 3
I1110 01:20:11.550129 29732 registrar.cpp:506] Successfully updated the registry in 6.962944ms
I1110 01:20:11.550396 29726 log.cpp:596] Attempting to truncate the log to 3
I1110 01:20:11.550614 29720 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I1110 01:20:11.551375 29723 slave.cpp:4263] Received ping from slave-observer(531)@172.17.0.3:52462
I1110 01:20:11.551326 29734 master.cpp:5225] Registered agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1110 01:20:11.551720 29730 replica.cpp:537] Replica received write request for position 4 from __req_res__(7326)@172.17.0.3:52462
I1110 01:20:11.551892 29723 slave.cpp:1115] Registered with master master@172.17.0.3:52462; given agent ID d28fbae1-c3dc-45fa-8384-32ab9395a975-S0
I1110 01:20:11.551975 29723 fetcher.cpp:86] Clearing fetcher cache
I1110 01:20:11.552170 29732 hierarchical.cpp:485] Added agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 (3a31be8bf679) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: {})
I1110 01:20:11.552338 29721 status_update_manager.cpp:184] Resuming sending status updates
I1110 01:20:11.552486 29730 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 709727ns
I1110 01:20:11.552655 29723 slave.cpp:1138] Checkpointing SlaveInfo to '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/meta/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/slave.info'
I1110 01:20:11.552609 29730 replica.cpp:708] Persisted action TRUNCATE at position 4
I1110 01:20:11.553383 29731 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I1110 01:20:11.553409 29723 slave.cpp:1175] Forwarding total oversubscribed resources {}
I1110 01:20:11.553653 29732 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:11.553671 29727 master.cpp:5624] Received update of agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) with total oversubscribed resources {}
I1110 01:20:11.553755 29732 hierarchical.cpp:1309] Performed allocation for agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 in 1.528714ms
I1110 01:20:11.553975 29731 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 525057ns
I1110 01:20:11.554072 29731 leveldb.cpp:399] Deleting ~2 keys from leveldb took 59750ns
I1110 01:20:11.554065 29732 hierarchical.cpp:555] Agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 (3a31be8bf679) updated with oversubscribed resources {} (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I1110 01:20:11.554105 29731 replica.cpp:708] Persisted action TRUNCATE at position 4
I1110 01:20:11.554260 29732 hierarchical.cpp:1694] No allocations performed
I1110 01:20:11.554314 29732 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:11.554345 29727 master.cpp:6574] Sending 1 offers to framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:11.554379 29732 hierarchical.cpp:1309] Performed allocation for agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 in 239597ns
I1110 01:20:11.556370 29724 scheduler.cpp:675] Enqueuing event OFFERS received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.559177 29730 scheduler.cpp:235] Sending ACCEPT call to http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.560282 29734 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1110 01:20:11.561323 29733 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54634
I1110 01:20:11.562417 29733 master.cpp:3581] Processing ACCEPT call for offers: [ d28fbae1-c3dc-45fa-8384-32ab9395a975-O0 ] on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:11.562584 29733 master.cpp:3173] Authorizing framework principal 'test-principal' to launch task c96eb523-0365-49b2-8b3b-78976ff28797
I1110 01:20:11.563097 29733 master.cpp:3173] Authorizing framework principal 'test-principal' to launch task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba
I1110 01:20:11.567248 29733 master.cpp:8337] Adding task c96eb523-0365-49b2-8b3b-78976ff28797 with resources cpus(*):0.1; mem(*):32; disk(*):32 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 (3a31be8bf679)
I1110 01:20:11.567651 29733 master.cpp:8337] Adding task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba with resources cpus(*):0.1; mem(*):32; disk(*):32 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 (3a31be8bf679)
I1110 01:20:11.567845 29733 master.cpp:4438] Launching task group { 08848440-4c0e-4ad6-a0a9-b5947c5d21ba, c96eb523-0365-49b2-8b3b-78976ff28797 } of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) with resources cpus(*):0.2; mem(*):64; disk(*):64 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:11.568495 29724 slave.cpp:1547] Got assigned task group containing tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.569128 29729 hierarchical.cpp:1018] Recovered cpus(*):1.7; mem(*):928; disk(*):928; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):0.3; mem(*):96; disk(*):96) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.569211 29729 hierarchical.cpp:1055] Framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 filtered agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 for 5secs
I1110 01:20:11.570461 29724 slave.cpp:1709] Launching task group containing tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.571297 29724 paths.cpp:530] Trying to chown '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06' to user 'mesos'
I1110 01:20:11.580168 29724 slave.cpp:6319] Launching executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 with resources cpus(*):0.1; mem(*):32; disk(*):32 in work directory '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06'
I1110 01:20:11.580930 29734 containerizer.cpp:940] Starting container a283035b-25d3-4b48-b59a-964e5a4dfa06 for executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.581110 29724 slave.cpp:2031] Queued task group containing tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] for executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.581214 29724 slave.cpp:868] Successfully attached file '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06'
I1110 01:20:11.585572 29722 containerizer.cpp:1480] Launching 'mesos-containerizer' with flags '--command=""{""arguments"":[""mesos-default-executor"",""--launcher_dir=\/mesos\/mesos-1.2.0\/_build\/src""],""shell"":false,""value"":""\/mesos\/mesos-1.2.0\/_build\/src\/mesos-default-executor""}"" --help=""false"" --pipe_read=""60"" --pipe_write=""61"" --pre_exec_commands=""[]"" --runtime_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06"" --unshare_namespace_mnt=""false"" --user=""mesos"" --working_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06""'
I1110 01:20:11.588587 29722 launcher.cpp:127] Forked child with pid '10191' for container 'a283035b-25d3-4b48-b59a-964e5a4dfa06'
I1110 01:20:11.592996 29734 fetcher.cpp:345] Starting to fetch URIs for container: a283035b-25d3-4b48-b59a-964e5a4dfa06, directory: /tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06
I1110 01:20:11.777189 10229 executor.cpp:189] Version: 1.2.0
I1110 01:20:11.786099 29719 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor'
I1110 01:20:11.787382 29722 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54638
I1110 01:20:11.787693 29722 slave.cpp:3086] Received Subscribe request for HTTP executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.790436 29730 slave.cpp:2276] Sending queued task group task group containing tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ] to executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (via HTTP)
I1110 01:20:11.795663 10221 default_executor.cpp:130] Received SUBSCRIBED event
I1110 01:20:11.797111 10221 default_executor.cpp:134] Subscribed executor on 3a31be8bf679
I1110 01:20:11.797611 10221 default_executor.cpp:130] Received LAUNCH_GROUP event
I1110 01:20:11.801981 29729 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.802435 29729 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.803306 29723 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54640
I1110 01:20:11.803452 29723 http.cpp:353] Processing call LAUNCH_NESTED_CONTAINER
I1110 01:20:11.803827 29727 containerizer.cpp:1685] Starting nested container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d
I1110 01:20:11.803865 29723 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54640
I1110 01:20:11.803978 29723 http.cpp:353] Processing call LAUNCH_NESTED_CONTAINER
I1110 01:20:11.804236 29727 containerizer.cpp:1709] Trying to chown '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d' to user 'mesos'
I1110 01:20:11.814858 29727 containerizer.cpp:1685] Starting nested container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611
I1110 01:20:11.815129 29727 containerizer.cpp:1709] Trying to chown '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611' to user 'mesos'
I1110 01:20:11.824666 29727 containerizer.cpp:1480] Launching 'mesos-containerizer' with flags '--command=""{""shell"":true,""value"":""sleep 1000""}"" --help=""false"" --pipe_read=""63"" --pipe_write=""64"" --pre_exec_commands=""[]"" --runtime_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d"" --unshare_namespace_mnt=""false"" --user=""mesos"" --working_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d""'
I1110 01:20:11.826855 29727 launcher.cpp:127] Forked child with pid '10240' for container 'a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d'
I1110 01:20:11.828918 29727 containerizer.cpp:1480] Launching 'mesos-containerizer' with flags '--command=""{""shell"":true,""value"":""sleep 1000""}"" --help=""false"" --pipe_read=""65"" --pipe_write=""66"" --pre_exec_commands=""[]"" --runtime_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611"" --unshare_namespace_mnt=""false"" --user=""mesos"" --working_directory=""/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611""'
I1110 01:20:11.831428 29727 launcher.cpp:127] Forked child with pid '10241' for container 'a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611'
I1110 01:20:11.834421 29731 fetcher.cpp:345] Starting to fetch URIs for container: a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d, directory: /tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d
I1110 01:20:11.837882 29731 fetcher.cpp:345] Starting to fetch URIs for container: a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611, directory: /tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611
I1110 01:20:11.847651 10227 default_executor.cpp:470] Successfully launched child containers [ a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d, a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 ] for tasks [ c96eb523-0365-49b2-8b3b-78976ff28797, 08848440-4c0e-4ad6-a0a9-b5947c5d21ba ]
I1110 01:20:11.849225 29728 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor'
I1110 01:20:11.850085 10226 default_executor.cpp:546] Waiting for child container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d of task 'c96eb523-0365-49b2-8b3b-78976ff28797'
I1110 01:20:11.850145 29734 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor'
I1110 01:20:11.850405 10226 default_executor.cpp:546] Waiting for child container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 of task '08848440-4c0e-4ad6-a0a9-b5947c5d21ba'
I1110 01:20:11.850746 29726 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54639
I1110 01:20:11.851114 29726 slave.cpp:3740] Handling status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.851552 29726 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54639
I1110 01:20:11.851727 29726 slave.cpp:3740] Handling status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.852295 29726 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.852826 29726 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.853938 29724 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.854076 29724 status_update_manager.cpp:500] Creating StatusUpdate stream for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.854460 29726 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54641
I1110 01:20:11.854559 29726 http.cpp:353] Processing call WAIT_NESTED_CONTAINER
I1110 01:20:11.854610 29724 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to the agent
I1110 01:20:11.855126 29724 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.855190 29724 status_update_manager.cpp:500] Creating StatusUpdate stream for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.855200 29726 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54642
I1110 01:20:11.855409 29726 http.cpp:353] Processing call WAIT_NESTED_CONTAINER
I1110 01:20:11.855608 29724 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to the agent
I1110 01:20:11.855803 29726 slave.cpp:4181] Forwarding the update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to master@172.17.0.3:52462
I1110 01:20:11.856199 29726 slave.cpp:4075] Status update manager successfully handled status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.856346 29725 master.cpp:5760] Status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 from agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:11.856439 29725 master.cpp:5822] Forwarding status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.856598 29726 slave.cpp:4181] Forwarding the update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to master@172.17.0.3:52462
I1110 01:20:11.856828 29726 slave.cpp:4075] Status update manager successfully handled status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.856998 29725 master.cpp:7715] Updating the state of task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1110 01:20:11.857322 29725 master.cpp:5760] Status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 from agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:11.857386 29725 master.cpp:5822] Forwarding status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.857787 29725 master.cpp:7715] Updating the state of task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I1110 01:20:11.858124 10226 default_executor.cpp:130] Received ACKNOWLEDGED event
I1110 01:20:11.858530 29725 scheduler.cpp:675] Enqueuing event UPDATE received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.859519 29732 scheduler.cpp:675] Enqueuing event UPDATE received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.859676 10233 default_executor.cpp:130] Received ACKNOWLEDGED event
../../src/tests/default_executor_tests.cpp:338: Failure
Value of: runningUpdate1->status().task_id()
  Actual: 08848440-4c0e-4ad6-a0a9-b5947c5d21ba
Expected: taskInfo1.task_id()
Which is: c96eb523-0365-49b2-8b3b-78976ff28797
../../src/tests/default_executor_tests.cpp:342: Failure
Value of: runningUpdate2->status().task_id()
  Actual: c96eb523-0365-49b2-8b3b-78976ff28797
Expected: taskInfo2.task_id()
Which is: 08848440-4c0e-4ad6-a0a9-b5947c5d21ba
I1110 01:20:11.861587 29733 scheduler.cpp:235] Sending ACKNOWLEDGE call to http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.861948 29733 scheduler.cpp:235] Sending ACKNOWLEDGE call to http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.862280 29733 scheduler.cpp:235] Sending KILL call to http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:11.862632 29721 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1110 01:20:11.863528 29719 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54634
I1110 01:20:11.863664 29719 master.cpp:4870] Processing ACKNOWLEDGE call 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87 for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0
I1110 01:20:11.864003 29732 status_update_manager.cpp:395] Received status update acknowledgement (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
W1110 01:20:11.864294 29732 status_update_manager.cpp:769] Unexpected status update acknowledgement (received 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87, expecting d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
E1110 01:20:11.864575 29726 slave.cpp:3015] Failed to handle status update acknowledgement (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000: Duplicate acknowledgement
I1110 01:20:11.864804 29723 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1110 01:20:11.865231 29723 process.cpp:3570] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I1110 01:20:11.866297 29722 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54634
I1110 01:20:11.866420 29722 master.cpp:4870] Processing ACKNOWLEDGE call d91c7deb-4646-4b4e-ba1a-5650a256e8d2 for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0
I1110 01:20:11.867036 29726 status_update_manager.cpp:395] Received status update acknowledgement (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.867076 29722 http.cpp:391] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:54634
I1110 01:20:11.867291 29722 master.cpp:4762] Telling agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) to kill task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
W1110 01:20:11.867297 29726 status_update_manager.cpp:769] Unexpected status update acknowledgement (received d91c7deb-4646-4b4e-ba1a-5650a256e8d2, expecting 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.867449 29733 slave.cpp:2344] Asked to kill task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
E1110 01:20:11.867710 29733 slave.cpp:3015] Failed to handle status update acknowledgement (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000: Duplicate acknowledgement
I1110 01:20:11.869391 10228 default_executor.cpp:130] Received KILL event
I1110 01:20:11.869498 10228 default_executor.cpp:810] Received kill for task 'c96eb523-0365-49b2-8b3b-78976ff28797'
I1110 01:20:11.869544 10228 default_executor.cpp:694] Shutting down
I1110 01:20:11.870112 10221 default_executor.cpp:782] Killing child container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d
I1110 01:20:11.870338 10221 default_executor.cpp:782] Killing child container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611
I1110 01:20:11.870965 29729 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.871399 29730 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1'
I1110 01:20:11.871984 29723 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54643
I1110 01:20:11.872088 29723 http.cpp:353] Processing call KILL_NESTED_CONTAINER
I1110 01:20:11.872284 29726 containerizer.cpp:1973] Destroying container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d in RUNNING state
I1110 01:20:11.872340 29723 http.cpp:277] HTTP POST for /slave(571)/api/v1 from 172.17.0.3:54643
I1110 01:20:11.872416 29723 http.cpp:353] Processing call KILL_NESTED_CONTAINER
I1110 01:20:11.872597 29726 launcher.cpp:143] Asked to destroy container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d
I1110 01:20:11.877090 29726 containerizer.cpp:1973] Destroying container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 in RUNNING state
I1110 01:20:11.877320 29726 launcher.cpp:143] Asked to destroy container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611
I1110 01:20:11.962539 29729 containerizer.cpp:2336] Container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d has exited
I1110 01:20:11.963811 29733 provisioner.cpp:324] Ignoring destroy request for unknown container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d
I1110 01:20:11.963851 29729 containerizer.cpp:2336] Container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 has exited
I1110 01:20:11.964437 29729 containerizer.cpp:2252] Checkpointing termination state to nested container's runtime directory '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/afdfa733-0ffd-4290-a69f-700b7d13cf6d/termination'
I1110 01:20:11.965940 29728 provisioner.cpp:324] Ignoring destroy request for unknown container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611
I1110 01:20:11.966202 29732 containerizer.cpp:2252] Checkpointing termination state to nested container's runtime directory '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_wVf7JJ/containers/a283035b-25d3-4b48-b59a-964e5a4dfa06/containers/969c8095-ccfc-46d1-802d-4c55f0b66611/termination'
I1110 01:20:11.970046 10231 default_executor.cpp:663] Successfully waited for child container a283035b-25d3-4b48-b59a-964e5a4dfa06.afdfa733-0ffd-4290-a69f-700b7d13cf6d of task 'c96eb523-0365-49b2-8b3b-78976ff28797' in state TASK_KILLED
I1110 01:20:11.970501 10231 default_executor.cpp:663] Successfully waited for child container a283035b-25d3-4b48-b59a-964e5a4dfa06.969c8095-ccfc-46d1-802d-4c55f0b66611 of task '08848440-4c0e-4ad6-a0a9-b5947c5d21ba' in state TASK_KILLED
I1110 01:20:11.970559 10231 default_executor.cpp:768] Terminating after 1secs
I1110 01:20:11.971288 29723 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor'
I1110 01:20:11.972218 29728 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54639
I1110 01:20:11.972488 29728 slave.cpp:3740] Handling status update TASK_KILLED (UUID: 48f76bc6-3855-40fc-b22b-e26b1048fc89) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.974179 29719 process.cpp:3570] Handling HTTP event for process 'slave(571)' with path: '/slave(571)/api/v1/executor'
I1110 01:20:11.975229 29726 status_update_manager.cpp:323] Received status update TASK_KILLED (UUID: 48f76bc6-3855-40fc-b22b-e26b1048fc89) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.975278 29729 http.cpp:277] HTTP POST for /slave(571)/api/v1/executor from 172.17.0.3:54639
I1110 01:20:11.975517 29729 slave.cpp:3740] Handling status update TASK_KILLED (UUID: a3be1364-0830-4dd3-8be5-2bbbafde1029) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.976048 29729 slave.cpp:4075] Status update manager successfully handled status update TASK_KILLED (UUID: 48f76bc6-3855-40fc-b22b-e26b1048fc89) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.978132 29720 status_update_manager.cpp:323] Received status update TASK_KILLED (UUID: a3be1364-0830-4dd3-8be5-2bbbafde1029) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:11.978482 29725 slave.cpp:4075] Status update manager successfully handled status update TASK_KILLED (UUID: a3be1364-0830-4dd3-8be5-2bbbafde1029) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:12.494274 29725 hierarchical.cpp:1880] Filtered offer with cpus(*):1.7; mem(*):928; disk(*):928; ports(*):[31000-32000] on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:12.494357 29725 hierarchical.cpp:1694] No allocations performed
I1110 01:20:12.494402 29725 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:12.494491 29725 hierarchical.cpp:1286] Performed allocation for 1 agents in 915780ns
I1110 01:20:13.071280 29734 containerizer.cpp:2336] Container a283035b-25d3-4b48-b59a-964e5a4dfa06 has exited
I1110 01:20:13.071339 29734 containerizer.cpp:1973] Destroying container a283035b-25d3-4b48-b59a-964e5a4dfa06 in RUNNING state
I1110 01:20:13.071746 29734 launcher.cpp:143] Asked to destroy container a283035b-25d3-4b48-b59a-964e5a4dfa06
I1110 01:20:13.076637 29723 provisioner.cpp:324] Ignoring destroy request for unknown container a283035b-25d3-4b48-b59a-964e5a4dfa06
I1110 01:20:13.077929 29721 slave.cpp:4672] Executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 exited with status 0
I1110 01:20:13.078433 29732 master.cpp:5884] Executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679): exited with status 0
I1110 01:20:13.078538 29732 master.cpp:7840] Removing executor 'default' with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:13.079448 29730 hierarchical.cpp:1018] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):0.2; mem(*):64; disk(*):64) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:13.081049 29723 scheduler.cpp:675] Enqueuing event FAILURE received from http://172.17.0.3:52462/master/api/v1/scheduler

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: failure(0x7fff1aa9a950, @0x2ab91c02dd10 48-byte object <90-62 27-EC B8-2A 00-00 00-00 00-00 00-00 00-00 07-00 00-00 00-00 00-00 10-CE 01-1C B9-2A 00-00 70-CE 02-1C B9-2A 00-00 00-00 00-00 B8-2A 00-00>)
Stack trace:
I1110 01:20:13.496551 29730 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:13.496680 29730 hierarchical.cpp:1286] Performed allocation for 1 agents in 1.498625ms
I1110 01:20:13.497339 29729 master.cpp:6574] Sending 1 offers to framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:13.499797 29721 scheduler.cpp:675] Enqueuing event OFFERS received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:14.497707 29732 hierarchical.cpp:1694] No allocations performed
I1110 01:20:14.497795 29732 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:14.497895 29732 hierarchical.cpp:1286] Performed allocation for 1 agents in 410313ns
I1110 01:20:15.499423 29728 hierarchical.cpp:1694] No allocations performed
I1110 01:20:15.499526 29728 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:15.499658 29728 hierarchical.cpp:1286] Performed allocation for 1 agents in 547651ns
I1110 01:20:16.500463 29729 hierarchical.cpp:1694] No allocations performed
I1110 01:20:16.500581 29729 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:16.500699 29729 hierarchical.cpp:1286] Performed allocation for 1 agents in 505442ns
I1110 01:20:17.502176 29727 hierarchical.cpp:1694] No allocations performed
I1110 01:20:17.502262 29727 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:17.502367 29727 hierarchical.cpp:1286] Performed allocation for 1 agents in 464526ns
I1110 01:20:18.503680 29723 hierarchical.cpp:1694] No allocations performed
I1110 01:20:18.503762 29723 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:18.503851 29723 hierarchical.cpp:1286] Performed allocation for 1 agents in 425163ns
I1110 01:20:19.505476 29723 hierarchical.cpp:1694] No allocations performed
I1110 01:20:19.505586 29723 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:19.505705 29723 hierarchical.cpp:1286] Performed allocation for 1 agents in 590762ns
I1110 01:20:20.507310 29724 hierarchical.cpp:1694] No allocations performed
I1110 01:20:20.507390 29724 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:20.507477 29724 hierarchical.cpp:1286] Performed allocation for 1 agents in 411721ns
I1110 01:20:21.508368 29729 hierarchical.cpp:1694] No allocations performed
I1110 01:20:21.508458 29729 hierarchical.cpp:1789] No inverse offers to send out!
I1110 01:20:21.508564 29729 hierarchical.cpp:1286] Performed allocation for 1 agents in 432440ns
W1110 01:20:21.855908 29728 status_update_manager.cpp:478] Resending status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.856066 29728 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to the agent
I1110 01:20:21.856652 29734 slave.cpp:4181] Forwarding the update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to master@172.17.0.3:52462
W1110 01:20:21.857002 29727 status_update_manager.cpp:478] Resending status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.857069 29727 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to the agent
I1110 01:20:21.857378 29733 master.cpp:5760] Status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 from agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.857475 29733 master.cpp:5822] Forwarding status update TASK_RUNNING (UUID: 3b6a6ffd-d27d-4558-820e-fb9c95fa9f87) for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.857662 29722 slave.cpp:4181] Forwarding the update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 to master@172.17.0.3:52462
I1110 01:20:21.858206 29733 master.cpp:7715] Updating the state of task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_KILLED, status update state: TASK_RUNNING)
I1110 01:20:21.858988 29733 master.cpp:5760] Status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 from agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.859259 29733 master.cpp:5822] Forwarding status update TASK_RUNNING (UUID: d91c7deb-4646-4b4e-ba1a-5650a256e8d2) for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.859647 29725 hierarchical.cpp:1018] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1.9; mem(*):992; disk(*):992; ports(*):[31000-32000]) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.859845 29725 scheduler.cpp:675] Enqueuing event UPDATE received from http://172.17.0.3:52462/master/api/v1/scheduler
I1110 01:20:21.859979 29733 master.cpp:7715] Updating the state of task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_KILLED, status update state: TASK_RUNNING)
I1110 01:20:21.860970 29729 hierarchical.cpp:1018] Recovered cpus(*):0.1; mem(*):32; disk(*):32 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1.8; mem(*):960; disk(*):960; ports(*):[31000-32000]) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.861687 29725 scheduler.cpp:675] Enqueuing event UPDATE received from http://172.17.0.3:52462/master/api/v1/scheduler
../../src/tests/default_executor_tests.cpp:400: Failure
Value of: killedUpdate1->status().state()
  Actual: TASK_RUNNING
Expected: TASK_KILLED
I1110 01:20:21.864666 29721 master.cpp:1297] Framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) disconnected
I1110 01:20:21.864765 29721 master.cpp:2918] Disconnecting framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:21.864820 29721 master.cpp:2942] Deactivating framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:21.865016 29732 hierarchical.cpp:386] Deactivated framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
W1110 01:20:21.865586 29721 master.hpp:2264] Master attempted to send message to disconnected framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
W1110 01:20:21.865691 29721 master.hpp:2270] Unable to send event to framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default): connection closed
I1110 01:20:21.865777 29721 master.cpp:1310] Giving framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default) 0ns to failover
I1110 01:20:21.866277 29728 hierarchical.cpp:1018] Recovered cpus(*):1.8; mem(*):960; disk(*):960; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: {}) on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 from framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.867295 29733 master.cpp:6426] Framework failover timeout, removing framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:21.867328 29733 master.cpp:7170] Removing framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (default)
I1110 01:20:21.867539 29733 master.cpp:7715] Updating the state of task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I1110 01:20:21.867559 29731 slave.cpp:2575] Asked to shut down framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 by master@172.17.0.3:52462
I1110 01:20:21.867617 29731 slave.cpp:2600] Shutting down framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.867585 29733 master.cpp:7811] Removing task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.867707 29731 slave.cpp:4776] Cleaning up executor 'default' of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (via HTTP)
I1110 01:20:21.867904 29733 master.cpp:7715] Updating the state of task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I1110 01:20:21.868042 29729 gc.cpp:55] Scheduling '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default/runs/a283035b-25d3-4b48-b59a-964e5a4dfa06' for gc 6.99998999732444days in the future
I1110 01:20:21.867939 29733 master.cpp:7811] Removing task c96eb523-0365-49b2-8b3b-78976ff28797 with resources cpus(*):0.1; mem(*):32; disk(*):32 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000 on agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.868232 29731 slave.cpp:4864] Cleaning up framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.868252 29729 gc.cpp:55] Scheduling '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000/executors/default' for gc 6.99998999732444days in the future
I1110 01:20:21.868422 29725 status_update_manager.cpp:285] Closing status update streams for framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.868484 29725 status_update_manager.cpp:531] Cleaning up status update stream for task c96eb523-0365-49b2-8b3b-78976ff28797 of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.868777 29721 gc.cpp:55] Scheduling '/tmp/MesosContainerizer_DefaultExecutorTest_KillTask_0_8sXOVD/slaves/d28fbae1-c3dc-45fa-8384-32ab9395a975-S0/frameworks/d28fbae1-c3dc-45fa-8384-32ab9395a975-0000' for gc 6.99998999732444days in the future
I1110 01:20:21.868926 29720 hierarchical.cpp:337] Removed framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.869235 29725 status_update_manager.cpp:531] Cleaning up status update stream for task 08848440-4c0e-4ad6-a0a9-b5947c5d21ba of framework d28fbae1-c3dc-45fa-8384-32ab9395a975-0000
I1110 01:20:21.870568 29730 slave.cpp:787] Agent terminating
I1110 01:20:21.870795 29732 master.cpp:1258] Agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679) disconnected
I1110 01:20:21.870825 29732 master.cpp:2977] Disconnecting agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.870930 29732 master.cpp:2996] Deactivating agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 at slave(571)@172.17.0.3:52462 (3a31be8bf679)
I1110 01:20:21.871158 29726 hierarchical.cpp:584] Agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0 deactivated
I1110 01:20:21.876986 29733 master.cpp:1097] Master terminating
I1110 01:20:21.877754 29724 hierarchical.cpp:517] Removed agent d28fbae1-c3dc-45fa-8384-32ab9395a975-S0
[  FAILED  ] MesosContainerizer/DefaultExecutorTest.KillTask/0, where GetParam() = ""mesos"" (10406 ms)
{noformat}","https://builds.apache.org/job/Mesos/BUILDTOOL=autotools,COMPILER=gcc,CONFIGURATION=--verbose,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu:14.04,label_exp=(docker%7C%7CHadoop)&&(!ubuntu-us1)&&(!ubuntu-6)&&(!ubuntu-eu2)/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-11-10 01:58:13.047,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 05 18:25:20 UTC 2016,,,,,,,"0|i365m7:",9223372036854775807,,,,,anandmazumdar,,,,,,Mesosphere Sprint 47,,,,,,,,,,,2.0,,,,,,,,,,,"10/Nov/16 01:46;xujyan;[~vinodkone] any insight?","10/Nov/16 01:58;anandmazumdar;Similar to the logic that {{TASK_KILLED}} update for the tasks can be received in any order, we need similar logic to ensure that {{TASK_RUNNING}} update for tasks can be received in any order by the scheduler.","28/Nov/16 18:33;bbannier;Reviews:
https://reviews.apache.org/r/54133/
https://reviews.apache.org/r/54134/","05/Dec/16 18:25;anandmazumdar;{noformat}
commit b18a57681511b22ffe8df531d5d0d623011c2b08
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Mon Dec 5 10:23:11 2016 -0800

    Removed test assumptions about arrival order of status updates.

    Just like TASK_KILLED updates, TASK_RUNNING updates can arrive in any
    order. Adjusted the code to not assume a particual order.

    Also, cleaned up the code to minimize the mutated state.

    Review: https://reviews.apache.org/r/54134/

commit 69f7951b0aaaaab4f05748dd37750fed5fa72c07
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Mon Dec 5 10:22:10 2016 -0800

    Fixed missing includes in default_executor_tests.cpp.

    Review: https://reviews.apache.org/r/54133/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,
Parallel test running does not respect GTEST_FILTER,MESOS-6516,13016610,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,neilc,neilc,31/Oct/16 14:09,26/Mar/18 08:13,29/Oct/20 16:32,07/Nov/16 12:22,,,,,,,,,1.2.0,,,,,,test,,,,,0,mesosphere,,,,,,,,"Normally, you can use {{GTEST_FILTER}} to control which tests will be run by {{make check}}. However, this doesn't currently work if Mesos is configured with {{--enable-parallel-test-execution}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-10-31 14:51:46.042,,,false,MESOS-4809,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 07 12:22:56 UTC 2016,,,,,,,"0|i33pnv:",9223372036854775807,,,,,tillt,,,,,,Mesosphere Sprint 46,,,,,,,,,,,2.0,,1.2.0,,,,,,,,,"31/Oct/16 14:51;bbannier;It looks like this is an issue only if an empty filter expression is used for `GTEST_FILTER`.

Review: https://reviews.apache.org/r/53304/","07/Nov/16 12:22;tillt;{noformat}
commit b64ab6c56ef8b5549a1162316c78bc7a499fa923
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Mon Nov 7 12:52:18 2016 +0100

    Fixed parallel test runner for empty GTEST_FILTER.

    We need to correctly distinguish between the environment variable
    GTEST_FILTER being unset, and containing an empty string. In the first
    case we want to run all tests, while in the latter none at all.

    Review: https://reviews.apache.org/r/53304/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Use 'geteuid()' for the root privileges check.,MESOS-6504,13016262,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,28/Oct/16 20:40,26/Apr/17 16:54,29/Oct/20 16:32,22/Jan/17 19:23,,,,,,,,,1.2.0,,,,,,containerization,,,,,0,backend,isolator,mesosphere,user,,,,,"Currently, parts of code in Mesos check the root privileges using os::user() to compare to ""root"", which is not sufficient, since it compares the real user. When people change the mesos binary by 'setuid root', the process may not have the right permission to execute.

We should check the effective user id instead in our code. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-07 06:47:04.555,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 22 19:23:01 UTC 2017,,,,,,,"0|hzzz3w:x",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 47,Mesosphere Sprint 48,Mesosphere Sprint 50,,,,,,,,,3.0,,1.2.0,,,,,,,,,"30/Nov/16 18:58;gilbert;https://reviews.apache.org/r/54216/","07/Jan/17 06:47;avinash.mesos;[~gilbert] [~jieyu] will be able to finish this in the coming sprint? If not we should move it out of the sprint.","22/Jan/17 19:23;jieyu;commit 9228ebc239dac42825390bebc72053dbf3ae7b09
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Sun Jan 22 09:00:00 2017 -0800

    Fixed unsufficient root privileges check by geteuid().

    We should check the root privileges by using the effective user id,
    instead of comparing os::user() to 'root'.

    Review: https://reviews.apache.org/r/54216/",,,,,,,,,,,,,,,,,,,,,,,,,
PosixRLimitsIsolatorTest.TaskExceedingLimit fails on OS X,MESOS-6459,13014707,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,gkleiman,gkleiman,24/Oct/16 12:47,26/Mar/18 08:13,29/Oct/20 16:32,31/Oct/16 23:49,,,,,,,,,1.2.0,,,,,,,,,,,0,mesosphere,,,,,,,,"This test consistently fails on OS X:

{code}
31-7e9c-4248-acfd-21634150a657@172.28.128.1:64864 on agent 52cc4957-1a39-4d66-ace6-5622fac3b85e-S0
../../src/tests/containerizer/posix_rlimits_isolator_tests.cpp:120: Failure
Value of: statusFailed->state()
  Actual: TASK_FINISHED
Expected: TASK_FAILED
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-10-24 15:50:01.037,,,false,MESOS-6402,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 31 23:49:22 UTC 2016,,,,,,,"0|i35apr:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 46,,,,,,,,,,,2.0,,1.2.0,,,,,,,,,"24/Oct/16 13:15;gkleiman;I took a brief look at the Darwin kernel source code (https://github.com/opensource-apple/xnu/blob/27ffc00f33925b582391b1ef318b78b8bd3939d1/bsd/kern/kern_resource.c#L912-L1121), and it looks like {{RLIMIT_FSIZE}} is ignored, so I think that using another kind of rlimit should work.

cc/ [~bbannier]","24/Oct/16 15:50;jieyu;Nice triage [~gkleiman]! Let's use some other kind of rlimit.","31/Oct/16 14:01;bbannier;Review: https://reviews.apache.org/r/53301/","31/Oct/16 23:49;jieyu;commit b5683f234854e22f9696f6b8bb6aab6819f9d36f
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Mon Oct 31 16:42:17 2016 -0700

    Fixed rlimit test for execution under OS X.

    The previously use RLIMIT_NOFILE is ignored under OS X; use RLIMIT_CPU
    instead.

    Review: https://reviews.apache.org/r/53301/
",,,,,,,,,,,,,,,,,,,,,,,,
"Roles with quota assigned can ""game"" the system to receive excessive resources.",MESOS-6432,13014049,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,bbannier,bmahler,bmahler,20/Oct/16 21:01,26/Mar/18 08:13,29/Oct/20 16:32,06/Feb/17 01:45,,,,,,,,,1.2.0,,,,,,allocation,,,,,0,,,,,,,,,"The current implementation of quota allocation attempts to satisfy each resource quota for a role, but in doing so can far exceed the quota assigned to the role.

For example, if a role has quota for {{\[30,20,10\]}}, it can consume up to: {{\[∞, ∞, 10\]}} or {{\[∞, 20, ∞\]}} or {{\[30, ∞, ∞\]}} as only once each resource in the quota vector is satisfied do we stop allocating agent's resources to the role!

As a first step for preventing gaming, we could consider quota satisfied once any of the resources in the vector has quota satisfied. This approach works reasonably well for resources that are required and are present on every agent (cpus, mem, disk). However, it doesn't work well for resources that are optional / only present on some agents (e.g. gpus) (a.k.a. non-ubiquitous / scarce resources). For this we would need to determine which agents have resources that can satisfy the quota prior to performing the allocation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6375,MESOS-7030,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-18 10:10:38.011,,,false,MESOS-6514,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 06 01:45:41 UTC 2017,,,,,,,"0|hzzz2h:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 49,Mesosphere Sprint 50,Mesosphere Sprint 51,,,,,,,,,5.0,,1.2.0,,,,,,,,,"18/Jan/17 10:10;bbannier;Review: https://reviews.apache.org/r/55625/","18/Jan/17 10:48;bbannier;[~bmahler]: I have posted a patch implementing the fix you suggested in the original description (quota satisfied once a single resource kind is allocated up to the quota limit).

Before going forward with this patch we should carefully evaluate if this approach has the semantics we want, fixes the underlying issue, and is ultimately worth it while more general fixes like MESOS-3765 being considered. I believe we are replacing ""gamebility"" with a potentially steep penalty for ""unusual"" quota setups (e.g., a quota of {{cpus:100000;disk:1}} would with this change be satisfied once 1MB have been allocated towards the quota'ed role). OTOH, the gamebility we are fixing would be restricted to users with the ability to update or set quotas, i.e., operators should be able to curb gaming of quota by arbitrary users.","18/Jan/17 23:56;bmahler;Going to try to have this fixed in 1.2.","23/Jan/17 21:25;zhitao;Sorry I'm late to the party on this issue, but I have some pretty deep concern here about the patch: wouldn't this break the semantic that quota is a {{guarantee}} of resources allocated?

I strongly think we should move towards getting  MESOS-3765 started.

About [~bbannier]'s comment, I think even not ""unusual"" quota setups could be penalized with this patch: consider the case when any general type of resources is much more subscribed than another (i.e. when cpu is much more heavily subscribed than mem). An allocation could easily satisfy the mem part of a quota but unsatisfies cpu part, thus breaks the intended quota usage.","27/Jan/17 02:31;adam-mesos;Dropping this from Blocker to Critical, since it's not new behavior (existed since the introduction of Quota), and it doesn't have any reviews yet, so we won't block the 1.2 release for it.
[~bmahler], [~alexr] Do either of you have time to review this soon?","06/Feb/17 01:45;bmahler;{noformat}
commit 34de30cf69bac40eaa348036d1268bede762d170
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Sun Feb 5 15:45:10 2017 -0800

    Prevent gaming of the quota system due to demand:quota ratio mismatch.

    It is currently possible to game the quota system since the allocator
    will perform allocations until the quotas for all resource types are
    satisfied. This can lead to allocations for certain resource type
    far exceeding the set quota.

    In order to prevent this issue, this patch changes the notion of what
    constitutes a ""satisfied quota"". Where before a quota could only be
    satisfied when allocations had been met up to the set quota for every
    resource type, we now consider a quota satisfied as soon as at least
    one of the resource types is allocated up to the set quota. What this
    means is that if the allocation and quota vectors are of the same
    proportion, this new technique behaves the same as the previous
    technique. However, if the allocation and quota vectors have different
    resource type ratios, then the quota becomes satisfied as soon as
    the any quota guarantee is hit.

    Longer term, we can consider allocating the resource types that have
    not reached the quota guarantee, but this is non-trivial (consider
    the case of having to allocate only memory, this means we likely
    should only offer it in such a manner that it allows an existing
    container to increase its memory; it cannot be used to create a
    new container unless there is a mechanism to obtain best-effort
    cpu/disk).

    Also note that the quota guarantee is currently applied as a limit,
    whereas longer term we would like to (1) make these distinct, and
    (2) distinguish quota and fair sharing via non-revocable and
    revocable resources, respectively.

    Review: https://reviews.apache.org/r/55625/
{noformat}",,,,,,,,,,,,,,,,,,,,,,
"The python linter doesn't rebuild the virtual environment before linting when ""pip-requirements.txt"" has changed",MESOS-6430,13013995,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,klueska,klueska,klueska,20/Oct/16 18:10,29/Apr/19 09:26,29/Oct/20 16:32,16/Mar/17 02:59,,,,,,,,,,,,,,,build,,,,,0,,,,,,,,,"We need to detect if ""pip-requirements.txt"" changes and rebuild the virtual environment if it has.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-5676,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 16 02:59:58 UTC 2017,,,,,,,"0|i356bj:",9223372036854775807,,,,,kaysoky,,,,,,Mesosphere Sprint 45,,,,,,,,,,,2.0,,,,,,,,,,,"16/Mar/17 02:59;klueska;{noformat}
commit a97b658fddb2cf32ec60808ea56249ef73f98da2
Author: Kevin Klues <klueska@gmail.com>
Date:   Wed Mar 15 15:47:27 2017 -0700

    Updated pylint to rebuild 'virtualenv' when necessary.
    
    Review: https://reviews.apache.org/r/53074/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,
Report new PARTITION_AWARE task statuses in HTTP endpoints,MESOS-6388,13012132,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,neilc,neilc,neilc,13/Oct/16 20:13,24/Jan/17 01:34,29/Oct/20 16:32,24/Jan/17 01:34,,,,,,,,,1.2.0,,,,,,master,,,,,0,mesosphere,,,,,,,,"At a minimum, the {{/state-summary}} endpoint needs to be updated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-6394,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 24 01:34:34 UTC 2017,,,,,,,"0|hzzz3w:zx",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 48,Mesosphere Sprint 49,,,,,,,,,,1.0,,,,,,,,,,,"02/Dec/16 17:03;neilc;https://reviews.apache.org/r/54312/","24/Jan/17 01:34;neilc;{noformat}
commit b5711fdd12998a1b5bba206066b09a93ffc1a087
Author: Neil Conway <neil.conway@gmail.com>
Date:   Mon Jan 23 17:04:00 2017 -0800

    Added TASK_UNREACHABLE to master's state-summary endpoint.

    Review: https://reviews.apache.org/r/54312/
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,
'mesos-containerizer launch' should inherit agent environment variables.,MESOS-6323,13010333,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,jieyu,jieyu,jieyu,07/Oct/16 00:54,16/Sep/17 11:37,29/Oct/20 16:32,12/Oct/16 00:00,,,,,,,,,1.1.0,,,,,,,,,,,0,mesosphere,,,,,,,,"If some dynamic libraries that agent depends on are stored in a non standard location, and the operator starts the agent using LD_LIBRARY_PATH. When we actually fork/exec the 'mesos-containerizer launch' helper, we need to make sure it inherits agent's environment variables. Otherwise, it might throw linking errors. This makes sense because it's a Mesos controlled process.

However, the the helper actually fork/exec the user container (or executor), we need to make sure to strip the agent environment variables.

The tricky case is for default executor and command executor. These two are controlled by Mesos as well, we also want them to have agent environment variables. We need to somehow distinguish this from custom executor case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-7958,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-10-12 21:43:17.38,,,false,MESOS-6341,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 12 21:43:17 UTC 2016,,,,,,,"0|i34jv3:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 44,,,,,,,,,,,5.0,,1.1.0,,,,,,,,,"11/Oct/16 05:40;jieyu;https://reviews.apache.org/r/52729
https://reviews.apache.org/r/52730
https://reviews.apache.org/r/52731
https://reviews.apache.org/r/52732","12/Oct/16 00:00;jieyu;commit af2d406282d8da0ed93eacf997bf8f94aa77b492
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Mon Oct 10 22:26:12 2016 -0700

    Made mesos-containerizer launch helper inherit agent env variables.
    
    This patch addressed MESOS-6323. The idea is that we pass the
    environment variables that the command needs to the
    mesos-containerizer launch helper, and let the helper inherit agent
    environment variables.
    
    Review: https://reviews.apache.org/r/52732

commit 7d60be7c17387673045f9486cd2b3b23a1ebae6e
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Mon Oct 10 22:25:28 2016 -0700

    Added 'environment' flag to mesos-containerizer launch helper.
    
    The flag is optional. If set, the command to be launched will use the
    specified environment variables. If not set, the command will inherit
    the environment variables of the calling process.
    
    Review: https://reviews.apache.org/r/52731

commit c43c54aa97ebad486f04094ae79a59a4c95c10ca
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Mon Oct 10 22:23:11 2016 -0700

    Added an abstraction for Envp pointer expected by exec routines.
    
    Review: https://reviews.apache.org/r/52730

commit 90ad84b78f53c18cf0e7fa479c21eea4f5c587a8
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Mon Oct 10 22:22:42 2016 -0700

    Made execvp explicit in posix/shell.hpp.
    
    Review: https://reviews.apache.org/r/52729","12/Oct/16 21:43;kaysoky;{code}
commit 8c8ec608503394575a4f99fd725010b8920e5efa
Author: Joseph Wu <josephwu@apache.org>
Date:   Wed Oct 12 11:57:18 2016 -0700

    Windows: Implemented os::execvpe with _spawnvpe.
    
    Review: https://reviews.apache.org/r/52798
{code}",,,,,,,,,,,,,,,,,,,,,,,,,
Agent fails to kill empty parent container,MESOS-6322,13010312,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,anandmazumdar,greggomann,greggomann,06/Oct/16 23:31,08/Oct/16 04:18,29/Oct/20 16:32,08/Oct/16 04:18,,,,,,,,,1.1.0,,,,,,,,,,,0,mesosphere,,,,,,,,"I launched a pod using Marathon, which led to the launching of a task group on a Mesos agent. The pod spec was flawed, so this led to Marathon repeatedly re-launching multiple instances of the task group. After this went on for a few minutes, I told Marathon to scale the app to 0 instances, which sends {{TASK_KILLED}} for one task in each task group. After this, the Mesos agent reports {{TASK_KILLED}} status updates for all 3 tasks in the pod, but hitting the {{/containers}} endpoint on the agent reveals that the executor container for this task group is still running.

Here is the task group launching on the agent:
{code}
slave.cpp:1696] Launching task group containing tasks [ test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask1, test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask2, test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.clientTask ] for framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000
{code}
and here is the executor container starting:
{code}
mesos-agent[2994]: I1006 20:23:27.407563  3094 containerizer.cpp:965] Starting container bf38ff09-3da1-487a-8926-1f4cc88bce32 for executor 'instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601' of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000
{code}
and here is the output showing the {{TASK_KILLED}} updates for one task group:
{code}
mesos-agent[2994]: I1006 20:23:28.728224  3097 slave.cpp:2283] Asked to kill task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask1 of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000
mesos-agent[2994]: W1006 20:23:28.728304  3097 slave.cpp:2364] Transitioning the state of task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask1 of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 to TASK_KILLED because the executor is not registered
mesos-agent[2994]: I1006 20:23:28.728659  3097 slave.cpp:3609] Handling status update TASK_KILLED (UUID: 1cb8197a-7829-4a05-9cb1-14ba97519c42) for task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask1 of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 from @0.0.0.0:0
mesos-agent[2994]: I1006 20:23:28.728817  3097 slave.cpp:3609] Handling status update TASK_KILLED (UUID: e377e9fb-6466-4ce5-b32a-37d840b9f87c) for task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask2 of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 from @0.0.0.0:0
mesos-agent[2994]: I1006 20:23:28.728912  3097 slave.cpp:3609] Handling status update TASK_KILLED (UUID: 24d44b25-ea52-43a1-afdb-6c04389879d2) for task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.clientTask of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 from @0.0.0.0:0
{code}
however, if we grep the log for the executor's ID, the last line mentioning it is:
{code}
slave.cpp:3080] Creating a marker file for HTTP based executor 'instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601' of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 (via HTTP) at path '/var/lib/mesos/slave/meta/slaves/42838ca8-8d6b-475b-9b3b-59f3cd0d6834-S0/frameworks/42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000/executors/instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601/runs/bf38ff09-3da1-487a-8926-1f4cc88bce32/http.marker'
{code}
so it seems the executor never exited. If we hit the agent's {{/containers}} endpoint, we get output which includes this executor container:
{code}
{
    ""container_id"": ""bf38ff09-3da1-487a-8926-1f4cc88bce32"",
    ""executor_id"": ""instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601"",
    ""executor_name"": """",
    ""framework_id"": ""42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000"",
    ""source"": """",
    ""statistics"": {
      ""cpus_limit"": 0.1,
      ""cpus_nr_periods"": 17,
      ""cpus_nr_throttled"": 11,
      ""cpus_system_time_secs"": 0.02,
      ""cpus_throttled_time_secs"": 0.784142448,
      ""cpus_user_time_secs"": 0.09,
      ""disk_limit_bytes"": 10485760,
      ""disk_used_bytes"": 20480,
      ""mem_anon_bytes"": 11337728,
      ""mem_cache_bytes"": 0,
      ""mem_critical_pressure_counter"": 0,
      ""mem_file_bytes"": 0,
      ""mem_limit_bytes"": 33554432,
      ""mem_low_pressure_counter"": 0,
      ""mem_mapped_file_bytes"": 0,
      ""mem_medium_pressure_counter"": 0,
      ""mem_rss_bytes"": 11337728,
      ""mem_swap_bytes"": 0,
      ""mem_total_bytes"": 12013568,
      ""mem_unevictable_bytes"": 0,
      ""timestamp"": 1475792290.12373
    },
    ""status"": {
      ""executor_pid"": 9068,
      ""network_infos"": [
        {
          ""ip_addresses"": [
            {
              ""ip_address"": ""9.0.1.34"",
              ""protocol"": ""IPv4""
            }
          ],
          ""labels"": {},
          ""name"": ""dcos"",
          ""port_mappings"": [
            {
              ""container_port"": 8080,
              ""host_port"": 24758,
              ""protocol"": ""tcp""
            },
            {
              ""container_port"": 8081,
              ""host_port"": 24759,
              ""protocol"": ""tcp""
            }
          ]
        }
      ]
    }
  },
{code}
and looking through the output of {{ps}} on the agent, indeed we can locate the executor process:
{code}
$ ps aux | grep 9068
root      9068  0.0  0.1  96076 25380 ?        Ss   20:23   0:00 mesos-containerizer launch --command={""arguments"":[""mesos-default-executor""],""shell"":false,""user"":""root"",""value"":""\/opt\/mesosphere\/packages\/mesos--3f320308144e6b5acb70fc89bc7ea13c70302de0\/libexec\/mesos\/mesos-default-executor""} --help=false --pipe_read=49 --pipe_write=50 --pre_exec_commands=[{""arguments"":[""mesos-containerizer"",""mount"",""--help=false"",""--operation=make-rslave"",""--path=\/""],""shell"":false,""value"":""\/opt\/mesosphere\/packages\/mesos--3f320308144e6b5acb70fc89bc7ea13c70302de0\/libexec\/mesos\/mesos-containerizer""},{""shell"":true,""value"":""ifconfig lo up""}] --runtime_directory=/var/run/mesos/containers/bf38ff09-3da1-487a-8926-1f4cc88bce32 --unshare_namespace_mnt=false --user=root --working_directory=/var/lib/mesos/slave/slaves/42838ca8-8d6b-475b-9b3b-59f3cd0d6834-S0/frameworks/42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000/executors/instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601/runs/bf38ff09-3da1-487a-8926-1f4cc88bce32
core     20406  0.0  0.0   6764  1020 pts/1    S+   23:05   0:00 grep --colour=auto 9068
$ sudo cat /proc/9068/task/9068/children
9330
$ ps aux | grep 9330
root      9330  0.0  0.2 498040 32944 ?        Sl   20:23   0:00 mesos-default-executor
root     19330  0.0  0.0      0     0 ?        S    22:49   0:00 [kworker/0:2]
core     20573  0.0  0.0   6764   888 pts/1    S+   23:07   0:00 grep --colour=auto 9330
{code}
Looking at the executor's logs, we find stdout is:
{code}
Executing pre-exec command '{""arguments"":[""mesos-containerizer"",""mount"",""--help=false"",""--operation=make-rslave"",""--path=\/""],""shell"":false,""value"":""\/opt\/mesosphere\/packages\/mesos--3f320308144e6b5acb70fc89bc7ea13c70302de0\/libexec\/mesos\/mesos-containerizer""}'
Executing pre-exec command '{""shell"":true,""value"":""ifconfig lo up""}'
{code}
and stderr is:
{code}
I1006 20:23:28.817401  9330 executor.cpp:189] Version: 1.1.0
I1006 20:23:28.906349  9352 default_executor.cpp:123] Received SUBSCRIBED event
I1006 20:23:28.908797  9352 default_executor.cpp:127] Subscribed executor on 10.0.0.133
{code}
With this short executor log, it seems possible that the agent received the {{TASK_KILLED}} before any tasks were sent to the executor, and the agent removed those tasks from its data structures without terminating the parent container. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-5380,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-10-06 23:49:01.713,,,false,MESOS-2449,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 08 04:18:37 UTC 2016,,,,,,,"0|i34jqf:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 44,,,,,,,,,,,3.0,,1.1.0,,,,,,,,,"06/Oct/16 23:49;anandmazumdar;hmm, looks like we need similar logic that we had introduced for MESOS-5380 to guard against these cases. A bit surprised that we did not add the logic to the {{subscribe}} handler on the agent for HTTP based executors but only added it for driver based executors (https://reviews.apache.org/r/47381).","07/Oct/16 22:08;anandmazumdar;https://reviews.apache.org/r/52651/","08/Oct/16 04:18;anandmazumdar;{noformat}
commit 1fe48d388af378176be02b814bb3c7109b90a40d
Author: Anand Mazumdar <anand@apache.org>
Date:   Fri Oct 7 20:49:03 2016 -0700

    Minor test logic cleanup for HTTP based executor tests.

    The executor erroneously tried to subscribe with the agent after
    launching a task. This won't work post the fix for MESOS-6322.

commit e6ab3431100eaf90725eac24af9a1a99994877bc
Author: Anand Mazumdar <anand@apache.org>
Date:   Fri Oct 7 20:02:41 2016 -0700

    Fixed agent to correctly handle kill task of unregistered executor.

    This change brings in the same logic we had introduced for driver
    based executors to shut them down if all of their tasks have been
    killed before the executor could initially register (MESOS-5380).
    Note that the executors still need to commit suicide if they notice
    a disconnection with the agent before they could subscribe with
    the agent or if they did not receive a launch event within a
    timeout. See MESOS-5385 for more details.

    Review: https://reviews.apache.org/r/52651/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,
Implement clang-tidy check to catch incorrect flags hierarchies,MESOS-6320,13010183,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,bbannier,bbannier,06/Oct/16 15:05,26/Mar/18 08:14,29/Oct/20 16:32,24/Jan/17 20:19,,,,,,,,,1.2.0,,,,,,,,,,,0,clang-tidy,mesosphere,,,,,,,"Classes need to always use {{virtual}} inheritance when being derived from {{FlagsBase}}. Also, in order to compose such derived flags they should be inherited virtually again.

Some examples:
{code}
struct A : virtual FlagsBase {}; // OK
struct B : FlagsBase {}; // ERROR
struct C : A {}; // ERROR
{code}


We should implement a clang-tidy checker to catch such wrong inheritance issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-24 20:18:54.623,,,false,MESOS-4907,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 24 20:18:54 UTC 2017,,,,,,,"0|i36o3c:",9223372036854775807,,,,,mcypark,,,,,,Mesosphere Sprint 49,,,,,,,,,,,3.0,,,,,,,,,,,"21/Dec/16 20:59;bbannier;Review: https://github.com/mesos/clang-tools-extra/pull/4","24/Jan/17 20:18;mcypark;{noformat}
commit d76f8d298b9f302c92ce4d0ff7ebed9e116a95a6
Author: Benjamin Bannier <bbannier@gmail.com>
Date:   Wed Dec 21 19:33:30 2016 +0100

    [clang-tidy] Added Mesos check of custom Flags classes.

    This change fixes MESOS-6320.
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Agent recovery can fail after nested containers are launched,MESOS-6302,13009027,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,gilbert,greggomann,greggomann,01/Oct/16 01:51,13/Oct/16 15:54,29/Oct/20 16:32,05/Oct/16 23:57,,,,,,,,,1.1.0,,,,,,containerization,,,,,0,mesosphere,,,,,,,,"After launching a nested container which used a Docker image, I restarted the agent which ran that task group and saw the following in the agent logs during recovery:
{code}
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]: I1001 01:45:10.813596  4640 status_update_manager.cpp:203] Recovering status update manager
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]: I1001 01:45:10.813622  4640 status_update_manager.cpp:211] Recovering executor 'instance-testvolume.02c26bce-8778-11e6-9ff3-7a3cd7c1568e' of framework 118ca38d-daee-4b2d-b584-b5581738a3dd-0000
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]: I1001 01:45:10.814249  4639 docker.cpp:745] Recovering Docker containers
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]: I1001 01:45:10.815294  4642 containerizer.cpp:581] Recovering containerizer
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]: Failed to perform recovery: Collect failed: Unable to list rootfses belonged to container a7d576da-fd0f-4dc1-bd5a-6d0a93ac8a53: Unable to list the container directory: Failed to opendir '/var/lib/mesos/slave/provisioner/containers/a7d576da-fd0f-4dc1-bd5a-6d0a93ac8a53/backends': No such file or directory
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]: To remedy this do as follows:
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]: Step 1: rm -f /var/lib/mesos/slave/meta/slaves/latest
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]:         This ensures agent doesn't recover old live executors.
Oct 01 01:45:10 ip-10-0-3-133.us-west-2.compute.internal mesos-agent[4629]: Step 2: Restart the agent.
{code}
and the agent continues to restart in this fashion. Attached is the Marathon app definition that I used to launch the task group.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Oct/16 01:52;greggomann;read_write_app.json;https://issues.apache.org/jira/secure/attachment/12831206/read_write_app.json",,,,,1.0,,,,,,,,,,,,,,,,,,,,2016-10-03 08:02:25.27,,,false,MESOS-2449,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 05 23:57:28 UTC 2016,,,,,,,"0|i34btr:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 44,,,,,,,,,,,3.0,,,,,,,,,,,"03/Oct/16 08:02;gilbert;https://reviews.apache.org/r/52480/","05/Oct/16 23:57;jieyu;commit 8bab70c691a3efeda301f72956de4f80b258464e
Author: Gilbert Song songzihao1990@gmail.com
Date:   Mon Oct 3 15:28:39 2016 -0700

Fixed provisioner recovering with nested containers existed.

Previously, in provisioner recover, we firstly get all container
ids from the provisioner directory, and then find all rootfses
from each container's 'backends' directory. We made an assumption
that if a 'container_id' directory exists in the provisioner
directory, it must contain a 'backends' directory underneath,
which contains at least one rootfs for this container.

However, this is no longer true since we added support for nested
containers. Because we allow the case that a nested container is
specified with a container image while its parent does not have
an image specified. In this case, when the provisioner recovers,
it can still find the parent container's id in the provisioner
directory while no 'backends' directory exists, since all nested
containers backend information are under its parent container's
directory.

As a result, we should skip recovering the 'Info' struct in
provisioner for the parent container if it never provisions any
image.

Review: https://reviews.apache.org/r/52480/","05/Oct/16 23:57;jieyu;commit 945e8929a759f8041c890aebcb857411b31bdc1f
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Oct 5 16:56:38 2016 -0700

    Added unit test for provisioner 'RecoverNestedContainerNoParentImage'.
    
    Review: https://reviews.apache.org/r/52537/",,,,,,,,,,,,,,,,,,,,,,,,,
Recursive destroy in MesosContainerizer is problematic.,MESOS-6301,13008982,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,jieyu,jieyu,30/Sep/16 20:56,13/Oct/16 15:59,29/Oct/20 16:32,30/Sep/16 21:15,,,,,,,,,1.1.0,,,,,,,,,,,0,mesosphere,,,,,,,,"    When doing recursive destroy, we should return the collected future of
    nested container destroys. Intead, we should fail the corresponding
    termination and return that termination if nested container destroys
    failed.
    
    In addition, we cannot remove 'Container' struct from the internal map
    when the destroy of a nested container failed. This is to ensure that
    the top level container do not proceed with destroy if any of its
    nested container failed to destroy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2449,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 30 21:15:33 UTC 2016,,,,,,,"0|i34bjr:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 44,,,,,,,,,,,3.0,,,,,,,,,,,"30/Sep/16 21:15;jieyu;commit 02088986938c8aa01294f41f21e7f16e23dea3a0
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Sep 30 13:26:28 2016 -0700

    Fixed the MesosContainerizer destroy issue.
    
    When doing recursive destroy, we should return the collected future of
    nested container destroys. Intead, we should fail the corresponding
    termination and return that termination if nested container destroys
    failed.
    
    In addition, we cannot remove 'Container' struct from the internal map
    when the destroy of a nested container failed. This is to ensure that
    the top level container do not proceed with destroy if any of its
    nested container failed to destroy.
    
    Review: https://reviews.apache.org/r/52444",,,,,,,,,,,,,,,,,,,,,,,,,,,
A destroyed nested container is not reflected in the parent container's children map.,MESOS-6300,13008981,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,jieyu,jieyu,30/Sep/16 20:54,13/Oct/16 15:59,29/Oct/20 16:32,30/Sep/16 21:15,,,,,,,,,1.1.0,,,,,,,,,,,0,mesosphere,,,,,,,,We should update parent container's children map if it's nested container is terminated.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2449,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 30 21:15:19 UTC 2016,,,,,,,"0|i34bjj:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 44,,,,,,,,,,,2.0,,,,,,,,,,,"30/Sep/16 21:13;jieyu;https://reviews.apache.org/r/52444/","30/Sep/16 21:15;jieyu;commit 02088986938c8aa01294f41f21e7f16e23dea3a0
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Sep 30 13:26:28 2016 -0700

    Fixed the MesosContainerizer destroy issue.
    
    When doing recursive destroy, we should return the collected future of
    nested container destroys. Intead, we should fail the corresponding
    termination and return that termination if nested container destroys
    failed.
    
    In addition, we cannot remove 'Container' struct from the internal map
    when the destroy of a nested container failed. This is to ensure that
    the top level container do not proceed with destroy if any of its
    nested container failed to destroy.
    
    Review: https://reviews.apache.org/r/52444",,,,,,,,,,,,,,,,,,,,,,,,,,
HealthCheckTest.HealthyTaskViaHTTPWithoutType fails on some distros.,MESOS-6293,13008828,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,alexr,alexr,alexr,30/Sep/16 08:27,28/Oct/16 10:05,29/Oct/20 16:32,28/Oct/16 10:05,,,,,,,,,1.2.0,,,,,,,,,,,0,health-check,mesosphere,,,,,,,"I see consistent failures of this test in the internal CI in *some* distros, specifically CentOS 6, Ubuntu 14, 15, 16. The source of the health check failure is always the same: {{curl}} cannot connect to the target:
{noformat}
Received task health update, healthy: false
W0929 17:22:05.270992  2730 health_checker.cpp:204] Health check failed 1 times consecutively: HTTP health check failed: curl returned exited with status 7: curl: (7) couldn't connect to host
I0929 17:22:05.273634 26850 slave.cpp:3609] Handling status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 from executor(1)@172.30.2.20:58660
I0929 17:22:05.274178 26844 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000
I0929 17:22:05.274226 26844 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 to the agent
I0929 17:22:05.274314 26845 slave.cpp:4026] Forwarding the update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 to master@172.30.2.20:38955
I0929 17:22:05.274415 26845 slave.cpp:3920] Status update manager successfully handled status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000
I0929 17:22:05.274436 26845 slave.cpp:3936] Sending acknowledgement for status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 to executor(1)@172.30.2.20:58660
I0929 17:22:05.274534 26849 master.cpp:5661] Status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 from agent 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-S0 at slave(77)@172.30.2.20:38955 (ip-172-30-2-20.mesosphere.io)
../../src/tests/health_check_tests.cpp:1398: Failure
I0929 17:22:05.274567 26849 master.cpp:5723] Forwarding status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000
Value of: statusHealth.get().healthy()
  Actual: false
  Expected: true
I0929 17:22:05.274636 26849 master.cpp:7560] Updating the state of task aa0792d3-8d85-4c32-bd04-56a9b552ebda of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0929 17:22:05.274829 26844 sched.cpp:1025] Scheduler::statusUpdate took 43297ns
Received SHUTDOWN event
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-10-01 07:30:30.841,,,false,MESOS-5916,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 28 10:05:59 UTC 2016,,,,,,,"0|hzzz7o:zzr",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 44,Mesosphere Sprint 45,,,,,,,,,,3.0,,,,,,,,,,,"30/Sep/16 13:03;alexr;Run that test 1000 times on the CentOS 6 and could not reproduce the failure.","01/Oct/16 07:30;haosdent@gmail.com;Try in aws c4.4 xlarge with Ubuntu 14.04 and Ubuntu 16.04, curl version
{code}
# 16.04
curl --version
curl 7.47.0 (x86_64-pc-linux-gnu) libcurl/7.47.0 GnuTLS/3.4.10 zlib/1.2.8 libidn/1.32 librtmp/2.3
Protocols: dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtmp rtsp smb smbs smtp smtps telnet tftp
Features: AsynchDNS IDN IPv6 Largefile GSS-API Kerberos SPNEGO NTLM NTLM_WB SSL libz TLS-SRP UnixSockets
# 14.04
curl --version
curl 7.35.0 (x86_64-pc-linux-gnu) libcurl/7.35.0 OpenSSL/1.0.1f zlib/1.2.8 libidn/1.28 librtmp/2.3
Protocols: dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtmp rtsp smtp smtps telnet tftp
Features: AsynchDNS GSS-Negotiate IDN IPv6 Largefile NTLM NTLM_WB SSL libz TLS-SRP
{code}

Run with stress {{stress -c 16 -m 16 -i 16 -d 1}} after 3000 runs and could not reproduce the failure.
{code}
MESOS_VERBOSE=1 GLOG_v=1 GTEST_REPEAT=-1 GTEST_BREAK_ON_FAILURE=1 GTEST_FILTER=""HealthCheckTest.HealthyTaskViaHTTPWithoutType"" bin/mesos-tests.sh
{code}

Try in a virualbox slow vm (Ubuntu 14.04), could not reproduce as well.","01/Oct/16 10:36;haosdent@gmail.com;Try in Cent OS 6 (curl 7.19) with stress, could not reproduce after 3000 runs.","27/Oct/16 14:09;alexr;https://reviews.apache.org/r/53226/","27/Oct/16 14:15;haosdent@gmail.com;As [~alexr] investigation result, the test case would fail when we set {{LIBPROCESS_IP}} in the environment because running test cases because master would bind to {{LIBPROCESS_IP}} and didn't listen on {{127.0.0.1}}.","28/Oct/16 10:05;alexr;{noformat}
Commit: 2ab0fc3835506fec913c1d027ebc2332f4d025e2 [2ab0fc3]
Author: Alexander Rukletsov <alexr@apache.org>
Date: 27 October 2016 at 16:01:50 GMT+2
Commit Date: 28 October 2016 at 11:50:11 GMT+2

Fixed HealthyTaskViaHTTPWithoutType test.

Review: https://reviews.apache.org/r/53226
{noformat}",,,,,,,,,,,,,,,,,,,,,,
Support nested containers for logger in Mesos Containerizer.,MESOS-6290,13008779,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,gilbert,gilbert,gilbert,30/Sep/16 01:08,12/Oct/16 21:52,29/Oct/20 16:32,12/Oct/16 21:45,,,,,,,,,1.1.0,,,,,,containerization,,,,,0,containerizer,logger,mesosphere,,,,,,"Currently, there are two issues in mesos containerizer using logger for nested contaienrs:

1. An empty executorinfo is passed to logger when launching a nested container, it would potentially break some logger modules if any module tries to access the required proto field (e.g., executorId).

2. The logger does not reocver the nested containers yet in MesosContainerizer::recover.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-30 05:38:35.472,,,false,MESOS-2449,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 12 21:45:40 UTC 2016,,,,,,,"0|i34aan:",9223372036854775807,,,,,kaysoky,,,,,,Mesosphere Sprint 44,,,,,,,,,,,2.0,,1.1.0,,,,,,,,,"30/Sep/16 05:38;jieyu;Chatted with [~gilbert]. The existing logger interfaces need to be changed to support nested container. We should make the interfaces aware of 'container' rather than 'executor'.

cc [~kaysoky]
","10/Oct/16 13:49;alexr;Retargeting this for 1.2, please speak up if you want to land this in 1.1.0.","12/Oct/16 00:17;gilbert;[~vinodkone] [~alexr], we may still want this landed in Mesos 1.1.0. Would try to merge it asap.","12/Oct/16 00:19;gilbert;https://reviews.apache.org/r/52412/","12/Oct/16 21:45;kaysoky;{code}
commit 1e514ca023526f949e0ed5b91d43636a1b2fd172
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Oct 12 14:38:43 2016 -0700

    Supported `ContainerLogger` with nested containers.
    
    For nested containers, the sandbox directory still exists.  However,
    ExecutorInfo's no longer map directly one-to-one to containers.
    That means a nested container does not have an associated ExecutorInfo.
    
    The `ExecutorInfo` parameter provides metadata for the `ContainerLogger`
    including the FrameworkID, ExecutorID, environment variables, and
    arbitrary Labels.  For nested containers, the top-level parent's
    `ExecutorInfo` should be sufficient to provide the same metadata.
    
    Review: https://reviews.apache.org/r/52412/
{code}",,,,,,,,,,,,,,,,,,,,,,,
The default executor should maintain launcher_dir.,MESOS-6288,13008744,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gkleiman,alexr,alexr,29/Sep/16 22:12,31/Oct/16 14:25,29/Oct/20 16:32,31/Oct/16 14:25,,,,,,,,,1.2.0,,,,,,,,,,,0,health-check,mesosphere,,,,,,,"Both command and docker executors require {{launcher_dir}} is provided in a flag. This directory contains mesos binaries, e.g. a tcp checker necessary for TCP health check. The default executor should obtain somehow (a flag, env var) and maintain this directory for health checker to use.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6119,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-10-06 16:33:44.96,,,false,MESOS-6355,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 31 14:25:15 UTC 2016,,,,,,,"0|hzzz6s:r",9223372036854775807,,,,,alexr,,,,,,Mesosphere Sprint 44,Mesosphere Sprint 45,Mesosphere Sprint 46,,,,,,,,,3.0,,,,,,,,,,,"06/Oct/16 16:33;gkleiman;Patches:

https://reviews.apache.org/r/52878/
https://reviews.apache.org/r/52879/
https://reviews.apache.org/r/52880/
","31/Oct/16 14:25;alexr;{noformat}
Commit: 1e57459b7d3f571bdf18fec29b070e78ce719319 [1e57459]
Author: Gastón Kleiman <gaston@mesosphere.com>
Date: 31 October 2016 at 12:35:13 GMT+1
Committer: Alexander Rukletsov <alexr@apache.org>
Commit Date: 31 October 2016 at 14:21:27 GMT+1

Added the '--launcher_dir' flag to the default executor.

Review: https://reviews.apache.org/r/53227/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos containerizer should figure out the correct sandbox directory for nested launch.,MESOS-6263,13008101,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,28/Sep/16 01:12,28/Sep/16 05:18,29/Oct/20 16:32,28/Sep/16 05:18,,,,,,,,,1.1.0,,,,,,containerization,,,,,0,containerizer,mesosphere,,,,,,,"Currently the mesos containerizer take the sandbox directory from the agent. Ideally, a nested sandbox dir can be figured out by the containerizer. And there is no need to pass it from the agent. We should remove the `directory` parameter in nested launch interface.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2449,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 28 05:18:48 UTC 2016,,,,,,,"0|i3463z:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 43,,,,,,,,,,,3.0,,,,,,,,,,,"28/Sep/16 04:26;gilbert;https://reviews.apache.org/r/52334/","28/Sep/16 05:18;gilbert;commit af5723034a962f000ab3e6050fd52ebbe083cb4b
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Sep 27 22:10:57 2016 -0700

    Removed 'directory' in nested container launch interface.
    
    In this patch, the containerizer launch method for nested containers
    should be responsible for figuring out the sandbox directory for the
    nested container. So we can avoid passing an extra parameter to the
    nested launch interface, and there is no need for the agent to figure
    out the nested sandbox.
    
    Review: https://reviews.apache.org/r/52334/

commit bb6527029b10835b04fc0a7de6826430bc13cc35
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Sep 27 21:35:33 2016 -0700

    Added a parent DESTROYING check for nested container launch.
    
    Review: https://reviews.apache.org/r/52333/

",,,,,,,,,,,,,,,,,,,,,,,,,,
Default executor should kill all other tasks in a task group if any task exits with a non-zero exit status.,MESOS-6262,13008052,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,anandmazumdar,anandmazumdar,27/Sep/16 20:48,28/Sep/16 04:48,29/Oct/20 16:32,28/Sep/16 04:48,,,,,,,,,1.1.0,,,,,,,,,,,0,mesosphere,,,,,,,,The default restart policy for a task group is to kill all active containers if any of the tasks terminates with a non-zero exit status code for now. The default executor needs to honor this default policy.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2449,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 28 04:48:11 UTC 2016,,,,,,,"0|i345t3:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 43,,,,,,,,,,,3.0,,,,,,,,,,,"27/Sep/16 21:04;anandmazumdar;https://reviews.apache.org/r/52317/","28/Sep/16 04:48;anandmazumdar;{noformat}
commit 75c277f9eabf3d27a4143ba86e37f9b0611c3fe4
Author: Anand Mazumdar <anand@apache.org>
Date:   Tue Sep 27 21:42:38 2016 -0700

    Fixed default executor to honor the default task group restart policy.

    This change fixes the default executor to honor the default task
    group restart policy i.e., if any task fails with a non-zero
    exit status, the other tasks in the group are killed.

    Review: https://reviews.apache.org/r/52317/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Composing containerizer needs to properly handle nested container launch,MESOS-6260,13008019,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,vinodkone,vinodkone,vinodkone,27/Sep/16 18:31,29/Sep/16 23:52,29/Oct/20 16:32,29/Sep/16 23:52,,,,,,,,,1.1.0,,,,,,,,,,,0,,,,,,,,,"Right now if the agent is started with --containerizers=""docker,mesos"" , nested container launches will fail because the composing containerizer doesn't implement the nested `launch` method. This results in it using the default `launch` method defined in the based class, which just returns an Error.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2449,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 29 23:52:40 UTC 2016,,,,,,,"0|i345lr:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 43,,,,,,,,,,,3.0,,,,,,,,,,,"27/Sep/16 22:26;vinodkone;https://reviews.apache.org/r/52321/","29/Sep/16 04:13;vinodkone;https://reviews.apache.org/r/52330/","29/Sep/16 04:13;vinodkone;commit 31350f9d3d468f24130c522931189bbf92869b10
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Tue Sep 27 15:10:36 2016 -0700

    Fixed composing containerizer to handled nested containers.
    
    Made composing containerizer nesting aware so that operators
    can enable both mesos and docker containerizers on agents.
    Ofcourse docker containerizer is not nesting aware.
    
    Review: https://reviews.apache.org/r/52321
","29/Sep/16 23:52;vinodkone;commit 700f04002bc78ec1b8fd813ea307b0f1f95770e9
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Tue Sep 27 20:12:53 2016 -0700

    Fixed the race between launch and destroy in composing containerizer.
    
    Review: https://reviews.apache.org/r/52330

commit 4dd3bd01f1a74ead5c9ea9d9357fb5273ef18d8d
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Tue Sep 27 17:38:45 2016 -0700

    Fixed logging in composing containerizer.
    
    In addition to removing quotes, removed Container ID in the
    returned error messages because the caller knows it already.
    
    Review: https://reviews.apache.org/r/52329

",,,,,,,,,,,,,,,,,,,,,,,,
CNI isolator should not `CHECK` for `resolv.conf` under `rootContainerDir`,MESOS-6259,13008002,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,avinash.mesos,avinash.mesos,avinash.mesos,27/Sep/16 17:55,27/Sep/16 19:08,29/Oct/20 16:32,27/Sep/16 19:08,1.1.0,,,,,,,,1.1.0,,,,,,containerization,,,,,0,,,,,,,,,,linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-27 19:08:37.282,,,false,MESOS-2449,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 27 19:08:37 UTC 2016,,,,,,,"0|i345hz:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 43,,,,,,,,,,,1.0,,,,,,,,,,,"27/Sep/16 19:08;jieyu;commit d216d989c9983518670c777b7b78e1003f40833f
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Tue Sep 27 12:05:12 2016 -0700

    Removed `CHECK` for no-existent resolv.conf for root containers.
    
    Review: https://reviews.apache.org/r/52305/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Driver based schedulers performing explicit acknowledgements cannot acknowledge updates from HTTP based executors.,MESOS-6245,13007284,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,anandmazumdar,anandmazumdar,23/Sep/16 22:24,26/Sep/16 18:19,29/Oct/20 16:32,26/Sep/16 18:19,,,,,,,,,1.0.2,1.1.0,,,,,,,,,,0,mesosphere,,,,,,,,"It seems that the agent code sets {{StatusUpdate}}->{{slave_id}} but does not set the {{TaskStatus}}->{{slave_id}} if it's not already set. On the driver, when we receive such a status update and if it has explicit ACK enabled, it would pass the {{TaskStatus}} to the scheduler. But, the scheduler has no way of acking this update due to {{slave_id}} not being present. Note that, implicit acknowledgements still work since they use the {{slave_id}} from {{StatusUpdate}}. Hence, we never noticed this in our tests as all of them use implicit acknowledgements on the driver.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2449,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 26 18:19:54 UTC 2016,,,,,,,"0|i34127:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 43,,,,,,,,,,,3.0,,,,,,,,,,,"24/Sep/16 01:07;anandmazumdar;https://reviews.apache.org/r/52231/","26/Sep/16 02:00;anandmazumdar;{noformat}
commit ec4c81a12559030791334359e7e1e2b6565cce01
Author: Anand Mazumdar <anand@apache.org>
Date:   Sun Sep 25 18:54:21 2016 -0700

    Fixed driver based schedulers to ACK updates from HTTP executors.

    Previously, driver based schedulers were not able to acknowledge
    status updates from HTTP based executors if they had explicit
    acknowledgements enabled. This was due to the fact that we
    were not populating `TaskStatus.slave_id` correctly if not
    set. It would also be great to validate `TaskStatus.slave_id`
    set by the executor and let them know if the value is incorrect.

    Review: https://reviews.apache.org/r/52231/
{noformat}

Keeping the JIRA open for backporting the fix to the 1.0.x branch.","26/Sep/16 18:19;anandmazumdar;{noformat}
commit c885b26975bb00bc8cc1357f706bcb175b25765a
Author: Anand Mazumdar <anand@apache.org>
Date:   Mon Sep 26 10:19:19 2016 -0700

    Added MESOS-6245 to 1.0.2 CHANGELOG.

commit 1bdd456605861767af51a7ec0a1ab755ec1cf295
Author: Anand Mazumdar <anand@apache.org>
Date:   Sun Sep 25 18:54:21 2016 -0700

    Fixed driver based schedulers to ACK updates from HTTP executors.

    Previously, driver based schedulers were not able to acknowledge
    status updates from HTTP based executors if they had explicit
    acknowledgements enabled. This was due to the fact that we
    were not populating `TaskStatus.slave_id` correctly if not
    set. It would also be great to validate `TaskStatus.slave_id`
    set by the executor and let them know if the value is incorrect.

    Review: https://reviews.apache.org/r/52231/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,
PAGE_SIZE was not declared in PPC64LE,MESOS-6217,13006559,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,haosdent@gmail.com,haosdent@gmail.com,haosdent@gmail.com,21/Sep/16 16:22,21/Sep/16 17:10,29/Oct/20 16:32,21/Sep/16 17:10,,,,,,,,,1.1.0,,,,,,,,,,,0,cgroups,ppc64,,,,,,,"When compile Mesos in PPC64LE, get this error

{code}
../../src/slave/containerizer/mesos/isolators/gpu/isolator.cpp  -fPIC -DPIC -o slave/containerizer/mesos/isolators/gpu/.libs/libmesos_no_3rdparty_la-isolator.o
../../src/slave/containerizer/mesos/isolators/cgroups/subsystems/memory.cpp: In member function 'virtual process::Future<Nothing> mesos::internal::slave::MemorySubsystem::update(const mesos::ContainerID&, const mesos::Resources&)':
../../src/slave/containerizer/mesos/isolators/cgroups/subsystems/memory.cpp:230:55: error: 'PAGE_SIZE' was not declared in this scope
   Bytes initialLimit(static_cast<uint64_t>(LONG_MAX / PAGE_SIZE * PAGE_SIZE));
                                                       ^
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-21 17:10:35.252,,,false,MESOS-4697,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 21 17:10:35 UTC 2016,,,,,,,"0|i33wmf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,"21/Sep/16 17:10;jieyu;commit 2f5e4492a4379c47e1ddb3370ac66725b210ee79
Author: haosdent huang <haosdent@gmail.com>
Date:   Wed Sep 21 10:10:18 2016 -0700

    Fixed compile error in ppc64le.
    
    `PAGE_SIZE` is not declared in ppc64le, use `os::pagesize()` instead
    to support different platforms.
    
    Review: https://reviews.apache.org/r/52090/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Health check grace period covers failures happening after first success.,MESOS-6170,13005163,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gkleiman,alexr,alexr,15/Sep/16 13:49,05/Oct/16 16:27,29/Oct/20 16:32,05/Oct/16 16:27,1.0.0,,,,,,,,1.1.0,,,,,,,,,,,0,health-check,mesosphere,,,,,,,"Currently, the health check library [ignores *all* failures|https://github.com/apache/mesos/blob/b053572bc424478cafcd60d1bce078f5132c4590/src/health-check/health_checker.cpp#L192-L197] from the task’s start (technically from the health check library initialization) [until after the grace period ends|https://github.com/apache/mesos/blob/b053572bc424478cafcd60d1bce078f5132c4590/include/mesos/v1/mesos.proto#L403].

This behaviour is misleading. Once the health check succeeds for the first time, grace period rule for failures should not be applied any more.

For example, if the grace period is set to 10 minutes, the task becomes healthy after 1 minute and fails after 2 minutes, the failure should be treated as a normal failure with all the consequences.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-28 17:01:32.42,,,false,MESOS-5916,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 05 16:27:21 UTC 2016,,,,,,,"0|i33o0v:",9223372036854775807,,,,,alexr,,,,,,Mesosphere Sprint 44,,,,,,,,,,,5.0,,1.1.0,,,,,,,,,"28/Sep/16 17:01;gkleiman;https://reviews.apache.org/r/52357/","05/Oct/16 16:27;alexr;{noformat}
Commit: 3eb8b20d7994c70ce81180b1ab01c3422426cb90 [3eb8b20]
Author: Gastón Kleiman <gaston@mesosphere.com>
Date: 5 October 2016 at 15:49:03 GMT+2
Committer: Alexander Rukletsov <alexr@apache.org>
Commit Date: 5 October 2016 at 18:08:35 GMT+2

Improved handling of health check failures within the grace period.

The health check library would ignore all failures until after the end
of the grace period.

This behaviour was misleading. With the changes in this commit, once a
health check succeeds for the first time, the grace period rule for
failures is not be applied any more.

Review: https://reviews.apache.org/r/52357/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Remove stout's Set type,MESOS-6159,13005024,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,Fixed,bbannier,bbannier,bbannier,14/Sep/16 23:06,26/Mar/18 08:10,29/Oct/20 16:32,21/Sep/16 15:40,,,,,,,,,1.1.0,,,,,,stout,,,,,0,tech-debt,,,,,,,,"stout provides a {{Set}} type which wraps a {{std::set}}. As only addition it provides new constructors,
{code}
Set(const T& t1);
Set(const T& t1, const T& t2);
Set(const T& t1, const T& t2, const T& t3);
Set(const T& t1, const T& t2, const T& t3, const T& t4);
{code}
which simplified creation of a {{Set}} from (up to four) known elements.

C++11 brought {{std::initializer_list}} which can be used to create a {{std::set}} from an arbitrary number of elements, so it appears that it should be possible to retire {{Set}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-21 15:40:57.988,,,false,MESOS-2664,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 21 15:40:57 UTC 2016,,,,,,,"0|i33n5z:",9223372036854775807,,,,,mcypark,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,"20/Sep/16 13:31;bbannier;Reviews:
https://reviews.apache.org/r/52036/
https://reviews.apache.org/r/52037/","21/Sep/16 15:40;mcypark;{noformat}
commit e48e9288ba654c5b85be62f577320580885cc0e9
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Wed Sep 21 14:08:32 2016 +0200

    Removed stout `Set`.

    Review: https://reviews.apache.org/r/52037/
{noformat}
{noformat}
commit 3e0ed792331ba9a56ceb79501e176f1c23f46cb5
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Wed Sep 21 14:08:12 2016 +0200

    Moved user of stout's `Set` to `std::set`.

    After the move to C++11 stout's `Set` offers no benefits over
    `std::set`, and will be removed in a subsequent change set.

    Review: https://reviews.apache.org/r/52036/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up queued tasks if a task group is killed before launch.,MESOS-6154,13004581,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,anandmazumdar,anandmazumdar,13/Sep/16 12:53,29/Sep/16 11:48,29/Oct/20 16:32,15/Sep/16 22:53,,,,,,,,,1.1.0,,,,,,,,,,,0,mesosphere,,,,,,,,"We are not properly cleaning up a queued task group when one of its task is killed i.e. https://github.com/apache/mesos/blob/aaa353acc515c0435a859113c9ee236247b51169/src/slave/slave.cpp#L6554 , we clean up the queued task but don't go around cleaning up the queued task group. Also, it would be great to add a test similar to we did for exercising the {{pending}} tasks workflow i.e. {{SlaveTest.KillTaskGroupBetweenRunTaskParts}} for {{queuedTasks}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6153,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2449,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 15 22:53:13 UTC 2016,,,,,,,"0|i33kfj:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 42,,,,,,,,,,,2.0,,,,,,,,,,,"15/Sep/16 16:06;anandmazumdar;https://reviews.apache.org/r/51904/","15/Sep/16 22:53;anandmazumdar;{noformat}
commit 81cd023eb9945a22c220edc966393dcfcdbce256
Author: Anand Mazumdar <anand@apache.org>
Date:   Wed Sep 14 20:56:14 2016 -0700

    Added logging when sending a queued task group to the executor.

    We were not logging the task group before sending the queued
    task group to the executor via a `LAUNCH_GROUP` event.

    Review: https://reviews.apache.org/r/51905

commit 0b1c1c89e871e0272871e8787008830080c60900
Author: Anand Mazumdar <anand@apache.org>
Date:   Wed Sep 14 20:29:02 2016 -0700

    Ensured that queued task groups killed before launch are cleaned up.

    Previously, we were not correctly sending TASK_KILLED status updates
    for a queued task group if some/all of its tasks were killed before
    launch on the agent. Also, we would have still sent the killed task
    group erroneously to the executor upon subscribing with the agent
    even if it was killed.

    Review: https://reviews.apache.org/r/51904
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Resource leak in slave.cpp.,MESOS-6153,13004524,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,js84,js84,13/Sep/16 08:05,29/Sep/16 11:48,29/Oct/20 16:32,29/Sep/16 11:48,,,,,,,,,1.1.0,,,,,,,,,,,0,coverity,mesosphere,,,,,,,"Coverity detected the following resource leak:
{code}
1. Condition this->queuedTasks.contains(taskId), taking true branch.
6547  if (queuedTasks.contains(taskId)) {
    	2. Condition terminal, taking true branch.
6548    if (terminal) {
    	3. alloc_fn: Storage is returned from allocation function operator new. [Note: The source code implementation of the function has been overridden by a builtin model.]
    	4. var_assign: Assigning: task = storage returned from new mesos::Task(mesos::internal::protobuf::createTask(this->queuedTasks.at(taskId), mesos::TaskState const(status->state()), this->frameworkId)).
6549      Task* task = new Task(protobuf::createTask(
6550          queuedTasks.at(taskId),
6551          status.state(),
6552          frameworkId));
6553
6554      queuedTasks.erase(taskId);
6555
6556      // This might be a queued task belonging to a task group.
6557      // If so, we need to update the other tasks belonging to this task group.
6558      Option<TaskGroupInfo> taskGroup = getQueuedTaskGroup(taskId);
6559
    	5. Condition taskGroup.isSome(), taking true branch.
6560      if (taskGroup.isSome()) {
    	6. No elements left in taskGroup->tasks(), leaving loop.
6561        foreach (const TaskInfo& task_, taskGroup->tasks()) {
6562          Task* task = new Task(
6563              protobuf::createTask(task_, status.state(), frameworkId));
6564
6565          tasks.push_back(task);
6566        }
    	7. Falling through to end of if statement.
6567      } else {
6568        tasks.push_back(task);
6569      }
    	
CID 1372871 (#1 of 1): Resource leak (RESOURCE_LEAK)
8. leaked_storage: Variable task going out of scope leaks the storage it points to.
6570    } else {
6571      return Error(""Cannot send non-terminal update for queued task"");
{code}

https://scan5.coverity.com/reports.htm#v39597/p10429/fileInstanceId=98881751&defectInstanceId=28450463",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-13 10:14:50.093,,,false,MESOS-2449,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 13 10:14:50 UTC 2016,,,,,,,"0|hzzz9g:zzzzi",9223372036854775807,,,,,anandmazumdar,,,,,,Mesosphere Sprint 42,,,,,,,,,,,1.0,,,,,,,,,,,"13/Sep/16 10:14;bbannier;Review: https://reviews.apache.org/r/51840/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Resource leak in libevent_ssl_socket.cpp.,MESOS-6152,13004523,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,js84,js84,13/Sep/16 08:03,22/Mar/19 16:40,29/Oct/20 16:32,13/Sep/16 09:48,,,,,,,,,0.28.3,1.0.2,1.1.0,,,,,,,,,0,coverity,,,,,,,,"Coverity detected the following resource leak.
IMO {code} if (fd == -1) {code} should be  {code} if (owned_fd == -1) {code}.


{code}
 // Duplicate the file descriptor because Libevent will take ownership
754  // and control the lifecycle separately.
755  //
756  // TODO(josephw): We can avoid duplicating the file descriptor in
757  // future versions of Libevent. In Libevent versions 2.1.2 and later,
758  // we may use `evbuffer_file_segment_new` and `evbuffer_add_file_segment`
759  // instead of `evbuffer_add_file`.
   	3. open_fn: Returning handle opened by dup.
   	4. var_assign: Assigning: owned_fd = handle returned from dup(fd).
760  int owned_fd = dup(fd);
   	CID 1372873: Argument cannot be negative (REVERSE_NEGATIVE) [select issue]
   	5. Condition fd == -1, taking true branch.
761  if (fd == -1) {
   	
CID 1372872 (#1 of 1): Resource leak (RESOURCE_LEAK)
6. leaked_handle: Handle variable owned_fd going out of scope leaks the handle.
762    return Failure(ErrnoError(""Failed to duplicate file descriptor""));
763  }
{code}

https://scan5.coverity.com/reports.htm#v39597/p10429/fileInstanceId=98881747&defectInstanceId=28450468",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-13 09:21:32.463,,,false,MESOS-7930,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 13 09:48:43 UTC 2016,,,,,,,"0|i33k2f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,"13/Sep/16 09:21;bbannier;Review: https://reviews.apache.org/r/51839/","13/Sep/16 09:48;jvanremoortere;{code}
commit fbaeb5f37fc88dc51b8d51343e5f9fa8b8c60fa8
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Tue Sep 13 11:43:19 2016 +0200

    Checked correct file descriptor after call to `dup`.
    
    Review: https://reviews.apache.org/r/51839/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Frameworks may RESERVE for an arbitrary role.,MESOS-6142,13003631,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,gkleiman,alexr,alexr,08/Sep/16 17:00,16/Nov/16 15:36,29/Oct/20 16:32,08/Nov/16 22:06,1.0.0,1.1.0,,,,,,,0.28.3,1.0.3,1.1.1,1.2.0,,,allocation,master,,,,0,mesosphere,reservations,,,,,,,"The master does not validate that resources from a reservation request have the same role the framework is registered with. As a result, frameworks may reserve resources for arbitrary roles.

I've modified the role in [the {{ReserveThenUnreserve}} test|https://github.com/apache/mesos/blob/bca600cf5602ed8227d91af9f73d689da14ad786/src/tests/reservation_tests.cpp#L117] to ""yoyo"" and observed the following in the test's log:
{noformat}
I0908 18:35:43.379122 2138112 master.cpp:3362] Processing ACCEPT call for offers: [ dfaf67e6-7c1c-4988-b427-c49842cb7bb7-O0 ] on agent dfaf67e6-7c1c-4988-b427-c49842cb7bb7-S0 at slave(1)@10.200.181.237:60116 (alexr.railnet.train) for framework dfaf67e6-7c1c-4988-b427-c49842cb7bb7-0000 (default) at scheduler-ca12a660-9f08-49de-be4e-d452aa3aa6da@10.200.181.237:60116
I0908 18:35:43.379170 2138112 master.cpp:3022] Authorizing principal 'test-principal' to reserve resources 'cpus(yoyo, test-principal):1; mem(yoyo, test-principal):512'
I0908 18:35:43.379678 2138112 master.cpp:3642] Applying RESERVE operation for resources cpus(yoyo, test-principal):1; mem(yoyo, test-principal):512 from framework dfaf67e6-7c1c-4988-b427-c49842cb7bb7-0000 (default) at scheduler-ca12a660-9f08-49de-be4e-d452aa3aa6da@10.200.181.237:60116 to agent dfaf67e6-7c1c-4988-b427-c49842cb7bb7-S0 at slave(1)@10.200.181.237:60116 (alexr.railnet.train)
I0908 18:35:43.379767 2138112 master.cpp:7341] Sending checkpointed resources cpus(yoyo, test-principal):1; mem(yoyo, test-principal):512 to agent dfaf67e6-7c1c-4988-b427-c49842cb7bb7-S0 at slave(1)@10.200.181.237:60116 (alexr.railnet.train)
I0908 18:35:43.380273 3211264 slave.cpp:2497] Updated checkpointed resources from  to cpus(yoyo, test-principal):1; mem(yoyo, test-principal):512
I0908 18:35:43.380574 2674688 hierarchical.cpp:760] Updated allocation of framework dfaf67e6-7c1c-4988-b427-c49842cb7bb7-0000 on agent dfaf67e6-7c1c-4988-b427-c49842cb7bb7-S0 from cpus(*):1; mem(*):512; disk(*):470841; ports(*):[31000-32000] to ports(*):[31000-32000]; cpus(yoyo, test-principal):1; disk(*):470841; mem(yoyo, test-principal):512 with RESERVE operation
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-26 14:25:17.248,,,false,MESOS-2018,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 08 22:06:57 UTC 2016,,,,,,,"0|hzzz6s:",9223372036854775807,,,,,alexr,,,,,,Mesosphere Sprint 44,Mesosphere Sprint 45,Mesosphere Sprint 46,,,,,,,,,3.0,,0.28.3,1.0.3,1.1.1,1.2.0,,,,,,"26/Sep/16 14:25;mcypark;It looks like there are actually a couple of {{TODO}} from [~bmahler] about this.
- https://github.com/apache/mesos/blob/1.0.1/src/master/allocator/mesos/hierarchical.cpp#L245-L246
- https://github.com/apache/mesos/blob/1.0.1/src/master/allocator/mesos/hierarchical.cpp#L430-L431","07/Oct/16 18:01;gkleiman;Patches:

https://reviews.apache.org/r/52642/
https://reviews.apache.org/r/53470/","08/Nov/16 22:06;alexr;{noformat}
Commit: 9d0ace758c8e2a444c15ef0c9395c9e4a15c553f [9d0ace7]
Author: Gastón Kleiman gaston@mesosphere.com
Date: 8 November 2016 at 21:57:29 GMT+1
Committer: Alexander Rukletsov alexr@apache.org
Commit Date: 8 November 2016 at 22:30:06 GMT+1

Improved the validation of RESERVE operations.

Don't allow a reservation if the framework role
doesn't match the role of all the resources.

Review: https://reviews.apache.org/r/52642/
{noformat}
{noformat}
Commit: a1780704308838d89eac75149cf2ba71da464b46 [a178070]
Author: Gastón Kleiman <gaston@mesosphere.com>
Date: 16 November 2016 at 16:11:37 GMT+1
Committer: Alexander Rukletsov <alexr@apache.org>
Commit Date: 16 November 2016 at 16:31:34 GMT+1

Added a test to ensure MESOS-6142 is fixed.

This test ensures that frameworks can't reserve resources using a role
different from the one they registered with.

Review: https://reviews.apache.org/r/53470/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,
TCP health checks are not portable.,MESOS-6119,13002376,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,alexr,alexr,alexr,02/Sep/16 15:07,03/Nov/16 12:43,29/Oct/20 16:32,03/Nov/16 12:43,,,,,,,,,1.2.0,,,,,,,,,,,0,health-check,mesosphere,,,,,,,"MESOS-3567 introduced a dependency on ""bash"" for TCP health checks, which is undesirable. We should implement a portable solution for TCP health checks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-5916,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 03 12:43:46 UTC 2016,,,,,,,"0|hzzz7h:",9223372036854775807,,,,,tillt,,,,,,Mesosphere Sprint 42,Mesosphere Sprint 43,Mesosphere Sprint 44,Mesosphere Sprint 46,,,,,,,,3.0,,,,,,,,,,,"02/Sep/16 16:45;alexr;https://reviews.apache.org/r/51605/
https://reviews.apache.org/r/51606/
https://reviews.apache.org/r/51607/","03/Nov/16 12:43;alexr;{noformat}
Commit: abecad3c5a3728d738b13c7726d1f9afd4118cc2 [abecad3]
Author: Alexander Rukletsov alexr@apache.org
Date: 2 September 2016 at 17:04:37 GMT+2
Commit Date: 3 November 2016 at 11:34:29 GMT+1

Added ""mesos-tcp-connect"" binary.

To remove dependency on bash for TCP health checks, introduce
a separate light-weight binary (without libmesos dependency) for
probing TCP connections.

Review: https://reviews.apache.org/r/51605
{noformat}
{noformat}
Commit: 0ddefe601686878e925808b312ab6d57e4f1d7c7 [0ddefe6]
Author: Alexander Rukletsov alexr@apache.org
Date: 2 September 2016 at 17:05:13 GMT+2
Commit Date: 3 November 2016 at 11:34:29 GMT+1

Libprocess: Added target for ""mesos-tcp-connect"" binary.

Review: https://reviews.apache.org/r/51606
{noformat}
{noformat}
Commit: fa57e4f9bded9d914418786c4bcf795327ec5a0e [fa57e4f]
Author: Alexander Rukletsov alexr@apache.org
Date: 2 September 2016 at 18:04:15 GMT+2
Commit Date: 3 November 2016 at 11:34:29 GMT+1

Used mesos-tcp-connect binary in TCP health checks.

Review: https://reviews.apache.org/r/51607
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Consider supporting TCP half-open in checks.,MESOS-6116,13001955,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Won't Fix,,alexr,alexr,01/Sep/16 09:40,06/Feb/19 17:28,29/Oct/20 16:32,06/Feb/19 17:28,,,,,,,,,,,,,,,,,,,,0,check,health-check,mesosphere,,,,,,A TCP half-open check does not complete the TCP handshake and hence the tested task is not notified that the someone is connecting. This is usually more performant than doing a complete TCP connection.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-7353,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-09-01 09:40:15.0,,,,,,,"0|i3348v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate using health checks without setting the type,MESOS-6110,13001494,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,haosdent@gmail.com,swsnider,swsnider,30/Aug/16 22:17,29/Sep/16 16:34,29/Oct/20 16:32,29/Sep/16 16:34,1.1.0,,,,,,,,1.1.0,,,,,,master,,,,,0,compatibility,health-check,mesosphere,,,,,,"When sending a task launch using the 1.0.x protos and the legacy (non-http) API, tasks with a healthcheck defined are rejected (TASK_ERROR) because the 'type' field is not set.

This field is marked optional in the proto and is not available before 1.1.0, so it should not be required in order to keep the mesos v1 api compatibility promise.

For backwards compatibility temporarily allow the use case when command health check is set without a type.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-08-31 18:30:06.824,,,false,MESOS-5916,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 29 16:34:49 UTC 2016,,,,,,,"0|hzzz9g:zzzr",9223372036854775807,,,,,alexr,,,,,,Mesosphere Sprint 42,Mesosphere Sprint 43,,,,,,,,,,3.0,,,,,,,,,,,"31/Aug/16 18:30;haosdent@gmail.com;Patch: https://reviews.apache.org/r/51560","31/Aug/16 19:08;swsnider;This cannot be deprecated until mesos 2.0, since the support policy (http://mesos.apache.org/documentation/latest/versioning/) states in the API Compatibility section that deprecation only starts for N-1 of the api in the N.0.0 release. I therefore believe it would be inappropriate to even add a log message until then.","31/Aug/16 19:15;kaysoky;That's true if we cut a 2.0 instead of a 1.4 or 1.5.  Either way, we need to give 6 months minimum.","01/Sep/16 00:12;swsnider;Hmm, this is confusing. Since this is an API change, I believe that we have to wait until 2.0 is cut to deprecate it, and then we have to wait 6 months before removing it. Is this what you mean?","29/Sep/16 16:34;alexr;{noformat}
Commit: 0a52cf7ea715f146ff581b82a8fd6efda15c208a [0a52cf7]
Author: haosdent huang haosdent@gmail.com
Date: 29 September 2016 at 17:27:08 GMT+2
Committer: Alexander Rukletsov alexr@apache.org

Ensured HealthCheck::HTTPCheckInfo compatible with the old one.

Review: https://reviews.apache.org/r/51803/
{noformat}
{noformat}
Commit: 2fe229d54228a26db637e0cc6b63c9b8bf528319 [2fe229d]
Author: haosdent huang haosdent@gmail.com
Date: 29 September 2016 at 17:27:21 GMT+2
Committer: Alexander Rukletsov alexr@apache.org
Commit Date: 29 September 2016 at 18:30:29 GMT+2

Supported command and http health checks without type.

Absence of type in command health check and HTTP health check are
supported for backwards compatibility and will be deprecated in
Mesos 2.0.

Review: https://reviews.apache.org/r/51560/
{noformat}
{noformat}
Commit: d7b1b667d8e48b0d5cf862a39c01832b2c05898d [d7b1b66]
Author: haosdent huang haosdent@gmail.com
Date: 29 September 2016 at 17:27:58 GMT+2
Committer: Alexander Rukletsov alexr@apache.org
Commit Date: 29 September 2016 at 18:30:31 GMT+2

Added test case HealthCheckTest.HealthyTaskViaHTTPWithoutType.

Review: https://reviews.apache.org/r/52301/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,
Potential FD double close in libevent's implementation of `sendfile`.,MESOS-6104,13001158,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,kaysoky,kaysoky,kaysoky,30/Aug/16 00:51,22/Mar/19 16:40,29/Oct/20 16:32,12/Sep/16 20:03,0.27.3,0.28.2,1.0.1,,,,,,0.28.3,1.0.2,1.1.0,,,,webui,,,,,0,mesosphere,ssl,,,,,,,"Repro copied from: https://reviews.apache.org/r/51509/

It is possible to make the master CHECK fail by repeatedly hitting the web UI and reloading the static assets:

1) Paste lots of text (16KB or more) of text into `src/webui/master/static/home.html`.  The more text, the more reliable the repro.

2) Start the master with SSL enabled:
{code}
LIBPROCESS_SSL_ENABLED=true LIBPROCESS_SSL_KEY_FILE=key.pem LIBPROCESS_SSL_CERT_FILE=cert.pem bin/mesos-master.sh --work_dir=/tmp/master
{code}

3) Run two instances of this python script repeatedly:
{code}
import socket
import ssl

s = ssl.wrap_socket(socket.socket())
s.connect((""localhost"", 5050))

s.sendall(""""""GET /static/home.html HTTP/1.1
User-Agent: foobar
Host: localhost:5050
Accept: */*
Connection: Keep-Alive

"""""")

# The HTTP part of the response
print s.recv(1000)
{code}

i.e. 
{code}
while python test.py; do :; done & while python test.py; do :; done
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6152,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-13 09:54:25.055,,,false,MESOS-7930,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 13 21:14:04 UTC 2016,,,,,,,"0|hzzza4:zzzr",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 41,Mesosphere Sprint 42,,,,,,,,,,3.0,,,,,,,,,,,"30/Aug/16 01:01;kaysoky;Added some fail-fast code here: https://reviews.apache.org/r/51511/","30/Aug/16 01:06;kaysoky;Note: Versions before 1.0.1 will not CHECK fail, as that CHECK was recently added.  The potential for double-close (and resulting undefined behavior) still exists though.","12/Sep/16 20:03;kaysoky;{code}
commit 00815ee93b69b1f93a4c8e42e0bc548a021804bc
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Mon Sep 12 12:40:09 2016 -0700

    Fixed potential FD double close in the libevent socket.
    
    `evbuffer_add_file` will take ownership of the file descriptor passed
    into it.  Normally, this file descriptor is owned by the `FileEncoder`
    in the libprocess's `SocketManager`.  Since there are two owners, one
    of the owners may close the file descriptor when it has been re-used.
    
    In this case, when multiple threads access the master's web UI at once
    with SSL enabled, the master may CHECK-fail due to a bad (closed)
    file descriptor.
    
    Review: https://reviews.apache.org/r/51509/
{code}","13/Sep/16 09:54;bbannier;Please make sure to also backport MESOS-6152 when backporting this fix.","13/Sep/16 21:14;kaysoky;{code}
commit 0221ad8c59ab2f682a087810f21cbf98022e904a
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Tue Sep 13 14:11:50 2016 -0700

    Added check in FileEncoder's destructor.
    
    This check will force libprocess to fail fast if a file descriptor
    is closed underneath it.
    
    Review: https://reviews.apache.org/r/51511/
{code}",,,,,,,,,,,,,,,,,,,,,,,
Unable to launch containers on CNI networks on CoreOS,MESOS-6052,12997965,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,avinash.mesos,avinash.mesos,avinash.mesos,17/Aug/16 17:03,30/Jan/17 18:36,29/Oct/20 16:32,07/Sep/16 23:09,1.0.0,,,,,,,,1.0.3,1.1.0,,,,,containerization,,,,,0,mesosphere,,,,,,,,"CoreOS does not have an `/etc/hosts`. Currently, in the `network/cni` isolator, if we don't see a `/etc/hosts` on the host filesystem we don't bind mount the containers `hosts` file to this target for the `command executor`. On distros such as CoreOS this fails the container launch since the `libprocess` initialization of the `command executor` fails cause it can't resolve its `hostname`.

We should be creating the `/etc/hosts` and `/etc/hostname` files when they are absent on the host filesystem since creating these files should not affect name resolution on the host network namespace, and it will allow the `/etc/hosts` file to be bind mounted correctly and allow name resolution in the containers network namespace as well. ",Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-07 23:09:06.317,,,false,MESOS-4641,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 25 19:04:52 UTC 2017,,,,,,,"0|hzzza4:zzv",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 41,Mesosphere Sprint 42,,,,,,,,,,1.0,,1.0.3,,,,,,,,,"06/Sep/16 21:00;avinash.mesos;https://reviews.apache.org/r/51643/","07/Sep/16 23:09;kaysoky;{code}
commit 3e52a107c4073778de9c14bf5fcdeb6e342821aa
Author: Avinash sridharan avinash@mesosphere.io
Date:   Wed Sep 7 13:30:53 2016 -0700

Modified network file setup in `network/cni` isolator.

In case /etc/hosts and /etc/hostname files are not present in the host
filesystem, we were ignoring these files and assuming that they would
not be required by the executor when it is launched in a new network
namespace. This assumption is incorrect, since the executor needs
/etc/hosts in the new network namespace to resolve its hostname.
Hence, we are explicitly creating these files in the host file system
in case they are not present, so that containers /etc/hosts and
/etc/hostname can be mounted on these mount points. This solves the
problem in distributions such as CoreOS that don't have /etc/hosts in
their host filesystem.

Review: https://reviews.apache.org/r/51643/
{code}","25/Jan/17 19:04;vinodkone;Backported to 1.0.3.

commit 23da4b9418a61e65b8ab5357c57ad9a3bd4d7fe9
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Wed Sep 7 13:30:53 2016 -0700

    Modified network file setup in `network/cni` isolator.
    
    In case /etc/hosts and /etc/hostname files are not present in the host
    filesystem, we were ignoring these files and assuming that they would
    not be required by the executor when it is launched in a new network
    namespace. This assumption is incorrect, since the executor needs
    /etc/hosts in the new network namespace to resolve its hostname.
    Hence, we are explicitly creating these files in the host file system
    in case they are not present, so that containers /etc/hosts and
    /etc/hostname can be mounted on these mount points. This solves the
    problem in distributions such as CoreOS that don't have /etc/hosts in
    their host filesystem.
    
    Review: https://reviews.apache.org/r/51643/
",,,,,,,,,,,,,,,,,,,,,,,,,
Aufs backend cannot support the image with numerous layers.,MESOS-6001,12995375,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,gilbert,gilbert,gilbert,05/Aug/16 22:59,07/Feb/17 18:19,29/Oct/20 16:32,26/Jan/17 02:04,,,,,,,,,1.2.0,,,,,,containerization,,,,,0,aufs,backend,containerizer,,,,,,"This issue was exposed in this unit test `ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller` by manually specifying the `bind` backend. Most likely mounting the aufs with specific options is limited by string length.

{noformat}
[20:13:07] :	 [Step 10/10] [ RUN      ] DockerRuntimeIsolatorTest.ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.615844 23416 cluster.cpp:155] Creating default 'local' authorizer
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.624106 23416 leveldb.cpp:174] Opened db in 8.148813ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627252 23416 leveldb.cpp:181] Compacted db in 3.126629ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627275 23416 leveldb.cpp:196] Created db iterator in 4410ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627282 23416 leveldb.cpp:202] Seeked to beginning of db in 763ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627287 23416 leveldb.cpp:271] Iterated through 0 keys in the db in 491ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627301 23416 replica.cpp:776] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627563 23434 recover.cpp:451] Starting replica recovery
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.627800 23437 recover.cpp:477] Replica is in EMPTY status
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628113 23431 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from __req_res__(5852)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628243 23430 recover.cpp:197] Received a recover response from a replica in EMPTY status
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628365 23437 recover.cpp:568] Updating replica status to STARTING
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628744 23432 master.cpp:375] Master dd755a55-0dd1-4d2d-9a49-812a666015cb (ip-172-30-2-138.mesosphere.io) started on 172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628758 23432 master.cpp:377] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/OZHDIQ/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/OZHDIQ/master"" --zk_session_timeout=""10secs""
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628893 23432 master.cpp:427] Master only allowing authenticated frameworks to register
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628900 23432 master.cpp:441] Master only allowing authenticated agents to register
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628902 23432 master.cpp:454] Master only allowing authenticated HTTP frameworks to register
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628906 23432 credentials.hpp:37] Loading credentials for authentication from '/tmp/OZHDIQ/credentials'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.628999 23432 master.cpp:499] Using default 'crammd5' authenticator
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629041 23432 http.cpp:883] Using default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629114 23432 http.cpp:883] Using default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629166 23432 http.cpp:883] Using default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629231 23432 master.cpp:579] Authorization enabled
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629290 23434 whitelist_watcher.cpp:77] No whitelist given
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629302 23430 hierarchical.cpp:151] Initialized hierarchical allocator process
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629921 23433 master.cpp:1851] Elected as the leading master!
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629933 23433 master.cpp:1547] Recovering from registrar
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.629992 23436 registrar.cpp:332] Recovering registrar
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.630861 23435 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.358536ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.630877 23435 replica.cpp:320] Persisted replica status to STARTING
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.630924 23435 recover.cpp:477] Replica is in STARTING status
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.631178 23435 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from __req_res__(5853)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.631285 23435 recover.cpp:197] Received a recover response from a replica in STARTING status
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.631433 23436 recover.cpp:568] Updating replica status to VOTING
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.633391 23433 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.912156ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.633409 23433 replica.cpp:320] Persisted replica status to VOTING
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.633438 23433 recover.cpp:582] Successfully joined the Paxos group
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.633479 23433 recover.cpp:466] Recover process terminated
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.633635 23435 log.cpp:553] Attempting to start the writer
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.634021 23432 replica.cpp:493] Replica received implicit promise request from __req_res__(5854)@172.30.2.138:44256 with proposal 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.636034 23432 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.995908ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.636049 23432 replica.cpp:342] Persisted promised to 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.636239 23432 coordinator.cpp:238] Coordinator attempting to fill missing positions
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.636672 23432 replica.cpp:388] Replica received explicit promise request from __req_res__(5855)@172.30.2.138:44256 for position 0 with proposal 2
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.637307 23432 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 614745ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.637318 23432 replica.cpp:708] Persisted action NOP at position 0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.637668 23432 replica.cpp:537] Replica received write request for position 0 from __req_res__(5856)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.637692 23432 leveldb.cpp:436] Reading position from leveldb took 10680ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.638314 23432 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 610038ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.638325 23432 replica.cpp:708] Persisted action NOP at position 0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.638569 23436 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.640446 23436 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.856131ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.640461 23436 replica.cpp:708] Persisted action NOP at position 0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.640645 23437 log.cpp:569] Writer started with ending position 0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.640940 23430 leveldb.cpp:436] Reading position from leveldb took 11341ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.641152 23430 registrar.cpp:365] Successfully fetched the registry (0B) in 11.14496ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.641185 23430 registrar.cpp:464] Applied 1 operations in 5010ns; attempting to update the registry
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.641381 23434 log.cpp:577] Attempting to append 209 bytes to the log
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.641425 23430 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.641706 23434 replica.cpp:537] Replica received write request for position 1 from __req_res__(5857)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.642320 23434 leveldb.cpp:341] Persisting action (228 bytes) to leveldb took 596016ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.642333 23434 replica.cpp:708] Persisted action APPEND at position 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.642608 23435 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644492 23435 leveldb.cpp:341] Persisting action (230 bytes) to leveldb took 1.868216ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644507 23435 replica.cpp:708] Persisted action APPEND at position 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644716 23432 registrar.cpp:509] Successfully updated the registry in 3.512064ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644759 23432 registrar.cpp:395] Successfully recovered registrar
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644811 23431 log.cpp:596] Attempting to truncate the log to 1
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644879 23433 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644949 23430 master.cpp:1655] Recovered 0 agents from the registry (170B); allowing 10mins for agents to re-register
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.644959 23437 hierarchical.cpp:178] Skipping recovery of hierarchical allocator: nothing to recover
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.645247 23431 replica.cpp:537] Replica received write request for position 2 from __req_res__(5858)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.645884 23431 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 618643ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.645896 23431 replica.cpp:708] Persisted action TRUNCATE at position 2
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.646080 23437 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.648093 23437 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.995217ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.648118 23437 leveldb.cpp:399] Deleting ~1 keys from leveldb took 10026ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.648125 23437 replica.cpp:708] Persisted action TRUNCATE at position 2
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.649564 23416 containerizer.cpp:200] Using isolation: docker/runtime,filesystem/linux,network/cni
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.652878 23416 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[20:13:07]W:	 [Step 10/10] E0805 20:13:07.656265 23416 shell.hpp:106] Command 'hadoop version 2>&1' failed; this is the output:
[20:13:07]W:	 [Step 10/10] sh: 1: hadoop: not found
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.656286 23416 fetcher.cpp:62] Skipping URI fetcher plugin 'hadoop' as it could not be created: Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.656338 23416 registry_puller.cpp:111] Creating registry puller with docker registry 'https://registry-1.docker.io'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.657330 23416 linux.cpp:148] Bind mounting '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn' and making it a shared mount
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663147 23416 cluster.cpp:434] Creating default 'local' authorizer
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663566 23436 slave.cpp:198] Mesos agent started on (506)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663583 23436 slave.cpp:199] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/OZHDIQ/store"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/http_credentials"" --image_providers=""docker"" --initialize_driver_logging=""true"" --isolation=""docker/runtime,filesystem/linux"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn""
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663796 23436 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/credential'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663868 23436 slave.cpp:336] Agent using credential for: test-principal
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663882 23436 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/http_credentials'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.663969 23436 http.cpp:883] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664010 23436 http.cpp:883] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664225 23416 sched.cpp:226] Version: 1.1.0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664423 23435 sched.cpp:330] New master detected at master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664451 23435 sched.cpp:396] Authenticating with master master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664428 23436 slave.cpp:519] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664458 23435 sched.cpp:403] Using default CRAM-MD5 authenticatee
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664463 23436 slave.cpp:527] Agent attributes: [  ]
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664470 23436 slave.cpp:532] Agent hostname: ip-172-30-2-138.mesosphere.io
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664588 23437 authenticatee.cpp:121] Creating new client SASL connection
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664810 23437 master.cpp:5900] Authenticating scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664873 23437 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1028)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.664939 23432 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/meta'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665006 23431 authenticator.cpp:98] Creating new server SASL connection
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665024 23435 status_update_manager.cpp:203] Recovering status update manager
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665174 23434 containerizer.cpp:527] Recovering containerizer
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665201 23431 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665221 23431 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665266 23431 authenticator.cpp:204] Received SASL authentication start
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665303 23431 authenticator.cpp:326] Authentication requires more steps
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665347 23431 authenticatee.cpp:259] Received SASL authentication step
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665436 23431 authenticator.cpp:232] Received SASL authentication step
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665457 23431 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-138.mesosphere.io' server FQDN: 'ip-172-30-2-138.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665465 23431 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665482 23431 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665494 23431 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-138.mesosphere.io' server FQDN: 'ip-172-30-2-138.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665503 23431 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665510 23431 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665524 23431 authenticator.cpp:318] Authentication success
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665575 23436 authenticatee.cpp:299] Authentication success
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665596 23435 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1028)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665624 23431 master.cpp:5930] Successfully authenticated principal 'test-principal' at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665705 23436 sched.cpp:502] Successfully authenticated with master master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665715 23436 sched.cpp:820] Sending SUBSCRIBE call to master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665751 23436 sched.cpp:853] Will retry registration in 188.601026ms if necessary
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665796 23437 master.cpp:2425] Received SUBSCRIBE call for framework 'default' at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665817 23437 master.cpp:1887] Authorizing framework principal 'test-principal' to receive offers for role '*'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.665998 23430 master.cpp:2501] Subscribing framework default with checkpointing disabled and capabilities [  ]
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666132 23432 hierarchical.cpp:271] Added framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666148 23434 sched.cpp:743] Framework registered with dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666154 23432 hierarchical.cpp:1548] No allocations performed
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666173 23432 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666177 23434 sched.cpp:757] Scheduler::registered took 11084ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666189 23432 hierarchical.cpp:1192] Performed allocation for 0 agents in 43102ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666486 23431 metadata_manager.cpp:205] No images to load from disk. Docker provisioner image storage path '/tmp/OZHDIQ/store/storedImages' does not exist
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666558 23436 provisioner.cpp:255] Provisioner recovery complete
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666677 23435 slave.cpp:4872] Finished recovery
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666831 23435 slave.cpp:5044] Querying resource estimator for oversubscribable resources
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666919 23435 slave.cpp:895] New master detected at master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666929 23435 slave.cpp:954] Authenticating with master master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666931 23436 status_update_manager.cpp:177] Pausing sending status updates
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666944 23435 slave.cpp:965] Using default CRAM-MD5 authenticatee
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.666982 23435 slave.cpp:927] Detecting new master
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667006 23431 authenticatee.cpp:121] Creating new client SASL connection
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667014 23435 slave.cpp:5058] Received oversubscribable resources  from the resource estimator
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667162 23431 master.cpp:5900] Authenticating slave(506)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667225 23434 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1029)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667275 23434 authenticator.cpp:98] Creating new server SASL connection
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667418 23434 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667436 23434 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667492 23436 authenticator.cpp:204] Received SASL authentication start
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667515 23436 authenticator.cpp:326] Authentication requires more steps
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667546 23436 authenticatee.cpp:259] Received SASL authentication step
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667592 23436 authenticator.cpp:232] Received SASL authentication step
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667603 23436 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-138.mesosphere.io' server FQDN: 'ip-172-30-2-138.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667610 23436 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667619 23436 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667630 23436 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-138.mesosphere.io' server FQDN: 'ip-172-30-2-138.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667639 23436 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667642 23436 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667652 23436 authenticator.cpp:318] Authentication success
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667688 23436 authenticatee.cpp:299] Authentication success
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667713 23432 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1029)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667733 23434 master.cpp:5930] Successfully authenticated principal 'test-principal' at slave(506)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667783 23437 slave.cpp:1049] Successfully authenticated with master master@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667836 23437 slave.cpp:1455] Will retry registration in 4.197236ms if necessary
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.667901 23436 master.cpp:4554] Registering agent at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io) with id dd755a55-0dd1-4d2d-9a49-812a666015cb-S0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.668021 23430 registrar.cpp:464] Applied 1 operations in 13306ns; attempting to update the registry
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.668269 23433 log.cpp:577] Attempting to append 395 bytes to the log
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.668329 23434 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.668622 23433 replica.cpp:537] Replica received write request for position 3 from __req_res__(5859)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.669297 23433 leveldb.cpp:341] Persisting action (414 bytes) to leveldb took 658552ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.669309 23433 replica.cpp:708] Persisted action APPEND at position 3
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.669589 23432 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.672566 23432 leveldb.cpp:341] Persisting action (416 bytes) to leveldb took 2.962622ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.672580 23432 replica.cpp:708] Persisted action APPEND at position 3
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.672866 23435 registrar.cpp:509] Successfully updated the registry in 4.822784ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.672936 23434 log.cpp:596] Attempting to truncate the log to 3
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673001 23437 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673110 23436 master.cpp:4623] Registered agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673152 23432 hierarchical.cpp:478] Added agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 (ip-172-30-2-138.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673174 23430 slave.cpp:3739] Received ping from slave-observer(465)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673254 23430 slave.cpp:1095] Registered with master master@172.30.2.138:44256; given agent ID dd755a55-0dd1-4d2d-9a49-812a666015cb-S0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673266 23430 fetcher.cpp:86] Clearing fetcher cache
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673288 23433 replica.cpp:537] Replica received write request for position 4 from __req_res__(5860)@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673317 23432 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673333 23432 hierarchical.cpp:1215] Performed allocation for agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 in 160981ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673358 23432 status_update_manager.cpp:184] Resuming sending status updates
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673435 23437 master.cpp:5729] Sending 1 offers to framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673467 23430 slave.cpp:1118] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/meta/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/slave.info'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673566 23437 sched.cpp:917] Scheduler::resourceOffers took 40919ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673607 23430 slave.cpp:1155] Forwarding total oversubscribed resources 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673710 23437 master.cpp:5006] Received update of agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io) with total oversubscribed resources 
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673781 23437 hierarchical.cpp:542] Agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 (ip-172-30-2-138.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673823 23437 hierarchical.cpp:1548] No allocations performed
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673830 23437 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.673838 23437 hierarchical.cpp:1215] Performed allocation for agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 in 31940ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674163 23435 master.cpp:3346] Processing ACCEPT call for offers: [ dd755a55-0dd1-4d2d-9a49-812a666015cb-O0 ] on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io) for framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674186 23435 master.cpp:2981] Authorizing framework principal 'test-principal' to launch task ecd0633f-2f1e-4cfa-819f-590bfb95fa12
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674538 23437 master.cpp:7451] Adding task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 with resources cpus(*):1; mem(*):128 on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 (ip-172-30-2-138.mesosphere.io)
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674564 23437 master.cpp:3835] Launching task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256 with resources cpus(*):1; mem(*):128 on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io)
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674665 23430 slave.cpp:1495] Got assigned task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 for framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674713 23436 hierarchical.cpp:924] Recovered cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):128) on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 from framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674736 23436 hierarchical.cpp:961] Framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 filtered agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 for 5secs
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.674866 23430 slave.cpp:1614] Launching task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 for framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.675107 23430 paths.cpp:536] Trying to chown '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12/runs/f2c1fd6d-4d11-45cd-a916-e4d73d226451' to user 'root'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.678246 23433 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 4.916164ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.678267 23433 replica.cpp:708] Persisted action TRUNCATE at position 4
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.678629 23436 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679050 23430 slave.cpp:5764] Launching executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12/runs/f2c1fd6d-4d11-45cd-a916-e4d73d226451'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679200 23430 slave.cpp:1840] Queuing task 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' for executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679219 23437 containerizer.cpp:786] Starting container 'f2c1fd6d-4d11-45cd-a916-e4d73d226451' for executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679234 23430 slave.cpp:848] Successfully attached file '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12/runs/f2c1fd6d-4d11-45cd-a916-e4d73d226451'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679435 23430 metadata_manager.cpp:167] Looking for image 'mesosphere/inky'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.679572 23430 registry_puller.cpp:236] Pulling image 'mesosphere/inky' from 'docker-manifest://registry-1.docker.io:443mesosphere/inky?latest#https' to '/tmp/OZHDIQ/store/staging/HbsybX'
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.680943 23436 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.14361ms
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.681073 23436 leveldb.cpp:399] Deleting ~2 keys from leveldb took 60273ns
[20:13:07]W:	 [Step 10/10] I0805 20:13:07.681112 23436 replica.cpp:708] Persisted action TRUNCATE at position 4
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104004 23431 registry_puller.cpp:259] The manifest for image 'mesosphere/inky' is '{
[20:13:08]W:	 [Step 10/10]    ""name"": ""mesosphere/inky"",
[20:13:08]W:	 [Step 10/10]    ""tag"": ""latest"",
[20:13:08]W:	 [Step 10/10]    ""architecture"": ""amd64"",
[20:13:08]W:	 [Step 10/10]    ""fsLayers"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""blobSum"": ""sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ],
[20:13:08]W:	 [Step 10/10]    ""history"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6\"",\""parent\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""created\"":\""2014-08-15T00:31:36.407713553Z\"",\""container\"":\""5d55401ff99c7508c9d546926b711c78e3ccb36e39a848024b623b2aef4c2c06\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ENTRYPOINT [echo]\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":[\""echo\""],\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e\"",\""parent\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""created\"":\""2014-08-15T00:31:36.247988044Z\"",\""container\"":\""ff756d99367825677c3c18cc5054bfbb3674a7f52a9f916282fb46b8feaddfb7\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) CMD [inky]\""],\""Image\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""inky\""],\""Image\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f\"",\""parent\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""created\"":\""2014-08-15T00:31:36.068514721Z\"",\""container\"":\""696c3d66c8575dfff3ba71267bf194ae97f0478231042449c98aa0d9164d3c8c\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) MAINTAINER support@mesosphere.io\""],\""Image\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""1.1.2\"",\""author\"":\""support@mesosphere.io\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\""],\""Image\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\"",\""parent\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""created\"":\""2014-06-05T00:05:35.990887725Z\"",\""container\"":\""bb3475b3130b6a47104549a0291a6569d24e41fa57a7f094591f0d4611fd15bc\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) CMD [/bin/sh]\""],\""Image\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\""],\""Image\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16\"",\""parent\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""created\"":\""2014-06-05T00:05:35.692528634Z\"",\""container\"":\""fc203791c4d5024b1a976223daa1cc7b1ceeb5b3abf25a2fb73034eba6398026\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ADD file:88f36b32456f849299e5df807a1e3514cf1da798af9692a0004598e500be5901 in /\""],\""Image\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":null,\""Image\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":2433303}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229\"",\""parent\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""created\"":\""2014-06-05T00:05:35.589531476Z\"",\""container\"":\""f7d939e68b5afdd74637d9204c40fe00295e658923be395c761da3278b98e446\"",\""container_config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) MAINTAINER Jrme Petazzoni \\u003cjerome@docker.com\\u003e\""],\""Image\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""docker_version\"":\""0.10.0\"",\""author\"":\""Jrme Petazzoni \\u003cjerome@docker.com\\u003e\"",\""config\"":{\""Hostname\"":\""f7d939e68b5a\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":[\""HOME=/\"",\""PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\""],\""Cmd\"":null,\""Image\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":[],\""Labels\"":null},\""architecture\"":\""amd64\"",\""os\"":\""linux\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       },
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""id\"":\""511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"",\""comment\"":\""Imported from -\"",\""created\"":\""2013-06-13T14:03:50.821769-07:00\"",\""container_config\"":{\""Hostname\"":\""\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""PortSpecs\"":null,\""ExposedPorts\"":null,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":null,\""Cmd\"":null,\""Image\"":\""\"",\""Volumes\"":null,\""VolumeDriver\"":\""\"",\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""NetworkDisabled\"":false,\""MacAddress\"":\""\"",\""OnBuild\"":null,\""Labels\"":null},\""docker_version\"":\""0.4.0\"",\""architecture\"":\""x86_64\"",\""Size\"":0}\n""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ],
[20:13:08]W:	 [Step 10/10]    ""schemaVersion"": 1,
[20:13:08]W:	 [Step 10/10]    ""signatures"": [
[20:13:08]W:	 [Step 10/10]       {
[20:13:08]W:	 [Step 10/10]          ""header"": {
[20:13:08]W:	 [Step 10/10]             ""jwk"": {
[20:13:08]W:	 [Step 10/10]                ""crv"": ""P-256"",
[20:13:08]W:	 [Step 10/10]                ""kid"": ""4AYN:KH32:GJJD:I6BX:SJAZ:A3EC:P7IC:7O7C:22ZQ:3Z5O:75VQ:3QOT"",
[20:13:08]W:	 [Step 10/10]                ""kty"": ""EC"",
[20:13:08]W:	 [Step 10/10]                ""x"": ""o8bvrUwNpXKZdgoo2wQ7EHQzCVYhVuoOvjqGEXtRylU"",
[20:13:08]W:	 [Step 10/10]                ""y"": ""DCHyGr0Cbi-fZzqypQm16qKfefUMqCTk0rQME-q5GmA""
[20:13:08]W:	 [Step 10/10]             },
[20:13:08]W:	 [Step 10/10]             ""alg"": ""ES256""
[20:13:08]W:	 [Step 10/10]          },
[20:13:08]W:	 [Step 10/10]          ""signature"": ""f3fAob4XPT0pUW9TiPtxAE_zPAe0PdM2imxAeaCmJbBf6Lb-SuFPVGE4iqz1CO0VOijeYVuB1G1lv_a5Nnj5zg"",
[20:13:08]W:	 [Step 10/10]          ""protected"": ""eyJmb3JtYXRMZW5ndGgiOjEzNzA3LCJmb3JtYXRUYWlsIjoiQ24wIiwidGltZSI6IjIwMTYtMDgtMDVUMjA6MTM6MDdaIn0""
[20:13:08]W:	 [Step 10/10]       }
[20:13:08]W:	 [Step 10/10]    ]
[20:13:08]W:	 [Step 10/10] }'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104116 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104130 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104138 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104146 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104151 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104158 23431 registry_puller.cpp:369] Fetching blob 'sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66' for layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104164 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.104171 23431 registry_puller.cpp:369] Fetching blob 'sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4' for layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' of image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.504564 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.507129 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.508962 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.510915 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.512848 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.515400 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.517390 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.519486 23436 registry_puller.cpp:306] Extracting layer tar ball '/tmp/OZHDIQ/store/staging/HbsybX/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/OZHDIQ/store/staging/HbsybX/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/rootfs'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.606955 23434 metadata_manager.cpp:155] Successfully cached image 'mesosphere/inky'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.607501 23436 provisioner.cpp:312] Provisioning image rootfs '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/rootfses/427b7851-bf82-4553-80f3-da2d42cede77' for container f2c1fd6d-4d11-45cd-a916-e4d73d226451 using the 'aufs' backend
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.607787 23434 aufs.cpp:152] Provisioning image rootfs with aufs: 'dirs=/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/scratch/427b7851-bf82-4553-80f3-da2d42cede77/workdir:/tmp/OZHDIQ/store/layers/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs:/tmp/OZHDIQ/store/layers/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs:/tmp/OZHDIQ/store/layers/be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e/rootfs:/tmp/OZHDIQ/store/layers/53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f/rootfs:/tmp/OZHDIQ/store/layers/a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721/rootfs:/tmp/OZHDIQ/store/layers/120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16/rootfs:/tmp/OZHDIQ/store/layers/42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229/rootfs:/tmp/OZHDIQ/store/layers/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/rootfs'
[20:13:08]W:	 [Step 10/10] E0805 20:13:08.614994 23432 slave.cpp:4029] Container 'f2c1fd6d-4d11-45cd-a916-e4d73d226451' for executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 failed to start: Failed to mount rootfs '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/rootfses/427b7851-bf82-4553-80f3-da2d42cede77' with aufs: Invalid argument
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.615058 23436 containerizer.cpp:1637] Destroying container 'f2c1fd6d-4d11-45cd-a916-e4d73d226451'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.615072 23436 containerizer.cpp:1640] Waiting for the provisioner to complete for container 'f2c1fd6d-4d11-45cd-a916-e4d73d226451'
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.615279 23435 provisioner.cpp:455] Destroying container rootfs at '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/provisioner/containers/f2c1fd6d-4d11-45cd-a916-e4d73d226451/backends/aufs/rootfses/427b7851-bf82-4553-80f3-da2d42cede77' for container f2c1fd6d-4d11-45cd-a916-e4d73d226451
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616097 23430 slave.cpp:4135] Executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 has terminated with unknown status
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616173 23430 slave.cpp:3264] Handling status update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 from @0.0.0.0:0
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616320 23435 slave.cpp:6104] Terminating task ecd0633f-2f1e-4cfa-819f-590bfb95fa12
[20:13:08]W:	 [Step 10/10] W0805 20:13:08.616402 23432 containerizer.cpp:1466] Ignoring update for unknown container: f2c1fd6d-4d11-45cd-a916-e4d73d226451
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616528 23433 status_update_manager.cpp:323] Received status update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616545 23433 status_update_manager.cpp:500] Creating StatusUpdate stream for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616750 23433 status_update_manager.cpp:377] Forwarding update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 to the agent
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616827 23431 slave.cpp:3657] Forwarding the update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 to master@172.30.2.138:44256
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.616936 23431 slave.cpp:3551] Status update manager successfully handled status update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617010 23433 master.cpp:5141] Status update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 from agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io)
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617032 23433 master.cpp:5203] Forwarding status update TASK_FAILED (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617079 23433 master.cpp:6845] Updating the state of task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617187 23435 sched.cpp:1025] Scheduler::statusUpdate took 57204ns
[20:13:08] :	 [Step 10/10] ../../src/tests/containerizer/runtime_isolator_tests.cpp:309: Failure
[20:13:08] :	 [Step 10/10] Value of: statusRunning->state()
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617234 23436 hierarchical.cpp:924] Recovered cpus(*):1; mem(*):128 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 from framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08] :	 [Step 10/10]   Actual: TASK_FAILED
[20:13:08] :	 [Step 10/10] Expected: TASK_RUNNING
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617281 23432 master.cpp:4266] Processing ACKNOWLEDGE call 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9 for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256 on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617311 23432 master.cpp:6911] Removing task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 with resources cpus(*):1; mem(*):128 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io)
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617450 23430 status_update_manager.cpp:395] Received status update acknowledgement (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617480 23430 status_update_manager.cpp:531] Cleaning up status update stream for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617545 23430 slave.cpp:2650] Status update manager successfully handled status update acknowledgement (UUID: 4a37d8ce-6c60-4f3e-97bd-ea9148be4ce9) for task ecd0633f-2f1e-4cfa-819f-590bfb95fa12 of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617561 23430 slave.cpp:6145] Completing task ecd0633f-2f1e-4cfa-819f-590bfb95fa12
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617575 23430 slave.cpp:4246] Cleaning up executor 'ecd0633f-2f1e-4cfa-819f-590bfb95fa12' of framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617660 23435 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12/runs/f2c1fd6d-4d11-45cd-a916-e4d73d226451' for gc 6.99999285160889days in the future
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617688 23430 slave.cpp:4334] Cleaning up framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617708 23435 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000/executors/ecd0633f-2f1e-4cfa-819f-590bfb95fa12' for gc 6.9999928509363days in the future
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617748 23434 status_update_manager.cpp:285] Closing status update streams for framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.617772 23434 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/DockerRuntimeIsolatorTest_ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller_CL0mhn/slaves/dd755a55-0dd1-4d2d-9a49-812a666015cb-S0/frameworks/dd755a55-0dd1-4d2d-9a49-812a666015cb-0000' for gc 6.99999285021926days in the future
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.630481 23432 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.630504 23432 hierarchical.cpp:1192] Performed allocation for 1 agents in 155186ns
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.630609 23430 master.cpp:5729] Sending 1 offers to framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:08]W:	 [Step 10/10] I0805 20:13:08.630728 23430 sched.cpp:917] Scheduler::resourceOffers took 13371ns
[20:13:09]W:	 [Step 10/10] I0805 20:13:09.631413 23437 hierarchical.cpp:1548] No allocations performed
[20:13:09]W:	 [Step 10/10] I0805 20:13:09.631450 23437 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:09]W:	 [Step 10/10] I0805 20:13:09.631465 23437 hierarchical.cpp:1192] Performed allocation for 1 agents in 202676ns
[20:13:10]W:	 [Step 10/10] I0805 20:13:10.631609 23435 hierarchical.cpp:1548] No allocations performed
[20:13:10]W:	 [Step 10/10] I0805 20:13:10.631640 23435 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:10]W:	 [Step 10/10] I0805 20:13:10.631655 23435 hierarchical.cpp:1192] Performed allocation for 1 agents in 102058ns
[20:13:11]W:	 [Step 10/10] I0805 20:13:11.632261 23431 hierarchical.cpp:1548] No allocations performed
[20:13:11]W:	 [Step 10/10] I0805 20:13:11.632294 23431 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:11]W:	 [Step 10/10] I0805 20:13:11.632308 23431 hierarchical.cpp:1192] Performed allocation for 1 agents in 112653ns
[20:13:12]W:	 [Step 10/10] I0805 20:13:12.632477 23433 hierarchical.cpp:1548] No allocations performed
[20:13:12]W:	 [Step 10/10] I0805 20:13:12.632510 23433 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:12]W:	 [Step 10/10] I0805 20:13:12.632525 23433 hierarchical.cpp:1192] Performed allocation for 1 agents in 144467ns
[20:13:13]W:	 [Step 10/10] I0805 20:13:13.633517 23430 hierarchical.cpp:1548] No allocations performed
[20:13:13]W:	 [Step 10/10] I0805 20:13:13.633549 23430 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:13]W:	 [Step 10/10] I0805 20:13:13.633563 23430 hierarchical.cpp:1192] Performed allocation for 1 agents in 111395ns
[20:13:14]W:	 [Step 10/10] I0805 20:13:14.633985 23436 hierarchical.cpp:1548] No allocations performed
[20:13:14]W:	 [Step 10/10] I0805 20:13:14.634018 23436 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:14]W:	 [Step 10/10] I0805 20:13:14.634048 23436 hierarchical.cpp:1192] Performed allocation for 1 agents in 132707ns
[20:13:15]W:	 [Step 10/10] I0805 20:13:15.634266 23430 hierarchical.cpp:1548] No allocations performed
[20:13:15]W:	 [Step 10/10] I0805 20:13:15.634299 23430 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:15]W:	 [Step 10/10] I0805 20:13:15.634313 23430 hierarchical.cpp:1192] Performed allocation for 1 agents in 103933ns
[20:13:16]W:	 [Step 10/10] I0805 20:13:16.635295 23431 hierarchical.cpp:1548] No allocations performed
[20:13:16]W:	 [Step 10/10] I0805 20:13:16.635330 23431 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:16]W:	 [Step 10/10] I0805 20:13:16.635346 23431 hierarchical.cpp:1192] Performed allocation for 1 agents in 115517ns
[20:13:17]W:	 [Step 10/10] I0805 20:13:17.635922 23436 hierarchical.cpp:1548] No allocations performed
[20:13:17]W:	 [Step 10/10] I0805 20:13:17.635958 23436 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:17]W:	 [Step 10/10] I0805 20:13:17.635973 23436 hierarchical.cpp:1192] Performed allocation for 1 agents in 109700ns
[20:13:18]W:	 [Step 10/10] I0805 20:13:18.636693 23437 hierarchical.cpp:1548] No allocations performed
[20:13:18]W:	 [Step 10/10] I0805 20:13:18.636728 23437 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:18]W:	 [Step 10/10] I0805 20:13:18.636744 23437 hierarchical.cpp:1192] Performed allocation for 1 agents in 123133ns
[20:13:19]W:	 [Step 10/10] I0805 20:13:19.637589 23432 hierarchical.cpp:1548] No allocations performed
[20:13:19]W:	 [Step 10/10] I0805 20:13:19.637624 23432 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:19]W:	 [Step 10/10] I0805 20:13:19.637639 23432 hierarchical.cpp:1192] Performed allocation for 1 agents in 118581ns
[20:13:20]W:	 [Step 10/10] I0805 20:13:20.638517 23431 hierarchical.cpp:1548] No allocations performed
[20:13:20]W:	 [Step 10/10] I0805 20:13:20.638550 23431 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:20]W:	 [Step 10/10] I0805 20:13:20.638566 23431 hierarchical.cpp:1192] Performed allocation for 1 agents in 107979ns
[20:13:21]W:	 [Step 10/10] I0805 20:13:21.639577 23435 hierarchical.cpp:1548] No allocations performed
[20:13:21]W:	 [Step 10/10] I0805 20:13:21.639612 23435 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:21]W:	 [Step 10/10] I0805 20:13:21.639628 23435 hierarchical.cpp:1192] Performed allocation for 1 agents in 126299ns
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.640533 23430 hierarchical.cpp:1548] No allocations performed
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.640566 23430 hierarchical.cpp:1643] No inverse offers to send out!
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.640581 23430 hierarchical.cpp:1192] Performed allocation for 1 agents in 106384ns
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.667985 23437 slave.cpp:5044] Querying resource estimator for oversubscribable resources
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.668124 23434 slave.cpp:5058] Received oversubscribable resources  from the resource estimator
[20:13:22]W:	 [Step 10/10] I0805 20:13:22.674278 23433 slave.cpp:3739] Received ping from slave-observer(465)@172.30.2.138:44256
[20:13:23] :	 [Step 10/10] ../../src/tests/containerizer/runtime_isolator_tests.cpp:311: Failure
[20:13:23] :	 [Step 10/10] Failed to wait 15secs for statusFinished
[20:13:23] :	 [Step 10/10] ../../src/tests/containerizer/runtime_isolator_tests.cpp:301: Failure
[20:13:23] :	 [Step 10/10] Actual function call count doesn't match EXPECT_CALL(sched, statusUpdate(&driver, _))...
[20:13:23] :	 [Step 10/10]          Expected: to be called twice
[20:13:23] :	 [Step 10/10]            Actual: called once - unsatisfied and active
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.618680 23433 master.cpp:1284] Framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256 disconnected
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.618721 23433 master.cpp:2726] Disconnecting framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.618737 23433 master.cpp:2750] Deactivating framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.618883 23434 hierarchical.cpp:382] Deactivated framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:23]W:	 [Step 10/10] W0805 20:13:23.618918 23433 master.hpp:2131] Master attempted to send message to disconnected framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.618963 23433 master.cpp:1297] Giving framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256 0ns to failover
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619046 23434 hierarchical.cpp:924] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 from framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619258 23416 slave.cpp:767] Agent terminating
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619321 23432 master.cpp:1245] Agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io) disconnected
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619336 23432 master.cpp:2785] Disconnecting agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io)
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619371 23432 master.cpp:2804] Deactivating agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 at slave(506)@172.30.2.138:44256 (ip-172-30-2-138.mesosphere.io)
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.619431 23432 hierarchical.cpp:571] Agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0 deactivated
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.620216 23435 master.cpp:5581] Framework failover timeout, removing framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.620232 23435 master.cpp:6316] Removing framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000 (default) at scheduler-893e3efc-6e25-48f3-a487-d2ef50ffd5ba@172.30.2.138:44256
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.620357 23433 hierarchical.cpp:333] Removed framework dd755a55-0dd1-4d2d-9a49-812a666015cb-0000
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.621464 23416 master.cpp:1092] Master terminating
[20:13:23]W:	 [Step 10/10] I0805 20:13:23.621561 23433 hierarchical.cpp:510] Removed agent dd755a55-0dd1-4d2d-9a49-812a666015cb-S0
[20:13:23] :	 [Step 10/10] [  FAILED  ] DockerRuntimeIsolatorTest.ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller (16012 ms)
{noformat}","Ubuntu 14, Ubuntu 12
Or any other os with aufs module",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6000,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-31 19:22:40.044,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 07 18:17:39 UTC 2017,,,,,,,"0|hzzz3w:v",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 47,Mesosphere Sprint 48,Mesosphere Sprint 50,,,,,,,,,3.0,,1.2.0,,,,,,,,,"30/Nov/16 18:59;gilbert;https://reviews.apache.org/r/54213/
https://reviews.apache.org/r/54214/","26/Jan/17 02:04;gilbert;commit dd0da9e8982cd65c5178d872ae8723f5f7db09e4
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jan 25 07:51:44 2017 -0800

    Added unit test for aufs backend supporting many layers.
    
    Review: https://reviews.apache.org/r/54214/

commit 37f2e7d15168e8d9fa76b4fcd61a6c0c6c7c8370
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jan 25 07:51:39 2017 -0800

    Supported more layers through symlink for aufs backend.
    
    This issue is similar to MESOS-6000. We use the same solution
    (using symlinks) to resolve many-layer image issue with aufs
    backend.
    
    Review: https://reviews.apache.org/r/54213/","31/Jan/17 19:22;vinodkone;Removing the 1.0.3 target version per conversation with [~jieyu]
","07/Feb/17 18:17;alexr;[~gilbert], [~jieyu], [~tillt] This has not been backported to 1.1.1, nor to 1.0.3, hence I'm removing 1.1.1 from target versions.",,,,,,,,,,,,,,,,,,,,,,,,
Re-evaluate libevent SSL socket EOF semantics in libprocess,MESOS-5999,12995363,Bug,Open,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,greggomann,greggomann,05/Aug/16 22:16,02/Apr/20 16:51,29/Oct/20 16:32,,,,,,,,,,,,,,,,libprocess,,,,,0,foundations,mesosphere,,,,,,,"While debugging some issues related to libprocess finalization/reinitialization, [~bmahler] pointed out that libprocess doesn't strictly adhere to the expected behavior of Unix sockets after an EOF is received. If a socket receives EOF, this means only that the writer on the other end has closed the write end of its socket. However, the other end may still be interested in reading. Libprocess currently treats a received EOF as if {{shutdown()}} has been called on the socket, and both ends have been closed for both reading and writing (see [here|https://github.com/apache/mesos/blob/1.0.0/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L349-L360] and [here|https://github.com/apache/mesos/blob/1.0.0/3rdparty/libprocess/src/process.cpp#L692-L697]).

We should consider changing the EOF semantics of the {{Socket}} object to more closely match those of Unix sockets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-10108,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-08-05 22:16:00.0,,,,,,,"0|i31zlb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSL Socket CHECK can fail after socket receives EOF,MESOS-5986,12994955,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,greggomann,greggomann,greggomann,04/Aug/16 16:30,22/Mar/19 16:40,29/Oct/20 16:32,08/Aug/16 18:18,1.0.0,,,,,,,,0.28.3,1.0.1,,,,,libprocess,,,,,0,mesosphere,,,,,,,,"While writing a test for MESOS-3753, I encountered a bug where [this check|https://github.com/apache/mesos/blob/853821cafcca3550b9c7bdaba5262d73869e2ee1/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L708] fails at the very end of the test body, while objects in the stack frame are being destroyed. After adding some debug logging output, I produced the following:
{code}
I0804 08:32:33.263211 273793024 libevent_ssl_socket.cpp:681] *** in send()17
I0804 08:32:33.263209 273256448 process.cpp:2970] Cleaning up __limiter__(3)@127.0.0.1:55688
I0804 08:32:33.263263 275939328 libevent_ssl_socket.cpp:152] *** in initialize(): 14
I0804 08:32:33.263206 272719872 process.cpp:2865] Resuming (61)@127.0.0.1:55688 at 2016-08-04 15:32:33.263261952+00:00
I0804 08:32:33.263327 275939328 libevent_ssl_socket.cpp:584] *** in recv()14
I0804 08:32:33.263337 272719872 hierarchical.cpp:571] Agent e2a49340-34ec-403f-a5a4-15e29c4a2434-S0 deactivated
I0804 08:32:33.263322 275402752 process.cpp:2865] Resuming help@127.0.0.1:55688 at 2016-08-04 15:32:33.263343104+00:00
I0804 08:32:33.263510 275939328 libevent_ssl_socket.cpp:322] *** in event_callback(bev)
I0804 08:32:33.263536 275939328 libevent_ssl_socket.cpp:353] *** in event_callback check for EOF/CONNECTED/ERROR: 19
I0804 08:32:33.263592 275939328 libevent_ssl_socket.cpp:159] *** in shutdown(): 19
I0804 08:32:33.263622 1985901312 process.cpp:3170] Donating thread to (87)@127.0.0.1:55688 while waiting
I0804 08:32:33.263639 274329600 process.cpp:2865] Resuming __http__(12)@127.0.0.1:55688 at 2016-08-04 15:32:33.263653888+00:00
I0804 08:32:33.263659 1985901312 process.cpp:2865] Resuming (87)@127.0.0.1:55688 at 2016-08-04 15:32:33.263671040+00:00
I0804 08:32:33.263730 1985901312 process.cpp:2970] Cleaning up (87)@127.0.0.1:55688
I0804 08:32:33.263741 275939328 libevent_ssl_socket.cpp:322] *** in event_callback(bev)
I0804 08:32:33.263736 274329600 process.cpp:2970] Cleaning up __http__(12)@127.0.0.1:55688
I0804 08:32:33.263778 275939328 libevent_ssl_socket.cpp:353] *** in event_callback check for EOF/CONNECTED/ERROR: 17
I0804 08:32:33.263818 275939328 libevent_ssl_socket.cpp:159] *** in shutdown(): 17
I0804 08:32:33.263839 272183296 process.cpp:2865] Resuming help@127.0.0.1:55688 at 2016-08-04 15:32:33.263857920+00:00
I0804 08:32:33.263933 273793024 process.cpp:2865] Resuming __gc__@127.0.0.1:55688 at 2016-08-04 15:32:33.263951104+00:00
I0804 08:32:33.264034 275939328 libevent_ssl_socket.cpp:681] *** in send()17
I0804 08:32:33.264020 272719872 process.cpp:2865] Resuming __http__(11)@127.0.0.1:55688 at 2016-08-04 15:32:33.264041984+00:00
I0804 08:32:33.264036 274329600 process.cpp:2865] Resuming status-update-manager(3)@127.0.0.1:55688 at 2016-08-04 15:32:33.264056064+00:00
I0804 08:32:33.264071 272719872 process.cpp:2970] Cleaning up __http__(11)@127.0.0.1:55688
I0804 08:32:33.264088 274329600 process.cpp:2970] Cleaning up status-update-manager(3)@127.0.0.1:55688
I0804 08:32:33.264086 275939328 libevent_ssl_socket.cpp:721] *** sending on socket: 17, data: 0

I0804 08:32:33.264112 272183296 process.cpp:2865] Resuming (89)@127.0.0.1:55688 at 2016-08-04 15:32:33.264126976+00:00
I0804 08:32:33.264118 275402752 process.cpp:2865] Resuming help@127.0.0.1:55688 at 2016-08-04 15:32:33.264144896+00:00
I0804 08:32:33.264149 272183296 process.cpp:2970] Cleaning up (89)@127.0.0.1:55688
I0804 08:32:33.264202 275939328 libevent_ssl_socket.cpp:281] *** in send_callback(bev)
I0804 08:32:33.264400 273793024 process.cpp:3170] Donating thread to (86)@127.0.0.1:55688 while waiting
I0804 08:32:33.264413 273256448 process.cpp:2865] Resuming (76)@127.0.0.1:55688 at 2016-08-04 15:32:33.264428032+00:00
I0804 08:32:33.296268 275939328 libevent_ssl_socket.cpp:300] *** in send_callback(): 17
I0804 08:32:33.296419 273256448 process.cpp:2970] Cleaning up (76)@127.0.0.1:55688
I0804 08:32:33.296357 273793024 process.cpp:2865] Resuming (86)@127.0.0.1:55688 at 2016-08-04 15:32:33.296414976+00:00
I0804 08:32:33.296464 273793024 process.cpp:2970] Cleaning up (86)@127.0.0.1:55688
I0804 08:32:33.296497 275939328 libevent_ssl_socket.cpp:104] *** releasing SSL socket
I0804 08:32:33.296517 275939328 libevent_ssl_socket.cpp:106] *** released SSL socket: 19
I0804 08:32:33.296515 274329600 process.cpp:2865] Resuming help@127.0.0.1:55688 at 2016-08-04 15:32:33.296532992+00:00
I0804 08:32:33.296550 275939328 libevent_ssl_socket.cpp:721] *** sending on socket: 17, data: 0

I0804 08:32:33.296583 273793024 process.cpp:2865] Resuming (77)@127.0.0.1:55688 at 2016-08-04 15:32:33.296616960+00:00
F0804 08:32:33.296623 275939328 libevent_ssl_socket.cpp:723] Check failed: 'self->send_request.get()' Must be non NULL
*** Check failure stack trace: ***
{code}

The {{in send()17}} line indicates the beginning of {{send()}} for the SSL socket using FD 17. {{in shutdown(): 17}} indicates the beginning of {{shutdown()}} for the same socket, while {{sending on socket: 17}} indicates the execution of the lambda from {{send()}} on the event loop. Since {{shutdown()}} was called in between the call to {{send()}} and the execution of its lambda, it looks like the {{Socket}} was destroyed before the lambda could run. It's unclear why this would happen, since {{send()}}'s lambda captures a shared copy of the socket's {{this}} pointer in order to keep it alive.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-7930,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 08 18:19:20 UTC 2016,,,,,,,"0|i31x2n:",9223372036854775807,,,,,bmahler,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"04/Aug/16 16:31;greggomann;Marking this as a blocker for 1.0.1 for now. Once I have better repro steps and a good understanding of the root cause, I'll update the ticket as necessary.","04/Aug/16 21:55;greggomann;Review here:
https://reviews.apache.org/r/50741/

I think it makes sense to simply remove the CHECK and replace it with a log message, since the SSL socket may receive an EOF at any time. The {{event_callback}} handler sets {{send_request}} to NULL immediately, which will prevent us from sending anything on the socket after an EOF or ERROR has been received. The SSL socket's {{send()}} method can ensure that a {{send_request}} is present before it tosses its callback on the event loop's function queue, but we have no guarantees that the {{send_request}} will still be there when the callback is actually executed.","08/Aug/16 18:19;greggomann;{code}
commit f5822f3c13f4fdacbb390341940d3379248a9837
Author: Greg Mann greg@mesosphere.io
Date:   Fri Aug 5 18:19:33 2016 -0700

Removed incorrect CHECK in SSL socket `send()`.

The lambda placed on the event loop by the libevent SSL
socket's `send()` method previously used a `CHECK` to
ensure that the socket's `send_request` member was not
`nullptr`. This patch removes this check, since
`send_request` may become `nullptr` any time the socket
receives an EOF or ERROR event.

Note that the current handling of events is incorrect
also, but we do not attempt a fix here. To be specific,
reading EOF should not deal with send requests at all
(see MESOS-5999). Also, the ERROR events are not
differentiated between reading and writing. Lastly,
when we receive an EOF we do not ensure that the caller
can read the bytes that remain in the buffer!

Review: https://reviews.apache.org/r/50741/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,
NvidiaVolume errors out if any binary is missing,MESOS-5982,12994666,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,klueska,klueska,klueska,03/Aug/16 18:41,03/Aug/16 22:51,29/Oct/20 16:32,03/Aug/16 22:48,1.0.0,,,,,,,,1.0.1,,,,,,,,,,,0,gpu,mesosphere,,,,,,,"We currently error out if a binary we were trying to add to the volume is not found on the host filesystem. However, these are not the semantics that we want. By design, we list all the binaries that *may* exist on the filesystem that we want to put in the volume, not all of the binaries that *must* exist. We should simply skip any unfound binaries and move on to the next one instead of erroring out.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-08-03 22:48:02.901,,,false,MESOS-4424,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 03 22:48:02 UTC 2016,,,,,,,"0|i31vaf:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 40,,,,,,,,,,,2.0,,,,,,,,,,,"03/Aug/16 22:48;bmahler;{noformat}
commit e31985119bdf69d496c0a2ed77ee0d7ddec2602b
Author: Kevin Klues <klueska@gmail.com>
Date:   Wed Aug 3 15:05:15 2016 -0700

    Updated 'NvidiaVolume' to not error out when binary is missing.

    Previously, when building the volume, we would error out if a binary
    we were trying to add was not found on the host filesystem. However,
    these are not the semantics that we want. By design, we list all the
    binaries the *may* exist on the filesystem that we want to put in the
    volume, not all of the binaries that *must* exist. To this end, we
    should simply skip any unfound binaries and move on to the next one
    instead of erroring out.

    Review: https://reviews.apache.org/r/50762/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,
HealthChecker should not decide when to kill tasks and when to stop performing health checks.,MESOS-5963,12994260,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,alexr,alexr,alexr,02/Aug/16 11:58,25/Nov/16 15:26,29/Oct/20 16:32,25/Nov/16 15:26,,,,,,,,,1.2.0,,,,,,,,,,,0,health-check,mesosphere,,,,,,,"Currently, {{HealthChecker}} library decides when a task should be killed based on its health status. Moreover, it stops checking it health after that. This seems unfortunate, because it's up to the executor and / or framework to decide both when to kill tasks and when to health check them. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-5916,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 25 15:26:03 UTC 2016,,,,,,,"0|hzzz6c:zx",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 46,Mesosphere Sprint 47,,,,,,,,,,5.0,,1.2.0,,,,,,,,,"14/Oct/16 12:54;alexr;https://reviews.apache.org/r/52865/
https://reviews.apache.org/r/52868/
https://reviews.apache.org/r/52869/
https://reviews.apache.org/r/52870/
https://reviews.apache.org/r/52871/","25/Nov/16 15:26;alexr;{noformat}
Commit: c428b8bcf3d48ab9d3e2ef0cef5e3057619d7670 [c428b8b]
Author: Alexander Rukletsov <rukletsov@gmail.com>
Date: 25 November 2016 at 16:13:00 GMT+1
Committer: Alexander Rukletsov <alexr@apache.org>

Refactored HealthChecker to never stop health checking.

Prior to this patch, HealthChecker would stop performing health
checks after it marks the task for kill. Since tasks' lifecycle
is managed by scheduler-executor, HealthChecker should never stop
health checking on its own.

Allowing health checks to run forever enables the scheduler
make the decision about how to deal with unhealthy tasks.

Review: https://reviews.apache.org/r/52865/
{noformat}
{noformat}
Commit: bd6186d20d6ffe5a565ea29a08e1e0eb86873e63 [bd6186d]
Author: Alexander Rukletsov <rukletsov@gmail.com>
Date: 25 November 2016 at 16:13:54 GMT+1
Committer: Alexander Rukletsov <alexr@apache.org>

Health checks may be stopped on demand.

Review: https://reviews.apache.org/r/52868/
{noformat}
{noformat}
Commit: 2edfcd34493dbac81c57513fd50197eb10834389 [2edfcd3]
Author: Alexander Rukletsov <rukletsov@gmail.com>
Date: 25 November 2016 at 16:14:05 GMT+1
Committer: Alexander Rukletsov <alexr@apache.org>

Ensured command executor stops health checking terminated tasks.

We stop health checking both when the task is reaped and
killed since these may be two different execution paths.

Review: https://reviews.apache.org/r/52869/
{noformat}
{noformat}
Commit: 2814d1e6135a85aeb30befd812264d80192f4b94 [2814d1e]
Author: Alexander Rukletsov rukletsov@gmail.com
Date: 25 November 2016 at 16:14:15 GMT+1
Committer: Alexander Rukletsov alexr@apache.org

Ensured docker executor stops health checking terminated tasks.

We stop health checking both when the task is reaped and
killed since these may be two different execution paths.

Review: https://reviews.apache.org/r/52870/
{noformat}
{noformat}
Commit: 46e348de89c5cc8998068b8e395cd8f2cb492873 [46e348d]
Author: Alexander Rukletsov <rukletsov@gmail.com>
Date: 25 November 2016 at 16:14:19 GMT+1
Committer: Alexander Rukletsov <alexr@apache.org>

Ensured default executor ignores health updates for terminated tasks.

After the task has been terminated, its health updates become
irrelevant and should be ignored. Also if the default executor
shuts down, we can safely stop all health checkers.

Technically health checking should be stopped right before
TASK_KILLING update is sent to avoid subsequent TASK_RUNNING
updates, but the default executor currently does not support
TASK_KILLING.

Review: https://reviews.apache.org/r/52871/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
All non-root tests fail on GPU machine,MESOS-5959,12994085,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,klueska,klueska,klueska,01/Aug/16 21:14,01/Aug/16 22:53,29/Oct/20 16:32,01/Aug/16 22:53,,,,,,,,,1.0.1,,,,,,,,,,,0,gpu,mesosphere,,,,,,,"A recent addition to ensure that {{NvidiaVolume::create()}} ran as root broke all non-root tests on GPU machines. The reason is that we unconditionally create this volume so long as we detect {{nvml.isAvailable()}} which will fail now that we are only allowed to create this volume if we have root permissions.

We should fix this by adding the proper conditions to determine when / if we should create this volume based on some combination of {{\-\-containerizer}} and {{\-\-isolation}} flags.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-08-01 22:53:26.943,,,false,MESOS-4424,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 01 22:53:26 UTC 2016,,,,,,,"0|i31rpj:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 40,,,,,,,,,,,2.0,,,,,,,,,,,"01/Aug/16 21:22;klueska;https://reviews.apache.org/r/50671/
https://reviews.apache.org/r/50672/","01/Aug/16 22:53;jieyu;commit 8dc71da12c9b91edd2fa6c7b9a0a088b7dbb0ad3
Author: Kevin Klues <klueska@gmail.com>
Date:   Mon Aug 1 15:52:43 2016 -0700

    Added extra conditions for deciding when to create ""NvidiaComponents"".
    
    A recent addition to ensure that 'NvidiaVolume::create()' ran as root
    broke all non-root tests on GPU machines.  The reason is that we
    unconditionally create this volume so long as we detect
    'nvml.isAvailable()', which will fail now that we are only allowed to
    create this volume if we have root permissions.
    
    We fix this by adding the proper conditions to determine when / if we
    should create this volume based on some combination of --containerizer
    and --isolation flags.
    
    Review: https://reviews.apache.org/r/50672/

commit 3043a693884ea7ac0efacd8fbf494710eacdef4c
Author: Kevin Klues <klueska@gmail.com>
Date:   Mon Aug 1 15:52:40 2016 -0700

    Updated containerizer.cpp to look for duplicates in '--containerizer'.
    
    Review: https://reviews.apache.org/r/50671/",,,,,,,,,,,,,,,,,,,,,,,,,,
NvidiaVolume::create() should check for root before creating volume,MESOS-5945,12993810,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,klueska,klueska,klueska,31/Jul/16 18:54,02/Aug/16 04:50,29/Oct/20 16:32,02/Aug/16 04:50,1.0.0,,,,,,,,1.0.1,,,,,,,,,,,0,gpu,mesosphere,,,,,,,"Without root, we cannot create the nvidia volume in {{/var/run/mesos}} or mount a {{tmpfs}} in cases where we need to override the {{noexec}} on the current file system.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-08-02 04:50:04.973,,,false,MESOS-4424,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 02 04:50:04 UTC 2016,,,,,,,"0|i31q0f:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 40,,,,,,,,,,,2.0,,,,,,,,,,,"02/Aug/16 04:50;jieyu;commit 7d66e4c562ccb77f84bf744137fc0b973267f157
Author: Kevin Klues <klueska@gmail.com>
Date:   Mon Aug 1 09:06:04 2016 -0700

    Added check for root permissions to 'NvidiaVolume::create()'.
    
    Review: https://reviews.apache.org/r/50644/",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Unable to run ""scratch"" Dockerfiles with Unified Containerizer.",MESOS-5927,12993470,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,philwinder,philwinder,29/Jul/16 07:34,03/Aug/16 00:06,29/Oct/20 16:32,03/Aug/16 00:06,1.0.0,,,,,,,,0.28.3,1.0.1,,,,,containerization,,,,,0,containerizer,filesystem,mesosphere,,,,,,"It is not possible to run Docker containers that are based upon the ""scratch"" container.

Setup: Mesos 1.0.0 with the following Mesos settings:

{code:none}
echo 'docker' | sudo tee /etc/mesos-slave/image_providers
echo 'filesystem/linux,docker/runtime' | sudo tee /etc/mesos-slave/isolation
{code}

Recreate: From a Master or Slave, run:

{code:none}
mesos-execute --command='echo ok' --docker_image=hello-seattle --master=localhost:5050 --name=test
{code}

Effect: The container will crash with messages from Mesos reporting it can't mount folder x/y/z. E.g. can't mount /tmp. This means you can't run any container that is not a ""fat"" container (i.e. one with a full OS). E.g. error: 
bq. Failed to enter chroot '/var/lib/mesos/provisioner/containers/fed6add8-0126-40e6-ae81-5859a0c1a2d4/backends/copy/rootfses/4feefc8b-fd5a-4835-95db-165e675f11cd': /tmp in chroot does not existI0729 07:49:56.753474  4362 exec.cpp:413] Executor asked to shutdown

Expected: Run without issues.

Use case: We use scratch based containers with static binaries to keep the image size down. This is a common practice.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-08-02 23:55:25.189,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 03 00:06:10 UTC 2016,,,,,,,"0|i31nwv:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 40,,,,,,,,,,,3.0,,,,,,,,,,,"02/Aug/16 23:55;gilbert;https://reviews.apache.org/r/50718/
https://reviews.apache.org/r/50719/
https://reviews.apache.org/r/50720/
https://reviews.apache.org/r/50721/
https://reviews.apache.org/r/50727/","03/Aug/16 00:06;jieyu;commit 1afdafd523389d73c4f8c1454bd95aebe051eabc
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Aug 2 16:50:07 2016 -0700

    Added a unit test for scratch based docker images.
    
    Review: https://reviews.apache.org/r/50727/

commit 0a35c429a3cf6752354dacf8ca70d036a15ad1ec
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Aug 2 16:50:01 2016 -0700

    Fixed the bind backend document in container-image.md.
    
    Review: https://reviews.apache.org/r/50721/

commit 5aa66ff187370168389628b8d447b4e83c3dbd22
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Aug 2 16:49:58 2016 -0700

    Added comments for tmpfs and sandbox mountpoints for bind backend.
    
    Review: https://reviews.apache.org/r/50720/

commit 98ffcb1b69b4e233beb428bb63fc99d9e09245c2
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Aug 2 16:49:43 2016 -0700

    Supported scratch containers in Unified Containerizer.
    
    Before this patch, we are assuming the 'tmpfs' mount point '/tmp'
    always exist in the container's new rootfs. However, this is not
    true. For the scratch container (which is a common case), '/tmp'
    may not exist, and there may only be an executable binary in the
    new rootfs. So we need to create the mount point for 'tmpfs' in
    fs::enter().
    
    However, this change may break some cases using the bind backend,
    because we are not able the create the '/tmp' mount point in a
    read-only filesystem. So we require users to make sure the
    directory '/tmp' must already exist in their single layer scratch
    images for the following reasons:
      1. For most cases, when operators prefer using the bind backend,
         the single-layer images are usually large in size, and '/tmp'
         exists in the rootfs.
      2. For scratch images, most of them contain more than one layer,
         which means the bind backend cannot be used in those cases.
         So we can create the '/tmp' mount point if it does not exist.
      3. If this is strictly a single layer scratch image, it is
         reasonable that we require users to make sure the mount point
         '/tmp' existed in the image rootfs if they are using the bind
         backend, because we already require the sandbox mount point
         to be existed in those iamges.
    
    Review: https://reviews.apache.org/r/50719/",,,,,,,,,,,,,,,,,,,,,,,,,,
"Ubuntu 14.04 LTS GPU Isolator ""/run"" directory is noexec",MESOS-5923,12993392,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,klueska,bill.zhao,bill.zhao,28/Jul/16 22:50,02/Aug/16 04:51,29/Oct/20 16:32,01/Aug/16 16:06,1.0.0,,,,,,,,1.0.1,,,,,,,,,,,0,gpu,mesosphere,,,,,,,"In Ubuntu 14.04 LTS the mount for /run directory is noexec.  It affect the {{/var/run/mesos/isolators/gpu/nvidia_352.63/bin}} directory which mesos GPU isolators depended on.

{{bill@billz:/var/run$ mount | grep noexec
proc on /proc type proc (rw,noexec,nosuid,nodev)
sysfs on /sys type sysfs (rw,noexec,nosuid,nodev)
devpts on /dev/pts type devpts (rw,noexec,nosuid,gid=5,mode=0620)
tmpfs on /run type tmpfs (rw,noexec,nosuid,size=10%,mode=0755)}}

The /var/run is link to /run:
{{bill@billz:/var$ ll
total 52
drwxr-xr-x 13 root root     4096 May  5 20:00 ./
drwxr-xr-x 27 root root     4096 Jul 14 17:29 ../
lrwxrwxrwx  1 root root        9 May  5 19:50 lock -> /run/lock/
drwxrwxr-x 19 root syslog   4096 Jul 28 08:00 log/
drwxr-xr-x  2 root root     4096 Aug  4  2015 opt/
lrwxrwxrwx  1 root root        4 May  5 19:50 run -> /run/}}

Current the work around is mount without noexec:
{{sudo mount -o remount,exec /run}}",Ubuntu 14.04 LTS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-07-29 03:25:13.472,,,false,MESOS-4424,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 01 16:06:34 UTC 2016,,,,,,,"0|i31nfj:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 40,,,,,,,,,,,3.0,,,,,,,,,,,"29/Jul/16 03:25;klueska;https://reviews.apache.org/r/50592/","01/Aug/16 16:06;jieyu;commit 48a492cd9d7d0a194735b9b4107a35b489c596e1
Author: Kevin Klues <klueska@gmail.com>
Date:   Mon Aug 1 09:06:07 2016 -0700

    Updated NvidiaVolume to mount as 'tmpfs' if parent fs is 'noexec'.
    
    This patch is in response to an issue we ran into on Ubuntu 14.04,
    where '/run' is being mounted as 'noexec' (MESOS-5923). Since our
    NvidiaVolume is created below this mount point, we are unable to
    execute any binaries we add to this volume. This causes problems, for
    example, when trying to execute 'nvidia-smi' from within a container
    that has this volume mounted in.
    
    To work around this issue, we detect if any mount point above the path
    where we create the volume is marked as 'noexec', and if so, we create
    a new 'tmpfs' mount for the volume without 'noexec' set.
    
    Review: https://reviews.apache.org/r/50592/
",,,,,,,,,,,,,,,,,,,,,,,,,,
Strict/RegistrarTest.UpdateQuota/0 is flaky,MESOS-5878,12991424,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Not A Problem,alexr,neilc,neilc,21/Jul/16 12:14,26/Apr/17 17:00,29/Oct/20 16:32,12/Aug/16 23:11,,,,,,,,,1.1.0,,,,,,test,,,,,0,mesosphere,,,,,,,,"Observed on ASF CI (https://builds.apache.org/job/Mesos/BUILDTOOL=autotools,COMPILER=clang,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu:14.04,label_exp=(docker%7C%7CHadoop)&&(!ubuntu-us1)&&(!ubuntu-6)/2539/consoleFull). Log file is attached. Note that this might have been uncovered due to the recent removal of {{os::sleep}} from {{Clock::settle}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/16 12:24;neilc;strict_registrar_update_quota.log;https://issues.apache.org/jira/secure/attachment/12819313/strict_registrar_update_quota.log",,,,,1.0,,,,,,,,,,,,,,,,,,,,2016-08-12 23:11:03.993,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 12 23:11:03 UTC 2016,,,,,,,"0|hzzzb0:zy",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 40,Mesosphere Sprint 41,,,,,,,,,,3.0,,,,,,,,,,,"12/Aug/16 23:11;alexr;Though the log is a bit confusing (the await future failure is right at the top), it seems that this future failure is related to the update registry failure (at the bottom):
{noformat}
I0721 05:03:51.022830 28189 replica.cpp:712] Persisted action at 13
I0721 05:03:51.022821 28178 leveldb.cpp:341] Persisting action (286 bytes) to leveldb took 23.227367ms
E0721 05:04:00.954085 28181 registrar.cpp:531] Registrar aborting: Failed to update 'registry': Failed to perform store within 10secs
I0721 05:04:33.684514 28178 replica.cpp:712] Persisted action at 13
{noformat}

If this hypothesis is true, than the test failure was most probably caused by the slow CI. Closing it for now; it the issue pops up again, we will investigate further.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Document MESOS_SANDBOX executor env variable.,MESOS-5864,12990932,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,jieyu,jieyu,19/Jul/16 21:53,17/Jul/17 12:04,29/Oct/20 16:32,22/Jul/16 01:10,,,,,,,,,1.1.0,,,,,,containerization,documentation,,,,0,containerizer,documentation,mesosphere,,,,,,And we should document the difference with MESOS_DIRECTORY.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-07-21 00:59:22.322,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 22 01:10:47 UTC 2016,,,,,,,"0|i31893:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 39,,,,,,,,,,,2.0,,,,,,,,,,,"21/Jul/16 00:59;gilbert;https://reviews.apache.org/r/50260/","22/Jul/16 01:10;jieyu;commit ca5710e0acaccbf6016ad8d65a71fba8ad1b70fe
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Thu Jul 21 18:10:11 2016 -0700

    Documented the executor envirment variable 'MESOS_SANDBOX'.
    
    Review: https://reviews.apache.org/r/50260/",,,,,,,,,,,,,,,,,,,,,,,,,,
Logrotate ContainerLogger module does not rotate logs when run as root with `--switch_user`.,MESOS-5856,12990618,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,sivaramsk,kaysoky,kaysoky,19/Jul/16 00:43,07/Dec/16 18:03,29/Oct/20 16:32,23/Nov/16 22:04,0.27.0,0.28.0,1.0.0,,,,,,1.2.0,,,,,,,,,,,1,logger,mesosphere,newbie,won't-backport,,,,,"The logrotate ContainerLogger module runs as the agent's user.  In most cases, this is {{root}}.

When {{logrotate}} is run as root, there is an additional check the configuration files must pass (because a root {{logrotate}} needs to be secured against non-root modifications to the configuration):
https://github.com/logrotate/logrotate/blob/fe80cb51a2571ca35b1a7c8ba0695db5a68feaba/config.c#L807-L815

Log rotation will fail under the following scenario:
1) The agent is run with {{--switch_user}} (default: true)
2) A task is launched with a non-root user specified
3) The logrotate module spawns a few companion processes (as root) and this creates the {{stdout}}, {{stderr}}, {{stdout.logrotate.conf}}, and {{stderr.logrotate.conf}} files (as root).  This step races with the next step.
4) The Mesos containerizer and Fetcher will {{chown}} the task's sandbox to the non-root user.  Including the files just created.
5) When {{logrotate}} is run, it will skip any non-root configuration files.  This means the files are not rotated.

----

Fix: The logrotate module's companion processes should call {{setuid}} and {{setgid}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-5218,MESOS-6027,MESOS-6271,MESOS-6747,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-07-21 17:08:44.962,,,false,MESOS-4086,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 23 22:04:00 UTC 2016,,,,,,,"0|hzzz6b:",9223372036854775807,,,,,kaysoky,,,,,,Mesosphere Sprint 44,Mesosphere Sprint 45,Mesosphere Sprint 46,Mesosphere Sprint 47,,,,,,,,3.0,,,,,,,,,,,"21/Jul/16 17:08;adam-mesos;If MESOS-5218 is addressed by the fix for MESOS-5845 then would this issue be resolved as well?
Please test the logrotate module against latest master to see if this issue is fixed now too.","19/Sep/16 19:39;kaysoky;Added a unit test to assert the expected behavior: https://reviews.apache.org/r/52059/","04/Oct/16 01:28;kaysoky;| https://reviews.apache.org/r/52308/ | Add user flag to logger companion binary |
| https://reviews.apache.org/r/52310/ | Switch user in logger companion binary |
| https://reviews.apache.org/r/53473/ | Change ContainerLogger interface | 
| https://reviews.apache.org/r/53699/ | Some tests |","23/Nov/16 22:04;kaysoky;{code}
commit 120274ac51e5ae10e9530201ab67e56fa29edd6e
Author: Sivaram Kannan <sivaramsk@gmail.com>
Date:   Wed Nov 23 11:14:36 2016 -0800

    Added flag for passing in a user to the logrotate module.
    
    This adds an optional field to the LogrotateContainerLogger's
    companion binary.  When specified, the companion binary should switch
    to the given user after being launched.
    
    Review: https://reviews.apache.org/r/52308/
{code}
{code}
commit d4ba90fac3b8d0507d8d952c413730b78c8be4ee
Author: Sivaram Kannan <sivaramsk@gmail.com>
Date:   Wed Nov 23 11:14:39 2016 -0800

    Added new parameter for user to ContainerLogger's prepare function.
    
    This new parameter allows the containerizer to pass along the
    user the container will eventually run as.  The ContainerLogger
    module may choose to launch subprocesses under that user if applicable.
    
    Review: https://reviews.apache.org/r/53473/
{code}
{code}
commit 1f93bdd9240d9d23456e4a5da1299dc4135512e7
Author: Sivaram Kannan <sivaramsk@gmail.com>
Date:   Wed Nov 23 11:14:41 2016 -0800

    Added test cases for the logrotate module with --switch_user.
    
    This adds two regression tests for running a task under a different
    user than the agent.  In both cases, the logrotate module should
    still rotate logs, but the user that owns the log files will differ.
    
    Review: https://reviews.apache.org/r/53699/
{code}
{code}
commit 842c4cf42766473764fe58d78bad430745abc043
Author: Joseph Wu <josephwu@apache.org>
Date:   Wed Nov 23 11:32:03 2016 -0800

    Added note about MESOS-5856 to the upgrade doc.
    
    The 1.2 release contains a breaking change to the ContainerLogger
    interface.  The change is an additional argument in one of the
    module's methods.
{code}",,,,,,,,,,,,,,,,,,,,,,,,
CMake build needs to generate protobufs before building libmesos,MESOS-5852,12989883,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,srbrahma,kaysoky,kaysoky,15/Jul/16 18:14,29/Apr/19 09:27,29/Oct/20 16:32,26/Jul/16 00:55,,,,,,,,,1.1.0,,,,,,build,cmake,,,,0,cmake,mesosphere,,,,,,,"The existing CMake lists place protobufs at the same level as other Mesos sources:
https://github.com/apache/mesos/blob/c4cecf9c279c5206faaf996fef0b1810b490b329/src/CMakeLists.txt#L415

This is incorrect, as protobuf changes need to be regenerated before we can build against them.

Note: in the autotools build, this is done by compiling protobufs into {{libmesos}}, which then builds {{libmesos_no_3rdparty}}:
https://github.com/apache/mesos/blob/c4cecf9c279c5206faaf996fef0b1810b490b329/src/Makefile.am#L1304-L1305",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-898,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 26 00:55:19 UTC 2016,,,,,,,"0|i311s7:",9223372036854775807,,,,,kaysoky,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,"26/Jul/16 00:41;kaysoky;Srini's review: https://reviews.apache.org/r/50088/","26/Jul/16 00:55;kaysoky;{code}
commit d7269f78a3b179355053369085893854a0dcef41
Author: Srinivas Brahmaroutu <srbrahma@us.ibm.com>
Date:   Tue Jul 19 11:22:59 2016 -0700

    CMake: Added mesos-protobuf target to build protobuf libraries.
    
    Protobufs need to be (re)generated before building other Mesos sources.
    To enforce this build order, we introduce a seperate build target
    that produces a shared library of protobufs.  The `libmesos` build
    target depends and links against this protobuf target.
    
    Review: https://reviews.apache.org/r/50088/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Modularize Network in replicated_log,MESOS-5828,12988108,Bug,Reviewable,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,guoger,guoger,guoger,10/Jul/16 10:08,23/Apr/19 03:12,29/Oct/20 16:32,,,,,,,,,,,,,,,,replicated log,,,,,1,,,,,,,,,Currently replicated_log relies on Zookeeper for coordinator election. This is done through network abstraction _ZookeeperNetwork_. We need to modularize this part in order to enable replicated_log when using Master contender/detector modules.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-7224,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-04-19 07:18:49.141,,,false,MESOS-1806,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 23 03:12:09 UTC 2019,,,,,,,"0|i30rbj:",9223372036854775807,,,,,kaysoky,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,"05/Aug/16 08:43;guoger;Updated patch chain summary:

||Reviews||Summary||
|https://reviews.apache.org/r/50837|Fixed minor code style.|
|https://reviews.apache.org/r/50491|Added PIDGroup to libprocess.|
|https://reviews.apache.org/r/50492|Switched replicated log to use PIDGroup.|
|https://reviews.apache.org/r/50490|Separated ZooKeeper PIDGroup implementation into its own cpp/hpp.|
|https://reviews.apache.org/r/50493|Added `base` to PIDGroup.|
|https://reviews.apache.org/r/50494|Remove `base` from ZooKeeperPIDGroup.|
|https://reviews.apache.org/r/50495|Added PIDGroup module struct.|
|https://reviews.apache.org/r/50496|Added static `createPIDGroup` method to LogProcess.|
|https://reviews.apache.org/r/50497|Added new constructors in Log and LogProcess.|
|https://reviews.apache.org/r/50498|Added --pid_group flag in master.|
|https://reviews.apache.org/r/50499|Added logic in master/main.cpp to use pid_group module.|
|https://reviews.apache.org/r/50838|Updated modules documentation to reflect PIDGroup module.|","19/Apr/19 07:18;carlone;Hi [~kaysoky]. Is this patching still in progress or stopped? 

I want to use etcd instead of ZK in some edge environments because ZK uses to much resource and is too complicated as a coordinator.","22/Apr/19 20:28;kaysoky;Progress on this has been paused for a while (although the bulk of the patches are still usable).

In the meantime, you can try using zetcd, which basically exposes a ZK API for etcd:
https://github.com/etcd-io/zetcd

See this thread too: https://issues.apache.org/jira/browse/MESOS-1806?focusedCommentId=15895593&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-15895593","23/Apr/19 03:12;carlone;Actually, I have made some modifications to MESOS-1806(based on Jay Guo's work), and it(etcd contender/detector) works fine for my test mesos cluster. 

I'll commit a patch if needed.

 

The problem is that Replicated Log only works when ZK is present. I think some abstraction would make it more flexible and elegant.

Anyway, I'll try zetcd. Thanks a lot!",,,,,,,,,,,,,,,,,,,,,,,,
MasterAPITest.Subscribe is flaky,MESOS-5812,12987892,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,zhitao,zhitao,08/Jul/16 18:48,26/Apr/17 17:00,29/Oct/20 16:32,17/Jul/16 21:55,,,,,,,,,1.1.0,,,,,,test,,,,,0,,,,,,,,,"This test seems to be flaky, although on Mac OS X and CentOS 7 the error a bit different.

On Mac OS X:
{noformat}[ RUN      ] ContentType/MasterAPITest.Subscribe/0
I0708 11:42:48.474665 1927435008 cluster.cpp:155] Creating default 'local' authorizer
I0708 11:42:48.480677 1927435008 leveldb.cpp:174] Opened db in 5727us
I0708 11:42:48.481494 1927435008 leveldb.cpp:181] Compacted db in 722us
I0708 11:42:48.481541 1927435008 leveldb.cpp:196] Created db iterator in 19us
I0708 11:42:48.481572 1927435008 leveldb.cpp:202] Seeked to beginning of db in 9us
I0708 11:42:48.481587 1927435008 leveldb.cpp:271] Iterated through 0 keys in the db in 7us
I0708 11:42:48.481617 1927435008 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0708 11:42:48.482030 350982144 recover.cpp:451] Starting replica recovery
I0708 11:42:48.482203 350982144 recover.cpp:477] Replica is in EMPTY status
I0708 11:42:48.484107 348299264 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (3780)@127.0.0.1:50325
I0708 11:42:48.484318 350982144 recover.cpp:197] Received a recover response from a replica in EMPTY status
I0708 11:42:48.484750 348835840 master.cpp:382] Master e055d60c-05ff-487e-82da-d0a43e52605c (localhost) started on 127.0.0.1:50325
I0708 11:42:48.484850 349908992 recover.cpp:568] Updating replica status to STARTING
I0708 11:42:48.484788 348835840 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/private/tmp/Sn2Kf4/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/private/tmp/Sn2Kf4/master"" --zk_session_timeout=""10secs""
W0708 11:42:48.485263 348835840 master.cpp:387] 
**************************************************
Master bound to loopback interface! Cannot communicate with remote schedulers or agents. You might want to set '--ip' flag to a routable IP address.
**************************************************
I0708 11:42:48.485291 348835840 master.cpp:434] Master only allowing authenticated frameworks to register
I0708 11:42:48.485314 348835840 master.cpp:448] Master only allowing authenticated agents to register
I0708 11:42:48.485335 348835840 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
I0708 11:42:48.485347 348835840 credentials.hpp:37] Loading credentials for authentication from '/private/tmp/Sn2Kf4/credentials'
I0708 11:42:48.485373 349372416 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 397us
I0708 11:42:48.485414 349372416 replica.cpp:320] Persisted replica status to STARTING
I0708 11:42:48.485608 350982144 recover.cpp:477] Replica is in STARTING status
I0708 11:42:48.485749 348835840 master.cpp:506] Using default 'crammd5' authenticator
I0708 11:42:48.485852 348835840 master.cpp:578] Using default 'basic' HTTP authenticator
I0708 11:42:48.486018 348835840 master.cpp:658] Using default 'basic' HTTP framework authenticator
I0708 11:42:48.486140 348835840 master.cpp:705] Authorization enabled
I0708 11:42:48.486486 350982144 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (3783)@127.0.0.1:50325
I0708 11:42:48.486758 352055296 recover.cpp:197] Received a recover response from a replica in STARTING status
I0708 11:42:48.487176 350982144 recover.cpp:568] Updating replica status to VOTING
I0708 11:42:48.487576 352055296 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 300us
I0708 11:42:48.487658 352055296 replica.cpp:320] Persisted replica status to VOTING
I0708 11:42:48.487736 350982144 recover.cpp:582] Successfully joined the Paxos group
I0708 11:42:48.487951 350982144 recover.cpp:466] Recover process terminated
I0708 11:42:48.489441 348835840 master.cpp:1973] The newly elected leader is master@127.0.0.1:50325 with id e055d60c-05ff-487e-82da-d0a43e52605c
I0708 11:42:48.489518 348835840 master.cpp:1986] Elected as the leading master!
I0708 11:42:48.489545 348835840 master.cpp:1673] Recovering from registrar
I0708 11:42:48.489637 350982144 registrar.cpp:332] Recovering registrar
I0708 11:42:48.490120 351518720 log.cpp:553] Attempting to start the writer
I0708 11:42:48.491161 350445568 replica.cpp:493] Replica received implicit promise request from (3784)@127.0.0.1:50325 with proposal 1
I0708 11:42:48.491461 350445568 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 252us
I0708 11:42:48.491528 350445568 replica.cpp:342] Persisted promised to 1
I0708 11:42:48.492337 348299264 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0708 11:42:48.493482 349372416 replica.cpp:388] Replica received explicit promise request from (3785)@127.0.0.1:50325 for position 0 with proposal 2
I0708 11:42:48.493854 349372416 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 283us
I0708 11:42:48.493904 349372416 replica.cpp:712] Persisted action at 0
I0708 11:42:48.495302 348299264 replica.cpp:537] Replica received write request for position 0 from (3786)@127.0.0.1:50325
I0708 11:42:48.495455 348299264 leveldb.cpp:436] Reading position from leveldb took 45us
I0708 11:42:48.495761 348299264 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 261us
I0708 11:42:48.495803 348299264 replica.cpp:712] Persisted action at 0
I0708 11:42:48.496484 350445568 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0708 11:42:48.496795 350445568 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 255us
I0708 11:42:48.496857 350445568 replica.cpp:712] Persisted action at 0
I0708 11:42:48.496896 350445568 replica.cpp:697] Replica learned NOP action at position 0
I0708 11:42:48.497445 350982144 log.cpp:569] Writer started with ending position 0
I0708 11:42:48.498523 350982144 leveldb.cpp:436] Reading position from leveldb took 80us
I0708 11:42:48.499307 349908992 registrar.cpp:365] Successfully fetched the registry (0B) in 9.63712ms
I0708 11:42:48.499464 349908992 registrar.cpp:464] Applied 1 operations in 36us; attempting to update the 'registry'
I0708 11:42:48.499953 351518720 log.cpp:577] Attempting to append 159 bytes to the log
I0708 11:42:48.500088 350982144 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0708 11:42:48.500880 348299264 replica.cpp:537] Replica received write request for position 1 from (3787)@127.0.0.1:50325
I0708 11:42:48.501186 348299264 leveldb.cpp:341] Persisting action (178 bytes) to leveldb took 259us
I0708 11:42:48.501231 348299264 replica.cpp:712] Persisted action at 1
I0708 11:42:48.501786 351518720 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0708 11:42:48.502118 351518720 leveldb.cpp:341] Persisting action (180 bytes) to leveldb took 311us
I0708 11:42:48.502260 351518720 replica.cpp:712] Persisted action at 1
I0708 11:42:48.502305 351518720 replica.cpp:697] Replica learned APPEND action at position 1
I0708 11:42:48.503475 349908992 registrar.cpp:509] Successfully updated the 'registry' in 3.944192ms
I0708 11:42:48.503909 349908992 registrar.cpp:395] Successfully recovered registrar
I0708 11:42:48.504003 350982144 log.cpp:596] Attempting to truncate the log to 1
I0708 11:42:48.504250 349372416 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0708 11:42:48.504546 350445568 master.cpp:1781] Recovered 0 agents from the Registry (121B) ; allowing 10mins for agents to re-register
I0708 11:42:48.506022 352055296 replica.cpp:537] Replica received write request for position 2 from (3788)@127.0.0.1:50325
I0708 11:42:48.506479 352055296 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 320us
I0708 11:42:48.506513 352055296 replica.cpp:712] Persisted action at 2
I0708 11:42:48.506978 351518720 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0708 11:42:48.507155 351518720 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 169us
I0708 11:42:48.507237 351518720 leveldb.cpp:399] Deleting ~1 keys from leveldb took 37us
I0708 11:42:48.507264 351518720 replica.cpp:712] Persisted action at 2
I0708 11:42:48.507285 351518720 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0708 11:42:48.521363 1927435008 cluster.cpp:432] Creating default 'local' authorizer
I0708 11:42:48.522498 350982144 slave.cpp:205] Agent started on 119)@127.0.0.1:50325
I0708 11:42:48.522538 350982144 slave.cpp:206] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/Users/zhitao/Uber/sync/zhitao-mesos1.dev.uber.com/home/uber/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX""
W0708 11:42:48.522903 350982144 slave.cpp:209] 
**************************************************
Agent bound to loopback interface! Cannot communicate with remote master(s). You might want to set '--ip' flag to a routable IP address.
**************************************************
I0708 11:42:48.522922 350982144 credentials.hpp:86] Loading credential for authentication from '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/credential'
W0708 11:42:48.522965 1927435008 scheduler.cpp:157] 
**************************************************
Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address.
**************************************************
I0708 11:42:48.522992 1927435008 scheduler.cpp:172] Version: 1.0.0
I0708 11:42:48.523066 350982144 slave.cpp:343] Agent using credential for: test-principal
I0708 11:42:48.523092 350982144 credentials.hpp:37] Loading credentials for authentication from '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/http_credentials'
I0708 11:42:48.523334 350982144 slave.cpp:395] Using default 'basic' HTTP authenticator
I0708 11:42:48.523973 352055296 scheduler.cpp:461] New master detected at master@127.0.0.1:50325
I0708 11:42:48.524050 350982144 slave.cpp:594] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0708 11:42:48.524196 350982144 slave.cpp:602] Agent attributes: [  ]
I0708 11:42:48.524224 350982144 slave.cpp:607] Agent hostname: localhost
I0708 11:42:48.525522 350445568 state.cpp:57] Recovering state from '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/meta'
I0708 11:42:48.525853 350445568 status_update_manager.cpp:200] Recovering status update manager
I0708 11:42:48.526165 350445568 slave.cpp:4856] Finished recovery
I0708 11:42:48.527223 349372416 status_update_manager.cpp:174] Pausing sending status updates
I0708 11:42:48.527231 352055296 slave.cpp:969] New master detected at master@127.0.0.1:50325
I0708 11:42:48.527276 352055296 slave.cpp:1028] Authenticating with master master@127.0.0.1:50325
I0708 11:42:48.527328 352055296 slave.cpp:1039] Using default CRAM-MD5 authenticatee
I0708 11:42:48.527561 352055296 slave.cpp:1001] Detecting new master
I0708 11:42:48.527582 348299264 authenticatee.cpp:121] Creating new client SASL connection
I0708 11:42:48.528666 349908992 master.cpp:6006] Authenticating slave(119)@127.0.0.1:50325
I0708 11:42:48.528880 352055296 authenticator.cpp:98] Creating new server SASL connection
I0708 11:42:48.529089 350445568 http.cpp:381] HTTP POST for /master/api/v1/scheduler from 127.0.0.1:50918
I0708 11:42:48.529233 350445568 master.cpp:2272] Received subscription request for HTTP framework 'default'
I0708 11:42:48.529261 350445568 master.cpp:2012] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0708 11:42:48.529323 352055296 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0708 11:42:48.529357 352055296 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0708 11:42:48.529417 352055296 authenticator.cpp:204] Received SASL authentication start
I0708 11:42:48.529503 352055296 authenticator.cpp:326] Authentication requires more steps
I0708 11:42:48.529561 352055296 master.cpp:2370] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I0708 11:42:48.529721 349908992 authenticatee.cpp:259] Received SASL authentication step
I0708 11:42:48.530005 348835840 authenticator.cpp:232] Received SASL authentication step
I0708 11:42:48.530241 348835840 authenticator.cpp:318] Authentication success
I0708 11:42:48.530254 350445568 hierarchical.cpp:271] Added framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:42:48.530900 349908992 authenticatee.cpp:299] Authentication success
I0708 11:42:48.531186 350982144 master.cpp:6036] Successfully authenticated principal 'test-principal' at slave(119)@127.0.0.1:50325
I0708 11:42:48.531657 348299264 slave.cpp:1123] Successfully authenticated with master master@127.0.0.1:50325
I0708 11:42:48.531935 349372416 master.cpp:4676] Registering agent at slave(119)@127.0.0.1:50325 (localhost) with id e055d60c-05ff-487e-82da-d0a43e52605c-S0
I0708 11:42:48.532304 349908992 registrar.cpp:464] Applied 1 operations in 55us; attempting to update the 'registry'
I0708 11:42:48.532908 348835840 log.cpp:577] Attempting to append 326 bytes to the log
I0708 11:42:48.533015 352055296 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0708 11:42:48.533641 349372416 replica.cpp:537] Replica received write request for position 3 from (3798)@127.0.0.1:50325
I0708 11:42:48.533867 349372416 leveldb.cpp:341] Persisting action (345 bytes) to leveldb took 186us
I0708 11:42:48.533917 349372416 replica.cpp:712] Persisted action at 3
I0708 11:42:48.537066 349908992 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0708 11:42:48.538169 349908992 leveldb.cpp:341] Persisting action (347 bytes) to leveldb took 914us
I0708 11:42:48.538226 349908992 replica.cpp:712] Persisted action at 3
I0708 11:42:48.538255 349908992 replica.cpp:697] Replica learned APPEND action at position 3
I0708 11:42:48.539247 352055296 registrar.cpp:509] Successfully updated the 'registry' in 6.895104ms
I0708 11:42:48.539302 348299264 log.cpp:596] Attempting to truncate the log to 3
I0708 11:42:48.539393 348299264 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0708 11:42:48.539798 348835840 master.cpp:4745] Registered agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0708 11:42:48.539881 348299264 hierarchical.cpp:478] Added agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 (localhost) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0708 11:42:48.539901 349908992 slave.cpp:1169] Registered with master master@127.0.0.1:50325; given agent ID e055d60c-05ff-487e-82da-d0a43e52605c-S0
I0708 11:42:48.540287 350445568 status_update_manager.cpp:181] Resuming sending status updates
I0708 11:42:48.540501 351518720 replica.cpp:537] Replica received write request for position 4 from (3799)@127.0.0.1:50325
I0708 11:42:48.540583 352055296 master.cpp:5835] Sending 1 offers to framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:42:48.540798 351518720 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 247us
I0708 11:42:48.540868 351518720 replica.cpp:712] Persisted action at 4
I0708 11:42:48.540895 349908992 slave.cpp:1229] Forwarding total oversubscribed resources 
I0708 11:42:48.541035 352055296 master.cpp:5128] Received update of agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost) with total oversubscribed resources 
I0708 11:42:48.541291 349908992 hierarchical.cpp:542] Agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 (localhost) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0708 11:42:48.541630 350982144 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0708 11:42:48.541911 350982144 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 189us
I0708 11:42:48.541965 350982144 leveldb.cpp:399] Deleting ~2 keys from leveldb took 28us
I0708 11:42:48.541987 350982144 replica.cpp:712] Persisted action at 4
I0708 11:42:48.542006 350982144 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0708 11:42:48.544836 352055296 http.cpp:381] HTTP POST for /master/api/v1 from 127.0.0.1:50920
I0708 11:42:48.544884 352055296 http.cpp:484] Processing call SUBSCRIBE
I0708 11:42:48.545382 352055296 master.cpp:7599] Added subscriber: a85e7341-ac15-4f18-9021-1a2efa326442 to the list of active subscribers
I0708 11:42:48.550048 348835840 http.cpp:381] HTTP POST for /master/api/v1/scheduler from 127.0.0.1:50919
I0708 11:42:48.550339 348835840 master.cpp:3468] Processing ACCEPT call for offers: [ e055d60c-05ff-487e-82da-d0a43e52605c-O0 ] on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost) for framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:42:48.550390 348835840 master.cpp:3106] Authorizing framework principal 'test-principal' to launch task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1
W0708 11:42:48.551434 348835840 validation.cpp:650] Executor default for task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0708 11:42:48.551477 348835840 validation.cpp:662] Executor default for task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0708 11:42:48.551803 348835840 master.cpp:7565] Adding task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 (localhost)
I0708 11:42:48.551949 348835840 master.cpp:3957] Launching task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default) with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:42:48.552151 352055296 slave.cpp:1569] Got assigned task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 for framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:42:48.552592 352055296 slave.cpp:1688] Launching task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 for framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:42:48.553282 352055296 paths.cpp:528] Trying to chown '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-S0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000/executors/default/runs/62add906-b60f-43ec-ab06-0514a798de26' to user 'zhitao'
I0708 11:42:48.566201 352055296 slave.cpp:5748] Launching executor default of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 with resources  in work directory '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-S0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000/executors/default/runs/62add906-b60f-43ec-ab06-0514a798de26'
I0708 11:42:48.567876 352055296 executor.cpp:188] Version: 1.0.0
I0708 11:42:48.568428 352055296 slave.cpp:1914] Queuing task 'd94e54c0-8c89-43bd-be2f-adeb8cf70cb1' for executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
E0708 11:42:48.571115 352591872 process.cpp:2104] Failed to shutdown socket with fd 254: Socket is not connected
W0708 11:42:48.570768 352055296 executor.cpp:739] Dropping SUBSCRIBE: Executor is in state DISCONNECTED

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: disconnected(0x7fad21fcebf0)
Stack trace:
../../src/tests/api_tests.cpp:1537: Failure
Failed to wait 15secs for event
E0708 11:43:03.556205 352591872 process.cpp:2104] Failed to shutdown socket with fd 235: Socket is not connected
E0708 11:43:03.556584 352591872 process.cpp:2104] Failed to shutdown socket with fd 223: Socket is not connected
I0708 11:43:03.557134 349908992 master.cpp:1410] Framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default) disconnected
I0708 11:43:03.557176 349908992 master.cpp:2851] Disconnecting framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:43:03.557209 349908992 master.cpp:2875] Deactivating framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:43:03.557415 349908992 master.cpp:1423] Giving framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default) 0ns to failover
I0708 11:43:03.557456 348835840 hierarchical.cpp:382] Deactivated framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:43:03.557878 350445568 master.cpp:5687] Framework failover timeout, removing framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:43:03.557945 350445568 master.cpp:6422] Removing framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:43:03.558076 352055296 slave.cpp:2292] Asked to shut down framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 by master@127.0.0.1:50325
I0708 11:43:03.558106 352055296 slave.cpp:2317] Shutting down framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:43:03.558131 352055296 slave.cpp:4481] Shutting down executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
W0708 11:43:03.558147 352055296 slave.hpp:768] Unable to send event to executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000: unknown connection type
I0708 11:43:03.558188 350445568 master.cpp:6959] Updating the state of task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I0708 11:43:03.558507 350445568 master.cpp:7025] Removing task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:43:03.558709 350445568 master.cpp:7054] Removing executor 'default' with resources  of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:43:03.559051 349372416 hierarchical.cpp:333] Removed framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:43:03.567955 350982144 slave.cpp:4163] Executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 exited with status 0
I0708 11:43:03.568176 350982144 slave.cpp:4267] Cleaning up executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
W0708 11:43:03.568258 348299264 master.cpp:5369] Ignoring unknown exited executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:43:03.568584 348299264 gc.cpp:55] Scheduling '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-S0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000/executors/default/runs/62add906-b60f-43ec-ab06-0514a798de26' for gc 6.99999342143407days in the future
I0708 11:43:03.568864 350982144 slave.cpp:4355] Cleaning up framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:43:03.568879 352055296 gc.cpp:55] Scheduling '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-S0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000/executors/default' for gc 6.99999341739556days in the future
I0708 11:43:03.569056 350445568 status_update_manager.cpp:282] Closing status update streams for framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:43:03.569247 350982144 slave.cpp:841] Agent terminating
I0708 11:43:03.569239 348835840 gc.cpp:55] Scheduling '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-S0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000' for gc 6.99999341315852days in the future
I0708 11:43:03.569524 350982144 master.cpp:1371] Agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost) disconnected
I0708 11:43:03.569577 350982144 master.cpp:2910] Disconnecting agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:43:03.569767 350982144 master.cpp:2929] Deactivating agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:43:03.570020 349372416 hierarchical.cpp:571] Agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 deactivated
../../src/tests/api_tests.cpp:1509: Failure
Actual function call count doesn't match EXPECT_CALL(*executor, acknowledged(_, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
../../src/tests/api_tests.cpp:1505: Failure
Actual function call count doesn't match EXPECT_CALL(*executor, launch(_, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
../../src/tests/api_tests.cpp:1503: Failure
Actual function call count doesn't match EXPECT_CALL(*executor, subscribed(_, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
../../src/tests/api_tests.cpp:1496: Failure
Actual function call count doesn't match EXPECT_CALL(*scheduler, update(_, _))...
         Expected: to be called twice
           Actual: never called - unsatisfied and active
I0708 11:43:03.572598 1927435008 master.cpp:1218] Master terminating
I0708 11:43:03.572844 352055296 hierarchical.cpp:510] Removed agent e055d60c-05ff-487e-82da-d0a43e52605c-S0
[  FAILED  ] ContentType/MasterAPITest.Subscribe/0, where GetParam() = application/x-protobuf (15105 ms)
{noformat}

On CentOS 7
{noformat}
[ RUN      ] ContentType/MasterAPITest.Subscribe/0
I0708 15:42:16.042171 29138 cluster.cpp:155] Creating default 'local' authorizer
I0708 15:42:16.154358 29138 leveldb.cpp:174] Opened db in 111.818825ms
I0708 15:42:16.197175 29138 leveldb.cpp:181] Compacted db in 42.714984ms
I0708 15:42:16.197293 29138 leveldb.cpp:196] Created db iterator in 32582ns
I0708 15:42:16.197324 29138 leveldb.cpp:202] Seeked to beginning of db in 4050ns
I0708 15:42:16.197343 29138 leveldb.cpp:271] Iterated through 0 keys in the db in 538ns
I0708 15:42:16.197417 29138 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0708 15:42:16.198655 29157 recover.cpp:451] Starting replica recovery
I0708 15:42:16.199364 29161 recover.cpp:477] Replica is in EMPTY status
I0708 15:42:16.200865 29161 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (16431)@172.17.0.3:34502
I0708 15:42:16.201282 29158 recover.cpp:197] Received a recover response from a replica in EMPTY status
I0708 15:42:16.203222 29160 recover.cpp:568] Updating replica status to STARTING
I0708 15:42:16.204633 29158 master.cpp:382] Master 2aea5b7f-ec9f-4fda-8f34-877d8adf064f (0382d073a49a) started on 172.17.0.3:34502
I0708 15:42:16.204675 29158 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/Lu916I/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-1.1.0/_inst/share/mesos/webui"" --work_dir=""/tmp/Lu916I/master"" --zk_session_timeout=""10secs""
I0708 15:42:16.205265 29158 master.cpp:434] Master only allowing authenticated frameworks to register
I0708 15:42:16.205283 29158 master.cpp:448] Master only allowing authenticated agents to register
I0708 15:42:16.205294 29158 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
I0708 15:42:16.205307 29158 credentials.hpp:37] Loading credentials for authentication from '/tmp/Lu916I/credentials'
I0708 15:42:16.205705 29158 master.cpp:506] Using default 'crammd5' authenticator
I0708 15:42:16.205940 29158 master.cpp:578] Using default 'basic' HTTP authenticator
I0708 15:42:16.206192 29158 master.cpp:658] Using default 'basic' HTTP framework authenticator
I0708 15:42:16.206374 29158 master.cpp:705] Authorization enabled
I0708 15:42:16.206866 29172 hierarchical.cpp:151] Initialized hierarchical allocator process
I0708 15:42:16.207018 29172 whitelist_watcher.cpp:77] No whitelist given
I0708 15:42:16.210026 29165 master.cpp:1973] The newly elected leader is master@172.17.0.3:34502 with id 2aea5b7f-ec9f-4fda-8f34-877d8adf064f
I0708 15:42:16.210187 29165 master.cpp:1986] Elected as the leading master!
I0708 15:42:16.210330 29165 master.cpp:1673] Recovering from registrar
I0708 15:42:16.210577 29171 registrar.cpp:332] Recovering registrar
I0708 15:42:16.239378 29160 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 35.540287ms
I0708 15:42:16.239485 29160 replica.cpp:320] Persisted replica status to STARTING
I0708 15:42:16.239938 29161 recover.cpp:477] Replica is in STARTING status
I0708 15:42:16.242017 29165 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (16434)@172.17.0.3:34502
I0708 15:42:16.242527 29167 recover.cpp:197] Received a recover response from a replica in STARTING status
I0708 15:42:16.243140 29167 recover.cpp:568] Updating replica status to VOTING
I0708 15:42:16.281746 29167 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.318978ms
I0708 15:42:16.281828 29167 replica.cpp:320] Persisted replica status to VOTING
I0708 15:42:16.282094 29170 recover.cpp:582] Successfully joined the Paxos group
I0708 15:42:16.282440 29170 recover.cpp:466] Recover process terminated
I0708 15:42:16.283365 29170 log.cpp:553] Attempting to start the writer
I0708 15:42:16.285605 29167 replica.cpp:493] Replica received implicit promise request from (16435)@172.17.0.3:34502 with proposal 1
I0708 15:42:16.315435 29167 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.761608ms
I0708 15:42:16.315528 29167 replica.cpp:342] Persisted promised to 1
I0708 15:42:16.317147 29159 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0708 15:42:16.318914 29160 replica.cpp:388] Replica received explicit promise request from (16436)@172.17.0.3:34502 for position 0 with proposal 2
I0708 15:42:16.348886 29160 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 29.896283ms
I0708 15:42:16.349161 29160 replica.cpp:712] Persisted action at 0
I0708 15:42:16.350939 29170 replica.cpp:537] Replica received write request for position 0 from (16437)@172.17.0.3:34502
I0708 15:42:16.351029 29170 leveldb.cpp:436] Reading position from leveldb took 42967ns
I0708 15:42:16.382378 29170 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 31.28917ms
I0708 15:42:16.382464 29170 replica.cpp:712] Persisted action at 0
I0708 15:42:16.383646 29169 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0708 15:42:16.415894 29169 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 32.189511ms
I0708 15:42:16.416015 29169 replica.cpp:712] Persisted action at 0
I0708 15:42:16.416056 29169 replica.cpp:697] Replica learned NOP action at position 0
I0708 15:42:16.417312 29168 log.cpp:569] Writer started with ending position 0
I0708 15:42:16.418628 29167 leveldb.cpp:436] Reading position from leveldb took 56748ns
I0708 15:42:16.420019 29165 registrar.cpp:365] Successfully fetched the registry (0B) in 209.31712ms
I0708 15:42:16.420155 29165 registrar.cpp:464] Applied 1 operations in 30566ns; attempting to update the 'registry'
I0708 15:42:16.420994 29172 log.cpp:577] Attempting to append 168 bytes to the log
I0708 15:42:16.421149 29157 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0708 15:42:16.422169 29162 replica.cpp:537] Replica received write request for position 1 from (16438)@172.17.0.3:34502
I0708 15:42:16.457743 29162 leveldb.cpp:341] Persisting action (187 bytes) to leveldb took 35.505294ms
I0708 15:42:16.457844 29162 replica.cpp:712] Persisted action at 1
I0708 15:42:16.459228 29172 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0708 15:42:16.495947 29172 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 36.653391ms
I0708 15:42:16.496048 29172 replica.cpp:712] Persisted action at 1
I0708 15:42:16.496091 29172 replica.cpp:697] Replica learned APPEND action at position 1
I0708 15:42:16.497947 29172 registrar.cpp:509] Successfully updated the 'registry' in 77.703936ms
I0708 15:42:16.498132 29172 registrar.cpp:395] Successfully recovered registrar
I0708 15:42:16.498169 29171 log.cpp:596] Attempting to truncate the log to 1
I0708 15:42:16.498294 29162 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0708 15:42:16.498668 29171 master.cpp:1781] Recovered 0 agents from the Registry (129B) ; allowing 10mins for agents to re-register
I0708 15:42:16.498919 29162 hierarchical.cpp:178] Skipping recovery of hierarchical allocator: nothing to recover
I0708 15:42:16.499577 29171 replica.cpp:537] Replica received write request for position 2 from (16439)@172.17.0.3:34502
I0708 15:42:16.521065 29171 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 21.423468ms
I0708 15:42:16.521160 29171 replica.cpp:712] Persisted action at 2
I0708 15:42:16.522766 29171 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0708 15:42:16.546223 29171 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 23.402601ms
I0708 15:42:16.546380 29171 leveldb.cpp:399] Deleting ~1 keys from leveldb took 70830ns
I0708 15:42:16.546411 29171 replica.cpp:712] Persisted action at 2
I0708 15:42:16.546445 29171 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0708 15:42:16.560467 29138 cluster.cpp:432] Creating default 'local' authorizer
I0708 15:42:16.565003 29162 slave.cpp:205] Agent started on 449)@172.17.0.3:34502
I0708 15:42:16.565520 29138 scheduler.cpp:172] Version: 1.1.0
I0708 15:42:16.565150 29162 slave.cpp:206] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""true"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-1.1.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContentType_MasterAPITest_Subscribe_0_be660M""
I0708 15:42:16.566128 29162 credentials.hpp:86] Loading credential for authentication from '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/credential'
I0708 15:42:16.566423 29162 slave.cpp:343] Agent using credential for: test-principal
I0708 15:42:16.566520 29171 scheduler.cpp:461] New master detected at master@172.17.0.3:34502
I0708 15:42:16.566543 29162 credentials.hpp:37] Loading credentials for authentication from '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/http_credentials'
I0708 15:42:16.566557 29171 scheduler.cpp:470] Waiting for 0ns before initiating a re-(connection) attempt with the master
I0708 15:42:16.566838 29162 slave.cpp:395] Using default 'basic' HTTP authenticator
I0708 15:42:16.568023 29162 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0708 15:42:16.568527 29162 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0708 15:42:16.569443 29162 slave.cpp:594] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0708 15:42:16.569535 29162 slave.cpp:602] Agent attributes: [  ]
I0708 15:42:16.569552 29162 slave.cpp:607] Agent hostname: 0382d073a49a
I0708 15:42:16.571897 29165 state.cpp:57] Recovering state from '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/meta'
I0708 15:42:16.572376 29165 status_update_manager.cpp:200] Recovering status update manager
I0708 15:42:16.572638 29165 slave.cpp:4856] Finished recovery
I0708 15:42:16.573194 29165 slave.cpp:5028] Querying resource estimator for oversubscribable resources
I0708 15:42:16.574082 29165 slave.cpp:969] New master detected at master@172.17.0.3:34502
I0708 15:42:16.574111 29165 slave.cpp:1028] Authenticating with master master@172.17.0.3:34502
I0708 15:42:16.574174 29165 slave.cpp:1039] Using default CRAM-MD5 authenticatee
I0708 15:42:16.574213 29162 status_update_manager.cpp:174] Pausing sending status updates
I0708 15:42:16.574323 29165 slave.cpp:1001] Detecting new master
I0708 15:42:16.574525 29165 authenticatee.cpp:121] Creating new client SASL connection
I0708 15:42:16.574851 29160 slave.cpp:5042] Received oversubscribable resources  from the resource estimator
I0708 15:42:16.575621 29164 scheduler.cpp:349] Connected with the master at http://172.17.0.3:34502/master/api/v1/scheduler
I0708 15:42:16.577546 29164 scheduler.cpp:231] Sending SUBSCRIBE call to http://172.17.0.3:34502/master/api/v1/scheduler
I0708 15:42:16.579020 29168 master.cpp:6006] Authenticating slave(449)@172.17.0.3:34502
I0708 15:42:16.579133 29165 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(926)@172.17.0.3:34502
I0708 15:42:16.579236 29168 process.cpp:3322] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0708 15:42:16.579448 29157 authenticator.cpp:98] Creating new server SASL connection
I0708 15:42:16.579684 29165 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0708 15:42:16.579722 29165 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0708 15:42:16.579831 29165 authenticator.cpp:204] Received SASL authentication start
I0708 15:42:16.579910 29165 authenticator.cpp:326] Authentication requires more steps
I0708 15:42:16.580013 29165 authenticatee.cpp:259] Received SASL authentication step
I0708 15:42:16.580111 29165 authenticator.cpp:232] Received SASL authentication step
I0708 15:42:16.580143 29165 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '0382d073a49a' server FQDN: '0382d073a49a' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0708 15:42:16.580157 29165 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0708 15:42:16.580196 29165 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0708 15:42:16.580227 29165 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '0382d073a49a' server FQDN: '0382d073a49a' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0708 15:42:16.580240 29165 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0708 15:42:16.580251 29165 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0708 15:42:16.580271 29165 authenticator.cpp:318] Authentication success
I0708 15:42:16.580420 29165 authenticatee.cpp:299] Authentication success
I0708 15:42:16.580525 29165 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(926)@172.17.0.3:34502
I0708 15:42:16.580840 29165 slave.cpp:1123] Successfully authenticated with master master@172.17.0.3:34502
I0708 15:42:16.581131 29165 slave.cpp:1529] Will retry registration in 814473ns if necessary
I0708 15:42:16.581560 29168 master.cpp:6036] Successfully authenticated principal 'test-principal' at slave(449)@172.17.0.3:34502
I0708 15:42:16.581795 29168 master.cpp:4676] Registering agent at slave(449)@172.17.0.3:34502 (0382d073a49a) with id 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0
I0708 15:42:16.583050 29162 registrar.cpp:464] Applied 1 operations in 131284ns; attempting to update the 'registry'
I0708 15:42:16.584233 29170 slave.cpp:1529] Will retry registration in 27.411836ms if necessary
I0708 15:42:16.584384 29158 master.cpp:4664] Ignoring register agent message from slave(449)@172.17.0.3:34502 (0382d073a49a) as admission is already in progress
I0708 15:42:16.585019 29168 log.cpp:577] Attempting to append 337 bytes to the log
I0708 15:42:16.585113 29162 http.cpp:381] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:33142
I0708 15:42:16.585156 29159 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0708 15:42:16.585417 29162 master.cpp:2272] Received subscription request for HTTP framework 'default'
I0708 15:42:16.585486 29162 master.cpp:2012] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0708 15:42:16.586509 29159 replica.cpp:537] Replica received write request for position 3 from (16448)@172.17.0.3:34502
I0708 15:42:16.587302 29168 master.cpp:2370] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I0708 15:42:16.588059 29170 master.hpp:2010] Sending heartbeat to 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.588745 29168 hierarchical.cpp:271] Added framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.588819 29168 hierarchical.cpp:1537] No allocations performed
I0708 15:42:16.588851 29168 hierarchical.cpp:1632] No inverse offers to send out!
I0708 15:42:16.588910 29168 hierarchical.cpp:1172] Performed allocation for 0 agents in 138375ns
I0708 15:42:16.593391 29162 scheduler.cpp:662] Enqueuing event SUBSCRIBED received from http://172.17.0.3:34502/master/api/v1/scheduler
I0708 15:42:16.594115 29162 scheduler.cpp:662] Enqueuing event HEARTBEAT received from http://172.17.0.3:34502/master/api/v1/scheduler
I0708 15:42:16.612622 29162 slave.cpp:1529] Will retry registration in 35.186867ms if necessary
I0708 15:42:16.613113 29169 master.cpp:4664] Ignoring register agent message from slave(449)@172.17.0.3:34502 (0382d073a49a) as admission is already in progress
I0708 15:42:16.621047 29159 leveldb.cpp:341] Persisting action (356 bytes) to leveldb took 34.409256ms
I0708 15:42:16.621134 29159 replica.cpp:712] Persisted action at 3
I0708 15:42:16.622661 29159 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0708 15:42:16.646806 29159 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 24.085822ms
I0708 15:42:16.646906 29159 replica.cpp:712] Persisted action at 3
I0708 15:42:16.646986 29159 replica.cpp:697] Replica learned APPEND action at position 3
I0708 15:42:16.649273 29157 registrar.cpp:509] Successfully updated the 'registry' in 66.121984ms
I0708 15:42:16.649538 29167 slave.cpp:1529] Will retry registration in 111.475397ms if necessary
I0708 15:42:16.649603 29158 log.cpp:596] Attempting to truncate the log to 3
I0708 15:42:16.649811 29157 master.cpp:4664] Ignoring register agent message from slave(449)@172.17.0.3:34502 (0382d073a49a) as admission is already in progress
I0708 15:42:16.650069 29160 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0708 15:42:16.650713 29160 slave.cpp:3760] Received ping from slave-observer(404)@172.17.0.3:34502
I0708 15:42:16.650879 29160 slave.cpp:1169] Registered with master master@172.17.0.3:34502; given agent ID 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0
I0708 15:42:16.651007 29160 fetcher.cpp:86] Clearing fetcher cache
I0708 15:42:16.651065 29158 hierarchical.cpp:478] Added agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 (0382d073a49a) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0708 15:42:16.651480 29160 slave.cpp:1192] Checkpointing SlaveInfo to '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/meta/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0/slave.info'
I0708 15:42:16.651499 29166 status_update_manager.cpp:181] Resuming sending status updates
I0708 15:42:16.650825 29157 master.cpp:4745] Registered agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0708 15:42:16.651847 29158 hierarchical.cpp:1632] No inverse offers to send out!
I0708 15:42:16.652433 29158 hierarchical.cpp:1195] Performed allocation for agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 in 1.317746ms
I0708 15:42:16.651897 29172 replica.cpp:537] Replica received write request for position 4 from (16450)@172.17.0.3:34502
I0708 15:42:16.653264 29157 master.cpp:5835] Sending 1 offers to framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default)
I0708 15:42:16.654682 29160 slave.cpp:1229] Forwarding total oversubscribed resources 
I0708 15:42:16.656188 29165 master.cpp:5128] Received update of agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a) with total oversubscribed resources 
I0708 15:42:16.656200 29164 scheduler.cpp:662] Enqueuing event OFFERS received from http://172.17.0.3:34502/master/api/v1/scheduler
I0708 15:42:16.656708 29159 hierarchical.cpp:542] Agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 (0382d073a49a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0708 15:42:16.657385 29159 hierarchical.cpp:1537] No allocations performed
I0708 15:42:16.657438 29159 hierarchical.cpp:1632] No inverse offers to send out!
I0708 15:42:16.657519 29159 hierarchical.cpp:1195] Performed allocation for agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 in 516462ns
I0708 15:42:16.660909 29163 process.cpp:3322] Handling HTTP event for process 'master' with path: '/master/api/v1'
I0708 15:42:16.661958 29163 http.cpp:381] HTTP POST for /master/api/v1 from 172.17.0.3:33143
I0708 15:42:16.662125 29163 http.cpp:484] Processing call SUBSCRIBE
I0708 15:42:16.663280 29164 master.cpp:7599] Added subscriber: 726edf8d-ad3d-4d08-9243-de3dc2df5f4a to the list of active subscribers
I0708 15:42:16.671409 29161 scheduler.cpp:231] Sending ACCEPT call to http://172.17.0.3:34502/master/api/v1/scheduler
I0708 15:42:16.672615 29165 process.cpp:3322] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0708 15:42:16.676375 29169 http.cpp:381] HTTP POST for /master/api/v1/scheduler from 172.17.0.3:33141
I0708 15:42:16.677199 29169 master.cpp:3468] Processing ACCEPT call for offers: [ 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-O0 ] on agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a) for framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default)
I0708 15:42:16.677291 29169 master.cpp:3106] Authorizing framework principal 'test-principal' to launch task d8bd1ba3-055a-4420-820c-8e85fdde7c08
W0708 15:42:16.679435 29169 validation.cpp:650] Executor default for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0708 15:42:16.679492 29169 validation.cpp:662] Executor default for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0708 15:42:16.680003 29169 master.cpp:7573] Notifying all active subscribers about TASK_ADDED event
I0708 15:42:16.680454 29172 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 27.707387ms
I0708 15:42:16.680685 29172 replica.cpp:712] Persisted action at 4
I0708 15:42:16.681685 29168 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0708 15:42:16.680449 29169 master.cpp:7565] Adding task d8bd1ba3-055a-4420-820c-8e85fdde7c08 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 (0382d073a49a)
I0708 15:42:16.682688 29169 master.cpp:3957] Launching task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default) with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a)
I0708 15:42:16.683289 29171 slave.cpp:1569] Got assigned task d8bd1ba3-055a-4420-820c-8e85fdde7c08 for framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.683903 29171 slave.cpp:1688] Launching task d8bd1ba3-055a-4420-820c-8e85fdde7c08 for framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.684563 29171 paths.cpp:528] Trying to chown '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000/executors/default/runs/d8931456-e1d5-4875-9fb2-cf66e66d7fa3' to user 'mesos'
I0708 15:42:16.699834 29171 slave.cpp:5748] Launching executor default of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 with resources  in work directory '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000/executors/default/runs/d8931456-e1d5-4875-9fb2-cf66e66d7fa3'
I0708 15:42:16.702018 29171 executor.cpp:188] Version: 1.1.0
I0708 15:42:16.702541 29171 slave.cpp:1914] Queuing task 'd8bd1ba3-055a-4420-820c-8e85fdde7c08' for executor 'default' of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.702777 29171 slave.cpp:922] Successfully attached file '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000/executors/default/runs/d8931456-e1d5-4875-9fb2-cf66e66d7fa3'
I0708 15:42:16.704201 29169 executor.cpp:389] Connected with the agent
I0708 15:42:16.704911 29159 executor.cpp:290] Sending SUBSCRIBE call to http://172.17.0.3:34502/slave(449)/api/v1/executor
I0708 15:42:16.706003 29170 process.cpp:3322] Handling HTTP event for process 'slave(449)' with path: '/slave(449)/api/v1/executor'
I0708 15:42:16.706897 29157 http.cpp:270] HTTP POST for /slave(449)/api/v1/executor from 172.17.0.3:33144
I0708 15:42:16.707108 29157 slave.cpp:2735] Received Subscribe request for HTTP executor 'default' of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.707819 29168 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 26.083403ms
I0708 15:42:16.707986 29168 leveldb.cpp:399] Deleting ~2 keys from leveldb took 117548ns
I0708 15:42:16.708031 29168 replica.cpp:712] Persisted action at 4
I0708 15:42:16.708076 29168 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0708 15:42:16.708082 29157 slave.cpp:2079] Sending queued task 'd8bd1ba3-055a-4420-820c-8e85fdde7c08' to executor 'default' of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (via HTTP)
I0708 15:42:16.710268 29163 executor.cpp:707] Enqueuing event SUBSCRIBED received from http://172.17.0.3:34502/slave(449)/api/v1/executor
I0708 15:42:16.712131 29169 executor.cpp:707] Enqueuing event LAUNCH received from http://172.17.0.3:34502/slave(449)/api/v1/executor
I0708 15:42:16.713174 29172 executor.cpp:290] Sending UPDATE call to http://172.17.0.3:34502/slave(449)/api/v1/executor
I0708 15:42:16.713984 29170 process.cpp:3322] Handling HTTP event for process 'slave(449)' with path: '/slave(449)/api/v1/executor'
I0708 15:42:16.714614 29170 http.cpp:270] HTTP POST for /slave(449)/api/v1/executor from 172.17.0.3:33145
I0708 15:42:16.714753 29170 slave.cpp:3285] Handling status update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.715451 29172 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.715498 29172 status_update_manager.cpp:497] Creating StatusUpdate stream for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.715996 29172 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 to the agent
I0708 15:42:16.716584 29172 slave.cpp:3678] Forwarding the update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 to master@172.17.0.3:34502
I0708 15:42:16.716956 29172 slave.cpp:3572] Status update manager successfully handled status update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.717159 29171 master.cpp:5273] Status update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 from agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a)
I0708 15:42:16.717265 29171 master.cpp:5321] Forwarding status update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.718080 29168 executor.cpp:707] Enqueuing event ACKNOWLEDGED received from http://172.17.0.3:34502/slave(449)/api/v1/executor
I0708 15:42:16.718299 29171 master.cpp:7573] Notifying all active subscribers about TASK_UPDATED event
I0708 15:42:16.719683 29171 master.cpp:6959] Updating the state of task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0708 15:42:16.720386 29171 scheduler.cpp:662] Enqueuing event UPDATE received from http://172.17.0.3:34502/master/api/v1/scheduler
I0708 15:42:16.726471 29164 hierarchical.cpp:1537] No allocations performed
W0708 15:42:16.726742 29163 status_update_manager.cpp:475] Resending status update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.726840 29163 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 to the agent
I0708 15:42:16.727401 29163 slave.cpp:3678] Forwarding the update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 to master@172.17.0.3:34502
I0708 15:42:16.727880 29163 master.cpp:5273] Status update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 from agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a)
I0708 15:42:16.728035 29163 master.cpp:5321] Forwarding status update TASK_RUNNING (UUID: a93b73fb-5289-4aec-80e9-1cad44bec619) for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.728570 29163 master.cpp:6959] Updating the state of task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0708 15:42:16.728003 29164 hierarchical.cpp:1632] No inverse offers to send out!
I0708 15:42:16.730080 29164 hierarchical.cpp:1172] Performed allocation for 1 agents in 3.858387ms
I0708 15:42:16.751055 29160 master.cpp:1410] Framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default) disconnected
I0708 15:42:16.751116 29160 master.cpp:2851] Disconnecting framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default)
I0708 15:42:16.751149 29160 master.cpp:2875] Deactivating framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default)
I0708 15:42:16.751242 29160 master.cpp:1423] Giving framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default) 0ns to failover
I0708 15:42:16.751602 29160 hierarchical.cpp:382] Deactivated framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.755091 29157 master.cpp:5687] Framework failover timeout, removing framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default)
I0708 15:42:16.755164 29157 master.cpp:6422] Removing framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (default)
I0708 15:42:16.755425 29157 master.cpp:7573] Notifying all active subscribers about TASK_UPDATED event
I0708 15:42:16.755795 29157 master.cpp:6959] Updating the state of task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I0708 15:42:16.756032 29166 slave.cpp:2292] Asked to shut down framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 by master@172.17.0.3:34502
I0708 15:42:16.756093 29166 slave.cpp:2317] Shutting down framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.756172 29166 slave.cpp:4481] Shutting down executor 'default' of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (via HTTP)
I0708 15:42:16.757699 29157 master.cpp:7025] Removing task d8bd1ba3-055a-4420-820c-8e85fdde7c08 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 on agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a)
I0708 15:42:16.758779 29161 hierarchical.cpp:924] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 from framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.759784 29161 executor.cpp:707] Enqueuing event SHUTDOWN received from http://172.17.0.3:34502/slave(449)/api/v1/executor
I0708 15:42:16.761289 29157 master.cpp:7054] Removing executor 'default' with resources  of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 on agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a)
I0708 15:42:16.763124 29157 hierarchical.cpp:333] Removed framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.777029 29163 slave.cpp:4163] Executor 'default' of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 exited with status 0
I0708 15:42:16.777218 29163 slave.cpp:4267] Cleaning up executor 'default' of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 (via HTTP)
I0708 15:42:16.777710 29163 slave.cpp:4355] Cleaning up framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.778026 29163 gc.cpp:55] Scheduling '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000/executors/default/runs/d8931456-e1d5-4875-9fb2-cf66e66d7fa3' for gc 6.99999163714074days in the future
W0708 15:42:16.778028 29167 master.cpp:5369] Ignoring unknown exited executor 'default' of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000 on agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a)
I0708 15:42:16.778195 29157 status_update_manager.cpp:282] Closing status update streams for framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.778239 29163 gc.cpp:55] Scheduling '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000/executors/default' for gc 6.99999163714074days in the future
I0708 15:42:16.778257 29157 status_update_manager.cpp:528] Cleaning up status update stream for task d8bd1ba3-055a-4420-820c-8e85fdde7c08 of framework 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000
I0708 15:42:16.778327 29163 gc.cpp:55] Scheduling '/tmp/ContentType_MasterAPITest_Subscribe_0_be660M/slaves/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0/frameworks/2aea5b7f-ec9f-4fda-8f34-877d8adf064f-0000' for gc 6.99999163714074days in the future
I0708 15:42:16.797328 29138 slave.cpp:841] Agent terminating
I0708 15:42:16.799114 29165 master.cpp:1371] Agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a) disconnected
I0708 15:42:16.800149 29165 master.cpp:2910] Disconnecting agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a)
I0708 15:42:16.800727 29165 master.cpp:2929] Deactivating agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 at slave(449)@172.17.0.3:34502 (0382d073a49a)
I0708 15:42:16.801389 29165 hierarchical.cpp:571] Agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0 deactivated
../../src/tests/api_tests.cpp:1496: Failure
Actual function call count doesn't match EXPECT_CALL(*scheduler, update(_, _))...
         Expected: to be called twice
           Actual: called once - unsatisfied and active
I0708 15:42:16.806820 29138 master.cpp:1218] Master terminating
I0708 15:42:16.807718 29160 hierarchical.cpp:510] Removed agent 2aea5b7f-ec9f-4fda-8f34-877d8adf064f-S0
[  FAILED  ] ContentType/MasterAPITest.Subscribe/0, where GetParam() = application/x-protobuf (780 ms)
{noformat}","mac os X
centos 7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-07-08 23:41:19.997,,,false,MESOS-4791,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 17 21:55:34 UTC 2016,,,,,,,"0|i30pzr:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 39,,,,,,,,,,,3.0,,,,,,,,,,,"08/Jul/16 21:26;zhitao;Updated snippet of Mac OS part, to reflect log for the first failure.","08/Jul/16 23:41;anandmazumdar;Fix for one of the issues: https://reviews.apache.org/r/49837","15/Jul/16 22:18;vinodkone;[~anandmazumdar] assigning this to you since you started working on one of the issues (though I can't tell which from your comment).","15/Jul/16 22:28;anandmazumdar;Yep, I forgot updating this JIRA thereafter.

https://reviews.apache.org/r/49837 : Flaky fix that showed up on ASF.
https://reviews.apache.org/r/49844 : The FD leak that was discovered while fixing the flakiness.
","17/Jul/16 21:55;anandmazumdar;{noformat}
commit 44e10cb2a8665c13c3d98184f8c27a608309020f
Author: Anand Mazumdar <anand@apache.org>
Date:   Sun Jul 17 14:28:05 2016 -0700

    Fixed a subscriber FD leak when running tests.

    This FD leak would only surface when running tests. We hold on to
    a reference of the \`Connection\` object in the client so that it is
    not destroyed while the connection is active. When running tests,
    the IP:Port of libprocess remain the same which means the objects
    keep on accumulating. In a real world cluster, we remove the
    subscriber upon noticing a \_disconnection\_ i.e. this means the
    socket has already been already closed upstream by Libprocess on
    the server side.

    Review: https://reviews.apache.org/r/49844/

commit 6ac4b234929cd85c8dc543e581999b3b5b38d132
Author: Anand Mazumdar <anand@apache.org>
Date:   Sun Jul 17 14:27:50 2016 -0700

    Fixed the flaky `MasterAPITest.Subscribe` test.

    The test was flaky because `Clock::settle()` does not ensure
    that data in flight (on the wire) is also flushed. Hence, sometimes
    the update used to be sent by the master but never received by the
    library before the test was completed. It would be good if libprocess
    could add some logic for recognizing local HTTP events and sending
    them directly as it does for messages using `send()`.

    Review: https://reviews.apache.org/r/49837/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,
CNI isolator should prepare network related /etc/* files for containers using host mode but specify container images.,MESOS-5806,12987642,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,qianzhang,jieyu,jieyu,08/Jul/16 00:10,25/Jul/16 16:49,29/Oct/20 16:32,19/Jul/16 15:57,,,,,,,,,1.0.0,,,,,,containerization,network,,,,0,mesosphere,,,,,,,,"Currently, the CNI isolator will just ignore those containers that want to join the host network (i.e., not specifying NetworkInfo). However, if the container specifies a container image, we need to make sure that it has access to host /etc/* files. We should perform the bind mount for the container. This is also what docker does when a container is running in host mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-07-15 06:22:17.363,,,false,MESOS-4641,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 25 16:49:11 UTC 2016,,,,,,,"0|i30ohb:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 39,,,,,,,,,,,5.0,,,,,,,,,,,"15/Jul/16 06:22;qianzhang;RR: https://reviews.apache.org/r/50065/","15/Jul/16 23:26;jieyu;commit b1f71f1ba6c672982e84d4e607288537abed41c7
Author: Qian Zhang <zhangqxa@cn.ibm.com>
Date:   Fri Jul 15 14:31:58 2016 -0700

    Enhancement for containers which have image and join host network.
    
    For the containers which have image and join host network, we enhanced
    'network/cni' isolator to make sure they have access to host /etc/hosts
    , /etc/hostname and /etc/resolv.conf files.
    
    Review: https://reviews.apache.org/r/50065/","18/Jul/16 16:45;jieyu;commit f1f3851efb41d2869dd7f1ae01247450a4acac2f
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Jul 15 22:20:19 2016 -0700

    Handled systems that miss /etc/hostname in CNI isolator.
    
    Review: https://reviews.apache.org/r/50108

commit d8665b3437e4affab9d2890444b4d162af0b8679
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Jul 15 18:55:01 2016 -0700

    Handled /etc/* file being dead links in CNI isolator.
    
    Review: https://reviews.apache.org/r/50107

commit 0bf29fac911912cd89192f01aebd11ba7fc7605d
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Jul 15 18:37:04 2016 -0700

    Ignored /etc/* mounts to host filesystems if host network is used.
    
    Review: https://reviews.apache.org/r/50102","18/Jul/16 18:52;jieyu;Re-open this one since i got this error on CoreOS host:
{noformat}Failed to launch container: Collect failed: Failed to setup hostname and network files: Unable to find '/etc/hosts'{noformat}

","19/Jul/16 15:57;jieyu;commit 65e821bb5c3fbdd494ceb383a12f165532459fd9
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Mon Jul 18 14:20:53 2016 -0700

    Ignored /etc/hosts if it does not exist in CNI isolator.
    
    Review: https://reviews.apache.org/r/50162","25/Jul/16 16:49;jieyu;commit caa13aba1ad48b6617ab1ac3d8cf8ee18befc977
Author: Qian Zhang <zhangqxa@cn.ibm.com>
Date:   Mon Jul 25 09:48:16 2016 -0700

    Added CniIsolatorTest.ROOT_INTERNET_CURL_LaunchContainerInHostNetwork.
    
    Review: https://reviews.apache.org/r/50224/",,,,,,,,,,,,,,,,,,,,,,
Missing License Information for Bundled NVML headers,MESOS-5766,12986432,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,klueska,klueska,klueska,02/Jul/16 19:18,05/Jul/16 17:21,29/Oct/20 16:32,05/Jul/16 17:21,,,,,,,,,1.0.0,,,,,,,,,,,0,gpu,mesosphere,,,,,,,See Summary,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-07-05 17:21:19.344,,,false,MESOS-4424,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 05 17:21:19 UTC 2016,,,,,,,"0|i30h0n:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 38,,,,,,,,,,,1.0,,,,,,,,,,,"02/Jul/16 19:20;klueska;https://reviews.apache.org/r/49558/","05/Jul/16 17:21;bmahler;{noformat}
commit ad4556f796ef355c9c8bf227cad984fe692e4c77
Author: Kevin Klues klueska@gmail.com
Date:   Sat Jul 2 13:26:24 2016 -0700

Added Nvidia License information for our bundled NVML header.

Review: https://reviews.apache.org/r/49558/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
"When start an agent with `--resources`, the GPU resource can be fractional",MESOS-5742,12984814,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Not A Problem,klueska,Sunzhe,Sunzhe,29/Jun/16 09:17,30/Jun/16 01:07,29/Oct/20 16:32,30/Jun/16 01:07,1.0.0,,,,,,,,,,,,,,,,,,,0,gpu,,,,,,,,"So far, the GPU resource is not fractional, only integer values are allowed. But when starting agents with {{\-\-resources='gpu:1.2'}}, it can also work without any warning or error. And in the webui the GPU resource is `1.2`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-30 00:19:50.101,,,false,MESOS-4424,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 30 01:05:37 UTC 2016,,,,,,,"0|i30avr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,"29/Jun/16 23:57;Sunzhe;cc [~klueska] [~bmahler]
Is that right with {{--resources='gpu:1.2'}}?","30/Jun/16 00:19;klueska;If you don't also set the {{\-\-isolators=gpu/nvidia}} flag, then you can set {{\-\-resources=""gpus:   ""}} to whatever you want. The idea being that you only want the semantics enforced by the {{gpu/nvidia}} isolator (i.e. that only *non-fractional* GPUs can be specified) when the {{gpu/nvidia}} isolator is enabled.  Some other isolator might be built in the future that allows fractional GPUs.","30/Jun/16 01:05;Sunzhe;Yes, I see. Because I noticed many people have implemented GPU resources without what you have done. They did it only with {{--resources=}} and other flags when starting agents. I wonder whether it can influence the existing mechanism.

As you said, the existing mechanism need to set the {{\-\-isolation=gpu/nvidia}} flag, so the {{--resources=}} is limited to set fraction. Yes. you are right.

Thank you for your reply.",,,,,,,,,,,,,,,,,,,,,,,,,
Command executor health check does not work when the task specifies container image.,MESOS-5727,12984166,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,jieyu,jieyu,27/Jun/16 23:51,28/Aug/16 15:06,29/Oct/20 16:32,02/Jul/16 00:02,0.28.2,1.0.0,,,,,,,1.0.0,,,,,,containerization,,,,,0,containerizer,health-check,mesosphere,,,,,,"Since we launch the task after pivot_root, we no longer has the access to the mesos-health-check binary. The solution is to refactor health check to be a library (libprocess) so that it does not depend on the underlying filesystem.

One note here is that we should strive to keep both the command executor and the task in the same mount namespace so that Mesos CLI tooling does not need to find the mount namespace for the task. It just need to find the corresponding pid for the executor. This statement is *arguable*, see the comment below.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-2533,MESOS-3567,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-07-01 21:10:06.865,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 24 11:19:41 UTC 2016,,,,,,,"0|i30807:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 38,,,,,,,,,,,5.0,,,,,,,,,,,"01/Jul/16 21:10;gilbert;https://reviews.apache.org/r/49388/
https://reviews.apache.org/r/49389/
https://reviews.apache.org/r/49390/","02/Jul/16 00:02;jieyu;commit d63185e1df4e55bf6cedfc311547e654decad023
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Fri Jul 1 17:01:53 2016 -0700

    Added test for container image command task with health check.
    
    Review: https://reviews.apache.org/r/49390/

commit 1556d9a3a02de4e8a90b5b64d268754f95b12d77
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Fri Jul 1 17:01:50 2016 -0700

    Refactored HealthCheck from a binary to be a library.
    
    Review: https://reviews.apache.org/r/49389/

commit c097509c59ea61f786d175c5edc81efa533cc4fd
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Fri Jul 1 17:01:48 2016 -0700

    Added devolve method for TaskID and HealthCheck.
    
    Review: https://reviews.apache.org/r/49388/","24/Aug/16 11:19;alexr;Though the benefits of sharing the mount namespace between the command executor and its task are clear, there are disadvantages as well.

Once we {{pivot_root}}, we may loose access to helper and system binaries, which we may need in the executor for, e.g., health checking. Moreover, in the future pods case, an executor will have multiple tasks with their own mount namespaces, hence the executor must have its own mount namespace.

We will be {{unshare}}'ing the task's mount namespace: https://reviews.apache.org/r/51266/",,,,,,,,,,,,,,,,,,,,,,,,,
SSL downgrade support will leak sockets in CLOSE_WAIT status,MESOS-5691,12982185,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,kaysoky,kaysoky,kaysoky,23/Jun/16 01:58,22/Mar/19 16:40,29/Oct/20 16:32,23/Jun/16 02:25,0.24.0,0.25.0,0.26.0,0.27.0,0.28.0,,,,0.28.3,1.0.0,,,,,libprocess,,,,,0,libprocess,mesosphere,,,,,,,"Repro steps:
1) Start a master:
{code}
bin/mesos-master.sh --work_dir=/tmp/master
{code}

2) Start an agent with SSL and downgrade enabled:
{code}
# Taken from http://mesos.apache.org/documentation/latest/ssl/
openssl genrsa -des3 -f4 -passout pass:some_password -out key.pem 4096
openssl req -new -x509 -passin pass:some_password -days 365 -key key.pem -out cert.pem

SSL_KEY_FILE=key.pem SSL_CERT_FILE=cert.pem SSL_ENABLED=true SSL_SUPPORT_DOWNGRADE=true sudo -E bin/mesos-agent.sh --master=localhost:5050 --work_dir=/tmp/agent
{code}

3) Start a framework that launches lots of executors, one after another:
{code}
sudo src/balloon-framework --master=localhost:5050 --task_memory=64mb --task_memory_usage_limit=256mb --long_running
{code}

4) Check FDs, repeatedly
{code}
sudo lsof -i | grep mesos | grep CLOSE_WAIT | wc -l
{code}

The number of sockets in {{CLOSE_WAIT}} will increase linearly with the number of launched executors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-7930,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 23 02:25:47 UTC 2016,,,,,,,"0|i2zy4v:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 37,,,,,,,,,,,5.0,,,,,,,,,,,"23/Jun/16 02:08;kaysoky;Fix: https://reviews.apache.org/r/49127/","23/Jun/16 02:25;kaysoky;{code}
commit e6a9fd57425043f8fdbdc5be3a3d2c0aef43c098
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Wed Jun 22 19:24:21 2016 -0700

    Fix socket leak in SSL_ENABLE_DOWNGRADE code path.
    
    When an SSL-enabled Mesos actor attempts to link, it will open an SSL
    socket first.  If this fails, and downgrading to non-SSL is enabled,
    the Mesos actor will create a ""Poll"" socket instead.
    
    In this code path, we explicitly avoid calling `SocketManager::close`
    as ""closing"" a link will also trigger `ExitedEvents`.  Instead, the
    downgrade codepath tries to ""swap"" the `SocketManager`s state from the
    old SSL socket, to the new Poll socket via `swap_implementing_socket`.
    
    `swap_implementing_socket` leaks a `Socket` object, which is a
    reference-counted wrapper for an FD.  Besides the memory leak, leaking
    the `Socket` means that the reference count for the old SSL socket
    will always be above zero.  And we only `close` the socket when the
    reference count reaches zero.
    
    Review: https://reviews.apache.org/r/49127/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Port mapping isolator may fail in 'isolate' method.,MESOS-5674,12981173,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,21/Jun/16 02:19,26/Apr/17 16:53,29/Oct/20 16:32,22/Jun/16 23:02,,,,,,,,,1.0.0,,,,,,containerization,network,,,,0,isolator,mesosphere,networking,tests,,,,,"Port mapping isolator may return failure in isolate method, if a symlink to the network namespace handle using that ContainerId already existed. We should overwrite the symlink if it exist.

This affects a couple test failures:
{noformat}
PortMappingIsolatorTest.ROOT_TooManyContainers
PortMappingIsolatorTest.ROOT_ContainerARPExternal
PortMappingIsolatorTest.ROOT_ContainerCMPInternal
PortMappingIsolatorTest.ROOT_NC_HostToContainerTCP
{noformat}

Here is an example failure test log:
{noformat}
[00:28:37] :	 [Step 10/10] [ RUN      ] PortMappingIsolatorTest.ROOT_TooManyContainers
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.046444 24846 port_mapping_tests.cpp:229] Using eth0 as the public interface
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.046728 24846 port_mapping_tests.cpp:237] Using lo as the loopback interface
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.058758 24846 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ephemeral_ports:[30001-30999];ports:[31000-32000]
[00:28:37]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.059711 24846 port_mapping.cpp:1557] Using eth0 as the public interface
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.059998 24846 port_mapping.cpp:1582] Using lo as the loopback interface
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061126 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh3 = '1024'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061172 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh1 = '128'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061206 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_wmem = '4096	16384	4194304'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061256 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_synack_retries = '5'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061297 24846 port_mapping.cpp:1869] /proc/sys/net/core/rmem_max = '212992'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061331 24846 port_mapping.cpp:1869] /proc/sys/net/core/somaxconn = '128'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061360 24846 port_mapping.cpp:1869] /proc/sys/net/core/wmem_max = '212992'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061390 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_rmem = '4096	87380	6291456'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061419 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_time = '7200'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061450 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh2 = '512'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061480 24846 port_mapping.cpp:1869] /proc/sys/net/core/netdev_max_backlog = '1000'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061511 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_intvl = '75'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061540 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_probes = '9'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061569 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_max_syn_backlog = '512'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.061599 24846 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_retries2 = '15'
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.069964 24846 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.070144 24846 resources.cpp:572] Parsing resources as JSON failed: ports:[31000-31499]
[00:28:37]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.070677 24867 port_mapping.cpp:2512] Using non-ephemeral ports {[31000,31500)} and ephemeral ports [30208,30720) for container container1 of executor ''
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.071688 24846 linux_launcher.cpp:281] Cloning child process with flags = CLONE_NEWNS | CLONE_NEWNET
[00:28:37]W:	 [Step 10/10] I0606 00:28:37.084079 24863 port_mapping.cpp:2576] Bind mounted '/proc/11997/ns/net' to '/run/netns/11997' for container container1
[00:28:37] :	 [Step 10/10] ../../src/tests/containerizer/port_mapping_tests.cpp:1438: Failure
[00:28:37] :	 [Step 10/10] (isolator.get()->isolate(containerId1, pid.get())).failure(): Failed to symlink the network namespace handle '/var/run/mesos/netns/container1' -> '/run/netns/11997': File exists
[00:28:37] :	 [Step 10/10] [  FAILED  ] PortMappingIsolatorTest.ROOT_TooManyContainers (57 ms)
{noformat}",Fedora 23 with network isolation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-22 23:02:11.508,,,false,MESOS-1585,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 22 23:02:11 UTC 2016,,,,,,,"0|i2zs53:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 37,,,,,,,,,,,3.0,,,,,,,,,,,"22/Jun/16 23:02;jieyu;commit 4cfccf4b5bd14419d7248103e709bbc7eaae8833
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jun 22 16:00:49 2016 -0700

    Fixed portmapping isolator tests ContainerId.
    
    Review: https://reviews.apache.org/r/48995/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Port mapping isolator may cause segfault if it bind mount root does not exist.,MESOS-5673,12981171,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,21/Jun/16 02:02,26/Apr/17 16:53,29/Oct/20 16:32,22/Jun/16 23:02,0.28.2,,,,,,,,0.28.3,1.0.0,,,,,containerization,,,,,0,isolator,mesosphere,networking,tests,,,,,"A check is needed for port mapping isolator for its bind mount root. Otherwise, non-existed port-mapping bind mount root may cause segmentation fault for some cases. Here is the test log:

{noformat}
[00:57:42] :	 [Step 10/10] [----------] 11 tests from PortMappingIsolatorTest
[00:57:42] :	 [Step 10/10] [ RUN      ] PortMappingIsolatorTest.ROOT_NC_ContainerToContainerTCP
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.723029 24841 port_mapping_tests.cpp:229] Using eth0 as the public interface
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.723348 24841 port_mapping_tests.cpp:237] Using lo as the loopback interface
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.735090 24841 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ephemeral_ports:[30001-30999];ports:[31000-32000]
[00:57:42]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.736006 24841 port_mapping.cpp:1557] Using eth0 as the public interface
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.736331 24841 port_mapping.cpp:1582] Using lo as the loopback interface
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737501 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh3 = '1024'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737545 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh1 = '128'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737578 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_wmem = '4096	16384	4194304'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737608 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_synack_retries = '5'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737637 24841 port_mapping.cpp:1869] /proc/sys/net/core/rmem_max = '212992'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737666 24841 port_mapping.cpp:1869] /proc/sys/net/core/somaxconn = '128'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737694 24841 port_mapping.cpp:1869] /proc/sys/net/core/wmem_max = '212992'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737720 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_rmem = '4096	87380	6291456'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737746 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_time = '7200'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737772 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/neigh/default/gc_thresh2 = '512'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737798 24841 port_mapping.cpp:1869] /proc/sys/net/core/netdev_max_backlog = '1000'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737828 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_intvl = '75'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737854 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_keepalive_probes = '9'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737879 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_max_syn_backlog = '512'
[00:57:42]W:	 [Step 10/10] I0604 00:57:42.737905 24841 port_mapping.cpp:1869] /proc/sys/net/ipv4/tcp_retries2 = '15'
[00:57:42]W:	 [Step 10/10] F0604 00:57:42.737968 24841 port_mapping_tests.cpp:448] CHECK_SOME(isolator): Failed to get realpath for bind mount root '/var/run/netns': Not found 
[00:57:42]W:	 [Step 10/10] *** Check failure stack trace: ***
[00:57:42]W:	 [Step 10/10]     @     0x7f8bd52583d2  google::LogMessage::Fail()
[00:57:42]W:	 [Step 10/10]     @     0x7f8bd525832b  google::LogMessage::SendToLog()
[00:57:42]W:	 [Step 10/10]     @     0x7f8bd5257d21  google::LogMessage::Flush()
[00:57:42]W:	 [Step 10/10]     @     0x7f8bd525ab92  google::LogMessageFatal::~LogMessageFatal()
[00:57:42]W:	 [Step 10/10]     @           0xa62171  _CheckFatal::~_CheckFatal()
[00:57:42]W:	 [Step 10/10]     @          0x1931b17  mesos::internal::tests::PortMappingIsolatorTest_ROOT_NC_ContainerToContainerTCP_Test::TestBody()
[00:57:42]W:	 [Step 10/10]     @          0x19e17b6  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[00:57:42]W:	 [Step 10/10]     @          0x19dc864  testing::internal::HandleExceptionsInMethodIfSupported<>()
[00:57:42]W:	 [Step 10/10]     @          0x19bd2ae  testing::Test::Run()
[00:57:42]W:	 [Step 10/10]     @          0x19bda66  testing::TestInfo::Run()
[00:57:42]W:	 [Step 10/10]     @          0x19be0b7  testing::TestCase::Run()
[00:57:42]W:	 [Step 10/10]     @          0x19c4bf5  testing::internal::UnitTestImpl::RunAllTests()
[00:57:42]W:	 [Step 10/10]     @          0x19e247d  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[00:57:42]W:	 [Step 10/10]     @          0x19dd3a4  testing::internal::HandleExceptionsInMethodIfSupported<>()
[00:57:42]W:	 [Step 10/10]     @          0x19c38d1  testing::UnitTest::Run()
[00:57:42]W:	 [Step 10/10]     @           0xfd28cb  RUN_ALL_TESTS()
[00:57:42]W:	 [Step 10/10]     @           0xfd24b1  main
[00:57:42]W:	 [Step 10/10]     @     0x7f8bceb89580  __libc_start_main
[00:57:42]W:	 [Step 10/10]     @           0xa607c9  _start
[00:57:43]W:	 [Step 10/10] /mnt/teamcity/temp/agentTmp/custom_script659125926639545396: line 3: 24841 Aborted                 (core dumped) GLOG_v=1 ./bin/mesos-tests.sh --verbose --gtest_filter=""$GTEST_FILTER""
[00:57:43]W:	 [Step 10/10] Process exited with code 134
{noformat}",Fedora 23 with network isolation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-22 23:02:42.353,,,false,MESOS-1585,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 22 23:02:42 UTC 2016,,,,,,,"0|i2zs4n:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 37,,,,,,,,,,,3.0,,,,,,,,,,,"22/Jun/16 23:02;jieyu;commit f9a0db8ef8ec5e9b22b41ddbbe6efb64b05da310
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jun 22 16:00:46 2016 -0700

    Fixed portmapping isolator bind mount root non-existed case.
    
    Review: https://reviews.apache.org/r/48994/",,,,,,,,,,,,,,,,,,,,,,,,,,,
MemoryPressureMesosTest.CGROUPS_ROOT_Statistics is flaky.,MESOS-5671,12981168,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,21/Jun/16 01:50,06/Jul/18 21:27,29/Oct/20 16:32,22/Jun/16 23:04,,,,,,,,,1.0.0,,,,,,containerization,test,,,,0,cgroups,flaky-test,mesosphere,,,,,,"{noformat}
[00:48:29] :	 [Step 10/10] [ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics
[00:48:29]W:	 [Step 10/10] 1+0 records in
[00:48:29]W:	 [Step 10/10] 1+0 records out
[00:48:29]W:	 [Step 10/10] 1048576 bytes (1.0 MB) copied, 0.000517638 s, 2.0 GB/s
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.000998 25413 cluster.cpp:155] Creating default 'local' authorizer
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.020459 25413 leveldb.cpp:174] Opened db in 19.338463ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.022897 25413 leveldb.cpp:181] Compacted db in 2.416906ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.022919 25413 leveldb.cpp:196] Created db iterator in 4037ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.022927 25413 leveldb.cpp:202] Seeked to beginning of db in 769ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.022932 25413 leveldb.cpp:271] Iterated through 0 keys in the db in 390ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.022944 25413 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.023272 25432 recover.cpp:451] Starting replica recovery
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.023425 25434 recover.cpp:477] Replica is in EMPTY status
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.023748 25434 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (19361)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.023849 25429 recover.cpp:197] Received a recover response from a replica in EMPTY status
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024019 25435 recover.cpp:568] Updating replica status to STARTING
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024338 25432 master.cpp:382] Master 0e92ffa4-4f26-4cea-84d3-9c67612de1bd (ip-172-30-2-56.mesosphere.io) started on 172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024348 25432 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/jBjY5p/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/jBjY5p/master"" --zk_session_timeout=""10secs""
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024502 25432 master.cpp:434] Master only allowing authenticated frameworks to register
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024508 25432 master.cpp:448] Master only allowing authenticated agents to register
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024513 25432 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024516 25432 credentials.hpp:37] Loading credentials for authentication from '/tmp/jBjY5p/credentials'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024603 25432 master.cpp:506] Using default 'crammd5' authenticator
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024644 25432 master.cpp:578] Using default 'basic' HTTP authenticator
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024701 25432 master.cpp:658] Using default 'basic' HTTP framework authenticator
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024770 25432 master.cpp:705] Authorization enabled
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024883 25435 whitelist_watcher.cpp:77] No whitelist given
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.024885 25434 hierarchical.cpp:142] Initialized hierarchical allocator process
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.025539 25433 master.cpp:1969] The newly elected leader is master@172.30.2.56:53790 with id 0e92ffa4-4f26-4cea-84d3-9c67612de1bd
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.025555 25433 master.cpp:1982] Elected as the leading master!
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.025560 25433 master.cpp:1669] Recovering from registrar
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.025611 25432 registrar.cpp:332] Recovering registrar
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.026397 25431 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.288187ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.026438 25431 replica.cpp:320] Persisted replica status to STARTING
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.026486 25431 recover.cpp:477] Replica is in STARTING status
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.026793 25432 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (19364)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.026897 25429 recover.cpp:197] Received a recover response from a replica in STARTING status
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.027031 25428 recover.cpp:568] Updating replica status to VOTING
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.028960 25432 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.874668ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.028975 25432 replica.cpp:320] Persisted replica status to VOTING
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.029007 25432 recover.cpp:582] Successfully joined the Paxos group
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.029047 25432 recover.cpp:466] Recover process terminated
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.029209 25430 log.cpp:553] Attempting to start the writer
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.029614 25429 replica.cpp:493] Replica received implicit promise request from (19365)@172.30.2.56:53790 with proposal 1
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.031486 25429 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.850474ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.031502 25429 replica.cpp:342] Persisted promised to 1
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.031726 25431 coordinator.cpp:238] Coordinator attempting to fill missing positions
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.032245 25428 replica.cpp:388] Replica received explicit promise request from (19366)@172.30.2.56:53790 for position 0 with proposal 2
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.034101 25428 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.831441ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.034117 25428 replica.cpp:712] Persisted action at 0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.034561 25433 replica.cpp:537] Replica received write request for position 0 from (19367)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.034589 25433 leveldb.cpp:436] Reading position from leveldb took 10586ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.036419 25433 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.817267ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.036434 25433 replica.cpp:712] Persisted action at 0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.036679 25429 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.038661 25429 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.96521ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.038677 25429 replica.cpp:712] Persisted action at 0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.038682 25429 replica.cpp:697] Replica learned NOP action at position 0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.038839 25435 log.cpp:569] Writer started with ending position 0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.039198 25433 leveldb.cpp:436] Reading position from leveldb took 10572ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.039412 25433 registrar.cpp:365] Successfully fetched the registry (0B) in 13.778944ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.039448 25433 registrar.cpp:464] Applied 1 operations in 4778ns; attempting to update the 'registry'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.039643 25428 log.cpp:577] Attempting to append 205 bytes to the log
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.039696 25432 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.039945 25430 replica.cpp:537] Replica received write request for position 1 from (19368)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.041738 25430 leveldb.cpp:341] Persisting action (224 bytes) to leveldb took 1.771112ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.041754 25430 replica.cpp:712] Persisted action at 1
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.041977 25432 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.043805 25432 leveldb.cpp:341] Persisting action (226 bytes) to leveldb took 1.810425ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.043820 25432 replica.cpp:712] Persisted action at 1
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.043825 25432 replica.cpp:697] Replica learned APPEND action at position 1
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.044040 25430 registrar.cpp:509] Successfully updated the 'registry' in 4.556032ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.044100 25430 registrar.cpp:395] Successfully recovered registrar
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.044124 25428 log.cpp:596] Attempting to truncate the log to 1
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.044215 25431 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.044244 25430 master.cpp:1777] Recovered 0 agents from the Registry (166B) ; allowing 10mins for agents to re-register
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.044317 25433 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.044497 25433 replica.cpp:537] Replica received write request for position 2 from (19369)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.046368 25433 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.851883ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.046383 25433 replica.cpp:712] Persisted action at 2
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.046583 25430 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.048426 25430 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.821628ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.048455 25430 leveldb.cpp:399] Deleting ~1 keys from leveldb took 14283ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.048463 25430 replica.cpp:712] Persisted action at 2
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.048468 25430 replica.cpp:697] Replica learned TRUNCATE action at position 2
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.055145 25413 containerizer.cpp:203] Using isolation: cgroups/mem,filesystem/posix,network/cni
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.058349 25413 linux_launcher.cpp:101] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.069301 25413 cluster.cpp:432] Creating default 'local' authorizer
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.069707 25431 slave.cpp:203] Agent started on 485)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.069718 25431 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""cgroups/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p""
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.069916 25431 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/credential'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.069967 25431 slave.cpp:341] Agent using credential for: test-principal
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.069984 25431 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/http_credentials'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070050 25431 slave.cpp:393] Using default 'basic' HTTP authenticator
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070127 25431 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
[00:48:30]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070282 25431 slave.cpp:592] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070309 25431 slave.cpp:600] Agent attributes: [  ]
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070314 25431 slave.cpp:605] Agent hostname: ip-172-30-2-56.mesosphere.io
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070484 25413 sched.cpp:224] Version: 1.0.0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070667 25433 sched.cpp:328] New master detected at master@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070711 25429 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/meta'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070749 25433 sched.cpp:394] Authenticating with master master@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070758 25433 sched.cpp:401] Using default CRAM-MD5 authenticatee
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070793 25430 status_update_manager.cpp:200] Recovering status update manager
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070904 25432 authenticatee.cpp:121] Creating new client SASL connection
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.070914 25430 containerizer.cpp:518] Recovering containerizer
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071049 25432 master.cpp:5943] Authenticating scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071105 25428 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(984)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071164 25434 authenticator.cpp:98] Creating new server SASL connection
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071241 25434 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071254 25434 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071292 25434 authenticator.cpp:204] Received SASL authentication start
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071336 25434 authenticator.cpp:326] Authentication requires more steps
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071374 25434 authenticatee.cpp:259] Received SASL authentication step
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071553 25434 authenticator.cpp:232] Received SASL authentication step
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071574 25434 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-56' server FQDN: 'ip-172-30-2-56' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071586 25434 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071594 25434 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071604 25434 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-56' server FQDN: 'ip-172-30-2-56' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071615 25434 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071619 25434 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071630 25434 authenticator.cpp:318] Authentication success
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071684 25428 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(984)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071687 25431 authenticatee.cpp:299] Authentication success
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071704 25434 master.cpp:5973] Successfully authenticated principal 'test-principal' at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071826 25431 sched.cpp:484] Successfully authenticated with master master@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071841 25431 sched.cpp:800] Sending SUBSCRIBE call to master@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071954 25431 sched.cpp:833] Will retry registration in 731.385085ms if necessary
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.071996 25434 master.cpp:2539] Received SUBSCRIBE call for framework 'default' at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072013 25434 master.cpp:2008] Authorizing framework principal 'test-principal' to receive offers for role '*'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072180 25430 master.cpp:2615] Subscribing framework default with checkpointing disabled and capabilities [  ]
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072305 25429 hierarchical.cpp:264] Added framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072326 25429 hierarchical.cpp:1488] No allocations performed
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072335 25429 hierarchical.cpp:1583] No inverse offers to send out!
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072347 25429 hierarchical.cpp:1139] Performed allocation for 0 agents in 26673ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072351 25431 provisioner.cpp:253] Provisioner recovery complete
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072371 25430 sched.cpp:723] Framework registered with 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072403 25430 sched.cpp:737] Scheduler::registered took 11852ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072587 25433 slave.cpp:4840] Finished recovery
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072760 25433 slave.cpp:5012] Querying resource estimator for oversubscribable resources
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072865 25431 status_update_manager.cpp:174] Pausing sending status updates
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072893 25432 slave.cpp:962] New master detected at master@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072906 25432 slave.cpp:1024] Authenticating with master master@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072917 25432 slave.cpp:1035] Using default CRAM-MD5 authenticatee
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072948 25432 slave.cpp:997] Detecting new master
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072976 25432 slave.cpp:5026] Received oversubscribable resources  from the resource estimator
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.072974 25435 authenticatee.cpp:121] Creating new client SASL connection
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073099 25434 master.cpp:5943] Authenticating slave(485)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073142 25434 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(985)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073213 25431 authenticator.cpp:98] Creating new server SASL connection
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073268 25431 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073287 25431 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073320 25431 authenticator.cpp:204] Received SASL authentication start
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073353 25431 authenticator.cpp:326] Authentication requires more steps
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073390 25431 authenticatee.cpp:259] Received SASL authentication step
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073444 25435 authenticator.cpp:232] Received SASL authentication step
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073460 25435 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-56' server FQDN: 'ip-172-30-2-56' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073465 25435 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073472 25435 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073477 25435 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-56' server FQDN: 'ip-172-30-2-56' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073480 25435 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073484 25435 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073493 25435 authenticator.cpp:318] Authentication success
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073534 25431 authenticatee.cpp:299] Authentication success
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073561 25435 master.cpp:5973] Successfully authenticated principal 'test-principal' at slave(485)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073590 25433 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(985)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073698 25431 slave.cpp:1103] Successfully authenticated with master master@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073742 25431 slave.cpp:1506] Will retry registration in 17.704164ms if necessary
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073786 25434 master.cpp:4653] Registering agent at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io) with id 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.073874 25434 registrar.cpp:464] Applied 1 operations in 9493ns; attempting to update the 'registry'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.074077 25430 log.cpp:577] Attempting to append 390 bytes to the log
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.074152 25432 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.074385 25431 replica.cpp:537] Replica received write request for position 3 from (19391)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.076269 25431 leveldb.cpp:341] Persisting action (409 bytes) to leveldb took 1.86243ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.076284 25431 replica.cpp:712] Persisted action at 3
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.076551 25434 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078383 25434 leveldb.cpp:341] Persisting action (411 bytes) to leveldb took 1.815955ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078398 25434 replica.cpp:712] Persisted action at 3
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078404 25434 replica.cpp:697] Replica learned APPEND action at position 3
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078703 25432 registrar.cpp:509] Successfully updated the 'registry' in 4.813056ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078745 25429 log.cpp:596] Attempting to truncate the log to 3
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078806 25433 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078909 25431 master.cpp:4721] Registered agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078928 25428 slave.cpp:3742] Received ping from slave-observer(439)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.078991 25430 hierarchical.cpp:473] Added agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 (ip-172-30-2-56.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079001 25428 slave.cpp:1147] Registered with master master@172.30.2.56:53790; given agent ID 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079020 25428 fetcher.cpp:86] Clearing fetcher cache
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079093 25430 hierarchical.cpp:1583] No inverse offers to send out!
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079111 25430 hierarchical.cpp:1162] Performed allocation for agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 in 100093ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079150 25435 replica.cpp:537] Replica received write request for position 4 from (19392)@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079233 25429 master.cpp:5772] Sending 1 offers to framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079259 25434 status_update_manager.cpp:181] Resuming sending status updates
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079263 25428 slave.cpp:1170] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/meta/slaves/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0/slave.info'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079396 25428 slave.cpp:1207] Forwarding total oversubscribed resources 
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079427 25429 sched.cpp:897] Scheduler::resourceOffers took 25735ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079448 25428 master.cpp:5066] Received update of agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io) with total oversubscribed resources 
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079608 25413 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:256;disk:1024
[00:48:30]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079612 25434 hierarchical.cpp:531] Agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 (ip-172-30-2-56.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079645 25434 hierarchical.cpp:1488] No allocations performed
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079651 25434 hierarchical.cpp:1583] No inverse offers to send out!
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079660 25434 hierarchical.cpp:1162] Performed allocation for agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 in 26873ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079957 25428 master.cpp:3457] Processing ACCEPT call for offers: [ 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-O0 ] on agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io) for framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.079979 25428 master.cpp:3095] Authorizing framework principal 'test-principal' to launch task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.080334 25432 master.hpp:178] Adding task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb with resources cpus(*):1; mem(*):256; disk(*):1024 on agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 (ip-172-30-2-56.mesosphere.io)
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.080365 25432 master.cpp:3946] Launching task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790 with resources cpus(*):1; mem(*):256; disk(*):1024 on agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io)
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.080495 25429 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):768; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):256; disk(*):1024) on agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 from framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.080507 25428 slave.cpp:1546] Got assigned task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb for framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.080528 25429 hierarchical.cpp:928] Framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 filtered agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 for 5secs
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.080602 25428 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[00:48:30]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.080718 25428 slave.cpp:1665] Launching task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb for framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.080747 25428 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[00:48:30]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.081048 25428 paths.cpp:528] Trying to chown '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/slaves/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0/frameworks/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000/executors/497f3d0f-ed77-4aef-9326-cf76c2dcbafb/runs/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8' to user 'root'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.082818 25435 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 3.508394ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.082859 25435 replica.cpp:712] Persisted action at 4
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.083400 25435 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.085247 25435 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.827229ms
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.085294 25435 leveldb.cpp:399] Deleting ~2 keys from leveldb took 30113ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.085304 25435 replica.cpp:712] Persisted action at 4
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.085310 25435 replica.cpp:697] Replica learned TRUNCATE action at position 4
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.085690 25428 slave.cpp:5729] Launching executor 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/slaves/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0/frameworks/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000/executors/497f3d0f-ed77-4aef-9326-cf76c2dcbafb/runs/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.085846 25428 slave.cpp:1891] Queuing task '497f3d0f-ed77-4aef-9326-cf76c2dcbafb' for executor '497f3d0f-ed77-4aef-9326-cf76c2dcbafb' of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.085849 25429 containerizer.cpp:777] Starting container '9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8' for executor '497f3d0f-ed77-4aef-9326-cf76c2dcbafb' of framework '0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.085898 25428 slave.cpp:915] Successfully attached file '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/slaves/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0/frameworks/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000/executors/497f3d0f-ed77-4aef-9326-cf76c2dcbafb/runs/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.087308 25428 mem.cpp:602] Started listening for OOM events for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.087671 25428 mem.cpp:722] Started listening on low memory pressure events for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.088007 25428 mem.cpp:722] Started listening on medium memory pressure events for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.088412 25428 mem.cpp:722] Started listening on critical memory pressure events for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.088750 25428 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 288MB for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.089221 25428 mem.cpp:388] Updated 'memory.limit_in_bytes' to 288MB for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.089759 25430 containerizer.cpp:1271] Launching 'mesos-containerizer' with flags '--command=""{""shell"":true,""value"":""\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor""}"" --commands=""{""commands"":[]}"" --help=""false"" --pipe_read=""110"" --pipe_write=""111"" --sandbox=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_Statistics_AF5X0p/slaves/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0/frameworks/0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000/executors/497f3d0f-ed77-4aef-9326-cf76c2dcbafb/runs/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8"" --user=""root""'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.089825 25430 linux_launcher.cpp:281] Cloning child process with flags = 
[00:48:30]W:	 [Step 10/10] WARNING: Logging before InitGoogleLogging() is written to STDERR
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.153952 10096 process.cpp:1060] libprocess is initialized on 172.30.2.56:34658 with 8 worker threads
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.156230 10096 logging.cpp:199] Logging to STDERR
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.157129 10096 exec.cpp:161] Version: 1.0.0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.157197 10125 exec.cpp:211] Executor started at: executor(1)@172.30.2.56:34658 with pid 10096
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.157687 25431 slave.cpp:2879] Got registration for executor '497f3d0f-ed77-4aef-9326-cf76c2dcbafb' of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 from executor(1)@172.30.2.56:34658
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.158280 10129 exec.cpp:236] Executor registered on agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.158689 25433 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 288MB for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.159274 25435 slave.cpp:2056] Sending queued task '497f3d0f-ed77-4aef-9326-cf76c2dcbafb' to executor '497f3d0f-ed77-4aef-9326-cf76c2dcbafb' of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 at executor(1)@172.30.2.56:34658
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.159399 10129 exec.cpp:248] Executor::registered took 64598ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.159651 10128 exec.cpp:323] Executor asked to run task '497f3d0f-ed77-4aef-9326-cf76c2dcbafb'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.159704 10128 exec.cpp:332] Executor::launchTask took 30558ns
[00:48:30] :	 [Step 10/10] Received SUBSCRIBED event
[00:48:30] :	 [Step 10/10] Subscribed executor on ip-172-30-2-56.mesosphere.io
[00:48:30] :	 [Step 10/10] Received LAUNCH event
[00:48:30] :	 [Step 10/10] Starting task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb
[00:48:30] :	 [Step 10/10] sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
[00:48:30] :	 [Step 10/10] Forked command at 10134
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.163949 10126 exec.cpp:546] Executor sending status update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.164324 25431 slave.cpp:3262] Handling status update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 from executor(1)@172.30.2.56:34658
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.164824 25428 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.164849 25428 status_update_manager.cpp:497] Creating StatusUpdate stream for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165026 25428 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 to the agent
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165132 25433 slave.cpp:3660] Forwarding the update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 to master@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165230 25433 slave.cpp:3554] Status update manager successfully handled status update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165251 25433 slave.cpp:3570] Sending acknowledgement for status update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 to executor(1)@172.30.2.56:34658
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165329 25430 master.cpp:5211] Status update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 from agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io)
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165349 25430 master.cpp:5259] Forwarding status update TASK_RUNNING (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165410 25430 master.cpp:6871] Updating the state of task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165560 10128 exec.cpp:369] Executor received status update acknowledgement 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1 for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165628 25432 sched.cpp:1005] Scheduler::statusUpdate took 78385ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165765 25432 master.cpp:4365] Processing ACKNOWLEDGE call 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1 for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790 on agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.165927 25428 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.166052 25428 slave.cpp:2648] Status update manager successfully handled status update acknowledgement (UUID: 3e8d37f5-2ac4-4b5e-ba0a-83215b0eaae1) for task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.583686 25428 master.cpp:4269] Telling agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io) to kill task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.583760 25428 slave.cpp:2086] Asked to kill task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.584074 10125 exec.cpp:343] Executor asked to kill task '497f3d0f-ed77-4aef-9326-cf76c2dcbafb'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.584121 10125 exec.cpp:352] Executor::killTask took 27333ns
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.959868 25430 mem.cpp:625] OOM notifier is triggered for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.959900 25430 mem.cpp:644] OOM detected for container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.960912 25430 mem.cpp:685] Memory limit exceeded: Requested: 288MB Maximum Used: 288MB
[00:48:30]W:	 [Step 10/10] 
[00:48:30]W:	 [Step 10/10] MEMORY STATISTICS: 
[00:48:30]W:	 [Step 10/10] cache 297218048
[00:48:30]W:	 [Step 10/10] rss 4771840
[00:48:30]W:	 [Step 10/10] rss_huge 0
[00:48:30]W:	 [Step 10/10] mapped_file 0
[00:48:30]W:	 [Step 10/10] pgpgin 75849
[00:48:30]W:	 [Step 10/10] pgpgout 2121
[00:48:30]W:	 [Step 10/10] pgfault 19539
[00:48:30]W:	 [Step 10/10] pgmajfault 0
[00:48:30]W:	 [Step 10/10] inactive_anon 0
[00:48:30]W:	 [Step 10/10] active_anon 4771840
[00:48:30]W:	 [Step 10/10] inactive_file 296955904
[00:48:30]W:	 [Step 10/10] active_file 253952
[00:48:30]W:	 [Step 10/10] unevictable 0
[00:48:30]W:	 [Step 10/10] hierarchical_memory_limit 301989888
[00:48:30]W:	 [Step 10/10] total_cache 297218048
[00:48:30]W:	 [Step 10/10] total_rss 4771840
[00:48:30]W:	 [Step 10/10] total_rss_huge 0
[00:48:30]W:	 [Step 10/10] total_mapped_file 0
[00:48:30]W:	 [Step 10/10] total_pgpgin 75849
[00:48:30]W:	 [Step 10/10] total_pgpgout 2121
[00:48:30]W:	 [Step 10/10] total_pgfault 19539
[00:48:30]W:	 [Step 10/10] total_pgmajfault 0
[00:48:30]W:	 [Step 10/10] total_inactive_anon 0
[00:48:30]W:	 [Step 10/10] total_active_anon 4771840
[00:48:30]W:	 [Step 10/10] total_inactive_file 296873984
[00:48:30]W:	 [Step 10/10] total_active_file 253952
[00:48:30]W:	 [Step 10/10] total_unevictable 0
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.961012 25430 containerizer.cpp:1833] Container 9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 has reached its limit for resource mem(*):288 and will be terminated
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.961050 25430 containerizer.cpp:1580] Destroying container '9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8'
[00:48:30]W:	 [Step 10/10] I0617 00:48:30.962447 25431 cgroups.cpp:2676] Freezing cgroup /cgroup/freezer/mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:48:45] :	 [Step 10/10] ../../src/tests/containerizer/memory_pressure_tests.cpp:172: Failure
[00:48:45] :	 [Step 10/10] Failed to wait 15secs for killed
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585013 25429 master.cpp:1406] Framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790 disconnected
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585052 25429 master.cpp:2840] Disconnecting framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585072 25429 master.cpp:2864] Deactivating framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585110 25429 master.cpp:1419] Giving framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790 0ns to failover
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585193 25432 hierarchical.cpp:375] Deactivated framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585247 25431 master.cpp:5624] Framework failover timeout, removing framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585269 25431 master.cpp:6354] Removing framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (default) at scheduler-21f8a988-6288-4ec1-9d6a-b66ae746896a@172.30.2.56:53790
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585325 25431 master.cpp:6871] Updating the state of task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585352 25434 slave.cpp:2269] Asked to shut down framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 by master@172.30.2.56:53790
[00:48:45] :	 [Step 10/10] ../../src/tests/containerizer/memory_pressure_tests.cpp:128: Failure
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585373 25434 slave.cpp:2294] Shutting down framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:45] :	 [Step 10/10] Actual function call count doesn't match EXPECT_CALL(sched, statusUpdate(&driver, _))...
[00:48:45] :	 [Step 10/10]          Expected: to be called at least twice
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585387 25434 slave.cpp:4465] Shutting down executor '497f3d0f-ed77-4aef-9326-cf76c2dcbafb' of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 at executor(1)@172.30.2.56:34658
[00:48:45] :	 [Step 10/10]            Actual: called once - unsatisfied and active
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585476 25429 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):256; disk(*):1024 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 from framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585492 25431 master.cpp:6937] Removing task 497f3d0f-ed77-4aef-9326-cf76c2dcbafb with resources cpus(*):1; mem(*):256; disk(*):1024 of framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 on agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io)
[00:48:45]W:	 [Step 10/10] I0617 00:48:45.585698 25431 hierarchical.cpp:326] Removed framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000
[00:49:00] :	 [Step 10/10] ../../src/tests/cluster.cpp:551: Failure
[00:49:00] :	 [Step 10/10] Failed to wait 15secs for wait
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.596581 25413 slave.cpp:834] Agent terminating
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.596611 25413 slave.cpp:2269] Asked to shut down framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 by @0.0.0.0:0
[00:49:00]W:	 [Step 10/10] W0617 00:49:00.596624 25413 slave.cpp:2290] Ignoring shutdown framework 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-0000 because it is terminating
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.596742 25428 master.cpp:1367] Agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io) disconnected
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.596761 25428 master.cpp:2899] Disconnecting agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io)
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.596807 25428 master.cpp:2918] Deactivating agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 at slave(485)@172.30.2.56:53790 (ip-172-30-2-56.mesosphere.io)
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.596863 25428 hierarchical.cpp:560] Agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0 deactivated
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.598708 25413 master.cpp:1214] Master terminating
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.598848 25431 hierarchical.cpp:505] Removed agent 0e92ffa4-4f26-4cea-84d3-9c67612de1bd-S0
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.601809 25433 cgroups.cpp:2676] Freezing cgroup /cgroup/freezer/mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.602758 25434 cgroups.cpp:1409] Successfully froze cgroup /cgroup/freezer/mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 after 0ns
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.603759 25431 cgroups.cpp:2694] Thawing cgroup /cgroup/freezer/mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8
[00:49:00]W:	 [Step 10/10] I0617 00:49:00.604717 25433 cgroups.cpp:1438] Successfully thawed cgroup /cgroup/freezer/mesos_test_d7ff4961-cb6d-4d51-bb21-10129a5c5572/9b1880d4-4b70-4eaf-b72e-cf207c4ee6b8 after 0ns
[00:49:00]W:	 [Step 10/10] E0617 00:49:00.605662 25436 process.cpp:2050] Failed to shutdown socket with fd 111: Transport endpoint is not connected
[00:49:15] :	 [Step 10/10] ../../src/tests/mesos.cpp:937: Failure
[00:49:15] :	 [Step 10/10] Failed to wait 15secs for cgroups::destroy(hierarchy, cgroup)
[00:49:15] :	 [Step 10/10] [  FAILED  ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics (45618 ms)
{noformat}",CentOS 6 with SSL enabled,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-22 23:04:08.696,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 22 23:04:08 UTC 2016,,,,,,,"0|i2zs3z:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 37,,,,,,,,,,,2.0,,,,,,,,,,,"22/Jun/16 23:04;jieyu;commit cefe4ce43f8048bb191c8fbdcbb9779741faeeeb
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jun 22 16:00:39 2016 -0700

    Fixed memory pressure test cgroup statistics.
    
    Review: https://reviews.apache.org/r/48992/",,,,,,,,,,,,,,,,,,,,,,,,,,,
MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery is flaky.,MESOS-5670,12981167,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,21/Jun/16 01:46,06/Jul/18 21:27,29/Oct/20 16:32,22/Jun/16 23:03,,,,,,,,,1.0.0,,,,,,containerization,test,,,,0,cgroups,flaky-test,mesosphere,,,,,,"{noformat}
[03:36:29] :	 [Step 10/10] [ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.461802  2797 cluster.cpp:155] Creating default 'local' authorizer
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.469468  2797 leveldb.cpp:174] Opened db in 7.527163ms
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470188  2797 leveldb.cpp:181] Compacted db in 699544ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470206  2797 leveldb.cpp:196] Created db iterator in 4293ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470211  2797 leveldb.cpp:202] Seeked to beginning of db in 535ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470216  2797 leveldb.cpp:271] Iterated through 0 keys in the db in 321ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470230  2797 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470510  2815 recover.cpp:451] Starting replica recovery
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.470592  2817 recover.cpp:477] Replica is in EMPTY status
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471029  2813 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (19800)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471139  2816 recover.cpp:197] Received a recover response from a replica in EMPTY status
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471271  2818 recover.cpp:568] Updating replica status to STARTING
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471606  2811 master.cpp:382] Master 6d44b7c1-ac0b-4409-97df-a53fa2e39d09 (ip-172-30-2-29.mesosphere.io) started on 172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471619  2811 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/baXWq5/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/baXWq5/master"" --zk_session_timeout=""10secs""
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471745  2811 master.cpp:434] Master only allowing authenticated frameworks to register
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471753  2811 master.cpp:448] Master only allowing authenticated agents to register
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471757  2811 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471761  2811 credentials.hpp:37] Loading credentials for authentication from '/tmp/baXWq5/credentials'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471829  2811 master.cpp:506] Using default 'crammd5' authenticator
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471868  2811 master.cpp:578] Using default 'basic' HTTP authenticator
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471941  2811 master.cpp:658] Using default 'basic' HTTP framework authenticator
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.471977  2811 master.cpp:705] Authorization enabled
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472034  2817 hierarchical.cpp:142] Initialized hierarchical allocator process
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472038  2814 whitelist_watcher.cpp:77] No whitelist given
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472506  2811 master.cpp:1969] The newly elected leader is master@172.30.2.29:37328 with id 6d44b7c1-ac0b-4409-97df-a53fa2e39d09
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472522  2811 master.cpp:1982] Elected as the leading master!
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472527  2811 master.cpp:1669] Recovering from registrar
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.472573  2812 registrar.cpp:332] Recovering registrar
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.473511  2816 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.195002ms
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.473527  2816 replica.cpp:320] Persisted replica status to STARTING
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.473578  2816 recover.cpp:477] Replica is in STARTING status
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.473877  2815 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (19803)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.473989  2814 recover.cpp:197] Received a recover response from a replica in STARTING status
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474126  2817 recover.cpp:568] Updating replica status to VOTING
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474735  2811 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 547332ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474748  2811 replica.cpp:320] Persisted replica status to VOTING
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474783  2811 recover.cpp:582] Successfully joined the Paxos group
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474829  2811 recover.cpp:466] Recover process terminated
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.474969  2818 log.cpp:553] Attempting to start the writer
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.475361  2811 replica.cpp:493] Replica received implicit promise request from (19804)@172.30.2.29:37328 with proposal 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.475944  2811 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 559444ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.475956  2811 replica.cpp:342] Persisted promised to 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.476215  2815 coordinator.cpp:238] Coordinator attempting to fill missing positions
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.476660  2816 replica.cpp:388] Replica received explicit promise request from (19805)@172.30.2.29:37328 for position 0 with proposal 2
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.477262  2816 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 584333ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.477273  2816 replica.cpp:712] Persisted action at 0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.477699  2815 replica.cpp:537] Replica received write request for position 0 from (19806)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.477726  2815 leveldb.cpp:436] Reading position from leveldb took 8842ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.478277  2815 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 537361ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.478291  2815 replica.cpp:712] Persisted action at 0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.478569  2811 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479132  2811 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 545208ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479146  2811 replica.cpp:712] Persisted action at 0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479152  2811 replica.cpp:697] Replica learned NOP action at position 0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479317  2814 log.cpp:569] Writer started with ending position 0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479568  2811 leveldb.cpp:436] Reading position from leveldb took 8325ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479786  2814 registrar.cpp:365] Successfully fetched the registry (0B) in 7.192064ms
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479822  2814 registrar.cpp:464] Applied 1 operations in 3018ns; attempting to update the 'registry'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.479995  2818 log.cpp:577] Attempting to append 205 bytes to the log
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.480044  2818 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.480309  2811 replica.cpp:537] Replica received write request for position 1 from (19807)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.480928  2811 leveldb.cpp:341] Persisting action (224 bytes) to leveldb took 596433ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.480942  2811 replica.cpp:712] Persisted action at 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.481148  2815 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.481710  2815 leveldb.cpp:341] Persisting action (226 bytes) to leveldb took 545656ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.481722  2815 replica.cpp:712] Persisted action at 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.481727  2815 replica.cpp:697] Replica learned APPEND action at position 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.481958  2816 registrar.cpp:509] Successfully updated the 'registry' in 2.119168ms
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482014  2816 registrar.cpp:395] Successfully recovered registrar
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482045  2817 log.cpp:596] Attempting to truncate the log to 1
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482117  2817 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482166  2816 master.cpp:1777] Recovered 0 agents from the Registry (166B) ; allowing 10mins for agents to re-register
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482177  2817 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482404  2817 replica.cpp:537] Replica received write request for position 2 from (19808)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482975  2817 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 552763ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.482986  2817 replica.cpp:712] Persisted action at 2
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.483301  2813 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.483870  2813 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 547529ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.483896  2813 leveldb.cpp:399] Deleting ~1 keys from leveldb took 12161ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.483904  2813 replica.cpp:712] Persisted action at 2
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.483911  2813 replica.cpp:697] Replica learned TRUNCATE action at position 2
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.492995  2797 containerizer.cpp:201] Using isolation: cgroups/mem,filesystem/posix,network/cni
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.496548  2797 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.503572  2797 cluster.cpp:432] Creating default 'local' authorizer
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.503936  2817 slave.cpp:203] Agent started on 488)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.503952  2817 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""cgroups/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL""
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504148  2817 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/credential'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504189  2817 slave.cpp:341] Agent using credential for: test-principal
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504199  2817 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/http_credentials'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504245  2817 slave.cpp:393] Using default 'basic' HTTP authenticator
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504410  2797 sched.cpp:224] Version: 1.0.0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504416  2817 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
[03:36:29]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504580  2818 sched.cpp:328] New master detected at master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504613  2818 sched.cpp:394] Authenticating with master master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504622  2818 sched.cpp:401] Using default CRAM-MD5 authenticatee
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504649  2817 slave.cpp:592] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504673  2817 slave.cpp:600] Agent attributes: [  ]
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504678  2817 slave.cpp:605] Agent hostname: ip-172-30-2-29.mesosphere.io
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504703  2816 authenticatee.cpp:121] Creating new client SASL connection
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504830  2818 master.cpp:5943] Authenticating scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504887  2816 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(991)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.504982  2811 authenticator.cpp:98] Creating new server SASL connection
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505004  2816 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505105  2813 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505131  2813 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505138  2818 status_update_manager.cpp:200] Recovering status update manager
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505167  2813 authenticator.cpp:204] Received SASL authentication start
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505200  2813 authenticator.cpp:326] Authentication requires more steps
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505200  2814 containerizer.cpp:514] Recovering containerizer
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505241  2813 authenticatee.cpp:259] Received SASL authentication step
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505300  2812 authenticator.cpp:232] Received SASL authentication step
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505317  2812 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-29.mesosphere.io' server FQDN: 'ip-172-30-2-29.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505323  2812 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505331  2812 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505337  2812 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-29.mesosphere.io' server FQDN: 'ip-172-30-2-29.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505342  2812 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505347  2812 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505355  2812 authenticator.cpp:318] Authentication success
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505399  2813 authenticatee.cpp:299] Authentication success
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505421  2811 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(991)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505436  2812 master.cpp:5973] Successfully authenticated principal 'test-principal' at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505534  2816 sched.cpp:484] Successfully authenticated with master master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505553  2816 sched.cpp:800] Sending SUBSCRIBE call to master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505591  2816 sched.cpp:833] Will retry registration in 11.319315ms if necessary
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505672  2815 master.cpp:2539] Received SUBSCRIBE call for framework 'default' at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505702  2815 master.cpp:2008] Authorizing framework principal 'test-principal' to receive offers for role '*'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.505854  2818 master.cpp:2615] Subscribing framework default with checkpointing enabled and capabilities [  ]
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506031  2818 sched.cpp:723] Framework registered with 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506050  2816 hierarchical.cpp:264] Added framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506072  2816 hierarchical.cpp:1488] No allocations performed
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506073  2818 sched.cpp:737] Scheduler::registered took 28711ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506093  2816 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506126  2816 hierarchical.cpp:1139] Performed allocation for 0 agents in 59667ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506428  2818 provisioner.cpp:253] Provisioner recovery complete
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506570  2815 slave.cpp:4845] Finished recovery
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506747  2815 slave.cpp:5017] Querying resource estimator for oversubscribable resources
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506878  2813 slave.cpp:967] New master detected at master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506886  2814 status_update_manager.cpp:174] Pausing sending status updates
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506903  2813 slave.cpp:1029] Authenticating with master master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506924  2813 slave.cpp:1040] Using default CRAM-MD5 authenticatee
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506976  2813 slave.cpp:1002] Detecting new master
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.506989  2816 authenticatee.cpp:121] Creating new client SASL connection
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507069  2813 slave.cpp:5031] Received oversubscribable resources  from the resource estimator
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507145  2815 master.cpp:5943] Authenticating slave(488)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507202  2811 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(992)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507264  2817 authenticator.cpp:98] Creating new server SASL connection
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507374  2817 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507387  2817 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507433  2813 authenticator.cpp:204] Received SASL authentication start
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507467  2813 authenticator.cpp:326] Authentication requires more steps
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507511  2813 authenticatee.cpp:259] Received SASL authentication step
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507578  2811 authenticator.cpp:232] Received SASL authentication step
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507597  2811 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-29.mesosphere.io' server FQDN: 'ip-172-30-2-29.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507606  2811 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507617  2811 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507629  2811 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-29.mesosphere.io' server FQDN: 'ip-172-30-2-29.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507640  2811 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507648  2811 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507686  2811 authenticator.cpp:318] Authentication success
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507750  2817 authenticatee.cpp:299] Authentication success
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507766  2811 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(992)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507786  2813 master.cpp:5973] Successfully authenticated principal 'test-principal' at slave(488)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507863  2817 slave.cpp:1108] Successfully authenticated with master master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507910  2817 slave.cpp:1511] Will retry registration in 10.588836ms if necessary
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.507966  2812 master.cpp:4653] Registering agent at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io) with id 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.508059  2817 registrar.cpp:464] Applied 1 operations in 13429ns; attempting to update the 'registry'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.508244  2812 log.cpp:577] Attempting to append 390 bytes to the log
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.508296  2817 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.508546  2815 replica.cpp:537] Replica received write request for position 3 from (19831)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509158  2815 leveldb.cpp:341] Persisting action (409 bytes) to leveldb took 589901ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509171  2815 replica.cpp:712] Persisted action at 3
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509403  2815 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509980  2815 leveldb.cpp:341] Persisting action (411 bytes) to leveldb took 558737ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509992  2815 replica.cpp:712] Persisted action at 3
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.509999  2815 replica.cpp:697] Replica learned APPEND action at position 3
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510262  2818 registrar.cpp:509] Successfully updated the 'registry' in 2.178048ms
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510313  2811 log.cpp:596] Attempting to truncate the log to 3
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510375  2817 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510486  2818 slave.cpp:3747] Received ping from slave-observer(447)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510519  2816 master.cpp:4721] Registered agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510540  2818 slave.cpp:1152] Registered with master master@172.30.2.29:37328; given agent ID 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510577  2818 fetcher.cpp:86] Clearing fetcher cache
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510577  2815 hierarchical.cpp:473] Added agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 (ip-172-30-2-29.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510639  2811 replica.cpp:537] Replica received write request for position 4 from (19832)@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510658  2816 status_update_manager.cpp:181] Resuming sending status updates
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510730  2815 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510747  2815 hierarchical.cpp:1162] Performed allocation for agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 in 127305ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510766  2818 slave.cpp:1175] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/slave.info'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510848  2816 master.cpp:5772] Sending 1 offers to framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510892  2818 slave.cpp:1212] Forwarding total oversubscribed resources 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510956  2818 master.cpp:5066] Received update of agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io) with total oversubscribed resources 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.510987  2817 sched.cpp:897] Scheduler::resourceOffers took 30391ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511080  2816 hierarchical.cpp:531] Agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 (ip-172-30-2-29.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511124  2816 hierarchical.cpp:1488] No allocations performed
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511132  2797 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:256;disk:1024
[03:36:29]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511133  2816 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511167  2816 hierarchical.cpp:1162] Performed allocation for agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 in 57933ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511201  2811 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 542938ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511214  2811 replica.cpp:712] Persisted action at 4
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511431  2818 master.cpp:3457] Processing ACCEPT call for offers: [ 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-O0 ] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io) for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511461  2818 master.cpp:3095] Authorizing framework principal 'test-principal' to launch task e9fcbad2-73bf-409e-9f71-023b826b5286
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511560  2816 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511827  2811 master.hpp:177] Adding task e9fcbad2-73bf-409e-9f71-023b826b5286 with resources cpus(*):1; mem(*):256; disk(*):1024 on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 (ip-172-30-2-29.mesosphere.io)
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511859  2811 master.cpp:3946] Launching task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328 with resources cpus(*):1; mem(*):256; disk(*):1024 on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io)
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511968  2814 slave.cpp:1551] Got assigned task e9fcbad2-73bf-409e-9f71-023b826b5286 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.511984  2815 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):768; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):256; disk(*):1024) on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 from framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512009  2815 hierarchical.cpp:928] Framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 filtered agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for 5secs
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512022  2814 slave.cpp:5654] Checkpointing FrameworkInfo to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/framework.info'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512127  2816 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 544409ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512138  2814 slave.cpp:5665] Checkpointing framework pid 'scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328' to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/framework.pid'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512153  2816 leveldb.cpp:399] Deleting ~2 keys from leveldb took 13134ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512162  2816 replica.cpp:712] Persisted action at 4
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512167  2816 replica.cpp:697] Replica learned TRUNCATE action at position 4
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512245  2814 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[03:36:29]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512377  2814 slave.cpp:1670] Launching task e9fcbad2-73bf-409e-9f71-023b826b5286 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512408  2814 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[03:36:29]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.512596  2814 paths.cpp:528] Trying to chown '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' to user 'root'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.517411  2814 slave.cpp:6136] Checkpointing ExecutorInfo to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/executor.info'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.517659  2814 slave.cpp:5734] Launching executor e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.517853  2814 slave.cpp:6159] Checkpointing TaskInfo to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82/tasks/e9fcbad2-73bf-409e-9f71-023b826b5286/task.info'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.517861  2818 containerizer.cpp:773] Starting container '8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' for executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework '6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.518013  2814 slave.cpp:1896] Queuing task 'e9fcbad2-73bf-409e-9f71-023b826b5286' for executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.518056  2814 slave.cpp:920] Successfully attached file '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.519455  2817 mem.cpp:602] Started listening for OOM events for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.519815  2817 mem.cpp:722] Started listening on low memory pressure events for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.520133  2817 mem.cpp:722] Started listening on medium memory pressure events for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.520447  2817 mem.cpp:722] Started listening on critical memory pressure events for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.520769  2817 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 288MB for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.521339  2817 mem.cpp:388] Updated 'memory.limit_in_bytes' to 288MB for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.521926  2816 containerizer.cpp:1267] Launching 'mesos-containerizer' with flags '--command=""{""shell"":true,""value"":""\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor""}"" --commands=""{""commands"":[]}"" --help=""false"" --pipe_read=""119"" --pipe_write=""120"" --sandbox=""/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82"" --user=""root""'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.521984  2816 linux_launcher.cpp:281] Cloning child process with flags = 
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.544052  2816 containerizer.cpp:1302] Checkpointing executor's forked pid 20673 to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82/pids/forked.pid'
[03:36:29]W:	 [Step 10/10] WARNING: Logging before InitGoogleLogging() is written to STDERR
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.603862 20687 process.cpp:1060] libprocess is initialized on 172.30.2.29:44617 with 8 worker threads
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.605692 20687 logging.cpp:199] Logging to STDERR
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.606240 20687 exec.cpp:161] Version: 1.0.0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.606302 20704 exec.cpp:211] Executor started at: executor(1)@172.30.2.29:44617 with pid 20687
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.606724  2814 slave.cpp:2884] Got registration for executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 from executor(1)@172.30.2.29:44617
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.606885  2814 slave.cpp:2970] Checkpointing executor pid 'executor(1)@172.30.2.29:44617' to '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82/pids/libprocess.pid'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.607306 20703 exec.cpp:236] Executor registered on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.607925  2815 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 288MB for container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.608141 20703 exec.cpp:248] Executor::registered took 89576ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.608538  2816 slave.cpp:2061] Sending queued task 'e9fcbad2-73bf-409e-9f71-023b826b5286' to executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 at executor(1)@172.30.2.29:44617
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.608767 20705 exec.cpp:323] Executor asked to run task 'e9fcbad2-73bf-409e-9f71-023b826b5286'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.608811 20705 exec.cpp:332] Executor::launchTask took 26475ns
[03:36:29] :	 [Step 10/10] Received SUBSCRIBED event
[03:36:29] :	 [Step 10/10] Subscribed executor on ip-172-30-2-29.mesosphere.io
[03:36:29] :	 [Step 10/10] Received LAUNCH event
[03:36:29] :	 [Step 10/10] Starting task e9fcbad2-73bf-409e-9f71-023b826b5286
[03:36:29] :	 [Step 10/10] Forked command at 20710
[03:36:29] :	 [Step 10/10] sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.611716 20705 exec.cpp:546] Executor sending status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.611974  2815 slave.cpp:3267] Handling status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 from executor(1)@172.30.2.29:44617
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.612499  2818 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.612527  2818 status_update_manager.cpp:497] Creating StatusUpdate stream for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.612751  2818 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.725725  2818 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 to the agent
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.725908  2817 slave.cpp:3665] Forwarding the update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 to master@172.30.2.29:37328
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.725999  2817 slave.cpp:3559] Status update manager successfully handled status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726016  2817 slave.cpp:3575] Sending acknowledgement for status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 to executor(1)@172.30.2.29:44617
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726124  2813 master.cpp:5211] Status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 from agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io)
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726157  2813 master.cpp:5259] Forwarding status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726238  2813 master.cpp:6871] Updating the state of task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726300 20701 exec.cpp:369] Executor received status update acknowledgement bea75e2e-9827-4410-9864-288f29c0a618 for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726363  2818 sched.cpp:1005] Scheduler::statusUpdate took 77055ns
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726517  2814 master.cpp:4365] Processing ACKNOWLEDGE call bea75e2e-9827-4410-9864-288f29c0a618 for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328 on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726757  2816 status_update_manager.cpp:392] Received status update acknowledgement (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:29]W:	 [Step 10/10] I0618 03:36:29.726812  2816 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:30]W:	 [Step 10/10] I0618 03:36:30.472790  2817 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):768; ports(*):[31000-32000] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:30]W:	 [Step 10/10] I0618 03:36:30.472841  2817 hierarchical.cpp:1488] No allocations performed
[03:36:30]W:	 [Step 10/10] I0618 03:36:30.472847  2817 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:30]W:	 [Step 10/10] I0618 03:36:30.472864  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 181038ns
[03:36:31]W:	 [Step 10/10] I0618 03:36:31.474026  2814 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):768; ports(*):[31000-32000] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:31]W:	 [Step 10/10] I0618 03:36:31.474076  2814 hierarchical.cpp:1488] No allocations performed
[03:36:31]W:	 [Step 10/10] I0618 03:36:31.474083  2814 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:31]W:	 [Step 10/10] I0618 03:36:31.474097  2814 hierarchical.cpp:1139] Performed allocation for 1 agents in 180187ns
[03:36:32]W:	 [Step 10/10] I0618 03:36:32.475332  2817 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):768; ports(*):[31000-32000] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:32]W:	 [Step 10/10] I0618 03:36:32.475383  2817 hierarchical.cpp:1488] No allocations performed
[03:36:32]W:	 [Step 10/10] I0618 03:36:32.475389  2817 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:32]W:	 [Step 10/10] I0618 03:36:32.475402  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 176560ns
[03:36:33]W:	 [Step 10/10] I0618 03:36:33.476011  2814 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):768; ports(*):[31000-32000] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:33]W:	 [Step 10/10] I0618 03:36:33.476059  2814 hierarchical.cpp:1488] No allocations performed
[03:36:33]W:	 [Step 10/10] I0618 03:36:33.476066  2814 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:33]W:	 [Step 10/10] I0618 03:36:33.476080  2814 hierarchical.cpp:1139] Performed allocation for 1 agents in 194002ns
[03:36:33]W:	 [Step 10/10] 512+0 records in
[03:36:33]W:	 [Step 10/10] 512+0 records out
[03:36:33]W:	 [Step 10/10] 536870912 bytes (537 MB, 512 MiB) copied, 4.23412 s, 127 MB/s
[03:36:34]W:	 [Step 10/10] I0618 03:36:34.477355  2814 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):768; ports(*):[31000-32000] on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:34]W:	 [Step 10/10] I0618 03:36:34.477406  2814 hierarchical.cpp:1488] No allocations performed
[03:36:34]W:	 [Step 10/10] I0618 03:36:34.477413  2814 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:34]W:	 [Step 10/10] I0618 03:36:34.477427  2814 hierarchical.cpp:1139] Performed allocation for 1 agents in 184403ns
[03:36:35]W:	 [Step 10/10] I0618 03:36:35.477726  2811 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:35]W:	 [Step 10/10] I0618 03:36:35.477774  2811 hierarchical.cpp:1139] Performed allocation for 1 agents in 202326ns
[03:36:35]W:	 [Step 10/10] I0618 03:36:35.477824  2818 master.cpp:5772] Sending 1 offers to framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:35]W:	 [Step 10/10] I0618 03:36:35.477948  2818 sched.cpp:897] Scheduler::resourceOffers took 9712ns
[03:36:36]W:	 [Step 10/10] I0618 03:36:36.478219  2814 hierarchical.cpp:1488] No allocations performed
[03:36:36]W:	 [Step 10/10] I0618 03:36:36.478235  2814 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:36]W:	 [Step 10/10] I0618 03:36:36.478245  2814 hierarchical.cpp:1139] Performed allocation for 1 agents in 47187ns
[03:36:37]W:	 [Step 10/10] I0618 03:36:37.478663  2811 hierarchical.cpp:1488] No allocations performed
[03:36:37]W:	 [Step 10/10] I0618 03:36:37.478678  2811 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:37]W:	 [Step 10/10] I0618 03:36:37.478693  2811 hierarchical.cpp:1139] Performed allocation for 1 agents in 45629ns
[03:36:38]W:	 [Step 10/10] I0618 03:36:38.479481  2817 hierarchical.cpp:1488] No allocations performed
[03:36:38]W:	 [Step 10/10] I0618 03:36:38.479516  2817 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:38]W:	 [Step 10/10] I0618 03:36:38.479532  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 98966ns
[03:36:39]W:	 [Step 10/10] I0618 03:36:39.480494  2813 hierarchical.cpp:1488] No allocations performed
[03:36:39]W:	 [Step 10/10] I0618 03:36:39.480526  2813 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:39]W:	 [Step 10/10] I0618 03:36:39.480543  2813 hierarchical.cpp:1139] Performed allocation for 1 agents in 87017ns
[03:36:40]W:	 [Step 10/10] I0618 03:36:40.481472  2812 hierarchical.cpp:1488] No allocations performed
[03:36:40]W:	 [Step 10/10] I0618 03:36:40.481504  2812 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:40]W:	 [Step 10/10] I0618 03:36:40.481519  2812 hierarchical.cpp:1139] Performed allocation for 1 agents in 122806ns
[03:36:41]W:	 [Step 10/10] I0618 03:36:41.482342  2813 hierarchical.cpp:1488] No allocations performed
[03:36:41]W:	 [Step 10/10] I0618 03:36:41.482378  2813 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:41]W:	 [Step 10/10] I0618 03:36:41.482393  2813 hierarchical.cpp:1139] Performed allocation for 1 agents in 98739ns
[03:36:42]W:	 [Step 10/10] I0618 03:36:42.483055  2817 hierarchical.cpp:1488] No allocations performed
[03:36:42]W:	 [Step 10/10] I0618 03:36:42.483083  2817 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:42]W:	 [Step 10/10] I0618 03:36:42.483095  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 73620ns
[03:36:43]W:	 [Step 10/10] I0618 03:36:43.483800  2811 hierarchical.cpp:1488] No allocations performed
[03:36:43]W:	 [Step 10/10] I0618 03:36:43.483837  2811 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:43]W:	 [Step 10/10] I0618 03:36:43.483853  2811 hierarchical.cpp:1139] Performed allocation for 1 agents in 103486ns
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.484480  2818 hierarchical.cpp:1488] No allocations performed
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.484508  2818 hierarchical.cpp:1583] No inverse offers to send out!
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.484522  2818 hierarchical.cpp:1139] Performed allocation for 1 agents in 76447ns
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.507843  2815 slave.cpp:5017] Querying resource estimator for oversubscribable resources
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.507937  2815 slave.cpp:5031] Received oversubscribable resources  from the resource estimator
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.511128  2812 slave.cpp:3747] Received ping from slave-observer(447)@172.30.2.29:37328
[03:36:44] :	 [Step 10/10] ../../src/tests/containerizer/memory_pressure_tests.cpp:263: Failure
[03:36:44] :	 [Step 10/10] Failed to wait 15secs for _statusUpdateAcknowledgement
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727337  2815 master.cpp:1406] Framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328 disconnected
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727363  2815 master.cpp:2840] Disconnecting framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727396  2815 master.cpp:2864] Deactivating framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727478  2814 hierarchical.cpp:375] Deactivated framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:44]W:	 [Step 10/10] W0618 03:36:44.727489  2815 master.hpp:1967] Master attempted to send message to disconnected framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727519  2815 master.cpp:1419] Giving framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328 0ns to failover
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727556  2814 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):768; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):256; disk(*):1024) on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 from framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.727741  2814 containerizer.cpp:1576] Destroying container '8a5ba23c-d1d2-4708-ab2f-40a6c269ef82'
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728740  2813 master.cpp:5624] Framework failover timeout, removing framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728765  2813 master.cpp:6354] Removing framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (default) at scheduler-3e992438-052b-45f0-af6a-851091145739@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728817  2813 master.cpp:6871] Updating the state of task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728827  2817 slave.cpp:2274] Asked to shut down framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 by master@172.30.2.29:37328
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728853  2817 slave.cpp:2299] Shutting down framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728869  2817 slave.cpp:4470] Shutting down executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 at executor(1)@172.30.2.29:44617
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728896  2811 cgroups.cpp:2676] Freezing cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728937  2815 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):256; disk(*):1024 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 from framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:44] :	 [Step 10/10] Received SHUTDOWN event
[03:36:44] :	 [Step 10/10] Shutting down
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.728950  2813 master.cpp:6937] Removing task e9fcbad2-73bf-409e-9f71-023b826b5286 with resources cpus(*):1; mem(*):256; disk(*):1024 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 on agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io)
[03:36:44] :	 [Step 10/10] Sending SIGTERM to process tree at pid 20710
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.729131 20707 exec.cpp:410] Executor asked to shutdown
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.729141  2815 hierarchical.cpp:326] Removed framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.729179 20707 exec.cpp:425] Executor::shutdown took 6153ns
[03:36:44]W:	 [Step 10/10] I0618 03:36:44.729199 20707 exec.cpp:92] Scheduling shutdown of the executor in 5secs
[03:36:45]W:	 [Step 10/10] I0618 03:36:45.485015  2818 hierarchical.cpp:1488] No allocations performed
[03:36:45]W:	 [Step 10/10] I0618 03:36:45.485038  2818 hierarchical.cpp:1139] Performed allocation for 1 agents in 47043ns
[03:36:46]W:	 [Step 10/10] I0618 03:36:46.485332  2811 hierarchical.cpp:1488] No allocations performed
[03:36:46]W:	 [Step 10/10] I0618 03:36:46.485350  2811 hierarchical.cpp:1139] Performed allocation for 1 agents in 33542ns
[03:36:47]W:	 [Step 10/10] I0618 03:36:47.486548  2817 hierarchical.cpp:1488] No allocations performed
[03:36:47]W:	 [Step 10/10] I0618 03:36:47.486588  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 84621ns
[03:36:48]W:	 [Step 10/10] I0618 03:36:48.487707  2813 hierarchical.cpp:1488] No allocations performed
[03:36:48]W:	 [Step 10/10] I0618 03:36:48.487751  2813 hierarchical.cpp:1139] Performed allocation for 1 agents in 83039ns
[03:36:49]W:	 [Step 10/10] I0618 03:36:49.488706  2812 hierarchical.cpp:1488] No allocations performed
[03:36:49]W:	 [Step 10/10] I0618 03:36:49.488745  2812 hierarchical.cpp:1139] Performed allocation for 1 agents in 78192ns
[03:36:49]W:	 [Step 10/10] I0618 03:36:49.729018  2811 slave.cpp:4543] Killing executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 at executor(1)@172.30.2.29:44617
[03:36:50]W:	 [Step 10/10] I0618 03:36:50.489168  2817 hierarchical.cpp:1488] No allocations performed
[03:36:50]W:	 [Step 10/10] I0618 03:36:50.489207  2817 hierarchical.cpp:1139] Performed allocation for 1 agents in 87236ns
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.369570  2818 slave.cpp:2653] Status update manager successfully handled status update acknowledgement (UUID: bea75e2e-9827-4410-9864-288f29c0a618) for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.430644  2813 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 after 6.70171904secs
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.431812  2818 cgroups.cpp:2694] Thawing cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.432981  2817 cgroups.cpp:1438] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82 after 1.140992ms
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.433709  2816 slave.cpp:3793] executor(1)@172.30.2.29:44617 exited
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.443989  2813 containerizer.cpp:1812] Executor for container '8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' has exited
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.446597  2818 provisioner.cpp:411] Ignoring destroy request for unknown container 8a5ba23c-d1d2-4708-ab2f-40a6c269ef82
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.446734  2813 slave.cpp:4152] Executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 terminated with signal Killed
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.446758  2813 slave.cpp:4256] Cleaning up executor 'e9fcbad2-73bf-409e-9f71-023b826b5286' of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000 at executor(1)@172.30.2.29:44617
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.446943  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' for gc 6.99999482767407days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447018  2813 slave.cpp:4344] Cleaning up framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447038  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286' for gc 6.9999948270963days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447082  2816 status_update_manager.cpp:282] Closing status update streams for framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447098  2816 status_update_manager.cpp:528] Cleaning up status update stream for task e9fcbad2-73bf-409e-9f71-023b826b5286 of framework 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447100  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286/runs/8a5ba23c-d1d2-4708-ab2f-40a6c269ef82' for gc 6.99999482669037days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447103  2813 slave.cpp:839] Agent terminating
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447149  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000/executors/e9fcbad2-73bf-409e-9f71-023b826b5286' for gc 6.99999482630815days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447190  2816 master.cpp:1367] Agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io) disconnected
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447209  2816 master.cpp:2899] Disconnecting agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io)
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447211  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000' for gc 6.99999482555556days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447237  2816 master.cpp:2918] Deactivating agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 at slave(488)@172.30.2.29:37328 (ip-172-30-2-29.mesosphere.io)
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447254  2812 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_MBzwwL/meta/slaves/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0/frameworks/6d44b7c1-ac0b-4409-97df-a53fa2e39d09-0000' for gc 6.99999482534815days in the future
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.447300  2816 hierarchical.cpp:560] Agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0 deactivated
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.448766  2797 master.cpp:1214] Master terminating
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.448875  2814 hierarchical.cpp:505] Removed agent 6d44b7c1-ac0b-4409-97df-a53fa2e39d09-S0
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.460062  2813 cgroups.cpp:2676] Freezing cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.562192  2816 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617 after 102.104064ms
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.563100  2816 cgroups.cpp:2694] Thawing cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617
[03:36:51]W:	 [Step 10/10] I0618 03:36:51.564021  2815 cgroups.cpp:1438] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos_test_ecfecccd-6714-4ec7-b5eb-a3071b772617 after 901888ns
[03:36:51] :	 [Step 10/10] [  FAILED  ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery (22119 ms)
{noformat}","CentOS 6, Debian 8, Ubuntu 15, Ubuntu 16",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-22 23:03:50.914,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 22 23:03:50 UTC 2016,,,,,,,"0|i2zs3r:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 37,,,,,,,,,,,2.0,,,,,,,,,,,"22/Jun/16 23:03;jieyu;commit 87a78e8327f6f321351ef53eb07a44c86dde1a53
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jun 22 16:00:43 2016 -0700

    Fixed memory pressure test cgroup slave recovery.
    
    Review: https://reviews.apache.org/r/48993/",,,,,,,,,,,,,,,,,,,,,,,,,,,
CNI isolator should not return failure if /etc/hostname does not exist on host.,MESOS-5669,12981163,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,21/Jun/16 01:36,26/Apr/17 17:00,29/Oct/20 16:32,22/Jun/16 23:04,,,,,,,,,1.0.0,,,,,,containerization,test,,,,0,isolator,mesosphere,tests,,,,,,"/etc/hostname may not necessarily exist on every system (e.g., CentOS 6). Currently CNI isolator just return a failure if it does not exist on host, because the isolator need to mount it into the container. This is fine for /etc/host and /etc/resolv.conf, but we should make an exception for /etc/hostname, because hostname may still be accessible even if /etc/hostname doesn't exist.

This issue relates to 3 failure tests:
{noformat}
[22:45:21] :	 [Step 10/10] [ RUN      ] CniIsolatorTest.ROOT_INTERNET_CURL_LaunchCommandTask
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.647611 24647 cluster.cpp:155] Creating default 'local' authorizer
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.655230 24647 leveldb.cpp:174] Opened db in 7.510408ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.657680 24647 leveldb.cpp:181] Compacted db in 2.427309ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.657702 24647 leveldb.cpp:196] Created db iterator in 6209ns
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.657709 24647 leveldb.cpp:202] Seeked to beginning of db in 692ns
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.657713 24647 leveldb.cpp:271] Iterated through 0 keys in the db in 431ns
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.657727 24647 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.657888 24662 recover.cpp:451] Starting replica recovery
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.658051 24668 recover.cpp:477] Replica is in EMPTY status
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.658495 24664 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (18401)@172.30.2.247:42024
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.658583 24662 recover.cpp:197] Received a recover response from a replica in EMPTY status
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.658687 24664 recover.cpp:568] Updating replica status to STARTING
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659111 24664 master.cpp:382] Master 9a4a353b-91c5-43b9-8c37-19245c37758c (ip-172-30-2-247.mesosphere.io) started on 172.30.2.247:42024
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659126 24664 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/l8346Z/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/l8346Z/master"" --zk_session_timeout=""10secs""
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659267 24664 master.cpp:434] Master only allowing authenticated frameworks to register
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659276 24664 master.cpp:448] Master only allowing authenticated agents to register
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659278 24664 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659282 24664 credentials.hpp:37] Loading credentials for authentication from '/tmp/l8346Z/credentials'
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659375 24664 master.cpp:506] Using default 'crammd5' authenticator
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659415 24664 master.cpp:578] Using default 'basic' HTTP authenticator
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659495 24664 master.cpp:658] Using default 'basic' HTTP framework authenticator
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659569 24664 master.cpp:705] Authorization enabled
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659684 24666 hierarchical.cpp:142] Initialized hierarchical allocator process
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.659696 24665 whitelist_watcher.cpp:77] No whitelist given
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.660269 24666 master.cpp:1969] The newly elected leader is master@172.30.2.247:42024 with id 9a4a353b-91c5-43b9-8c37-19245c37758c
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.660281 24666 master.cpp:1982] Elected as the leading master!
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.660290 24666 master.cpp:1669] Recovering from registrar
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.660342 24662 registrar.cpp:332] Recovering registrar
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.661232 24669 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.48585ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.661254 24669 replica.cpp:320] Persisted replica status to STARTING
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.661326 24669 recover.cpp:477] Replica is in STARTING status
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.661667 24668 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (18404)@172.30.2.247:42024
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.661758 24665 recover.cpp:197] Received a recover response from a replica in STARTING status
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.661893 24664 recover.cpp:568] Updating replica status to VOTING
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.663851 24664 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.915617ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.663866 24664 replica.cpp:320] Persisted replica status to VOTING
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.663899 24664 recover.cpp:582] Successfully joined the Paxos group
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.663944 24664 recover.cpp:466] Recover process terminated
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.664088 24668 log.cpp:553] Attempting to start the writer
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.664556 24668 replica.cpp:493] Replica received implicit promise request from (18405)@172.30.2.247:42024 with proposal 1
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.666551 24668 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.971938ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.666566 24668 replica.cpp:342] Persisted promised to 1
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.666767 24667 coordinator.cpp:238] Coordinator attempting to fill missing positions
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.667230 24668 replica.cpp:388] Replica received explicit promise request from (18406)@172.30.2.247:42024 for position 0 with proposal 2
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.669271 24668 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 2.02399ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.669287 24668 replica.cpp:712] Persisted action at 0
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.669656 24669 replica.cpp:537] Replica received write request for position 0 from (18407)@172.30.2.247:42024
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.669680 24669 leveldb.cpp:436] Reading position from leveldb took 10808ns
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.671674 24669 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.977316ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.671689 24669 replica.cpp:712] Persisted action at 0
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.671907 24665 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.673920 24665 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.991274ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.673935 24665 replica.cpp:712] Persisted action at 0
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.673941 24665 replica.cpp:697] Replica learned NOP action at position 0
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.674190 24665 log.cpp:569] Writer started with ending position 0
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.674489 24663 leveldb.cpp:436] Reading position from leveldb took 9059ns
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.674718 24663 registrar.cpp:365] Successfully fetched the registry (0B) in 14.355968ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.674747 24663 registrar.cpp:464] Applied 1 operations in 3070ns; attempting to update the 'registry'
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.674935 24665 log.cpp:577] Attempting to append 209 bytes to the log
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.674978 24665 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.675242 24666 replica.cpp:537] Replica received write request for position 1 from (18408)@172.30.2.247:42024
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.677088 24666 leveldb.cpp:341] Persisting action (228 bytes) to leveldb took 1.823904ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.677103 24666 replica.cpp:712] Persisted action at 1
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.677299 24667 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679270 24667 leveldb.cpp:341] Persisting action (230 bytes) to leveldb took 1.952303ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679286 24667 replica.cpp:712] Persisted action at 1
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679291 24667 replica.cpp:697] Replica learned APPEND action at position 1
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679481 24663 registrar.cpp:509] Successfully updated the 'registry' in 4.715264ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679503 24666 log.cpp:596] Attempting to truncate the log to 1
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679560 24667 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679581 24663 registrar.cpp:395] Successfully recovered registrar
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679745 24664 master.cpp:1777] Recovered 0 agents from the Registry (170B) ; allowing 10mins for agents to re-register
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679774 24662 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.679986 24662 replica.cpp:537] Replica received write request for position 2 from (18409)@172.30.2.247:42024
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.681895 24662 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.891877ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.681910 24662 replica.cpp:712] Persisted action at 2
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.682160 24666 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.684331 24666 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.153217ms
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.684375 24666 leveldb.cpp:399] Deleting ~1 keys from leveldb took 26973ns
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.684383 24666 replica.cpp:712] Persisted action at 2
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.684389 24666 replica.cpp:697] Replica learned TRUNCATE action at position 2
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.691529 24647 containerizer.cpp:201] Using isolation: docker/runtime,filesystem/linux,network/cni
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.694491 24647 linux_launcher.cpp:101] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher
[22:45:21]W:	 [Step 10/10] E0619 22:45:21.699741 24647 shell.hpp:106] Command 'hadoop version 2>&1' failed; this is the output:
[22:45:21]W:	 [Step 10/10] sh: hadoop: command not found
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.699769 24647 fetcher.cpp:62] Skipping URI fetcher plugin 'hadoop' as it could not be created: Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.699823 24647 registry_puller.cpp:111] Creating registry puller with docker registry 'https://registry-1.docker.io'
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.700865 24647 linux.cpp:146] Bind mounting '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG' and making it a shared mount
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.707801 24647 cni.cpp:286] Bind mounting '/var/run/mesos/isolators/network/cni' and making it a shared mount
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.714337 24647 cluster.cpp:432] Creating default 'local' authorizer
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.714825 24668 slave.cpp:203] Agent started on 468)@172.30.2.247:42024
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.714839 24668 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/l8346Z/store"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG/http_credentials"" --image_providers=""docker"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""docker/runtime,filesystem/linux,network/cni"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --network_cni_config_dir=""/tmp/l8346Z/configs"" --network_cni_plugins_dir=""/tmp/l8346Z/plugins"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG""
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.715116 24668 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG/credential'
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.715195 24668 slave.cpp:341] Agent using credential for: test-principal
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.715214 24668 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_CVAWpG/http_credentials'
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.715296 24668 slave.cpp:393] Using default 'basic' HTTP authenticator
[22:45:21]W:	 [Step 10/10] I0619 22:45:21.715400 24668 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
{noformat}

{noformat}
[22:45:38] :	 [Step 10/10] [ RUN      ] CniIsolatorTest.ROOT_VerifyCheckpointedInfo
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.459836 24647 cluster.cpp:155] Creating default 'local' authorizer
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.470319 24647 leveldb.cpp:174] Opened db in 10.34226ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.472771 24647 leveldb.cpp:181] Compacted db in 2.403554ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.472795 24647 leveldb.cpp:196] Created db iterator in 4446ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.472801 24647 leveldb.cpp:202] Seeked to beginning of db in 810ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.472806 24647 leveldb.cpp:271] Iterated through 0 keys in the db in 393ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.472822 24647 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.473093 24665 recover.cpp:451] Starting replica recovery
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.473260 24663 recover.cpp:477] Replica is in EMPTY status
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.473647 24663 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (18464)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.473752 24665 recover.cpp:197] Received a recover response from a replica in EMPTY status
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.473896 24667 recover.cpp:568] Updating replica status to STARTING
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474319 24663 master.cpp:382] Master 64f1f7ac-e810-4fb1-b549-6e29fc62622b (ip-172-30-2-247.mesosphere.io) started on 172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474329 24663 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/qJWqSY/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/qJWqSY/master"" --zk_session_timeout=""10secs""
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474452 24663 master.cpp:434] Master only allowing authenticated frameworks to register
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474457 24663 master.cpp:448] Master only allowing authenticated agents to register
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474459 24663 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474463 24663 credentials.hpp:37] Loading credentials for authentication from '/tmp/qJWqSY/credentials'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474551 24663 master.cpp:506] Using default 'crammd5' authenticator
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474598 24663 master.cpp:578] Using default 'basic' HTTP authenticator
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474643 24663 master.cpp:658] Using default 'basic' HTTP framework authenticator
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474674 24663 master.cpp:705] Authorization enabled
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474771 24668 whitelist_watcher.cpp:77] No whitelist given
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.474798 24664 hierarchical.cpp:142] Initialized hierarchical allocator process
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.475177 24663 master.cpp:1969] The newly elected leader is master@172.30.2.247:42024 with id 64f1f7ac-e810-4fb1-b549-6e29fc62622b
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.475188 24663 master.cpp:1982] Elected as the leading master!
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.475191 24663 master.cpp:1669] Recovering from registrar
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.475244 24662 registrar.cpp:332] Recovering registrar
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.476292 24669 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.312046ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.476308 24669 replica.cpp:320] Persisted replica status to STARTING
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.476368 24669 recover.cpp:477] Replica is in STARTING status
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.476687 24668 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (18467)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.476824 24666 recover.cpp:197] Received a recover response from a replica in STARTING status
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.476953 24668 recover.cpp:568] Updating replica status to VOTING
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.478798 24668 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.793996ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.478813 24668 replica.cpp:320] Persisted replica status to VOTING
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.478844 24668 recover.cpp:582] Successfully joined the Paxos group
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.478889 24668 recover.cpp:466] Recover process terminated
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.479060 24665 log.cpp:553] Attempting to start the writer
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.479547 24667 replica.cpp:493] Replica received implicit promise request from (18468)@172.30.2.247:42024 with proposal 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.481433 24667 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.8684ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.481449 24667 replica.cpp:342] Persisted promised to 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.481667 24662 coordinator.cpp:238] Coordinator attempting to fill missing positions
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.482067 24668 replica.cpp:388] Replica received explicit promise request from (18469)@172.30.2.247:42024 for position 0 with proposal 2
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.483842 24668 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.754044ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.483858 24668 replica.cpp:712] Persisted action at 0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.484235 24665 replica.cpp:537] Replica received write request for position 0 from (18470)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.484261 24665 leveldb.cpp:436] Reading position from leveldb took 10298ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.486331 24665 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 2.057217ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.486346 24665 replica.cpp:712] Persisted action at 0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.486574 24669 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.488533 24669 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.941228ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.488548 24669 replica.cpp:712] Persisted action at 0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.488553 24669 replica.cpp:697] Replica learned NOP action at position 0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.488690 24666 log.cpp:569] Writer started with ending position 0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.489006 24662 leveldb.cpp:436] Reading position from leveldb took 11082ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.489244 24667 registrar.cpp:365] Successfully fetched the registry (0B) in 13.976832ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.489276 24667 registrar.cpp:464] Applied 1 operations in 3438ns; attempting to update the 'registry'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.489450 24662 log.cpp:577] Attempting to append 209 bytes to the log
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.489514 24665 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.489785 24662 replica.cpp:537] Replica received write request for position 1 from (18471)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.491642 24662 leveldb.cpp:341] Persisting action (228 bytes) to leveldb took 1.838371ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.491657 24662 replica.cpp:712] Persisted action at 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.491885 24665 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.493649 24665 leveldb.cpp:341] Persisting action (230 bytes) to leveldb took 1.743495ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.493665 24665 replica.cpp:712] Persisted action at 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.493670 24665 replica.cpp:697] Replica learned APPEND action at position 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.493930 24669 registrar.cpp:509] Successfully updated the 'registry' in 4.638976ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.493983 24667 log.cpp:596] Attempting to truncate the log to 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.493994 24669 registrar.cpp:395] Successfully recovered registrar
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.494034 24668 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.494197 24662 master.cpp:1777] Recovered 0 agents from the Registry (170B) ; allowing 10mins for agents to re-register
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.494210 24666 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.494396 24662 replica.cpp:537] Replica received write request for position 2 from (18472)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.496301 24662 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.884992ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.496315 24662 replica.cpp:712] Persisted action at 2
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.496574 24666 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.498500 24666 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.906093ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.498529 24666 leveldb.cpp:399] Deleting ~1 keys from leveldb took 13787ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.498538 24666 replica.cpp:712] Persisted action at 2
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.498543 24666 replica.cpp:697] Replica learned TRUNCATE action at position 2
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.505269 24647 containerizer.cpp:201] Using isolation: network/cni,filesystem/posix
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.508313 24647 linux_launcher.cpp:101] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.509832 24647 cluster.cpp:432] Creating default 'local' authorizer
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510205 24666 slave.cpp:203] Agent started on 469)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510213 24666 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""network/cni"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --network_cni_config_dir=""/tmp/qJWqSY/configs"" --network_cni_plugins_dir=""/tmp/qJWqSY/plugins"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru""
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510442 24666 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/credential'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510510 24666 slave.cpp:341] Agent using credential for: test-principal
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510521 24666 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/http_credentials'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510604 24666 slave.cpp:393] Using default 'basic' HTTP authenticator
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510696 24666 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
[22:45:38]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510915 24647 sched.cpp:224] Version: 1.0.0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510962 24666 slave.cpp:592] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510984 24666 slave.cpp:600] Agent attributes: [  ]
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.510989 24666 slave.cpp:605] Agent hostname: ip-172-30-2-247.mesosphere.io
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511077 24669 sched.cpp:328] New master detected at master@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511162 24669 sched.cpp:394] Authenticating with master master@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511173 24669 sched.cpp:401] Using default CRAM-MD5 authenticatee
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511294 24662 authenticatee.cpp:121] Creating new client SASL connection
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511371 24667 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/meta'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511494 24665 master.cpp:5943] Authenticating scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511523 24668 status_update_manager.cpp:200] Recovering status update manager
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511566 24662 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(957)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511612 24664 containerizer.cpp:514] Recovering containerizer
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511706 24667 authenticator.cpp:98] Creating new server SASL connection
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511800 24667 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511816 24667 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511865 24667 authenticator.cpp:204] Received SASL authentication start
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511934 24667 authenticator.cpp:326] Authentication requires more steps
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.511977 24667 authenticatee.cpp:259] Received SASL authentication step
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512080 24668 authenticator.cpp:232] Received SASL authentication step
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512102 24668 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-247' server FQDN: 'ip-172-30-2-247' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512112 24668 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512125 24668 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512136 24668 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-247' server FQDN: 'ip-172-30-2-247' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512145 24668 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512152 24668 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512166 24668 authenticator.cpp:318] Authentication success
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512228 24665 authenticatee.cpp:299] Authentication success
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512233 24668 master.cpp:5973] Successfully authenticated principal 'test-principal' at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512253 24667 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(957)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512434 24664 sched.cpp:484] Successfully authenticated with master master@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512445 24664 sched.cpp:800] Sending SUBSCRIBE call to master@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512490 24667 provisioner.cpp:253] Provisioner recovery complete
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512609 24664 sched.cpp:833] Will retry registration in 550.501359ms if necessary
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512648 24663 master.cpp:2539] Received SUBSCRIBE call for framework 'default' at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512665 24663 master.cpp:2008] Authorizing framework principal 'test-principal' to receive offers for role '*'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512678 24667 slave.cpp:4845] Finished recovery
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512763 24663 master.cpp:2615] Subscribing framework default with checkpointing disabled and capabilities [  ]
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512876 24664 hierarchical.cpp:264] Added framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512893 24664 hierarchical.cpp:1488] No allocations performed
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512905 24664 hierarchical.cpp:1583] No inverse offers to send out!
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512922 24664 hierarchical.cpp:1139] Performed allocation for 0 agents in 33065ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.512940 24667 slave.cpp:5017] Querying resource estimator for oversubscribable resources
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513025 24666 sched.cpp:723] Framework registered with 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513056 24666 sched.cpp:737] Scheduler::registered took 18725ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513074 24669 status_update_manager.cpp:174] Pausing sending status updates
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513089 24667 slave.cpp:967] New master detected at master@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513105 24667 slave.cpp:1029] Authenticating with master master@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513120 24667 slave.cpp:1040] Using default CRAM-MD5 authenticatee
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513169 24667 slave.cpp:1002] Detecting new master
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513192 24663 authenticatee.cpp:121] Creating new client SASL connection
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513260 24667 slave.cpp:5031] Received oversubscribable resources  from the resource estimator
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513324 24666 master.cpp:5943] Authenticating slave(469)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513365 24666 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(958)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513423 24665 authenticator.cpp:98] Creating new server SASL connection
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513484 24665 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513494 24665 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513525 24665 authenticator.cpp:204] Received SASL authentication start
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513563 24665 authenticator.cpp:326] Authentication requires more steps
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513594 24665 authenticatee.cpp:259] Received SASL authentication step
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513635 24665 authenticator.cpp:232] Received SASL authentication step
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513653 24665 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-247' server FQDN: 'ip-172-30-2-247' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513661 24665 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513667 24665 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513674 24665 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-247' server FQDN: 'ip-172-30-2-247' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513677 24665 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513680 24665 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513689 24665 authenticator.cpp:318] Authentication success
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513727 24665 authenticatee.cpp:299] Authentication success
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513737 24664 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(958)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513754 24666 master.cpp:5973] Successfully authenticated principal 'test-principal' at slave(469)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513859 24669 slave.cpp:1108] Successfully authenticated with master master@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513921 24669 slave.cpp:1511] Will retry registration in 834760ns if necessary
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.513974 24666 master.cpp:4653] Registering agent at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) with id 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.514077 24668 registrar.cpp:464] Applied 1 operations in 12262ns; attempting to update the 'registry'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.514245 24666 log.cpp:577] Attempting to append 395 bytes to the log
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.514282 24666 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.514566 24662 replica.cpp:537] Replica received write request for position 3 from (18487)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.515151 24665 slave.cpp:1511] Will retry registration in 1.465145ms if necessary
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.515202 24667 master.cpp:4641] Ignoring register agent message from slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) as admission is already in progress
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.517513 24663 slave.cpp:1511] Will retry registration in 70.844019ms if necessary
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.517555 24664 master.cpp:4641] Ignoring register agent message from slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) as admission is already in progress
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.518628 24662 leveldb.cpp:341] Persisting action (414 bytes) to leveldb took 4.043654ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.518643 24662 replica.cpp:712] Persisted action at 3
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.518877 24665 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.520764 24665 leveldb.cpp:341] Persisting action (416 bytes) to leveldb took 1.869511ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.520779 24665 replica.cpp:712] Persisted action at 3
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.520784 24665 replica.cpp:697] Replica learned APPEND action at position 3
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521023 24663 registrar.cpp:509] Successfully updated the 'registry' in 6.930176ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521083 24668 log.cpp:596] Attempting to truncate the log to 3
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521152 24665 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521239 24667 master.cpp:4721] Registered agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521272 24665 slave.cpp:3747] Received ping from slave-observer(424)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521280 24664 hierarchical.cpp:473] Added agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 (ip-172-30-2-247.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521340 24665 slave.cpp:1152] Registered with master master@172.30.2.247:42024; given agent ID 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521354 24665 fetcher.cpp:86] Clearing fetcher cache
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521428 24664 hierarchical.cpp:1583] No inverse offers to send out!
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521455 24664 hierarchical.cpp:1162] Performed allocation for agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 in 131318ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521443 24669 replica.cpp:537] Replica received write request for position 4 from (18488)@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521466 24662 status_update_manager.cpp:181] Resuming sending status updates
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521502 24668 master.cpp:5772] Sending 1 offers to framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521553 24665 slave.cpp:1175] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/meta/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0/slave.info'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521667 24665 slave.cpp:1212] Forwarding total oversubscribed resources 
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521709 24665 master.cpp:5066] Received update of agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) with total oversubscribed resources 
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521725 24668 sched.cpp:897] Scheduler::resourceOffers took 35814ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521827 24665 hierarchical.cpp:531] Agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 (ip-172-30-2-247.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521860 24665 hierarchical.cpp:1488] No allocations performed
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521870 24665 hierarchical.cpp:1583] No inverse offers to send out!
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521884 24665 hierarchical.cpp:1162] Performed allocation for agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 in 36469ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.521885 24647 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:128
[22:45:38]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.522244 24666 master.cpp:3457] Processing ACCEPT call for offers: [ 64f1f7ac-e810-4fb1-b549-6e29fc62622b-O0 ] on agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) for framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.522267 24666 master.cpp:3095] Authorizing framework principal 'test-principal' to launch task 123bdde2-b542-4206-9554-249c053f63d2
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.522642 24666 master.hpp:177] Adding task 123bdde2-b542-4206-9554-249c053f63d2 with resources cpus(*):1; mem(*):128 on agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 (ip-172-30-2-247.mesosphere.io)
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.522666 24666 master.cpp:3946] Launching task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024 with resources cpus(*):1; mem(*):128 on agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.522780 24662 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):128) on agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 from framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.522799 24667 slave.cpp:1551] Got assigned task 123bdde2-b542-4206-9554-249c053f63d2 for framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.522804 24662 hierarchical.cpp:928] Framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 filtered agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 for 5secs
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.522893 24667 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[22:45:38]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.523059 24667 slave.cpp:1670] Launching task 123bdde2-b542-4206-9554-249c053f63d2 for framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.523108 24667 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[22:45:38]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.523439 24669 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.965378ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.523454 24669 replica.cpp:712] Persisted action at 4
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.523521 24667 paths.cpp:528] Trying to chown '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2/runs/e533d091-9fc2-4161-b6b3-4c99a88be466' to user 'root'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.526239 24665 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.528328 24665 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.028744ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.528360 24665 leveldb.cpp:399] Deleting ~2 keys from leveldb took 16691ns
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.528368 24665 replica.cpp:712] Persisted action at 4
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.528374 24665 replica.cpp:697] Replica learned TRUNCATE action at position 4
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.528923 24667 slave.cpp:5734] Launching executor 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2/runs/e533d091-9fc2-4161-b6b3-4c99a88be466'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.529093 24667 slave.cpp:1896] Queuing task '123bdde2-b542-4206-9554-249c053f63d2' for executor '123bdde2-b542-4206-9554-249c053f63d2' of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.529100 24669 containerizer.cpp:773] Starting container 'e533d091-9fc2-4161-b6b3-4c99a88be466' for executor '123bdde2-b542-4206-9554-249c053f63d2' of framework '64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.529126 24667 slave.cpp:920] Successfully attached file '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2/runs/e533d091-9fc2-4161-b6b3-4c99a88be466'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.529799 24663 containerizer.cpp:1120] Overwriting environment variable 'LIBPROCESS_IP', original: '172.30.2.247', new: '0.0.0.0', for container e533d091-9fc2-4161-b6b3-4c99a88be466
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.530079 24666 containerizer.cpp:1267] Launching 'mesos-containerizer' with flags '--command=""{""shell"":true,""value"":""\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor""}"" --commands=""{""commands"":[]}"" --help=""false"" --pipe_read=""96"" --pipe_write=""106"" --sandbox=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2/runs/e533d091-9fc2-4161-b6b3-4c99a88be466"" --user=""root""'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.530154 24666 linux_launcher.cpp:281] Cloning child process with flags = CLONE_NEWUTS | CLONE_NEWNS
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.533272 24662 cni.cpp:683] Bind mounted '/proc/7922/ns/net' to '/var/run/mesos/isolators/network/cni/e533d091-9fc2-4161-b6b3-4c99a88be466/ns' for container e533d091-9fc2-4161-b6b3-4c99a88be466
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.533452 24662 cni.cpp:977] Invoking CNI plugin 'mockPlugin' with network configuration '{""args"":{""org.apache.mesos"":{""network_info"":{""name"":""__MESOS_TEST__""}}},""name"":""__MESOS_TEST__"",""type"":""mockPlugin""}'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.606812 24663 cni.cpp:1066] Got assigned IPv4 address '172.17.42.1/16' from CNI network '__MESOS_TEST__' for container e533d091-9fc2-4161-b6b3-4c99a88be466
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.607293 24666 cni.cpp:808] DNS nameservers for container e533d091-9fc2-4161-b6b3-4c99a88be466 are:
[22:45:38]W:	 [Step 10/10] nameserver 172.30.0.2
[22:45:38]W:	 [Step 10/10] Failed to synchronize with agent (it's probably exited)
[22:45:38]W:	 [Step 10/10] E0619 22:45:38.707609 24662 slave.cpp:4039] Container 'e533d091-9fc2-4161-b6b3-4c99a88be466' for executor '123bdde2-b542-4206-9554-249c053f63d2' of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 failed to start: Collect failed: Failed to setup hostname and network files: WARNING: Logging before InitGoogleLogging() is written to STDERR
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.645313  7938 cni.cpp:1449] Set hostname to 'e533d091-9fc2-4161-b6b3-4c99a88be466'
[22:45:38]W:	 [Step 10/10] Mount point '/etc/hostname' does not exist on the host filesystem
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.707772 24669 containerizer.cpp:1576] Destroying container 'e533d091-9fc2-4161-b6b3-4c99a88be466'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.707787 24669 containerizer.cpp:1624] Waiting for the isolators to complete for container 'e533d091-9fc2-4161-b6b3-4c99a88be466'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.708878 24667 cgroups.cpp:2676] Freezing cgroup /cgroup/freezer/mesos/e533d091-9fc2-4161-b6b3-4c99a88be466
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.807951 24664 containerizer.cpp:1812] Executor for container 'e533d091-9fc2-4161-b6b3-4c99a88be466' has exited
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.810672 24666 cgroups.cpp:1409] Successfully froze cgroup /cgroup/freezer/mesos/e533d091-9fc2-4161-b6b3-4c99a88be466 after 101.766144ms
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.811637 24668 cgroups.cpp:2694] Thawing cgroup /cgroup/freezer/mesos/e533d091-9fc2-4161-b6b3-4c99a88be466
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.812523 24664 cgroups.cpp:1438] Successfully thawed cgroup /cgroup/freezer/mesos/e533d091-9fc2-4161-b6b3-4c99a88be466 after 864us
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.908664 24668 cni.cpp:1217] Unmounted the network namespace handle '/var/run/mesos/isolators/network/cni/e533d091-9fc2-4161-b6b3-4c99a88be466/ns' for container e533d091-9fc2-4161-b6b3-4c99a88be466
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.908843 24668 cni.cpp:1228] Removed the container directory '/var/run/mesos/isolators/network/cni/e533d091-9fc2-4161-b6b3-4c99a88be466'
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.909222 24669 provisioner.cpp:411] Ignoring destroy request for unknown container e533d091-9fc2-4161-b6b3-4c99a88be466
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.909346 24664 slave.cpp:4152] Executor '123bdde2-b542-4206-9554-249c053f63d2' of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 exited with status 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.909437 24664 slave.cpp:3267] Handling status update TASK_FAILED (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 from @0.0.0.0:0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.909620 24664 slave.cpp:6074] Terminating task 123bdde2-b542-4206-9554-249c053f63d2
[22:45:38]W:	 [Step 10/10] W0619 22:45:38.909713 24665 containerizer.cpp:1418] Ignoring update for unknown container: e533d091-9fc2-4161-b6b3-4c99a88be466
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.909871 24666 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.909888 24666 status_update_manager.cpp:497] Creating StatusUpdate stream for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910080 24666 status_update_manager.cpp:374] Forwarding update TASK_FAILED (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 to the agent
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910163 24665 slave.cpp:3665] Forwarding the update TASK_FAILED (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 to master@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910253 24665 slave.cpp:3559] Status update manager successfully handled status update TASK_FAILED (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910490 24667 master.cpp:5211] Status update TASK_FAILED (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 from agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910512 24667 master.cpp:5259] Forwarding status update TASK_FAILED (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910560 24667 master.cpp:6871] Updating the state of task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
[22:45:38] :	 [Step 10/10] ../../src/tests/containerizer/cni_isolator_tests.cpp:292: Failure
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910698 24668 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):128 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 from framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38] :	 [Step 10/10] Value of: statusRunning->state()
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910755 24666 sched.cpp:1005] Scheduler::statusUpdate took 50939ns
[22:45:38] :	 [Step 10/10]   Actual: TASK_FAILED
[22:45:38] :	 [Step 10/10] Expected: TASK_RUNNING
[22:45:38] :	 [Step 10/10] ../../src/tests/containerizer/cni_isolator_tests.cpp:296: Failure
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.910995 24662 master.cpp:4365] Processing ACKNOWLEDGE call 2b27727d-e7cd-4aea-b5a6-a83c83df5f01 for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024 on agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0
[22:45:38] :	 [Step 10/10] Value of: containers.get().size()
[22:45:38] :	 [Step 10/10]   Actual: 0
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911026 24662 master.cpp:6937] Removing task 123bdde2-b542-4206-9554-249c053f63d2 with resources cpus(*):1; mem(*):128 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 on agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:38] :	 [Step 10/10] Expected: 1u
[22:45:38] :	 [Step 10/10] Which is: 1
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911234 24665 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911278 24665 status_update_manager.cpp:528] Cleaning up status update stream for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911402 24669 master.cpp:1406] Framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024 disconnected
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911418 24669 master.cpp:2840] Disconnecting framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911414 24665 slave.cpp:2653] Status update manager successfully handled status update acknowledgement (UUID: 2b27727d-e7cd-4aea-b5a6-a83c83df5f01) for task 123bdde2-b542-4206-9554-249c053f63d2 of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911433 24669 master.cpp:2864] Deactivating framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911453 24665 slave.cpp:6115] Completing task 123bdde2-b542-4206-9554-249c053f63d2
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911470 24665 slave.cpp:4256] Cleaning up executor '123bdde2-b542-4206-9554-249c053f63d2' of framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911548 24669 master.cpp:1419] Giving framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024 0ns to failover
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911583 24662 hierarchical.cpp:375] Deactivated framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911640 24669 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2/runs/e533d091-9fc2-4161-b6b3-4c99a88be466' for gc 6.99998944950222days in the future
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911680 24665 slave.cpp:4344] Cleaning up framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911689 24669 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000/executors/123bdde2-b542-4206-9554-249c053f63d2' for gc 6.99998944832296days in the future
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911738 24664 status_update_manager.cpp:282] Closing status update streams for framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911805 24662 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_VerifyCheckpointedInfo_IIWOru/slaves/64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0/frameworks/64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000' for gc 6.99998944754667days in the future
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911918 24647 slave.cpp:839] Agent terminating
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.911991 24662 master.cpp:1367] Agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) disconnected
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.912009 24662 master.cpp:2899] Disconnecting agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.912029 24662 master.cpp:2918] Deactivating agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 at slave(469)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.912135 24665 hierarchical.cpp:560] Agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0 deactivated
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.912824 24669 master.cpp:5624] Framework failover timeout, removing framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.912842 24669 master.cpp:6354] Removing framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000 (default) at scheduler-17f04bf5-5b53-40c9-8a93-abe9a7966897@172.30.2.247:42024
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.913030 24669 hierarchical.cpp:326] Removed framework 64f1f7ac-e810-4fb1-b549-6e29fc62622b-0000
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.913905 24647 master.cpp:1214] Master terminating
[22:45:38]W:	 [Step 10/10] I0619 22:45:38.914031 24664 hierarchical.cpp:505] Removed agent 64f1f7ac-e810-4fb1-b549-6e29fc62622b-S0
[22:45:38] :	 [Step 10/10] [  FAILED  ] CniIsolatorTest.ROOT_VerifyCheckpointedInfo (457 ms)
{noformat}

{noformat}
[22:45:39] :	 [Step 10/10] [ RUN      ] CniIsolatorTest.ROOT_SlaveRecovery
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.224643 24647 cluster.cpp:155] Creating default 'local' authorizer
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.232614 24647 leveldb.cpp:174] Opened db in 7.839626ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.235198 24647 leveldb.cpp:181] Compacted db in 2.563679ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.235219 24647 leveldb.cpp:196] Created db iterator in 4353ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.235224 24647 leveldb.cpp:202] Seeked to beginning of db in 668ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.235231 24647 leveldb.cpp:271] Iterated through 0 keys in the db in 399ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.235246 24647 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.235555 24662 recover.cpp:451] Starting replica recovery
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.235777 24663 recover.cpp:477] Replica is in EMPTY status
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236134 24669 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (18550)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236197 24663 recover.cpp:197] Received a recover response from a replica in EMPTY status
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236351 24667 recover.cpp:568] Updating replica status to STARTING
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236580 24668 master.cpp:382] Master 032cd99a-1cdc-42d4-b94a-f7b00f37fb52 (ip-172-30-2-247.mesosphere.io) started on 172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236594 24668 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ghfuib/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/ghfuib/master"" --zk_session_timeout=""10secs""
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236723 24668 master.cpp:434] Master only allowing authenticated frameworks to register
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236729 24668 master.cpp:448] Master only allowing authenticated agents to register
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236732 24668 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236737 24668 credentials.hpp:37] Loading credentials for authentication from '/tmp/ghfuib/credentials'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236829 24668 master.cpp:506] Using default 'crammd5' authenticator
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236871 24668 master.cpp:578] Using default 'basic' HTTP authenticator
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236946 24668 master.cpp:658] Using default 'basic' HTTP framework authenticator
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.236991 24668 master.cpp:705] Authorization enabled
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.237077 24663 whitelist_watcher.cpp:77] No whitelist given
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.237159 24665 hierarchical.cpp:142] Initialized hierarchical allocator process
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.237638 24667 master.cpp:1969] The newly elected leader is master@172.30.2.247:42024 with id 032cd99a-1cdc-42d4-b94a-f7b00f37fb52
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.237650 24667 master.cpp:1982] Elected as the leading master!
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.237655 24667 master.cpp:1669] Recovering from registrar
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.237700 24669 registrar.cpp:332] Recovering registrar
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.239017 24662 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.616259ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.239032 24662 replica.cpp:320] Persisted replica status to STARTING
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.239084 24662 recover.cpp:477] Replica is in STARTING status
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.239437 24669 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (18553)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.239538 24662 recover.cpp:197] Received a recover response from a replica in STARTING status
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.239672 24663 recover.cpp:568] Updating replica status to VOTING
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.241654 24662 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.871972ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.241670 24662 replica.cpp:320] Persisted replica status to VOTING
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.241703 24662 recover.cpp:582] Successfully joined the Paxos group
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.241745 24662 recover.cpp:466] Recover process terminated
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.241880 24662 log.cpp:553] Attempting to start the writer
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.242295 24668 replica.cpp:493] Replica received implicit promise request from (18554)@172.30.2.247:42024 with proposal 1
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.244303 24668 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.98443ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.244318 24668 replica.cpp:342] Persisted promised to 1
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.244529 24663 coordinator.cpp:238] Coordinator attempting to fill missing positions
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.245007 24664 replica.cpp:388] Replica received explicit promise request from (18555)@172.30.2.247:42024 for position 0 with proposal 2
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.246898 24664 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.869865ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.246915 24664 replica.cpp:712] Persisted action at 0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.247295 24666 replica.cpp:537] Replica received write request for position 0 from (18556)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.247320 24666 leveldb.cpp:436] Reading position from leveldb took 10783ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.249264 24666 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.93015ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.249279 24666 replica.cpp:712] Persisted action at 0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.249492 24663 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.251349 24663 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.840655ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.251364 24663 replica.cpp:712] Persisted action at 0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.251369 24663 replica.cpp:697] Replica learned NOP action at position 0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.251634 24668 log.cpp:569] Writer started with ending position 0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.251905 24666 leveldb.cpp:436] Reading position from leveldb took 11014ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.252132 24664 registrar.cpp:365] Successfully fetched the registry (0B) in 14.413312ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.252176 24664 registrar.cpp:464] Applied 1 operations in 5009ns; attempting to update the 'registry'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.252378 24663 log.cpp:577] Attempting to append 209 bytes to the log
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.252437 24669 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.252768 24666 replica.cpp:537] Replica received write request for position 1 from (18557)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.254878 24666 leveldb.cpp:341] Persisting action (228 bytes) to leveldb took 2.087874ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.254894 24666 replica.cpp:712] Persisted action at 1
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.255100 24664 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.256983 24664 leveldb.cpp:341] Persisting action (230 bytes) to leveldb took 1.863178ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.256999 24664 replica.cpp:712] Persisted action at 1
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.257004 24664 replica.cpp:697] Replica learned APPEND action at position 1
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.257231 24663 registrar.cpp:509] Successfully updated the 'registry' in 5.034752ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.257283 24663 registrar.cpp:395] Successfully recovered registrar
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.257304 24665 log.cpp:596] Attempting to truncate the log to 1
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.257431 24666 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.257462 24668 master.cpp:1777] Recovered 0 agents from the Registry (170B) ; allowing 10mins for agents to re-register
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.257484 24662 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.257690 24668 replica.cpp:537] Replica received write request for position 2 from (18558)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.259577 24668 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.867119ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.259593 24668 replica.cpp:712] Persisted action at 2
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.259788 24667 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.261890 24667 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.084656ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.261920 24667 leveldb.cpp:399] Deleting ~1 keys from leveldb took 13997ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.261929 24667 replica.cpp:712] Persisted action at 2
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.261934 24667 replica.cpp:697] Replica learned TRUNCATE action at position 2
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.269104 24647 containerizer.cpp:201] Using isolation: network/cni,filesystem/posix
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.272172 24647 linux_launcher.cpp:101] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.273219 24647 cluster.cpp:432] Creating default 'local' authorizer
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.273654 24662 slave.cpp:203] Agent started on 471)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.273664 24662 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""network/cni"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --network_cni_config_dir=""/tmp/ghfuib/configs"" --network_cni_plugins_dir=""/tmp/ghfuib/plugins"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ""
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.273874 24662 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/credential'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.273952 24662 slave.cpp:341] Agent using credential for: test-principal
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.273967 24662 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/http_credentials'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274041 24662 slave.cpp:393] Using default 'basic' HTTP authenticator
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274193 24662 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
[22:45:39]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274448 24647 sched.cpp:224] Version: 1.0.0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274459 24662 slave.cpp:592] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274492 24662 slave.cpp:600] Agent attributes: [  ]
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274500 24662 slave.cpp:605] Agent hostname: ip-172-30-2-247.mesosphere.io
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274618 24669 sched.cpp:328] New master detected at master@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274714 24669 sched.cpp:394] Authenticating with master master@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274724 24669 sched.cpp:401] Using default CRAM-MD5 authenticatee
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274826 24667 authenticatee.cpp:121] Creating new client SASL connection
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274855 24662 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.274950 24667 master.cpp:5943] Authenticating scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275002 24669 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(961)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275041 24667 status_update_manager.cpp:200] Recovering status update manager
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275116 24668 authenticator.cpp:98] Creating new server SASL connection
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275132 24662 containerizer.cpp:514] Recovering containerizer
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275185 24668 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275197 24668 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275246 24666 authenticator.cpp:204] Received SASL authentication start
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275322 24666 authenticator.cpp:326] Authentication requires more steps
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275370 24666 authenticatee.cpp:259] Received SASL authentication step
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275445 24667 authenticator.cpp:232] Received SASL authentication step
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275462 24667 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-247' server FQDN: 'ip-172-30-2-247' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275468 24667 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275485 24667 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275492 24667 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-247' server FQDN: 'ip-172-30-2-247' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275497 24667 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275501 24667 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275511 24667 authenticator.cpp:318] Authentication success
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275563 24667 authenticatee.cpp:299] Authentication success
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275574 24664 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(961)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275586 24665 master.cpp:5973] Successfully authenticated principal 'test-principal' at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275640 24666 sched.cpp:484] Successfully authenticated with master master@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275653 24666 sched.cpp:800] Sending SUBSCRIBE call to master@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275758 24666 sched.cpp:833] Will retry registration in 1.75141928secs if necessary
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275781 24664 master.cpp:2539] Received SUBSCRIBE call for framework 'default' at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275804 24664 master.cpp:2008] Authorizing framework principal 'test-principal' to receive offers for role '*'
[22:45:39]W:	 [Step 10/10] W0619 22:45:39.275826 24665 cni.cpp:503] The checkpointed CNI plugin output '/var/run/mesos/isolators/network/cni/422a6a27-4327-4dc1-9a4c-7de578226eab/__MESOS_TEST__/eth0/network.info' for container 422a6a27-4327-4dc1-9a4c-7de578226eab does not exist
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275856 24665 cni.cpp:407] Removing unknown orphaned container 422a6a27-4327-4dc1-9a4c-7de578226eab
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.275918 24662 master.cpp:2615] Subscribing framework default with checkpointing enabled and capabilities [  ]
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.278825 24667 hierarchical.cpp:264] Added framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.278898 24667 hierarchical.cpp:1488] No allocations performed
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.278906 24667 hierarchical.cpp:1583] No inverse offers to send out!
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.278916 24667 hierarchical.cpp:1139] Performed allocation for 0 agents in 28334ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.278957 24662 sched.cpp:723] Framework registered with 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279021 24662 sched.cpp:737] Scheduler::registered took 46037ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279216 24669 provisioner.cpp:253] Provisioner recovery complete
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279381 24663 slave.cpp:4845] Finished recovery
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279583 24663 slave.cpp:5017] Querying resource estimator for oversubscribable resources
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279724 24667 status_update_manager.cpp:174] Pausing sending status updates
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279791 24668 slave.cpp:967] New master detected at master@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279808 24668 slave.cpp:1029] Authenticating with master master@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279826 24668 slave.cpp:1040] Using default CRAM-MD5 authenticatee
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279878 24668 slave.cpp:1002] Detecting new master
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279916 24666 authenticatee.cpp:121] Creating new client SASL connection
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.279953 24668 slave.cpp:5031] Received oversubscribable resources  from the resource estimator
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280045 24666 master.cpp:5943] Authenticating slave(471)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280129 24665 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(962)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280186 24665 authenticator.cpp:98] Creating new server SASL connection
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280266 24665 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280279 24665 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280308 24665 authenticator.cpp:204] Received SASL authentication start
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280345 24665 authenticator.cpp:326] Authentication requires more steps
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280383 24665 authenticatee.cpp:259] Received SASL authentication step
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280447 24669 authenticator.cpp:232] Received SASL authentication step
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280468 24669 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-247' server FQDN: 'ip-172-30-2-247' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280474 24669 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280483 24669 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280488 24669 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-247' server FQDN: 'ip-172-30-2-247' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280493 24669 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280496 24669 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280504 24669 authenticator.cpp:318] Authentication success
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280544 24669 authenticatee.cpp:299] Authentication success
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280568 24668 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(962)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280596 24665 master.cpp:5973] Successfully authenticated principal 'test-principal' at slave(471)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280673 24669 slave.cpp:1108] Successfully authenticated with master master@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280725 24669 slave.cpp:1511] Will retry registration in 8.06966ms if necessary
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280796 24667 master.cpp:4653] Registering agent at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) with id 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.280905 24663 registrar.cpp:464] Applied 1 operations in 11081ns; attempting to update the 'registry'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.281116 24669 log.cpp:577] Attempting to append 395 bytes to the log
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.281182 24664 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.281452 24663 replica.cpp:537] Replica received write request for position 3 from (18575)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.283769 24663 leveldb.cpp:341] Persisting action (414 bytes) to leveldb took 2.297945ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.283785 24663 replica.cpp:712] Persisted action at 3
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.283993 24663 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.285805 24663 leveldb.cpp:341] Persisting action (416 bytes) to leveldb took 1.793213ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.285820 24663 replica.cpp:712] Persisted action at 3
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.285826 24663 replica.cpp:697] Replica learned APPEND action at position 3
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286088 24668 registrar.cpp:509] Successfully updated the 'registry' in 5.161984ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286118 24666 log.cpp:596] Attempting to truncate the log to 3
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286172 24666 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286332 24667 slave.cpp:3747] Received ping from slave-observer(426)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286325 24665 master.cpp:4721] Registered agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286382 24668 hierarchical.cpp:473] Added agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 (ip-172-30-2-247.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286411 24667 slave.cpp:1152] Registered with master master@172.30.2.247:42024; given agent ID 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286424 24667 fetcher.cpp:86] Clearing fetcher cache
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286480 24665 status_update_manager.cpp:181] Resuming sending status updates
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286545 24669 replica.cpp:537] Replica received write request for position 4 from (18576)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286556 24668 hierarchical.cpp:1583] No inverse offers to send out!
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286579 24668 hierarchical.cpp:1162] Performed allocation for agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 in 176726ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286625 24667 slave.cpp:1175] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/slave.info'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286660 24663 master.cpp:5772] Sending 1 offers to framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286782 24667 slave.cpp:1212] Forwarding total oversubscribed resources 
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286842 24667 master.cpp:5066] Received update of agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) with total oversubscribed resources 
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286912 24663 sched.cpp:897] Scheduler::resourceOffers took 41729ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.286981 24667 hierarchical.cpp:531] Agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 (ip-172-30-2-247.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287026 24667 hierarchical.cpp:1488] No allocations performed
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287036 24667 hierarchical.cpp:1583] No inverse offers to send out!
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287051 24667 hierarchical.cpp:1162] Performed allocation for agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 in 40073ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287082 24647 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:128
[22:45:39]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287443 24668 master.cpp:3457] Processing ACCEPT call for offers: [ 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-O0 ] on agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) for framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287467 24668 master.cpp:3095] Authorizing framework principal 'test-principal' to launch task 44eba68b-0f7c-437f-935f-26a52ac3f64b
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287804 24667 master.hpp:177] Adding task 44eba68b-0f7c-437f-935f-26a52ac3f64b with resources cpus(*):1; mem(*):128 on agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 (ip-172-30-2-247.mesosphere.io)
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287829 24667 master.cpp:3946] Launching task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024 with resources cpus(*):1; mem(*):128 on agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287947 24664 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):128) on agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 from framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287981 24665 slave.cpp:1551] Got assigned task 44eba68b-0f7c-437f-935f-26a52ac3f64b for framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.287981 24664 hierarchical.cpp:928] Framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 filtered agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 for 5secs
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288043 24665 slave.cpp:5654] Checkpointing FrameworkInfo to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/framework.info'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288200 24665 slave.cpp:5665] Checkpointing framework pid 'scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024' to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/framework.pid'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288331 24665 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[22:45:39]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288467 24669 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.901433ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288480 24669 replica.cpp:712] Persisted action at 4
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288480 24665 slave.cpp:1670] Launching task 44eba68b-0f7c-437f-935f-26a52ac3f64b for framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288516 24665 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[22:45:39]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288709 24669 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.288784 24665 paths.cpp:528] Trying to chown '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' to user 'root'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.290657 24669 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.915037ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.290714 24669 leveldb.cpp:399] Deleting ~2 keys from leveldb took 27510ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.290726 24669 replica.cpp:712] Persisted action at 4
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.290736 24669 replica.cpp:697] Replica learned TRUNCATE action at position 4
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.292919 24665 slave.cpp:6136] Checkpointing ExecutorInfo to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/executor.info'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.293200 24665 slave.cpp:5734] Launching executor 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.293373 24669 containerizer.cpp:773] Starting container 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' for executor '44eba68b-0f7c-437f-935f-26a52ac3f64b' of framework '032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.293403 24665 slave.cpp:6159] Checkpointing TaskInfo to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d/tasks/44eba68b-0f7c-437f-935f-26a52ac3f64b/task.info'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.293581 24665 slave.cpp:1896] Queuing task '44eba68b-0f7c-437f-935f-26a52ac3f64b' for executor '44eba68b-0f7c-437f-935f-26a52ac3f64b' of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.293622 24665 slave.cpp:920] Successfully attached file '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.294059 24662 containerizer.cpp:1120] Overwriting environment variable 'LIBPROCESS_IP', original: '172.30.2.247', new: '0.0.0.0', for container ef0b6221-1073-42f0-adfc-cfe75ddb3a5d
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.294361 24664 containerizer.cpp:1267] Launching 'mesos-containerizer' with flags '--command=""{""shell"":true,""value"":""\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor""}"" --commands=""{""commands"":[]}"" --help=""false"" --pipe_read=""96"" --pipe_write=""107"" --sandbox=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d"" --user=""root""'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.294427 24664 linux_launcher.cpp:281] Cloning child process with flags = CLONE_NEWUTS | CLONE_NEWNS
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.296911 24664 containerizer.cpp:1302] Checkpointing executor's forked pid 7982 to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d/pids/forked.pid'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.297456 24663 cni.cpp:683] Bind mounted '/proc/7982/ns/net' to '/var/run/mesos/isolators/network/cni/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d/ns' for container ef0b6221-1073-42f0-adfc-cfe75ddb3a5d
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.297610 24663 cni.cpp:977] Invoking CNI plugin 'mockPlugin' with network configuration '{""args"":{""org.apache.mesos"":{""network_info"":{""name"":""__MESOS_TEST__""}}},""name"":""__MESOS_TEST__"",""type"":""mockPlugin""}'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.311471 24667 cni.cpp:1066] Got assigned IPv4 address '172.17.42.1/16' from CNI network '__MESOS_TEST__' for container ef0b6221-1073-42f0-adfc-cfe75ddb3a5d
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.311671 24667 cni.cpp:1217] Unmounted the network namespace handle '/var/run/mesos/isolators/network/cni/422a6a27-4327-4dc1-9a4c-7de578226eab/ns' for container 422a6a27-4327-4dc1-9a4c-7de578226eab
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.311774 24667 cni.cpp:1228] Removed the container directory '/var/run/mesos/isolators/network/cni/422a6a27-4327-4dc1-9a4c-7de578226eab'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.311911 24667 cni.cpp:808] DNS nameservers for container ef0b6221-1073-42f0-adfc-cfe75ddb3a5d are:
[22:45:39]W:	 [Step 10/10] nameserver 172.30.0.2
[22:45:39]W:	 [Step 10/10] EFailed to synchronize with agent (it's probably exited)0619 22:45:39.412294 24666 slave.cpp:4039] Container 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' for executor '44eba68b-0f7c-437f-935f-26a52ac3f64b' of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 failed to start: Collect failed: Failed to setup hostname and network files: WARNING: Logging before InitGoogleLogging() is written to STDERR
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.352311  7998 cni.cpp:1449] Set hostname to 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d'
[22:45:39]W:	 [Step 10/10] Mount point '/etc/hostname' does not exist on the host filesystem
[22:45:39]W:	 [Step 10/10] 
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.412427 24664 containerizer.cpp:1576] Destroying container 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.412444 24664 containerizer.cpp:1624] Waiting for the isolators to complete for container 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.413815 24662 cgroups.cpp:2676] Freezing cgroup /cgroup/freezer/mesos/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.512704 24665 containerizer.cpp:1812] Executor for container 'ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' has exited
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.516521 24662 cgroups.cpp:1409] Successfully froze cgroup /cgroup/freezer/mesos/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d after 102.685184ms
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.517462 24664 cgroups.cpp:2694] Thawing cgroup /cgroup/freezer/mesos/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.518301 24662 cgroups.cpp:1438] Successfully thawed cgroup /cgroup/freezer/mesos/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d after 813824ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.614039 24664 cni.cpp:1217] Unmounted the network namespace handle '/var/run/mesos/isolators/network/cni/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d/ns' for container ef0b6221-1073-42f0-adfc-cfe75ddb3a5d
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.614209 24664 cni.cpp:1228] Removed the container directory '/var/run/mesos/isolators/network/cni/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.614532 24662 provisioner.cpp:411] Ignoring destroy request for unknown container ef0b6221-1073-42f0-adfc-cfe75ddb3a5d
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.614665 24668 slave.cpp:4152] Executor '44eba68b-0f7c-437f-935f-26a52ac3f64b' of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 exited with status 1
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.614742 24668 slave.cpp:3267] Handling status update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 from @0.0.0.0:0
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.614964 24669 slave.cpp:6074] Terminating task 44eba68b-0f7c-437f-935f-26a52ac3f64b
[22:45:39]W:	 [Step 10/10] W0619 22:45:39.615054 24669 containerizer.cpp:1418] Ignoring update for unknown container: ef0b6221-1073-42f0-adfc-cfe75ddb3a5d
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.615197 24668 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.615211 24668 status_update_manager.cpp:497] Creating StatusUpdate stream for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.615552 24668 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.623800 24668 status_update_manager.cpp:374] Forwarding update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 to the agent
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.623888 24662 slave.cpp:3665] Forwarding the update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 to master@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.623988 24662 slave.cpp:3559] Status update manager successfully handled status update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624162 24668 master.cpp:5211] Status update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 from agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624181 24668 master.cpp:5259] Forwarding status update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624243 24668 master.cpp:6871] Updating the state of task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624356 24666 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):128 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 from framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624442 24669 sched.cpp:1005] Scheduler::statusUpdate took 38999ns
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624614 24664 master.cpp:4365] Processing ACKNOWLEDGE call addda641-2dda-4d21-bc97-f2d8f9e28fe7 for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024 on agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0
[22:45:39] :	 [Step 10/10] ../../src/tests/containerizer/cni_isolator_tests.cpp:487: Failure
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624634 24664 master.cpp:6937] Removing task 44eba68b-0f7c-437f-935f-26a52ac3f64b with resources cpus(*):1; mem(*):128 of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 on agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:39] :	 [Step 10/10] Value of: statusRunning->state()
[22:45:39] :	 [Step 10/10]   Actual: TASK_FAILED
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624804 24665 status_update_manager.cpp:392] Received status update acknowledgement (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39] :	 [Step 10/10] Expected: TASK_RUNNING
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.624847 24665 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_FAILED (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628197 24665 status_update_manager.cpp:528] Cleaning up status update stream for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628276 24665 slave.cpp:2653] Status update manager successfully handled status update acknowledgement (UUID: addda641-2dda-4d21-bc97-f2d8f9e28fe7) for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628291 24665 slave.cpp:6115] Completing task 44eba68b-0f7c-437f-935f-26a52ac3f64b
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628304 24665 slave.cpp:4256] Cleaning up executor '44eba68b-0f7c-437f-935f-26a52ac3f64b' of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628487 24669 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' for gc 6.99999272627556days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628530 24669 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b' for gc 6.99999272575111days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628592 24663 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' for gc 6.99999272505778days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628623 24665 slave.cpp:4344] Cleaning up framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628635 24663 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b' for gc 6.9999927244563days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628679 24663 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000' for gc 6.99999272379556days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628686 24667 status_update_manager.cpp:282] Closing status update streams for framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628693 24665 slave.cpp:839] Agent terminating
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628718 24663 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000' for gc 6.9999927235763days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628746 24665 master.cpp:1367] Agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io) disconnected
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628760 24665 master.cpp:2899] Disconnecting agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628783 24665 master.cpp:2918] Deactivating agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 at slave(471)@172.30.2.247:42024 (ip-172-30-2-247.mesosphere.io)
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.628832 24665 hierarchical.cpp:560] Agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0 deactivated
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.629724 24647 containerizer.cpp:201] Using isolation: network/cni,filesystem/posix
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.633214 24647 linux_launcher.cpp:101] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.634099 24647 cluster.cpp:432] Creating default 'local' authorizer
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.634585 24664 slave.cpp:203] Agent started on 472)@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.634599 24664 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""network/cni"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --network_cni_config_dir=""/tmp/ghfuib/configs"" --network_cni_plugins_dir=""/tmp/ghfuib/plugins"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ""
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.634822 24664 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/credential'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.634886 24664 slave.cpp:341] Agent using credential for: test-principal
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.634897 24664 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/http_credentials'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.634953 24664 slave.cpp:393] Using default 'basic' HTTP authenticator
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.635114 24664 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
[22:45:39]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.635422 24664 slave.cpp:592] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.635444 24664 slave.cpp:600] Agent attributes: [  ]
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.635449 24664 slave.cpp:605] Agent hostname: ip-172-30-2-247.mesosphere.io
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.635941 24668 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.635964 24668 state.cpp:697] No checkpointed resources found at '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/resources/resources.info'
[22:45:39]W:	 [Step 10/10] W0619 22:45:39.636117 24669 master.cpp:4232] Cannot kill task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024 because it is unknown; performing reconciliation
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636142 24669 master.cpp:5510] Performing explicit task state reconciliation for 1 tasks of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39] :	 [Step 10/10] ../../src/tests/containerizer/cni_isolator_tests.cpp:504: Failure
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636162 24669 master.cpp:5600] Sending explicit reconciliation state TASK_LOST for task 44eba68b-0f7c-437f-935f-26a52ac3f64b of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39] :	 [Step 10/10] Value of: statusKilled->state()
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636317 24669 sched.cpp:1005] Scheduler::statusUpdate took 28596ns
[22:45:39] :	 [Step 10/10]   Actual: TASK_LOST
[22:45:39] :	 [Step 10/10] Expected: TASK_KILLED
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636548 24647 sched.cpp:1964] Asked to stop the driver
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636605 24669 sched.cpp:1167] Stopping framework '032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000'
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636728 24662 master.cpp:6342] Processing TEARDOWN call for framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] W0619 22:45:39.636739 24668 state.cpp:544] Failed to find executor libprocess pid/http marker file
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636747 24662 master.cpp:6354] Removing framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 (default) at scheduler-fb3e96c6-1106-4910-80d7-e83d75960307@172.30.2.247:42024
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636823 24664 hierarchical.cpp:375] Deactivated framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.636973 24667 hierarchical.cpp:326] Removed framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637154 24663 fetcher.cpp:86] Clearing fetcher cache
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637192 24663 slave.cpp:4933] Recovering framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637208 24663 slave.cpp:5858] Recovering executor '44eba68b-0f7c-437f-935f-26a52ac3f64b' of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637316 24663 slave.cpp:6074] Terminating task 44eba68b-0f7c-437f-935f-26a52ac3f64b
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637331 24663 slave.cpp:6115] Completing task 44eba68b-0f7c-437f-935f-26a52ac3f64b
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637421 24666 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' for gc 6.99999262296296days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637454 24666 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b/runs/ef0b6221-1073-42f0-adfc-cfe75ddb3a5d' for gc 6.99999262252444days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637460 24663 slave.cpp:4344] Cleaning up framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637477 24666 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b' for gc 6.99999262231407days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637514 24666 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000/executors/44eba68b-0f7c-437f-935f-26a52ac3f64b' for gc 6.99999262212741days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637534 24667 status_update_manager.cpp:282] Closing status update streams for framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637547 24666 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000' for gc 6.99999262155259days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637574 24666 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_SlaveRecovery_UbK9bJ/meta/slaves/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0/frameworks/032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000' for gc 6.99999262134222days in the future
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637648 24668 status_update_manager.cpp:200] Recovering status update manager
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637660 24668 status_update_manager.cpp:208] Recovering executor '44eba68b-0f7c-437f-935f-26a52ac3f64b' of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637676 24668 status_update_manager.cpp:233] Skipping recovering updates of executor '44eba68b-0f7c-437f-935f-26a52ac3f64b' of framework 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-0000 because its latest run ef0b6221-1073-42f0-adfc-cfe75ddb3a5d is completed
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.637729 24663 slave.cpp:839] Agent terminating
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.639163 24647 master.cpp:1214] Master terminating
[22:45:39]W:	 [Step 10/10] I0619 22:45:39.639279 24667 hierarchical.cpp:505] Removed agent 032cd99a-1cdc-42d4-b94a-f7b00f37fb52-S0
[22:45:39] :	 [Step 10/10] [  FAILED  ] CniIsolatorTest.ROOT_SlaveRecovery (418 ms)
{noformat}",CentOS 6 with/without SSL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-22 23:04:27.533,,,false,MESOS-4641,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 22 23:04:27 UTC 2016,,,,,,,"0|i2zs2v:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 37,,,,,,,,,,,3.0,,,,,,,,,,,"22/Jun/16 23:04;jieyu;commit 3805ebb5d0926ba2af1fec2c0644b82c5b71ddc0
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jun 22 16:00:36 2016 -0700

    Added exception for etc hostname mount in cni isolator.
    
    Review: https://reviews.apache.org/r/48991/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add CGROUP namespace to linux ns helper.,MESOS-5668,12981161,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,21/Jun/16 01:23,06/Jul/18 21:27,29/Oct/20 16:32,22/Jun/16 23:04,,,,,,,,,1.0.0,,,,,,containerization,,,,,0,cgroups,linux,mesosphere,namespace,,,,,"Since linux kernel 4.6, CGROUP namespace is added. we need to support the handle for the cgroup namespace of the process.

This also relates to two test failures on Ubuntu 16:
{noformat}
[22:41:26] :	 [Step 10/10] [ RUN      ] NsTest.ROOT_setns
[22:41:26] :	 [Step 10/10] ../../src/tests/containerizer/ns_tests.cpp:75: Failure
[22:41:26] :	 [Step 10/10] nstype: Unknown namespace 'cgroup'
[22:41:26] :	 [Step 10/10] [  FAILED  ] NsTest.ROOT_setns (1 ms)
{noformat}

{noformat}
[22:41:26] :	 [Step 10/10] [ RUN      ] NsTest.ROOT_getns
[22:41:26] :	 [Step 10/10] ../../src/tests/containerizer/ns_tests.cpp:160: Failure
[22:41:26] :	 [Step 10/10] nstype: Unknown namespace 'cgroup'
[22:41:26] :	 [Step 10/10] [  FAILED  ] NsTest.ROOT_getns (0 ms)
{noformat}",Ubuntu 16 with/without SSL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-22 23:04:43.266,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 22 23:04:43 UTC 2016,,,,,,,"0|i2zs2f:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 37,,,,,,,,,,,3.0,,,,,,,,,,,"22/Jun/16 23:04;jieyu;commit 1498c04bebd7e191021173c4d5f6705e4674c366
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jun 22 16:00:33 2016 -0700

    Added CGROUP namespace to ns helper.
    
    Review: https://reviews.apache.org/r/48990/",,,,,,,,,,,,,,,,,,,,,,,,,,,
CniIsolatorTest.ROOT_INTERNET_CURL_LaunchCommandTask fails on CentOS 7.,MESOS-5667,12981159,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,21/Jun/16 01:03,26/Apr/17 17:00,29/Oct/20 16:32,22/Jun/16 23:05,,,,,,,,,1.0.0,,,,,,containerization,test,,,,0,containerizer,isolator,mesosphere,,,,,,"{noformat}
[22:41:54] :	 [Step 10/10] [ RUN      ] CniIsolatorTest.ROOT_INTERNET_CURL_LaunchCommandTask
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.348641 30896 cluster.cpp:155] Creating default 'local' authorizer
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.353384 30896 leveldb.cpp:174] Opened db in 4.634552ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354763 30896 leveldb.cpp:181] Compacted db in 1.360201ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354784 30896 leveldb.cpp:196] Created db iterator in 3421ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354790 30896 leveldb.cpp:202] Seeked to beginning of db in 633ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354797 30896 leveldb.cpp:271] Iterated through 0 keys in the db in 401ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354811 30896 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.354990 30913 recover.cpp:451] Starting replica recovery
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.355123 30915 recover.cpp:477] Replica is in EMPTY status
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.355391 30915 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (18695)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.355479 30912 recover.cpp:197] Received a recover response from a replica in EMPTY status
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.355581 30914 recover.cpp:568] Updating replica status to STARTING
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356091 30910 master.cpp:382] Master 27c796db-6f98-4d61-96c0-f583f22787ff (ip-172-30-2-105.mesosphere.io) started on 172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356104 30910 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/KhgYrQ/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/KhgYrQ/master"" --zk_session_timeout=""10secs""
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356237 30910 master.cpp:434] Master only allowing authenticated frameworks to register
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356245 30910 master.cpp:448] Master only allowing authenticated agents to register
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356247 30910 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356251 30910 credentials.hpp:37] Loading credentials for authentication from '/tmp/KhgYrQ/credentials'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356351 30910 master.cpp:506] Using default 'crammd5' authenticator
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356389 30910 master.cpp:578] Using default 'basic' HTTP authenticator
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356439 30910 master.cpp:658] Using default 'basic' HTTP framework authenticator
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356467 30910 master.cpp:705] Authorization enabled
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356531 30913 whitelist_watcher.cpp:77] No whitelist given
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356549 30912 hierarchical.cpp:142] Initialized hierarchical allocator process
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356868 30916 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.232816ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356884 30916 replica.cpp:320] Persisted replica status to STARTING
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.356945 30916 recover.cpp:477] Replica is in STARTING status
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357100 30917 master.cpp:1969] The newly elected leader is master@172.30.2.105:40724 with id 27c796db-6f98-4d61-96c0-f583f22787ff
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357115 30917 master.cpp:1982] Elected as the leading master!
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357122 30917 master.cpp:1669] Recovering from registrar
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357213 30910 registrar.cpp:332] Recovering registrar
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357429 30913 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (18698)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357549 30914 recover.cpp:197] Received a recover response from a replica in STARTING status
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.357728 30913 recover.cpp:568] Updating replica status to VOTING
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.358937 30913 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.14792ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.358952 30913 replica.cpp:320] Persisted replica status to VOTING
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.358986 30913 recover.cpp:582] Successfully joined the Paxos group
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.359041 30913 recover.cpp:466] Recover process terminated
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.359180 30916 log.cpp:553] Attempting to start the writer
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.359578 30917 replica.cpp:493] Replica received implicit promise request from (18699)@172.30.2.105:40724 with proposal 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.360752 30917 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.157449ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.360767 30917 replica.cpp:342] Persisted promised to 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.360982 30914 coordinator.cpp:238] Coordinator attempting to fill missing positions
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.361426 30910 replica.cpp:388] Replica received explicit promise request from (18700)@172.30.2.105:40724 for position 0 with proposal 2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.362571 30910 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.124969ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.362587 30910 replica.cpp:712] Persisted action at 0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.362999 30911 replica.cpp:537] Replica received write request for position 0 from (18701)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.363030 30911 leveldb.cpp:436] Reading position from leveldb took 14967ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.364264 30911 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.214497ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.364279 30911 replica.cpp:712] Persisted action at 0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.364470 30910 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.365622 30910 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.131398ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.365636 30910 replica.cpp:712] Persisted action at 0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.365643 30910 replica.cpp:697] Replica learned NOP action at position 0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.365769 30915 log.cpp:569] Writer started with ending position 0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366080 30913 leveldb.cpp:436] Reading position from leveldb took 8794ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366284 30915 registrar.cpp:365] Successfully fetched the registry (0B) in 9.053952ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366315 30915 registrar.cpp:464] Applied 1 operations in 3436ns; attempting to update the 'registry'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366487 30911 log.cpp:577] Attempting to append 209 bytes to the log
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366539 30917 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.366839 30917 replica.cpp:537] Replica received write request for position 1 from (18702)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.367966 30917 leveldb.cpp:341] Persisting action (228 bytes) to leveldb took 1.106053ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.367982 30917 replica.cpp:712] Persisted action at 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.368201 30915 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.371786 30915 leveldb.cpp:341] Persisting action (230 bytes) to leveldb took 3.566076ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.371803 30915 replica.cpp:712] Persisted action at 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.371809 30915 replica.cpp:697] Replica learned APPEND action at position 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372032 30910 registrar.cpp:509] Successfully updated the 'registry' in 5.693952ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372097 30910 registrar.cpp:395] Successfully recovered registrar
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372107 30911 log.cpp:596] Attempting to truncate the log to 1
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372151 30910 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372218 30911 master.cpp:1777] Recovered 0 agents from the Registry (170B) ; allowing 10mins for agents to re-register
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372242 30915 hierarchical.cpp:169] Skipping recovery of hierarchical allocator: nothing to recover
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.372467 30914 replica.cpp:537] Replica received write request for position 2 from (18703)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.373693 30914 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.207676ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.373708 30914 replica.cpp:712] Persisted action at 2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.373920 30913 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.375115 30913 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.17978ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.375145 30913 leveldb.cpp:399] Deleting ~1 keys from leveldb took 14216ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.375154 30913 replica.cpp:712] Persisted action at 2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.375159 30913 replica.cpp:697] Replica learned TRUNCATE action at position 2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.383839 30896 containerizer.cpp:201] Using isolation: docker/runtime,filesystem/linux,network/cni
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.388789 30896 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[22:41:54]W:	 [Step 10/10] E0619 22:41:54.393234 30896 shell.hpp:106] Command 'hadoop version 2>&1' failed; this is the output:
[22:41:54]W:	 [Step 10/10] sh: hadoop: command not found
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.393265 30896 fetcher.cpp:62] Skipping URI fetcher plugin 'hadoop' as it could not be created: Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.393316 30896 registry_puller.cpp:111] Creating registry puller with docker registry 'https://registry-1.docker.io'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.395668 30896 cluster.cpp:432] Creating default 'local' authorizer
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396100 30914 slave.cpp:203] Agent started on 469)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396116 30914 slave.cpp:204] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/KhgYrQ/store"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/http_credentials"" --image_providers=""docker"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""docker/runtime,filesystem/linux,network/cni"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --network_cni_config_dir=""/tmp/KhgYrQ/configs"" --network_cni_plugins_dir=""/tmp/KhgYrQ/plugins"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI""
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396380 30914 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/credential'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396495 30914 slave.cpp:341] Agent using credential for: test-principal
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396509 30914 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/http_credentials'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396586 30914 slave.cpp:393] Using default 'basic' HTTP authenticator
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396698 30914 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
[22:41:54]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396780 30896 sched.cpp:224] Version: 1.0.0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.396991 30914 slave.cpp:592] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397020 30914 slave.cpp:600] Agent attributes: [  ]
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397029 30914 slave.cpp:605] Agent hostname: ip-172-30-2-105.mesosphere.io
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397040 30916 sched.cpp:328] New master detected at master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397068 30916 sched.cpp:394] Authenticating with master master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397078 30916 sched.cpp:401] Using default CRAM-MD5 authenticatee
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397188 30916 authenticatee.cpp:121] Creating new client SASL connection
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397467 30914 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/meta'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397476 30912 master.cpp:5943] Authenticating scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397544 30913 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(953)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397614 30915 status_update_manager.cpp:200] Recovering status update manager
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397668 30912 authenticator.cpp:98] Creating new server SASL connection
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397709 30915 containerizer.cpp:514] Recovering containerizer
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397869 30912 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397886 30912 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397927 30912 authenticator.cpp:204] Received SASL authentication start
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.397964 30912 authenticator.cpp:326] Authentication requires more steps
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398000 30912 authenticatee.cpp:259] Received SASL authentication step
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398052 30912 authenticator.cpp:232] Received SASL authentication step
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398066 30912 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-105.mesosphere.io' server FQDN: 'ip-172-30-2-105.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398073 30912 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398087 30912 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398098 30912 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-105.mesosphere.io' server FQDN: 'ip-172-30-2-105.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398103 30912 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398108 30912 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398116 30912 authenticator.cpp:318] Authentication success
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398162 30914 authenticatee.cpp:299] Authentication success
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398181 30913 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(953)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398200 30912 master.cpp:5973] Successfully authenticated principal 'test-principal' at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398270 30914 sched.cpp:484] Successfully authenticated with master master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398280 30914 sched.cpp:800] Sending SUBSCRIBE call to master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398342 30914 sched.cpp:833] Will retry registration in 869.123866ms if necessary
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398381 30916 master.cpp:2539] Received SUBSCRIBE call for framework 'default' at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398398 30916 master.cpp:2008] Authorizing framework principal 'test-principal' to receive offers for role '*'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398483 30916 master.cpp:2615] Subscribing framework default with checkpointing disabled and capabilities [  ]
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398679 30916 sched.cpp:723] Framework registered with 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398701 30916 sched.cpp:737] Scheduler::registered took 10291ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398784 30910 hierarchical.cpp:264] Added framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398802 30910 hierarchical.cpp:1488] No allocations performed
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398808 30910 hierarchical.cpp:1583] No inverse offers to send out!
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.398818 30910 hierarchical.cpp:1139] Performed allocation for 0 agents in 22451ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399222 30916 metadata_manager.cpp:205] No images to load from disk. Docker provisioner image storage path '/tmp/KhgYrQ/store/storedImages' does not exist
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399318 30910 provisioner.cpp:253] Provisioner recovery complete
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399453 30913 slave.cpp:4845] Finished recovery
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399690 30913 slave.cpp:5017] Querying resource estimator for oversubscribable resources
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399796 30911 slave.cpp:967] New master detected at master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399811 30911 slave.cpp:1029] Authenticating with master master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399801 30914 status_update_manager.cpp:174] Pausing sending status updates
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399821 30911 slave.cpp:1040] Using default CRAM-MD5 authenticatee
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399855 30911 slave.cpp:1002] Detecting new master
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399879 30915 authenticatee.cpp:121] Creating new client SASL connection
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.399910 30911 slave.cpp:5031] Received oversubscribable resources  from the resource estimator
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400044 30915 master.cpp:5943] Authenticating slave(469)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400099 30910 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(954)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400151 30910 authenticator.cpp:98] Creating new server SASL connection
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400316 30910 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400329 30910 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400367 30910 authenticator.cpp:204] Received SASL authentication start
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400398 30910 authenticator.cpp:326] Authentication requires more steps
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400431 30910 authenticatee.cpp:259] Received SASL authentication step
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400516 30917 authenticator.cpp:232] Received SASL authentication step
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400530 30917 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-105.mesosphere.io' server FQDN: 'ip-172-30-2-105.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400537 30917 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400544 30917 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400550 30917 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-105.mesosphere.io' server FQDN: 'ip-172-30-2-105.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400554 30917 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400558 30917 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400566 30917 authenticator.cpp:318] Authentication success
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400609 30914 authenticatee.cpp:299] Authentication success
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400640 30912 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(954)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400682 30917 master.cpp:5973] Successfully authenticated principal 'test-principal' at slave(469)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400738 30911 slave.cpp:1108] Successfully authenticated with master master@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400790 30911 slave.cpp:1511] Will retry registration in 13.364855ms if necessary
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400848 30913 master.cpp:4653] Registering agent at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io) with id 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.400950 30914 registrar.cpp:464] Applied 1 operations in 16921ns; attempting to update the 'registry'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.401154 30915 log.cpp:577] Attempting to append 395 bytes to the log
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.401213 30914 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.401515 30914 replica.cpp:537] Replica received write request for position 3 from (18725)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.402851 30914 leveldb.cpp:341] Persisting action (414 bytes) to leveldb took 1.317458ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.402866 30914 replica.cpp:712] Persisted action at 3
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.403101 30917 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404217 30917 leveldb.cpp:341] Persisting action (416 bytes) to leveldb took 1.100393ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404233 30917 replica.cpp:712] Persisted action at 3
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404239 30917 replica.cpp:697] Replica learned APPEND action at position 3
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404495 30915 registrar.cpp:509] Successfully updated the 'registry' in 3.521792ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404561 30913 log.cpp:596] Attempting to truncate the log to 3
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404621 30915 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404690 30910 master.cpp:4721] Registered agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404726 30915 slave.cpp:3747] Received ping from slave-observer(429)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404747 30916 hierarchical.cpp:473] Added agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 (ip-172-30-2-105.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404825 30915 slave.cpp:1152] Registered with master master@172.30.2.105:40724; given agent ID 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404840 30915 fetcher.cpp:86] Clearing fetcher cache
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404880 30910 replica.cpp:537] Replica received write request for position 4 from (18726)@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404911 30916 hierarchical.cpp:1583] No inverse offers to send out!
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404932 30913 status_update_manager.cpp:181] Resuming sending status updates
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.404942 30916 hierarchical.cpp:1162] Performed allocation for agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 in 168147ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405025 30911 master.cpp:5772] Sending 1 offers to framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405082 30915 slave.cpp:1175] Checkpointing SlaveInfo to '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/meta/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/slave.info'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405177 30911 sched.cpp:897] Scheduler::resourceOffers took 55063ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405239 30915 slave.cpp:1212] Forwarding total oversubscribed resources 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405299 30911 master.cpp:5066] Received update of agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io) with total oversubscribed resources 
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405318 30896 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:128
[22:41:54]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405387 30911 hierarchical.cpp:531] Agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 (ip-172-30-2-105.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405421 30911 hierarchical.cpp:1488] No allocations performed
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405431 30911 hierarchical.cpp:1583] No inverse offers to send out!
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405447 30911 hierarchical.cpp:1162] Performed allocation for agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 in 40224ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405643 30914 master.cpp:3457] Processing ACCEPT call for offers: [ 27c796db-6f98-4d61-96c0-f583f22787ff-O0 ] on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io) for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.405668 30914 master.cpp:3095] Authorizing framework principal 'test-principal' to launch task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406030 30912 master.hpp:177] Adding task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 with resources cpus(*):1; mem(*):128 on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 (ip-172-30-2-105.mesosphere.io)
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406056 30912 master.cpp:3946] Launching task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724 with resources cpus(*):1; mem(*):128 on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406158 30916 slave.cpp:1551] Got assigned task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406193 30912 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):1; mem(*):128) on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 from framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406214 30912 hierarchical.cpp:928] Framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 filtered agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 for 5secs
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406250 30916 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[22:41:54]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406347 30910 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.44747ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406359 30910 replica.cpp:712] Persisted action at 4
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406381 30916 slave.cpp:1670] Launching task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406420 30916 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
[22:41:54]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406555 30914 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.406793 30916 paths.cpp:528] Trying to chown '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4' to user 'root'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.408360 30914 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.635458ms
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.408453 30914 leveldb.cpp:399] Deleting ~2 keys from leveldb took 53370ns
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.408469 30914 replica.cpp:712] Persisted action at 4
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.408480 30914 replica.cpp:697] Replica learned TRUNCATE action at position 4
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.411355 30916 slave.cpp:5734] Launching executor d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.411485 30916 slave.cpp:1896] Queuing task 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' for executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.411516 30915 containerizer.cpp:773] Starting container '548370b5-05f2-4e33-8f6f-015aa3fd1af4' for executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework '27c796db-6f98-4d61-96c0-f583f22787ff-0000'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.411521 30916 slave.cpp:920] Successfully attached file '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.411733 30914 metadata_manager.cpp:167] Looking for image 'alpine'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.412009 30911 registry_puller.cpp:235] Pulling image 'library/alpine' from 'docker-manifest://registry-1.docker.io:443library/alpine?latest#https' to '/tmp/KhgYrQ/store/staging/0cVlJm'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.870712 30915 registry_puller.cpp:258] The manifest for image 'library/alpine' is '{
[22:41:54]W:	 [Step 10/10]    ""schemaVersion"": 1,
[22:41:54]W:	 [Step 10/10]    ""name"": ""library/alpine"",
[22:41:54]W:	 [Step 10/10]    ""tag"": ""latest"",
[22:41:54]W:	 [Step 10/10]    ""architecture"": ""amd64"",
[22:41:54]W:	 [Step 10/10]    ""fsLayers"": [
[22:41:54]W:	 [Step 10/10]       {
[22:41:54]W:	 [Step 10/10]          ""blobSum"": ""sha256:fae91920dcd4542f97c9350b3157139a5d901362c2abec284de5ebd1b45b4957""
[22:41:54]W:	 [Step 10/10]       }
[22:41:54]W:	 [Step 10/10]    ],
[22:41:54]W:	 [Step 10/10]    ""history"": [
[22:41:54]W:	 [Step 10/10]       {
[22:41:54]W:	 [Step 10/10]          ""v1Compatibility"": ""{\""architecture\"":\""amd64\"",\""config\"":{\""Hostname\"":\""571cde9b03ce\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":null,\""Cmd\"":null,\""Image\"":\""\"",\""Volumes\"":null,\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""OnBuild\"":null,\""Labels\"":null},\""container\"":\""571cde9b03ce6f46b78b8e9c5089d03034863a4ab9f05d3e4997d0e5e80a2a6e\"",\""container_config\"":{\""Hostname\"":\""571cde9b03ce\"",\""Domainname\"":\""\"",\""User\"":\""\"",\""AttachStdin\"":false,\""AttachStdout\"":false,\""AttachStderr\"":false,\""Tty\"":false,\""OpenStdin\"":false,\""StdinOnce\"":false,\""Env\"":null,\""Cmd\"":[\""/bin/sh\"",\""-c\"",\""#(nop) ADD file:701fd33a2f463fd4bd459779276897ef01dcf998dd47f6c8eae34fa5e0886046 in /\""],\""Image\"":\""\"",\""Volumes\"":null,\""WorkingDir\"":\""\"",\""Entrypoint\"":null,\""OnBuild\"":null,\""Labels\"":null},\""created\"":\""2016-06-02T21:43:31.291506236Z\"",\""docker_version\"":\""1.9.1\"",\""id\"":\""e43bd3919b4ed702040fe5d0b19c9a0778ae7d61f169cf98112a842746168e6b\"",\""os\"":\""linux\""}""
[22:41:54]W:	 [Step 10/10]       }
[22:41:54]W:	 [Step 10/10]    ],
[22:41:54]W:	 [Step 10/10]    ""signatures"": [
[22:41:54]W:	 [Step 10/10]       {
[22:41:54]W:	 [Step 10/10]          ""header"": {
[22:41:54]W:	 [Step 10/10]             ""jwk"": {
[22:41:54]W:	 [Step 10/10]                ""crv"": ""P-256"",
[22:41:54]W:	 [Step 10/10]                ""kid"": ""IZ4C:AKG6:LLBK:4Y62:6YWU:OI2G:K2EN:ZOJH:GHRY:5PKA:PFEE:WZWD"",
[22:41:54]W:	 [Step 10/10]                ""kty"": ""EC"",
[22:41:54]W:	 [Step 10/10]                ""x"": ""hU3h5pMhA0tgT3mF41BH5EbsLy9Tv3O-bla53S8-25g"",
[22:41:54]W:	 [Step 10/10]                ""y"": ""Y9sM4tXh_3KKKeEhikWEGgTUlQLYJxPWCXcs_bVP4Pc""
[22:41:54]W:	 [Step 10/10]             },
[22:41:54]W:	 [Step 10/10]             ""alg"": ""ES256""
[22:41:54]W:	 [Step 10/10]          },
[22:41:54]W:	 [Step 10/10]          ""signature"": ""8SZVGFKd_Ovz9FtfNMoLRWkwayOY9zaTq4bgPnKPuFPK-48nhDTMlkMz52Nqm2SHCk2xtYYkhzLtE6wUctrjqA"",
[22:41:54]W:	 [Step 10/10]          ""protected"": ""eyJmb3JtYXRMZW5ndGgiOjEzNTgsImZvcm1hdFRhaWwiOiJDbjAiLCJ0aW1lIjoiMjAxNi0wNi0xOVQyMjo0MTo1NFoifQ""
[22:41:54]W:	 [Step 10/10]       }
[22:41:54]W:	 [Step 10/10]    ]
[22:41:54]W:	 [Step 10/10] }'
[22:41:54]W:	 [Step 10/10] I0619 22:41:54.870767 30915 registry_puller.cpp:368] Fetching blob 'sha256:fae91920dcd4542f97c9350b3157139a5d901362c2abec284de5ebd1b45b4957' for layer 'e43bd3919b4ed702040fe5d0b19c9a0778ae7d61f169cf98112a842746168e6b' of image 'library/alpine'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.357898 30910 hierarchical.cpp:1674] Filtered offer with cpus(*):1; mem(*):896; disk(*):1024; ports(*):[31000-32000] on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.357965 30910 hierarchical.cpp:1488] No allocations performed
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.357980 30910 hierarchical.cpp:1583] No inverse offers to send out!
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.358002 30910 hierarchical.cpp:1139] Performed allocation for 1 agents in 238814ns
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.474309 30911 registry_puller.cpp:305] Extracting layer tar ball '/tmp/KhgYrQ/store/staging/0cVlJm/sha256:fae91920dcd4542f97c9350b3157139a5d901362c2abec284de5ebd1b45b4957 to rootfs '/tmp/KhgYrQ/store/staging/0cVlJm/e43bd3919b4ed702040fe5d0b19c9a0778ae7d61f169cf98112a842746168e6b/rootfs'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.575764 30910 metadata_manager.cpp:155] Successfully cached image 'alpine'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.576198 30911 provisioner.cpp:294] Provisioning image rootfs '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/provisioner/containers/548370b5-05f2-4e33-8f6f-015aa3fd1af4/backends/copy/rootfses/4f5eb0d5-118b-4129-972d-0a7e6a374f6f' for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.576556 30910 copy.cpp:128] Copying layer path '/tmp/KhgYrQ/store/layers/e43bd3919b4ed702040fe5d0b19c9a0778ae7d61f169cf98112a842746168e6b/rootfs' to rootfs '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/provisioner/containers/548370b5-05f2-4e33-8f6f-015aa3fd1af4/backends/copy/rootfses/4f5eb0d5-118b-4129-972d-0a7e6a374f6f'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.676825 30916 containerizer.cpp:1267] Launching 'mesos-containerizer' with flags '--command=""{""arguments"":[""mesos-executor"",""--sandbox_directory=\/mnt\/mesos\/sandbox"",""--user=root"",""--rootfs=\/mnt\/teamcity\/temp\/buildTmp\/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI\/provisioner\/containers\/548370b5-05f2-4e33-8f6f-015aa3fd1af4\/backends\/copy\/rootfses\/4f5eb0d5-118b-4129-972d-0a7e6a374f6f""],""shell"":false,""user"":""root"",""value"":""\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-executor""}"" --commands=""{""commands"":[{""shell"":true,""value"":""#!\/bin\/sh\nset -x -e\n\/mnt\/teamcity\/work\/4240ba9ddd0997c3\/build\/src\/mesos-containerizer mount --help=\""false\"" --operation=\""make-rslave\"" --path=\""\/\""\nmount -n --rbind '\/mnt\/teamcity\/temp\/buildTmp\/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI\/slaves\/27c796db-6f98-4d61-96c0-f583f22787ff-S0\/frameworks\/27c796db-6f98-4d61-96c0-f583f22787ff-0000\/executors\/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2\/runs\/548370b5-05f2-4e33-8f6f-015aa3fd1af4' '\/mnt\/teamcity\/temp\/buildTmp\/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI\/provisioner\/containers\/548370b5-05f2-4e33-8f6f-015aa3fd1af4\/backends\/copy\/rootfses\/4f5eb0d5-118b-4129-972d-0a7e6a374f6f\/mnt\/mesos\/sandbox'\n""}]}"" --help=""false"" --pipe_read=""17"" --pipe_write=""20"" --sandbox=""/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4"" --user=""root""'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.676923 30916 linux_launcher.cpp:281] Cloning child process with flags = CLONE_NEWUTS | CLONE_NEWNS
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.681491 30913 cni.cpp:683] Bind mounted '/proc/13484/ns/net' to '/run/mesos/isolators/network/cni/548370b5-05f2-4e33-8f6f-015aa3fd1af4/ns' for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.681712 30913 cni.cpp:977] Invoking CNI plugin 'mockPlugin' with network configuration '{""args"":{""org.apache.mesos"":{""network_info"":{""name"":""__MESOS_TEST__""}}},""name"":""__MESOS_TEST__"",""type"":""mockPlugin""}'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.776078 30916 cni.cpp:1066] Got assigned IPv4 address '172.17.0.1/16' from CNI network '__MESOS_TEST__' for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.776463 30913 cni.cpp:808] DNS nameservers for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4 are:
[22:41:55]W:	 [Step 10/10] nameserver 172.30.0.2
[22:41:55]W:	 [Step 10/10] + /mnt/teamcity/work/4240ba9ddd0997c3/build/src/mesos-containerizer mount --help=false --operation=make-rslave --path=/
[22:41:55]W:	 [Step 10/10] + mount -n --rbind /mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4 /mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/provisioner/containers/548370b5-05f2-4e33-8f6f-015aa3fd1af4/backends/copy/rootfses/4f5eb0d5-118b-4129-972d-0a7e6a374f6f/mnt/mesos/sandbox
[22:41:55]W:	 [Step 10/10] WARNING: Logging before InitGoogleLogging() is written to STDERR
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.944355 13484 process.cpp:1060] libprocess is initialized on 172.17.0.1:60396 with 8 worker threads
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.946605 13484 logging.cpp:199] Logging to STDERR
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.947335 13484 exec.cpp:161] Version: 1.0.0
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.947404 13541 exec.cpp:211] Executor started at: executor(1)@172.17.0.1:60396 with pid 13484
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.947883 30917 slave.cpp:2884] Got registration for executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 from executor(1)@172.17.0.1:60396
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.948427 13543 exec.cpp:236] Executor registered on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.948524 30914 slave.cpp:2061] Sending queued task 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' to executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 at executor(1)@172.17.0.1:60396
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.949061 13543 exec.cpp:248] Executor::registered took 75489ns
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.949213 13543 exec.cpp:323] Executor asked to run task 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.949246 13543 exec.cpp:332] Executor::launchTask took 21245ns
[22:41:55] :	 [Step 10/10] Received SUBSCRIBED event
[22:41:55] :	 [Step 10/10] Subscribed executor on ip-172-30-2-105.mesosphere.io
[22:41:55] :	 [Step 10/10] Received LAUNCH event
[22:41:55] :	 [Step 10/10] Starting task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2
[22:41:55] :	 [Step 10/10] Forked command at 13550
[22:41:55] :	 [Step 10/10] sh -c 'ifconfig'
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.953589 13547 exec.cpp:546] Executor sending status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] Failed to exec: No such file or directory
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.953891 30917 slave.cpp:3267] Handling status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 from executor(1)@172.17.0.1:60396
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954368 30910 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954385 30910 status_update_manager.cpp:497] Creating StatusUpdate stream for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954545 30910 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to the agent
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954637 30911 slave.cpp:3665] Forwarding the update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to master@172.30.2.105:40724
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954711 30911 slave.cpp:3559] Status update manager successfully handled status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954732 30911 slave.cpp:3575] Sending acknowledgement for status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to executor(1)@172.17.0.1:60396
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954761 30914 master.cpp:5211] Status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 from agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954788 30914 master.cpp:5259] Forwarding status update TASK_RUNNING (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954843 30914 master.cpp:6871] Updating the state of task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954934 13548 exec.cpp:369] Executor received status update acknowledgement 5caccf6c-9e1e-44cc-93d4-6851987802cd for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.954967 30910 sched.cpp:1005] Scheduler::statusUpdate took 57021ns
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.955070 30914 master.cpp:4365] Processing ACKNOWLEDGE call 5caccf6c-9e1e-44cc-93d4-6851987802cd for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724 on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.955150 30911 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:55]W:	 [Step 10/10] I0619 22:41:55.955219 30911 slave.cpp:2653] Status update manager successfully handled status update acknowledgement (UUID: 5caccf6c-9e1e-44cc-93d4-6851987802cd) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56] :	 [Step 10/10] Command terminated with signal Aborted (pid: 13550)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.054153 13541 exec.cpp:546] Executor sending status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.054498 30913 slave.cpp:3267] Handling status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 from executor(1)@172.17.0.1:60396
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.054955 30917 slave.cpp:6074] Terminating task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055366 30912 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055409 30912 status_update_manager.cpp:374] Forwarding update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to the agent
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055485 30916 slave.cpp:3665] Forwarding the update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to master@172.30.2.105:40724
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055558 30916 slave.cpp:3559] Status update manager successfully handled status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56] :	 [Step 10/10] ../../src/tests/containerizer/cni_isolator_tests.cpp:216: Failure
[22:41:56] :	 [Step 10/10] Value of: statusFinished->state()
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055572 30916 slave.cpp:3575] Sending acknowledgement for status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 to executor(1)@172.17.0.1:60396
[22:41:56] :	 [Step 10/10]   Actual: TASK_FAILED
[22:41:56] :	 [Step 10/10] Expected: TASK_FINISHED
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055613 30914 master.cpp:5211] Status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 from agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055640 30914 master.cpp:5259] Forwarding status update TASK_FAILED (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055696 30914 master.cpp:6871] Updating the state of task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055773 30912 sched.cpp:1005] Scheduler::statusUpdate took 29145ns
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055780 13546 exec.cpp:369] Executor received status update acknowledgement 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055816 30916 hierarchical.cpp:891] Recovered cpus(*):1; mem(*):128 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 from framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055887 30911 master.cpp:4365] Processing ACKNOWLEDGE call 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34 for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724 on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055907 30911 master.cpp:6937] Removing task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 with resources cpus(*):1; mem(*):128 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 on agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.055971 30896 sched.cpp:1964] Asked to stop the driver
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056030 30913 sched.cpp:1167] Stopping framework '27c796db-6f98-4d61-96c0-f583f22787ff-0000'
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056040 30916 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056073 30916 status_update_manager.cpp:528] Cleaning up status update stream for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056151 30915 master.cpp:6342] Processing TEARDOWN call for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056172 30915 master.cpp:6354] Removing framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 (default) at scheduler-af10d6a3-1ebc-4377-b44d-8c0dfbffcb8e@172.30.2.105:40724
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056197 30916 slave.cpp:2653] Status update manager successfully handled status update acknowledgement (UUID: 3d3632b6-f69b-4ca1-8bac-0b4e8e471d34) for task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2 of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056216 30916 slave.cpp:6115] Completing task d7416b1b-cd1c-422a-bbaa-bb28913eeaf2
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056218 30913 hierarchical.cpp:375] Deactivated framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056248 30916 slave.cpp:2274] Asked to shut down framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 by master@172.30.2.105:40724
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056265 30916 slave.cpp:2299] Shutting down framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056277 30916 slave.cpp:4470] Shutting down executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 at executor(1)@172.17.0.1:60396
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056468 30914 hierarchical.cpp:326] Removed framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.056634 30911 containerizer.cpp:1576] Destroying container '548370b5-05f2-4e33-8f6f-015aa3fd1af4'
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.057258 13543 exec.cpp:410] Executor asked to shutdown
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.057303 13543 exec.cpp:425] Executor::shutdown took 6363ns
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.057324 13547 exec.cpp:92] Scheduling shutdown of the executor in 5secs
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.058279 30910 cgroups.cpp:2676] Freezing cgroup /sys/fs/cgroup/freezer/mesos/548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.059762 30912 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/548370b5-05f2-4e33-8f6f-015aa3fd1af4 after 1.460736ms
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.061364 30910 cgroups.cpp:2694] Thawing cgroup /sys/fs/cgroup/freezer/mesos/548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.062861 30915 cgroups.cpp:1438] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/548370b5-05f2-4e33-8f6f-015aa3fd1af4 after 1.478912ms
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.064016 30910 slave.cpp:3793] executor(1)@172.17.0.1:60396 exited
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.078352 30915 containerizer.cpp:1812] Executor for container '548370b5-05f2-4e33-8f6f-015aa3fd1af4' has exited
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.179833 30916 cni.cpp:1217] Unmounted the network namespace handle '/run/mesos/isolators/network/cni/548370b5-05f2-4e33-8f6f-015aa3fd1af4/ns' for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.179924 30916 cni.cpp:1228] Removed the container directory '/run/mesos/isolators/network/cni/548370b5-05f2-4e33-8f6f-015aa3fd1af4'
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.180981 30913 provisioner.cpp:434] Destroying container rootfs at '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/provisioner/containers/548370b5-05f2-4e33-8f6f-015aa3fd1af4/backends/copy/rootfses/4f5eb0d5-118b-4129-972d-0a7e6a374f6f' for container 548370b5-05f2-4e33-8f6f-015aa3fd1af4
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280364 30912 slave.cpp:4152] Executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 terminated with signal Killed
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280406 30912 slave.cpp:4256] Cleaning up executor 'd7416b1b-cd1c-422a-bbaa-bb28913eeaf2' of framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000 at executor(1)@172.17.0.1:60396
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280545 30915 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2/runs/548370b5-05f2-4e33-8f6f-015aa3fd1af4' for gc 6.99999675365926days in the future
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280575 30912 slave.cpp:4344] Cleaning up framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280647 30915 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000/executors/d7416b1b-cd1c-422a-bbaa-bb28913eeaf2' for gc 6.99999675293037days in the future
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280654 30914 status_update_manager.cpp:282] Closing status update streams for framework 27c796db-6f98-4d61-96c0-f583f22787ff-0000
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280710 30915 gc.cpp:55] Scheduling '/mnt/teamcity/temp/buildTmp/CniIsolatorTest_ROOT_INTERNET_CURL_LaunchCommandTask_GcX6XI/slaves/27c796db-6f98-4d61-96c0-f583f22787ff-S0/frameworks/27c796db-6f98-4d61-96c0-f583f22787ff-0000' for gc 6.99999675200296days in the future
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280745 30915 slave.cpp:839] Agent terminating
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280810 30912 master.cpp:1367] Agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io) disconnected
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280827 30912 master.cpp:2899] Disconnecting agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280844 30912 master.cpp:2918] Deactivating agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 at slave(469)@172.30.2.105:40724 (ip-172-30-2-105.mesosphere.io)
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.280912 30912 hierarchical.cpp:560] Agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0 deactivated
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.283011 30896 master.cpp:1214] Master terminating
[22:41:56]W:	 [Step 10/10] I0619 22:41:56.283140 30916 hierarchical.cpp:505] Removed agent 27c796db-6f98-4d61-96c0-f583f22787ff-S0
[22:41:56] :	 [Step 10/10] [  FAILED  ] CniIsolatorTest.ROOT_INTERNET_CURL_LaunchCommandTask (1945 ms)
{noformat}",CenOS 7 with/without SSL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-22 23:05:06.83,,,false,MESOS-4641,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 22 23:05:06 UTC 2016,,,,,,,"0|i2zs1z:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 37,,,,,,,,,,,2.0,,,,,,,,,,,"22/Jun/16 23:05;jieyu;commit 7fca36322efef219eef008088abe91fe9ae8b7f2
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jun 22 16:00:30 2016 -0700

    Fixed broken cni isolator test on centos7.
    
    Review: https://reviews.apache.org/r/48989/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Executors should not inherit environment variables from the agent.,MESOS-5657,12980726,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,20/Jun/16 06:53,20/Jun/16 18:32,29/Oct/20 16:32,20/Jun/16 18:32,,,,,,,,,1.0.0,,,,,,containerization,,,,,0,containerizer,mesosphere,,,,,,,"Currently executors are inheriting environment variables form the slave in mesos containerizer. This is problematic, because of two reasons:

1. When we use docker images (such as `mongo`) in unified containerizer, duplicated environment variables inherited from the slave lead to initialization failures, because LANG and/or LC_* environment variables are not set correctly.

2. When we are looking at the environment variables from the executor tasks, there are pages of environment variables listed, which is redundant and dangerous.

Depending on the reasons above, we propose that no longer allow executors to inherit environment variables from the slave. Instead, users should specify all environment variables they need by setting the slave flag `--executor_environment_variables` as a JSON format.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-20 18:32:21.576,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 20 18:32:21 UTC 2016,,,,,,,"0|i2zpxz:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 37,,,,,,,,,,,3.0,,,,,,,,,,,"20/Jun/16 18:32;jieyu;commit ce4b3056164a804bea52810173dbd7a418d12641
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Sun Jun 19 16:01:10 2016 -0700

    Forbid the executor to inherit from slave environment.
    
    Review: https://reviews.apache.org/r/44498/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Put initial scaffolding in place for implementing SUBSCRIBE call on v1 Master API.,MESOS-5609,12978548,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,anandmazumdar,anandmazumdar,14/Jun/16 00:25,21/Jun/16 04:10,29/Oct/20 16:32,21/Jun/16 04:10,,,,,,,,,1.0.0,,,,,,,,,,,0,mesosphere,,,,,,,,"As discussed on MESOS-5498, this ticket is for tracking work to put the initial scaffolding in place for streaming task status update events to a client that has subscribed to the {{api/v1}} Operator API endpoint. Other events/support for snapshots would be done as part of MESOS-5498.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-4791,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 21 04:10:41 UTC 2016,,,,,,,"0|i2zewf:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 37,,,,,,,,,,,5.0,,,,,,,,,,,"17/Jun/16 20:18;anandmazumdar;https://reviews.apache.org/r/48873/","21/Jun/16 04:10;anandmazumdar;{noformat}
commit b60ed386c4dfc8cb594bbc02038645a3e0efe817
Author: Anand Mazumdar <anand@apache.org>
Date:   Mon Jun 20 20:50:13 2016 -0700

    Added test for task added/update event.

    This change adds a basic test to ensure that a client is able
    to subscribe and receive task added/updated events.

    Review: https://reviews.apache.org/r/48880/

commit e878e16ba54419ae39c7fbcd3466afe1cc42a93a
Author: Anand Mazumdar <anand@apache.org>
Date:   Mon Jun 20 20:50:07 2016 -0700

    Added logic for subscribing to task added/updated events.

    This change sets the initial scaffolding in place allowing
    a client to subscribe to events via the '/api/vX' endpoint.
    Currently, only two events are supported i.e. `TASK_ADDED`/
    `TASK_UPDATED`.

    Review: https://reviews.apache.org/r/48879/

commit f384ac984f0062f4573a494b6e8db114bf666e55
Author: Anand Mazumdar <anand@apache.org>
Date:   Mon Jun 20 20:49:59 2016 -0700

    Moved `addTask()` definition to cpp from header.

    This change moves the definition of `addTask()` to cpp.
    This is needed later in the chain as this method needs
    access to the implementation of `Master` that is declared
    later in the header.

    Review: https://reviews.apache.org/r/48974/

commit 3cde2694bd8dee53fb12047deecab56ae0418a2e
Author: Anand Mazumdar <anand@apache.org>
Date:   Mon Jun 20 20:49:52 2016 -0700

    Moved `HttpConnection` struct before `Master`.

    This would be used later in the chain and makes the diff
    less verbose. This was done because the `Master` would need
    to save a list of `Subscriber`'s that have a dependency on
    `HttpConnection`.

    Review: https://reviews.apache.org/r/48874/

commit cdf648832743afadaa4f78a821e02fef9efc5645
Author: Anand Mazumdar <anand@apache.org>
Date:   Mon Jun 20 20:49:47 2016 -0700

    Added protobuf helpers for creating task added/updated events.

    This change adds logic for creating `TASK_ADDED`/`TASK_UPDATED`
    events from `Task`. This would be used by the master for sending
    events to a client subscribed to the `api/vX` endpoint.

    Review: https://reviews.apache.org/r/48878/

commit 9d3e01cb43d84673fbebe06d240eeaf3169feb25
Author: Anand Mazumdar <anand@apache.org>
Date:   Mon Jun 20 20:49:41 2016 -0700

    Added evolve logic for `master::Event` -> `v1::master::Event`.

    Review: https://reviews.apache.org/r/48877/

commit f0d07756e569c7b948c74ab4201370a097d1c91f
Author: Anand Mazumdar <anand@apache.org>
Date:   Mon Jun 20 20:49:37 2016 -0700

    Added `TASK_ADDED`/`TASK_UPDATED` events to master.proto.

    Review: https://reviews.apache.org/r/48876/

commit c37c6a219e728ec9d55aa3bf5c65e7d257b3e07c
Author: Anand Mazumdar <anand@apache.org>
Date:   Mon Jun 20 20:49:17 2016 -0700

    Fixed a couple of minor style typos.

    Review: https://reviews.apache.org/r/48875/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Modules using replicated log state API require zookeeper headers,MESOS-5577,12977087,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,avinash.mesos,avinash.mesos,avinash.mesos,09/Jun/16 04:16,29/Apr/19 09:27,29/Oct/20 16:32,14/Jun/16 05:10,1.0.0,,,,,,,,1.0.0,,,,,,modules,,,,,0,mesosphere,,,,,,,,The state API uses zookeeper client headers and hence the bundled zookeeper headers need to be installed during Mesos installation. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-14 05:10:36.488,,,false,MESOS-1384,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 14 05:10:36 UTC 2016,,,,,,,"0|hzzze6:zzzx",9223372036854775807,,,,,karya,,,,,,Mesosphere Sprint 36,Mesosphere Sprint 37,,,,,,,,,,1.0,,,,,,,,,,,"14/Jun/16 05:10;karya;{code}
commit bcccb82a7b2643efb0928797f429f1b0b64aecc4
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Tue Jun 14 01:06:36 2016 -0400

    Removed conditional for of setting `--prefix=/`.
    
    For bundled 3rdparty packages the `--prefix` flag should always be set
    to `/`, else the 3rdparty installation path might append the
    $MESOS_INSTALL to the installation path.
    
    Review: https://reviews.apache.org/r/48612/

commit a26ec3c1b9cfd2e85f3df08944d3e2a839f4eb9e
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Tue Jun 14 01:06:22 2016 -0400

    Added bundled zookeeper to the module-dependencies.
    
    Modules that want to use the `State` API for storing information in
    the replicated log require zookeeper client headers. With this change
    the bundled zookeeper client headers will be installed along with the
    Mesos libraries and headers under the Mesos installation path.
    
    Review: https://reviews.apache.org/r/48594/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Need to remove references to ""messages/messages.hpp"" from `State` API",MESOS-5561,12976638,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,avinash.mesos,avinash.mesos,avinash.mesos,07/Jun/16 21:47,09/Jun/16 17:31,29/Oct/20 16:32,09/Jun/16 17:31,,,,,,,,,1.0.0,,,,,,,,,,,0,mesosphere,,,,,,,,"In order to expose the `State` API for using replicated log in Mesos modules it is necessary that the `State` API does not reference headers that are not exposed as part of the Mesos installation. 

Currently include/mesos/state/protobuf.hpp references src/messages/messages.hpp making the `State` API unusable in a module. 

We need to move the protobuf `serialize`/`deserialize` functions out of messages.hpp and move them to `stout/protobuf.hpp`. This will help us remove references to messages.hpp from the `State` API.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-09 17:31:45.844,,,false,MESOS-1384,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 09 17:31:45 UTC 2016,,,,,,,"0|i2z4ef:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 36,,,,,,,,,,,2.0,,,,,,,,,,,"09/Jun/16 17:31;jieyu;commit 023bc0813d6b271a019fed2fe349a5fe529f29d8
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Thu Jun 9 10:31:06 2016 -0700

    Moved the `serialize`/ `deserialize` calls to `stout`.
    
    Review: https://reviews.apache.org/r/48378/

commit 3181f368eff6800020ed172f2cb42d28eaf76584
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Thu Jun 9 10:30:55 2016 -0700

    Added the `serialize`/deserialize` methods in `protobuf` namespace.
    
    Moved these methods from ::messages namespace to ::protobuf namespace in
    stout.
    
    Review: https://reviews.apache.org/r/48377/",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix method of populating device entries for `/dev/nvidia-uvm`, etc.",MESOS-5556,12976630,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,klueska,klueska,klueska,07/Jun/16 21:29,14/Jun/16 20:43,29/Oct/20 16:32,14/Jun/16 20:43,,,,,,,,,1.0.0,,,,,,,,,,,0,gpu,mesosphere,,,,,,,"Currently, the major/minor numbers of `/dev/nvidiactl` and `/dev/nvidia-uvm` are hard-coded. This causes problems for `/dev/nvidia-uvm` because its major number is part of the ""Experimental"" device range on Linux.

Because this range is experimental, there is no guarantee which device
number will be assigned to it on a given machine.  We should use `os:stat::rdev()` to extract the major/minor numbers programatically.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-14 20:43:04.273,,,false,MESOS-4424,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 14 20:43:04 UTC 2016,,,,,,,"0|hzzze6:zzx",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 36,Mesosphere Sprint 37,,,,,,,,,,2.0,,,,,,,,,,,"07/Jun/16 22:45;klueska;https://reviews.apache.org/r/48370/","14/Jun/16 20:43;bmahler;{noformat}
commit 3e9df075c585270faee802380463c1e1236509a0
Author: Kevin Klues <klueska@gmail.com>
Date:   Tue Jun 14 13:38:13 2016 -0700

    Fixed hard-coding of Nvidia control device numbers.

    Previously, the major/minor numbers of `/dev/nvidiactl` and
    `/dev/nvidia-uvm` were hard-coded. This actually caused problems for
    `/dev/nvidia-uvm` because its major number is part of the
    ""Experimental"" device range on Linux.

    Because this range is experimental, there is no guarantee which device
    number will be assigned to it on a given machine. We actually
    encountered this problem in the wild, prompting this change.

    We now use `os:stat::rdev()` to extract the major/minor numbers
    programatically.

    Review: https://reviews.apache.org/r/48370/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Change major/minor device types for Nvidia GPUs to `unsigned int`,MESOS-5554,12976628,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,klueska,klueska,klueska,07/Jun/16 21:22,15/Jun/16 00:40,29/Oct/20 16:32,15/Jun/16 00:40,,,,,,,,,1.0.0,,,,,,,,,,,0,gpu,mesosphere,,,,,,,"Currently, the GPU struct specifies the type of its `major` and `minor` fields as `dev_t`, which is actually a concatenation of both the major and minor device numbers accessible through the `major()` and `minor()` macros. These macros return an `unsigned int` when handed a `dev_t`, so it makes sense for these fields to be of that type instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-15 00:40:48.36,,,false,MESOS-4424,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 15 00:40:48 UTC 2016,,,,,,,"0|hzzze6:zzr",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 36,Mesosphere Sprint 37,,,,,,,,,,1.0,,,,,,,,,,,"07/Jun/16 22:45;klueska;https://reviews.apache.org/r/48368/","15/Jun/16 00:40;bmahler;{noformat}
commit 7acf83e443d26bee15bc900c9397abc2062d6443
Author: Kevin Klues <klueska@gmail.com>
Date:   Tue Jun 14 17:35:53 2016 -0700

    Fixed invalid usage of `dev_t` in NVIDIA GPU isolator.

    Previously, the GPU struct specified the type of its `major` and
    `minor` fields as `dev_t`, which is actually a concatenation of both
    the major and minor device numbers accessible through the `major()`
    and `minor()` macros. These macros return an `unsigned int` when
    handed a `dev_t`, so it makes sense for these fields to be of that
    type instead.

    https://reviews.apache.org/r/48368/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
http v1 SUBSCRIBED scheduler event always has nil http_interval_seconds,MESOS-5537,12975574,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,anandmazumdar,jdef,jdef,03/Jun/16 18:43,09/Jun/16 16:01,29/Oct/20 16:32,09/Jun/16 15:09,,,,,,,,,1.0.0,,,,,,,,,,,0,mesosphere,,,,,,,,"I'm writing a controller in Go to monitor heartbeats. I'd like to use the interval as communicated by the master, which should be specified in the SUBSCRIBED event. But it's not.

{code}
2016/06/03 18:34:04 {Type:SUBSCRIBED Subscribed:&Event_Subscribed{FrameworkID:&mesos.FrameworkID{Value:ffdb6d6e-0167-4fa2-98f9-2c3f8157fc25-0004,},HeartbeatIntervalSeconds:nil,} Offers:nil Rescind:nil Update:nil Message:nil Failure:nil Error:nil}
{code}

{code}
$ dpkg -l |grep -e mesos
ii  mesos                               0.28.0-2.0.16.ubuntu1404         amd64        Cluster resource manager with efficient resource isolation
{code}

I *am* seeing HEARTBEAT events. Just not seeing the interval specified in the SUBSCRIBED event.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-08 18:31:51.744,,,false,MESOS-3302,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 09 15:09:54 UTC 2016,,,,,,,"0|i2yyk7:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 36,,,,,,,,,,,1.0,,,,,,,,,,,"08/Jun/16 18:31;anandmazumdar;https://reviews.apache.org/r/48437/","09/Jun/16 15:09;vinodkone;commit 1c975d0474e97f7af929ea30a4043a59dcaef8a9
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Thu Jun 9 11:07:19 2016 -0400

    Removed redundant expectations for `disconnected` callbacks.
    
    After the recent test refactorings we no longer invoke
    `Shutdown()` at the end of the tests. This ensures that
    the `Master` object is destroyed strictly _after_ the scheduler
    library itself (MESOS-4029). This ensures that there cannot
    be any unexpected callbacks thereafter.
    
    Also, fixed a wrong expectation in
    `Teardown` test. We explicitly close the socket now after
    https://reviews.apache.org/r/44729/
    
    Review: https://reviews.apache.org/r/48442/

commit 49dd8fc9a0822cd9d1055a44ab91c8709ba734e9
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Thu Jun 9 11:07:14 2016 -0400

    Fixed `Subscribed` events to include heartbeat interval.
    
    This changes modifies the \`evolve\` function to pass around
    the heartbeat interval info. This might need some tinkering
    if we decide to make heartbeat interval configurable in the
    near future.
    
    Review: https://reviews.apache.org/r/48437/
",,,,,,,,,,,,,,,,,,,,,,,,,,
Re-enable style-check for stout.,MESOS-5531,12974851,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,karya,karya,karya,01/Jun/16 16:28,26/Apr/17 16:53,29/Oct/20 16:32,01/Jun/16 16:33,,,,,,,,,1.0.0,,,,,,,,,,,0,mesosphere,,,,,,,,"After the 3rdparty reorg, the mesos-style checker stopped checking stout.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-01 16:33:00.887,,,false,MESOS-4690,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 01 16:33:00 UTC 2016,,,,,,,"0|i2yu47:",9223372036854775807,,,,,tillt,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,"01/Jun/16 16:29;karya;RRs:

https://reviews.apache.org/r/48132/
https://reviews.apache.org/r/48133/","01/Jun/16 16:33;tillt;{noformat}
commit f4431a9e07346ce1ff6dcbfce92146631cda02ac
Author: Kapil Arya <kapil@mesosphere.io>
Date:   Wed Jun 1 18:15:25 2016 +0200

    Fixed a minor style issue in stout.

    Review: https://reviews.apache.org/r/48133/
{noformat}

{noformat}
commit 79a7102df42274808ac7ef5ea4c7ea43bafc6035
Author: Kapil Arya <kapil@mesosphere.io>
Date:   Wed Jun 1 18:06:47 2016 +0200

    Updated mesos-style to add 3rdparty/stout.

    Review: https://reviews.apache.org/r/48132/
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,
Confirm errors in authorized persistent volume tests,MESOS-5470,12973307,Bug,Open,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,,,greggomann,greggomann,27/May/16 05:49,26/Apr/17 17:01,29/Oct/20 16:32,,,,,,,,,,,,,,,,test,,,,,0,mesosphere,,,,,,,,The tests {{PersistentVolumeTest.BadACLDropCreateAndDestroy}} and {{PersistentVolumeTest.BadACLNoPrincipal}} check for a failed Destroy operation by confirming that the persistent volume is still contained in an offer received after the attempted operation. We should also explicitly check that the operation did not succeed due to failed authorization.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-7230,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-05-27 05:49:42.0,,,,,,,"0|i2ykyf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CNI should not store subnet of address in NetworkInfo,MESOS-5453,12972785,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,djosborne,djosborne,djosborne,25/May/16 17:35,31/May/16 02:11,29/Oct/20 16:32,27/May/16 22:20,,,,,,,,,1.0.0,,,,,,,,,,,0,mesosphere,,,,,,,,"When the CNI isolator executes the CNI plugin, that CNI plugin will return an IP Address and Subnet (192.168.0.1/32). Mesos should strip the subnet before storing the address in the Task.NetworkInfo.IPAddress.

Reason being - most current mesos components are not expecting a subnet in the Task's NetworkInfo.IPAddress, and instead expect just the IP address. This can cause errors in those components, such as Mesos-DNS failing to return a NetworkInfo address (and instead defaulting to the next configured IPSource), and Marathon generating invalid links to tasks (as it includes /32 in the link)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-27 22:20:14.325,,,false,MESOS-4641,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 27 22:59:39 UTC 2016,,,,,,,"0|i2yhqf:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 35,,,,,,,,,,,2.0,,,,,,,,,,,"27/May/16 17:19;djosborne;https://reviews.apache.org/r/47971/","27/May/16 22:20;jieyu;commit 0c3ea7661a0d6c733a3055102567b1ed925782be
Author: Dan Osborne <dan@projectcalico.org>
Date:   Fri May 27 15:18:28 2016 -0700

    Remove subnet prefix length from IP address.
    
    Remove the prefix length included in the IP address string that is
    returned by the CNI plugin before storing the value in
    NetworkInfo.IPAddress.
    
    Review: https://reviews.apache.org/r/47971/","27/May/16 22:52;djosborne;First time submitting - Is there something left for me to do to close this out?","27/May/16 22:59;jieyu;Thanks for contributing! Nope. I've already resolved this ticket.",,,,,,,,,,,,,,,,,,,,,,,,
Make the SASL dependency optional.,MESOS-5450,12972610,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hausdorff,hausdorff,hausdorff,25/May/16 05:26,22/Mar/17 19:19,29/Oct/20 16:32,07/Jun/16 20:23,,,,,,,,,1.0.0,,,,,,agent,,,,,0,mesosphere,,,,,,,,"Right now there is a hard dependency on SASL, which probably won't work well on Windows (at least) in the near future for our use cases.

In the future, it would be nice to have a pluggable authentication layer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-7286,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-25 06:24:41.983,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 07 20:23:14 UTC 2016,,,,,,,"0|i2ygnj:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 36,,,,,,,,,,,2.0,,,,,,,,,,,"25/May/16 06:24;vinodkone;Authentication is pluggable. But I guess we still depend on SASL? cc [~tillt]","25/May/16 07:03;hausdorff;Ah, yes, that's correct. What we're really trying to do is shed the hard SASL dependency. Thanks [~vinodkone]! :)","25/May/16 21:09;tillt;Let's please adapt the above description and set straight that we do have a pluggable authentication layer -- in fact, we actually have it for both, V0 and V1 (HTTP) authentication.

The challenge is the ""default"" implementation (SASL CRAM-MD5) which we linked into libmesos for historical/transitional reasons. This default implementation creates a hard dependency of libmesos towards CyrusSASL as described within our `src/Makefile.am`
https://github.com/apache/mesos/blob/8be9b5b5decd9ec2bcad547b1dff29b777cbc438/src/Makefile.am#L1867, https://github.com/apache/mesos/blob/8be9b5b5decd9ec2bcad547b1dff29b777cbc438/src/Makefile.am#L1868 and https://github.com/apache/mesos/blob/8be9b5b5decd9ec2bcad547b1dff29b777cbc438/src/Makefile.am#L678

I would also like to propose adding a long term solution, minimizing the platform specifics within Mesos core.

How about this:

We remove the default implementation of the authentication which makes SASL a hard dependency of libmesos right now. We ship the CRAM-MD5 authenticator module with Mesos, allowing installation just like we already do with other modules. We disable installing and building of the CRAM-MD5 authentication for Windows. 

All of this only needs platform specific patches in our build env - not in the code.

Then, instead of disabling authentication within Mesos for Windows as it is now proposed in a short termed solution, we actually add a trivial, not perfectly flexible but usable authentication module. Please mind that one great advantage of using SASL is the mechanism negotiation. Lets call this new variant our “test-authentication"" for now. That one will get included in all builds of Mesos, also within the Windows variant. For Windows, it would be the default authentication, for other platforms we stick with CRAM-MD5 by default but still offer the ""test-authentication"" as an option to be selected by the user when starting up the runnables (master, agent and framework).

Such authenticator / authenticatee pair might also pose as a great example and starting point for other developers, intending to add custom authentication variants not supported by SASL (or simply without SASL for any reason).
","25/May/16 21:11;tillt;How about we add another ticket using my proposal (or something with similar qualities) for the long-termed solution as another JIRA and make them link each other? ","07/Jun/16 20:22;jvanremoortere;https://reviews.apache.org/r/47821/","07/Jun/16 20:23;jvanremoortere;{code}
commit 99145d3e996e08eb7f77a55b1ba030f071fb0f08
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Tue Jun 7 15:59:46 2016 -0400

    Remove SASL dependency for Windows builds.
    
    SASL is currently a hard dependency for Mesos. Per MESOS-5450, we expect
    that some platforms will not support SASL (namely Windows), so this
    commit will begin the first step in a several-step process of removing
    it as a hard dependency.
    
    For this first step, this commit will shed SASL dependency in libmesos
    only for Windows builds. We do this by:
    
      (1) Adding a preprocessor symbol, `HAS_AUTHENTICATION` that wraps the
          SASL-dependent code, so that we can conditionally choose not to
          compile it.
      (2) Defining `HAS_AUTHENTICATION` on all Unix builds, and leaving it
          undefined on all Windows builds.
      (3) Logging an error and exiting the master if the user passes in
          flags that depend on SASL, such as `--authenticate`.
    
    Notably, what we do *not* do is:
    
      (1) Shed SASL dependency in the tests.
    
    The impact of this is that, on Windows, relevant libmesos tests will
    either not compile, or not pass. Naturally, as tracked by MESOS-5450,
    the second phase of this series will be to shed the test dependency as
    well.
    
    Review: https://reviews.apache.org/r/47821/
{code}",,,,,,,,,,,,,,,,,,,,,,
Allow libprocess/stout to build without first doing `make` in 3rdparty.,MESOS-5445,12972266,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,karya,karya,karya,24/May/16 02:46,29/Apr/19 09:27,29/Oct/20 16:32,05/Jul/16 19:05,,,,,,,,,1.0.0,,,,,,build,,,,,1,mesosphere,,,,,,,,"After the 3rdparty reorg, libprocess/stout are enable to build their dependencies and so one has to do `make` in 3rdpart/ before building libprocess/stout.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-04 19:49:35.282,,,false,MESOS-4690,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 05 19:05:37 UTC 2016,,,,,,,"0|hzzzd8:y",9223372036854775807,,,,,tillt,,,,,,Mesosphere Sprint 35,Mesosphere Sprint 36,Mesosphere Sprint 37,Mesosphere Sprint 38,,,,,,,,2.0,,,,,,,,,,,"24/May/16 02:50;karya;RR:

https://reviews.apache.org/r/47752/ : Added build-stamp files for gmock/glog/http-parse/libev.
https://reviews.apache.org/r/47753/ : Enabled building libprocess without building 3rdparty first.
https://reviews.apache.org/r/47754/ : Enabled building libprocess without building 3rdparty first.","04/Jun/16 19:49;tillt;I would like to commit these as they are a dependency of a chain that I am shepherding already. [~jvanremoortere] would you mind me doing so?","06/Jun/16 15:57;jvanremoortere;[~tillt] Great! Go for it :-)","05/Jul/16 19:05;karya;{code}
commit bf7afcecc6196d39e2931ead603bb395322522d8
Author: Till Toenshoff <toenshoff@me.com>
Date:   Tue Jul 5 16:13:37 2016 +0200

    Enabled building stout tests without building 3rdparty first.
    
    Doing a `make` inside stout now automatically build bundled
    dependencies.
    
    Review: https://reviews.apache.org/r/47754/

commit 78deb535c0ebc6d89b4d0e11f76ed0395e5e6e68
Author: Till Toenshoff <toenshoff@me.com>
Date:   Tue Jul 5 14:21:20 2016 +0200

    Enabled building libprocess without building 3rdparty first.
    
    Doing a `make` inside libprocess would now automatically build bundled
    dependencies.
    
    Review: https://reviews.apache.org/r/47753/

commit da298d0a68a2fabd7d7b3a26deb1a29f2f90e5c5
Author: Till Toenshoff <toenshoff@me.com>
Date:   Tue Jul 5 14:12:18 2016 +0200

    Added build-stamp files for gmock/glog/http-parse/libev.
    
    This would make it easier for libprocess to declare/build these
    dependencies.
    
    Review: https://reviews.apache.org/r/47752/
{code}",,,,,,,,,,,,,,,,,,,,,,,,
GPU resource broke framework data table in webUI,MESOS-5436,12971923,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,Fixed,haosdent@gmail.com,haosdent@gmail.com,haosdent@gmail.com,23/May/16 01:36,08/Jun/16 23:43,29/Oct/20 16:32,08/Jun/16 23:43,,,,,,,,,1.0.0,,,,,,,,,,,0,gpu,,,,,,,,"In agent_framework.html and master/static/agent.html, we add {{GPUs (Used / Allocated)}} in table header. But we didn't add the corresponding column to the table body as well.

On the other hand, we didn't provide statistics for gpus on monitor endpoints.
To provide those data in webui, it requires we implement gpus statistics in monitor endpoints firstly. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/May/16 16:53;haosdent@gmail.com;after_agent_framework_page.png;https://issues.apache.org/jira/secure/attachment/12805670/after_agent_framework_page.png","23/May/16 16:53;haosdent@gmail.com;after_agent_page.png;https://issues.apache.org/jira/secure/attachment/12805671/after_agent_page.png","23/May/16 16:53;haosdent@gmail.com;incorrect_agent_framework_page.png;https://issues.apache.org/jira/secure/attachment/12805672/incorrect_agent_framework_page.png","23/May/16 16:53;haosdent@gmail.com;incorrect_agent_page.png;https://issues.apache.org/jira/secure/attachment/12805673/incorrect_agent_page.png",,4.0,,,,,,,,,,,,,,,,,,,,2016-05-23 08:27:46.35,,,false,MESOS-4424,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 08 23:43:58 UTC 2016,,,,,,,"0|i2ycfj:",9223372036854775807,,,,,bmahler,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,"23/May/16 08:27;klueska;I think it's OK to just put 0 or ""N/A"" in cases where we don't yet have the proper statistics. We can fill them in once we have them.","23/May/16 18:11;haosdent@gmail.com;Patch: https://reviews.apache.org/r/47719/","08/Jun/16 23:43;bmahler;{noformat}
commit dfcf5c76934b254466527cdaa67ef2264f7d0dc6
Author: haosdent huang <haosdent@gmail.com>
Date:   Wed Jun 8 16:42:37 2016 -0700

    Fixed a webui bug when GPUs statistics are absent.

    Review: https://reviews.apache.org/r/47719/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,
`network/cni` isolator should skip the bind mounting of the CNI network information root directory if possible,MESOS-5413,12971156,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,qianzhang,qianzhang,qianzhang,19/May/16 13:45,31/May/16 02:12,29/Oct/20 16:32,23/May/16 16:47,,,,,,,,,1.0.0,,,,,,,,,,,0,,,,,,,,,"Currently in the create() method `network/cni` isolator, for the CNI network information root directory (i.e., {{/var/run/mesos/isolators/network/cni}}), we do a self bind mount and make sure it is a shared mount of its own peer group. However, we should not do a self bind mount if the mount containing the CNI network information root directory is already a shared mount in its own share peer group, just like what we did for `filesystem/linux` isolator in [MESOS-5239 | https://issues.apache.org/jira/browse/MESOS-5239].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-23 16:47:34.531,,,false,MESOS-4641,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 23 16:47:34 UTC 2016,,,,,,,"0|i2y7p3:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 35,,,,,,,,,,,3.0,,,,,,,,,,,"19/May/16 14:32;qianzhang;RR: https://reviews.apache.org/r/47594/","23/May/16 16:47;jieyu;commit ecb202aefd62c097d0c8e8e1b52b3e7f8ccbd81b
Author: Qian Zhang <zhangqxa@cn.ibm.com>
Date:   Mon May 23 09:45:14 2016 -0700

    Skipped the bind mount of CNI net info root dir if possible.
    
    Review: https://reviews.apache.org/r/47594/",,,,,,,,,,,,,,,,,,,,,,,,,,
Delete the /observe HTTP endpoint,MESOS-5408,12970604,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,qianzhang,vinodkone,vinodkone,18/May/16 01:36,31/May/16 02:12,29/Oct/20 16:32,20/May/16 03:13,,,,,,,,,1.0.0,,,,,,,,,,,0,,,,,,,,,"The ""/observe"" endpoint was introduced a long time ago for supporting functionality that was never implemented. We should just kill this endpoint and associated code to avoid tech debt.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-19 00:36:55.659,,,false,MESOS-3914,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 20 23:41:13 UTC 2016,,,,,,,"0|i2y4an:",9223372036854775807,,,,,vinodkone,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,"18/May/16 01:37;vinodkone;If anyone wants to work on it this week, I'm willing to shepherd.","19/May/16 00:36;qianzhang;[~vinodkone], I will work on it this week, thanks for shepherding.","19/May/16 00:42;vinodkone;Great. Please send an email to dev/user list as a heads up just in case and also update the CHANGELOG.","19/May/16 07:19;qianzhang;RR: https://reviews.apache.org/r/47581/","20/May/16 03:13;vinodkone;commit 17cbfdf6e4af3ec9519e286581439f8e3c8bc666
Author: Qian Zhang <zhangqxa@cn.ibm.com>
Date:   Thu May 19 20:10:10 2016 -0700

    Deleted the /observe HTTP endpoint.
    
    Review: https://reviews.apache.org/r/47581/
","20/May/16 06:17;gyliu;There are still document needs to be cleaned https://reviews.apache.org/r/47635/","20/May/16 23:41;vinodkone;Thanks [~gyliu]!

commit aafe80911c9885de3a97253ff6c553f43bab083a
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Fri May 20 16:40:11 2016 -0700

    Removed documentation about /observe endpoint.
    
    Review: https://reviews.apache.org/r/47635/
",,,,,,,,,,,,,,,,,,,,,
Make fields in authorization::Request protobuf optional.,MESOS-5405,12970546,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,tillt,alexr,alexr,17/May/16 21:27,07/Jun/16 09:36,29/Oct/20 16:32,07/Jun/16 07:39,,,,,,,,,1.0.0,,,,,,,,,,,0,mesosphere,security,,,,,,,"Currently {{authorization::Request}} protobuf declares {{subject}} and {{object}} as required fields. However, in the codebase we not always set them, which renders the message in the uninitialized state, for example:
 * https://github.com/apache/mesos/blob/0bfd6999ebb55ddd45e2c8566db17ab49bc1ffec/src/common/http.cpp#L603
 * https://github.com/apache/mesos/blob/0bfd6999ebb55ddd45e2c8566db17ab49bc1ffec/src/master/http.cpp#L2057

I believe that the reason why we don't see issues related to this is because we never send authz requests over the wire, i.e., never serialize/deserialize them. However, they are still invalid protobuf messages. Moreover, some external authorizers may serialize these messages.

We can either ensure all required fields are set or make both {{subject}} and {{object}} fields optional. This will also require updating local authorizer, which should properly handle the situation when these fields are absent. We may also want to notify authors of external authorizers to update their code accordingly.

It looks like no deprecation is necessary, mainly because we already—erroneously!—treat these fields as optional.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-17 23:33:33.042,,,false,MESOS-2948,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 07 09:36:36 UTC 2016,,,,,,,"0|i2y3xr:",9223372036854775807,,,,,adam-mesos,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"17/May/16 23:33;tillt;Seems the correct solution here is to make {{subject}} and {{object}} optional within the proto definition.

Keeping them mandatory would e.g. mandate a rather ugly fix for the first example:
{noformat}
        if (principal.isSome()) {
          authRequest.mutable_subject()->set_value(principal.get());
        } else {
          authRequest.mutable_subject();
        }
{noformat}
I firmly believe that this is not what we want for expressing {{ANY}} as used by our internal representation whenever an empty / missing {{subject}} or {{object}} got supplied.

Note that this bug was already present in the authorizer design document: https://docs.google.com/document/d/1-XARWJFUq0r_TgRHz_472NvLZNjbqE4G8c2JL44OSMQ/edit?pref=2&pli=1
I just added a comment to point this out.

I would like to propose the following changes to {{authorizer.proto}}:

1. make the {{subject}} and {{object}} themselves optional:
{noformat}
message Request {
  optional Subject subject = 1;
  optional Action action = 2;
  optional Object object = 3;
}
{noformat}

2. also make the {{subject}} and {{object}} {{value}} required to simplify the underlying implementations:
{noformat}
message Subject {
  required string value = 1;
}
message Object {
  required string value = 1;
}
{noformat} ","18/May/16 00:33;tillt;https://reviews.apache.org/r/47505/
","18/May/16 01:22;tillt;While my above approach would fix things, it would have a deep impact on the conceptual design of the authorizer. It would also make us lose the ability to add alternative {{value}} types - e.g. having a union styled {{object}}.

As an alternative, we could do the following:

1. fix all of those missing {{subject}} / {{object}} initializings. 
2. {{CHECK(request->has_subject())}} and {{CHECK(request->has_object())}} to {{authorizer:: authorized}} to make sure people do not fall into that trap anymore. ","18/May/16 01:29;vinodkone;Fixing the call sites sgtm.","18/May/16 02:17;tillt;Got a fix for the second variant at hand - reviewboard currently down - will upload ASAP.

Need a shepherd.","18/May/16 03:08;tillt;https://reviews.apache.org/r/47509","18/May/16 08:05;alexr;I'd rather vote for the original solution for the following reasons:
  * proto3 does not support required fields, if we want to migrate eventually we will have to get rid of them anyway;
  * the design is still error-prone: checks you've added do not protect if local authorizer is not used
  * the code is not concise; we require people to call certain function only in order to please proto rules

To overcome the drawback of your first solution, how about you keep fields in {{Subject}} and {{Object}} optional? This way, we can still upgrade to a union styled {{object}} if we would like to. I don't see why this change deeply impacts the design of the authorizer. We are allowed to express {{ANY}}, with the change you proposed, there will be _just_ another way to express it. Basically, we already do it everywhere in the codebase, we just legalize it. Moreover, while absence of a {{subject}} and {{subject}} with absent {{value}} mean the same thing now, we may even want to change it in the future, so this gives us an extra degree of freedom.","27/May/16 21:47;hartem;[~adam-mesos] Can you take a look at this one please? It's marked as a blocker for the release.","28/May/16 08:47;adam-mesos;[~tillt] I think I'm favoring [~alexr]'s approach of making subject and object optional (to handle object-less requests, like ACCESS_MESOS_LOG), leaving their values optional (to handle union-like objects), and then fixing all the accesses to handle missing subject/object/value correctly.
But maybe missing object doesn't mean the same as a missing object.value (even if the other fields are also missing). We could allow each action to define whether or not it requires an object, since some like ACCESS_MESOS_LOG might actually want to check that there is never an object set. And then, if the action allows/requires an object, we can define an object with no fields set (including no optional FrameworkInfo/ExecutorInfo/TaskInfo fields) as an ANY object.","30/May/16 20:25;alexr;cc [~arojas]","30/May/16 23:15;arojas;Well, I share Adam's opinion. Not to mention that one of the purposes of the design was to make the calling logic somewhat friendlier and easier to track and going for the second approach doesn't achieve that goal.","31/May/16 19:42;tillt;{noformat}
commit 79a98ef739dd67a301ed08cc35f446772f224abd
Author: Till Toenshoff <toenshoff@me.com>
Date:   Tue May 31 21:39:53 2016 +0200

    Updated authorizer.proto Request.

    `subject` and `object` are now optional fields of the `Request` proto
    message.

    Review: https://reviews.apache.org/r/47505/
{noformat}","31/May/16 21:45;js84;[~tillt] [~adam-mesos] [~mcypark]
This breaks some assumptions of the current `authorized` interface which assume `subject` and `object` are set (see below).

In order to accomodate for this these new optional fields i would propose the following 
1. Change getObjectApprover's signatures to accept Option<subject>, Option <action>
2. Change objectApprover->approved() signature to accept an Option<object>
(and adapt the logic in approved for the LocalAuthorizerObjectApprover to deal with the None -> Any conversion)

{noformat}
  Future<bool> authorized(const authorization::Request& request)
  {
    return getObjectApprover(request.subject(), request.action())
      .then([=](const Owned<ObjectApprover>& objectApprover) -> Future<bool> {
        ObjectApprover::Object object(request.object());
        Try<bool> result = objectApprover->approved(object);
        if (result.isError()) {
          return Failure(result.error());
        }
        return result.get();
      });
  }
{noformat}
","31/May/16 22:27;tillt;Additional work was done belonging to this issue:

Added {{Request}} sanity checks in {{LocalAuthorizer}}: https://reviews.apache.org/r/48085/
Updated comments in authorizer.proto.: https://reviews.apache.org/r/48093/

Note that the latter tries to supercede https://reviews.apache.org/r/47876 - by borrowing some inspirations from it - thanks [~adam-mesos]!","31/May/16 22:27;tillt;sgtm","31/May/16 23:30;js84;https://reviews.apache.org/r/48101/","06/Jun/16 08:22;js84;Added documentation and tests:

Fixed documentation for MESOS-5405.
https://reviews.apache.org/r/48263/

Added test for optional request.object field.
https://reviews.apache.org/r/48264/","07/Jun/16 07:39;tillt;{noformat}
commit ac7171c04e7718202e6adc946fc196715565012b
Author: Joerg Schad <joerg@mesosphere.io>
Date:   Tue Jun 7 09:33:16 2016 +0200

    Added test for optional `Request.Object` field.

    As MESOS-5405 changes the `Request.Object` field to optional. We need
    to check that it only matches an `ANY` ACL.

    Review: https://reviews.apache.org/r/48264/
{noformat}

{noformat}
commit bba8f54a6c1f3807ac433efc9f4ff933f709299c
Author: Joerg Schad <joerg@mesosphere.io>
Date:   Tue Jun 7 09:34:29 2016 +0200

    Added change of fields `Request` to optional to CHANGELOG.

    The changed semantic of the `Request` protobug message with MESOS-5405
    should be added to the CHANGELOG.

    Review: https://reviews.apache.org/r/48267/
{noformat}","07/Jun/16 07:40;tillt;{noformat}
commit b9998d1473d50fe8fe4001c69c44dbfba16d7fea
Author: Joerg Schad <joerg@mesosphere.io>
Date:   Tue May 31 20:57:18 2016 -0600

    Changed the ObjectApprover interface for optional subject and objects.

    Review 47505 changed the Request.object and subject to become optional
    fields.This patch adapts the ObjectApprover interface to deal with this
    behavior.

    Review: https://reviews.apache.org/r/48101/
{noformat}","07/Jun/16 09:36;tillt;{noformat}
commit 90871a48f4f1a345950862a53efb78e0b9aadedb
Author: Joerg Schad <joerg@mesosphere.io>
Date:   Tue Jun 7 11:34:53 2016 +0200

    Fixed documentation for MESOS-5405.

    As MESOS-5405 changes the fields in `Request` to optional, we need to
    update the documentation.

    Review: https://reviews.apache.org/r/48263/
{noformat}",,,,,,,,
Introduce ObjectApprover Interface to Authorizer.,MESOS-5403,12970537,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,js84,js84,js84,17/May/16 21:01,31/May/16 06:48,29/Oct/20 16:32,31/May/16 06:48,,,,,,,,,1.0.0,,,,,,,,,,,0,,,,,,,,,As outlined here (https://docs.google.com/document/d/1FuS79P8uj5PIBycrBlkJSBKOtmeO8ezAuiNXxwIA3qA) we plan to add the option of retrieving a FilterObject from the Authorizer with the goal of allowing for efficient authorization of a large number of (potentially large) objects. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-31 06:48:30.093,,,false,MESOS-4931,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 31 06:48:30 UTC 2016,,,,,,,"0|i2y3vr:",9223372036854775807,,,,,mcypark,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,"31/May/16 00:46;js84;https://reviews.apache.org/r/47558/","31/May/16 06:48;mcypark;{noformat}
commit 4c83d7f7adc5f5146426eff70b67c317a583c15d
Author: Joerg Schad <joerg@mesosphere.io>
Date:   Mon May 30 23:25:21 2016 -0700

    Added `ObjectApprover` interface to `Authorizer`.

    With the goal to provide more efficient authorization for multiple,
    potentially large objects, we extend the authorizer interface to support
    `ObjectApprover`. Retrieving an `ObjectApprover` for a given action is
    an asynchronous operation but following authorization of multiple
    objects can be done synchronously without copying.

    NOTE: This implies that `Authorizer` module writers need to ensure that
    an `ObjectApprover` will behave nicely (e.g., it must not block).

    Review: https://reviews.apache.org/r/47558/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Slave/Agent Rename Phase 1: Update terms in the website,MESOS-5397,12970483,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,haosdent@gmail.com,vinodkone,vinodkone,17/May/16 18:40,31/May/16 02:11,29/Oct/20 16:32,27/May/16 04:54,,,,,,,,,1.0.0,,,,,,project website,,,,,0,,,,,,,,,"The following files need to be updated

site/source/index.html.md
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-26 01:44:03.885,,,false,MESOS-1478,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 27 04:54:18 UTC 2016,,,,,,,"0|i2y3jz:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 35,,,,,,,,,,,1.0,,,,,,,,,,,"25/May/16 19:01;vinodkone;[~haosdent@gmail.com] is this something you want to take on?","26/May/16 01:44;haosdent@gmail.com;Got it, let me post the patch later.","26/May/16 17:16;haosdent@gmail.com;Patch: https://reviews.apache.org/r/47909/

Because we updated {{site/README.md}} before, only need to update {{site/source/index.html.erb}} here.","27/May/16 04:54;vinodkone;commit 085b3690e7cccd84bc45ffa9221335c5da61fec5
Author: haosdent huang <haosdent@gmail.com>
Date:   Thu May 26 21:53:14 2016 -0700

    Slave/Agent Rename Phase I - Updated terms in the website.
    
    Review: https://reviews.apache.org/r/47909/
",,,,,,,,,,,,,,,,,,,,,,,,
v1 Executor Protos not included in maven jar,MESOS-5390,12970147,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,dongdong,BenWhitehead,BenWhitehead,16/May/16 20:58,18/Jul/16 19:04,29/Oct/20 16:32,20/May/16 03:00,0.28.0,0.28.1,,,,,,,0.28.3,1.0.0,,,,,HTTP API,,,,,0,mesosphere,,,,,,,,"According to MESOS-4793 the Executor v1 HTTP API was released in Mesos 0.28.0 however the corresponding protos are not included in the maven jar for version 0.28.0 or 0.28.1.

Script to verify
{code}
wget https://repo.maven.apache.org/maven2/org/apache/mesos/mesos/0.28.1/mesos-0.28.1.jar && unzip -lf mesos-0.28.1.jar | grep ""v1\/executor"" | wc -l
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-3524,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-19 06:50:18.308,,,false,MESOS-4855,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 20 03:00:05 UTC 2016,,,,,,,"0|i2y1hb:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 35,,,,,,,,,,,1.0,,,,,,,,,,,"19/May/16 06:50;dongdong;Review Request submitted at: https://reviews.apache.org/r/47582/","20/May/16 03:00;vinodkone;commit 56560fd00dbd76f8c9a5187af52ce8d59545fee3
Author: zhou xing <xingzhou@cn.ibm.com>
Date:   Thu May 19 19:57:28 2016 -0700

    Included v1 Executor Protos in maven jar.
    
    Updated the Makefile to include v1 executor protos classes files
    in the generated mesos maven jar.
    
    Review: https://reviews.apache.org/r/47582/
",,,,,,,,,,,,,,,,,,,,,,,,,,
docker containerizer should prefix relative volume.container_path values with the path to the sandbox,MESOS-5389,12970113,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,jdef,jdef,16/May/16 19:11,26/Apr/17 16:53,29/Oct/20 16:32,13/Jul/16 22:56,,,,,,,,,1.0.0,,,,,,containerization,,,,,0,docker,mesosphere,storage,volumes,,,,,"docker containerizer currently requires absolute paths for values of volume.container_path. this is inconsistent with the mesos containerizer which requires relative container_path. it makes for a confusing API. both at the Mesos level as well as at the Marathon level.

ideally the docker containerizer would allow a framework to specify a relative path for volume.container_path and in such cases automatically convert it to an absolute path by prepending the sandbox directory to it.

/cc [~jieyu]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-5341,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-16 20:36:41.238,,,false,MESOS-4355,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 13 22:56:20 UTC 2016,,,,,,,"0|i2y19r:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 39,,,,,,,,,,,3.0,,,,,,,,,,,"16/May/16 20:36;gilbert;Thanks [~jdef], it should not be hard to do that, but a discussion may be needed about whether or not we want to do that. Because that's totally different from docker user's behavior. Only absolute path is allowed in docker daemon when they do `docker run -v ....`.

I understand that you want to make two containerizer consistent in behavior, but this issue belongs to the diff semantic for these two containerizes, and we do have similar difference on supporting docker runtime on these two containerizes. This difference also comes from the mesos containerizer support with/without image rootfs, but docker containerizer only use docker daemon.

Definitely we need more discussion. To me, I would prefer keep the current semantic. And if users want to use the docker containerizer with relative path, they can just specify the container_path to the sandbox (e.g., /mnt/mesos/sandbox/tmp).","17/May/16 06:06;gyliu;I think that for docker containerizer, we should keep the behaviour same as docker, the {{man docker run}} will report the following:

{code}
 -v|--volume[=[[HOST-DIR:]CONTAINER-DIR[:OPTIONS]]]
          Create a bind mount. If you specify, -v /HOST-DIR:/CONTAINER-DIR, Docker
          bind mounts /HOST-DIR in the host to /CONTAINER-DIR in the Docker
          container. If 'HOST-DIR' is omitted,  Docker automatically creates the new
          volume on the host.  The OPTIONS are a comma delimited list and can be:

       0

              item [rw|ro] item [z|Z] item [[r]shared|[r]slave|[r]private] item [nocopy]

       The  CONTAINER-DIR  must  be  an  absolute  path such as /src/docs. The HOST-DIR can be an absolute path or a name value. A name value must start with an alphanumeric character, followed by
       a-z0-9, _ (underscore), . (period) or - (hyphen). An absolute path starts with a / (forward slash).
{code}

It highlight that the {{The  CONTAINER-DIR  must  be  an  absolute  path such as /src/docs. }}.
","17/May/16 10:42;jdef;Disagree. The containerizers should both being able to accept relative
containerPath's otherwise the API's become confusing for the end user. The
user doesn't always know the path to the sandbox that Mesos (or dc/os) will
append so asking the user to specify an absolute containerPath (if they
want the vol mounted in their sandbox) is a non-starter.





-- 
James DeFelice
585.241.9488 (voice)
650.649.6071 (fax)
","17/May/16 10:44;jdef;s/will append/will use/","17/May/16 13:31;gyliu;Thannks [~james_defelice@elementkcom] I'm linking this with MESOS-5341 cause I was now enhancing docker containerizer with docker volume support, hope we can get more comments from [~jieyu] and [~gilbert]","13/Jul/16 19:29;gilbert;https://reviews.apache.org/r/49961/
https://reviews.apache.org/r/49997/
https://reviews.apache.org/r/49998/
https://reviews.apache.org/r/49999/","13/Jul/16 22:56;jieyu;commit 453c3cc939eb19adda25ff3be4483fe277adcc8b
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jul 13 15:52:21 2016 -0700

    Added docker test relative host path and relative container path.
    
    Review: https://reviews.apache.org/r/49999/

commit f7df37b7c9e3e9c20899ce906655087c3270c47b
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jul 13 15:52:18 2016 -0700

    Added docker test mount relative container path.
    
    Review: https://reviews.apache.org/r/49998/

commit 2b6f75c80ecf80c4fec2d3f22f1aa00798e0f419
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jul 13 15:52:16 2016 -0700

    Modified docker test mount absolute/relative host path.
    
    Review: https://reviews.apache.org/r/49997/

commit 35a2074b39dc2e97d84b0f0f5f16f39218fb5bff
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jul 13 15:52:12 2016 -0700

    Supported relative container path in docker containerizer.
    
    Review: https://reviews.apache.org/r/49961/",,,,,,,,,,,,,,,,,,,,,
MesosContainerizerLaunch flags execute arbitrary commands via shell.,MESOS-5388,12970098,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,jdef,jdef,16/May/16 18:28,02/Aug/16 23:51,29/Oct/20 16:32,02/Aug/16 23:51,1.0.0,,,,,,,,1.0.1,,,,,,containerization,,,,,0,containerizer,isolator,mesosphere,security,,,,,"For example, the docker volume isolator's containerPath is appended (without sanitation) to a command that's executed in this manner. As such, it's possible to inject arbitrary shell commands to be executed by mesos.

https://github.com/apache/mesos/blob/17260204c833c643adf3d8f36ad8a1a606ece809/src/slave/containerizer/mesos/launch.cpp#L206

Perhaps instead of strings these commands could/should be sent as string arrays that could be passed as argv arguments w/o shell interpretation?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-16 20:19:12.31,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 02 23:49:26 UTC 2016,,,,,,,"0|hzzzc4:zx",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 39,Mesosphere Sprint 40,,,,,,,,,,5.0,,,,,,,,,,,"16/May/16 20:19;gilbert;[~jdef]thanks for reporting the issue.

To better understand, is this issue referring that any arbitrary commands may be included in `container_path`? which make it dangerous since we will run a shell command including that `contianer_path`.

First, we should do more on users, and we have MESOS-4936 to support container capabilities.

Second, container_path should not be regarded as totally without sanitation (we should do more though), because in docker volume isolator prepare() we will `mkdir` for the container_path:
https://github.com/apache/mesos/blame/bd9d208972dbfae4e0d15b053b754de3e02a763e/src/slave/containerizer/mesos/isolators/docker/volume/isolator.cpp#L375~#L380
which will cause a failure in prepare() before we set CommandInof.commands, so many dangerous commands attached with the container_path should be filtered out.","17/May/16 06:35;gyliu;Yes, the {{docker volume isolator}} already filtered out the {{CommandInfo.commands}} as the {{commands}} in {{docker volume isolator}} is only for {{launchInfo}}

{code}
Future<Option<ContainerLaunchInfo>> DockerVolumeIsolatorProcess::_prepare(
    const ContainerID& containerId,
    const vector<string>& targets,
    const list<Future<string>>& futures)
{
  ContainerLaunchInfo launchInfo;  <<<<<<<<<
  launchInfo.set_namespaces(CLONE_NEWNS);

  vector<string> messages;
  vector<string> sources;
  foreach (const Future<string>& future, futures) {
    if (!future.isReady()) {
      messages.push_back(future.isFailed() ? future.failure() : ""discarded"");
      continue;
    }

    sources.push_back(strings::trim(future.get()));
  }

  if (!messages.empty()) {
    return Failure(strings::join(""\n"", messages));
  }

  CHECK_EQ(sources.size(), targets.size());

  for (size_t i = 0; i < sources.size(); i++) {
    const string& source = sources[i];
    const string& target = targets[i];

    LOG(INFO) << ""Mounting docker volume mount point '"" << source
              << ""' to '"" << target  << ""' for container "" << containerId;

    const string command = ""mount -n --rbind "" + source + "" "" + target;

    launchInfo.add_commands()->set_value(command); <<<<<<<<<
  }

  return launchInfo;
}
{code}","21/Jul/16 01:03;gilbert;https://reviews.apache.org/r/50215/
https://reviews.apache.org/r/50214/
https://reviews.apache.org/r/50534/
https://reviews.apache.org/r/50535/
https://reviews.apache.org/r/50216/
https://reviews.apache.org/r/50580/
https://reviews.apache.org/r/50581/","26/Jul/16 21:12;jieyu;commit 5299df79c85dcc4205b014a8bd360bc8a4cdf8e7
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Jul 26 14:11:20 2016 -0700

    Updated pre exec commands as non-shell in docker volume isolator.
    
    By adding apostrophes to mount 'source' and 'target', arbitraty commands
    defined by users postfixed to 'container_path' will take no effect.
    'mount' command will return an error for invalid mount 'target'.
    
    Review: https://reviews.apache.org/r/50215/","27/Jul/16 05:00;jieyu;This does not fully fix the problem. People can still inject arbitrary command using '; rm -rf /
","01/Aug/16 18:26;jieyu;commit 25626fcf8f63875ed0ccfe2ddb67a9998e5ba934
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Mon Aug 1 09:50:13 2016 -0700

    Supported non-shell command in MesosLaunch to avoid arbitrary commands.
    
    Currently all pre_exec_commands are executed as shell commands in Mesos
    Launch. It is not safe because arbitrary shell command may be included
    in some user facing api (e.g., container_path).  We should execute those
    command as a subprocess to prevent arbitrary shell command injection.
    
    Review: https://reviews.apache.org/r/50214/","01/Aug/16 20:06;jieyu;commit ca5eaad82f69309de427aab3ec2ed7976c9cc850
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Mon Aug 1 13:05:53 2016 -0700

    Updated docker volume isolator to return non-shell 'pre_exec_commands'.
    
    Review: https://reviews.apache.org/r/50535/

commit 202e1933c592f456420ec1c85fd9a212222d0df9
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Mon Aug 1 13:03:16 2016 -0700

    Updated mesos containerizer launch execute() to return 'EXIT_FAILURE'.
    
    Review: https://reviews.apache.org/r/50534/","02/Aug/16 00:07;jieyu;commit 9c6097f063405279efc07eec22457c2059653f07
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Mon Aug 1 17:07:00 2016 -0700

    Updated filesystem linux isolator pre exec commands to be non-shell.
    
    Review: https://reviews.apache.org/r/50216/","02/Aug/16 23:49;gilbert;commit 9c77899431f8e414f5965a424888a889f6327135
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Aug 2 15:21:38 2016 -0700

    Removed unused user variable in filesystem linux isolator prepare.
    
    Review: https://reviews.apache.org/r/50580/

commit 9579a298d1b3e38d9e70261ebe2c893893282d72
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Aug 2 15:46:31 2016 -0700

    Added logs for pre-exec commands to sandbox in MesosContainerizerLaunch.
    
    Review: https://reviews.apache.org/r/50581/",,,,,,,,,,,,,,,,,,,
Add `HANDLE` overloads for functions that take a file descriptor,MESOS-5386,12969894,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hausdorff,hausdorff,hausdorff,16/May/16 03:23,31/May/16 02:11,29/Oct/20 16:32,23/May/16 23:55,,,,,,,,,1.0.0,,,,,,stout,,,,,0,mesosphere,stout,windows-mvp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-23 23:55:02.428,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 23 23:55:10 UTC 2016,,,,,,,"0|i2xzx3:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 35,,,,,,,,,,,3.0,,,,,,,,,,,"23/May/16 23:55;jvanremoortere;https://reviews.apache.org/r/47409/
https://reviews.apache.org/r/47404/","23/May/16 23:55;jvanremoortere;{code}
commit 4155145c8973859c4dfb55856708c3c782660461
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Mon May 23 16:09:08 2016 -0700

    Libprocess: Implemented `HANDLE` versions of file descriptor functions.
    
    Review: https://reviews.apache.org/r/47409/

commit 490c532ce47f082d3359ee7e64d65b94019c4ef6
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Mon May 23 16:05:10 2016 -0700

    Stout: Implemented `HANDLE` versions of file descriptor functions.
    
    Review: https://reviews.apache.org/r/47404/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Implement os::setHostname,MESOS-5383,12969759,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hausdorff,hausdorff,hausdorff,14/May/16 11:57,31/May/16 02:12,29/Oct/20 16:32,23/May/16 22:18,,,,,,,,,1.0.0,,,,,,stout,,,,,0,mesosphere,stout,windows-mvp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-23 22:18:07.577,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 23 22:18:16 UTC 2016,,,,,,,"0|i2xz33:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 35,,,,,,,,,,,1.0,,,,,,,,,,,"23/May/16 22:18;jvanremoortere;https://reviews.apache.org/r/47386/","23/May/16 22:18;jvanremoortere;{code}
commit a50bb26a496d565bf44631eacbb4e59b896a83c4
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Fri May 20 13:55:15 2016 -0700

    Stout: Implemented `setHostname` on Windows.
    
    Review: https://reviews.apache.org/r/47386/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Implement os::fsync,MESOS-5382,12969758,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hausdorff,hausdorff,hausdorff,14/May/16 11:56,31/May/16 02:11,29/Oct/20 16:32,23/May/16 23:51,,,,,,,,,1.0.0,,,,,,stout,,,,,0,mesosphere,stout,windows-mvp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-23 23:50:58.08,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 23 23:51:13 UTC 2016,,,,,,,"0|i2xz2v:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 35,,,,,,,,,,,1.0,,,,,,,,,,,"23/May/16 23:50;jvanremoortere;https://reviews.apache.org/r/47387/","23/May/16 23:51;jvanremoortere;{code}
commit 79f07ef1447e1c74bb32372ac413f73d3ec56765
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Mon May 23 15:29:17 2016 -0700

    Updated Mesos to use `os::fsync`.

commit a8829e1ad07ba81099d9b9fabe1e93e2b36036e5
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Mon May 23 15:23:02 2016 -0700

    Stout: Implemented `os::fsync`.
    
    Review: https://reviews.apache.org/r/47387/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
The scheduler library should have a delay before initiating a connection with master.,MESOS-5359,12967052,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jvanz,anandmazumdar,anandmazumdar,10/May/16 17:22,20/Jun/16 14:40,29/Oct/20 16:32,20/Jun/16 14:40,1.0.0,,,,,,,,1.0.0,,,,,,,,,,,0,mesosphere,,,,,,,,"Currently, the scheduler library {{src/scheduler/scheduler.cpp}} does have an artificially induced delay when trying to initially establish a connection with the master. In the event of a master failover or ZK disconnect, a large number of frameworks can get disconnected and then thereby overwhelm the master with TCP SYN requests. 

On a large cluster with many agents, the master is already overwhelmed with handling connection requests from the agents. This compounds the issue further on the master.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-21 20:19:32.984,,,false,MESOS-3302,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 20 14:40:07 UTC 2016,,,,,,,"0|i2xifr:",9223372036854775807,,,,,anandmazumdar,,,,,,Mesosphere Sprint 37,,,,,,,,,,,3.0,,,,,,,,,,,"21/May/16 20:19;jvanz;Taking a look in the related issue MESOS-5330 and scheduler library (i. e.  MesosSchedulerDriver, SchedulerProcess classes ) I think I will change the SchedulerProcess class. Right?
Based on MESOS-5330 solution I have to move the link method call from detected() method  to authenticate() and doReliableRegistration()  methods: 

{code:title=sched.cpp|borderStyle=solid}
    if (master.isSome()) {                                                                                                                                                                                                                    
      LOG(INFO) << ""New master detected at "" << master.get().pid();                                                                                                                                                                           
      link(master.get().pid());                                                                                                                                                                                                               
                                                                                                                                                                                                                                              
      if (credential.isSome()) {                                                                                                                                                                                                              
        // Authenticate with the master.                                                                                                                                                                                                      
        // TODO(vinod): Do a backoff for authentication similar to what                                                                                                                                                                       
        // we do for registration.                                                                                                                                                                                                            
        authenticate();                                                                                                                                                                                                                       
      } else { 
{code}

Am I right path?


","21/May/16 20:38;anandmazumdar;[~jvanz] Thanks for taking this on. 

The scope of the JIRA is to make the changes for the new API i.e. the scheduler library here: {{src/scheduler/scheduler.cpp}} and not to the old driver based interface {{src/sched/sched.cpp}}. A possible solution can be to introduce a {{delay}} here: https://github.com/apache/mesos/blob/master/src/scheduler/scheduler.cpp#L473

For more information about the new API see: http://mesos.apache.org/documentation/latest/scheduler-http-api/

","24/May/16 03:06;jvanz;Cool! Thanks [~anandmazumdar]] for the code pointer.

Should this delay be configurable by some flag?","24/May/16 16:17;anandmazumdar;Ideally, yes. The delay should be configurable by a flag.

Have a look at how we are already doing so for the old driver based interface {{src/sched/flags.hpp}}. The only difference here is that we just need a single maximum connection backoff variable e.g., {{MESOS_CONNECTION_BACKOFF_MAX=~500ms}}. The scheduler library can then do a linear backoff after picking a random delay between 0 and maxBackoff for initiating the connection with the master.

","01/Jun/16 02:39;jvanz;I already have a preliminary patch. But before submitted it to review, I would like to ask something related which is the best approach considering the Mesos way to does stuff.

Right now, I added a {{flags}} member in the {{MesosProcess}}:

{code:title=src/scheduler/scherduler.ccp|borderStyle=solid}
  MesosProcess(
      const string& master,
      ContentType _contentType,
      const lambda::function<void()>& connected,
      const lambda::function<void()>& disconnected,
      const lambda::function<void(const queue<Event>&)>& received,
      const Option<Credential>& _credential,
      const Option<shared_ptr<MasterDetector>>& _detector,
      const mesos::v1::scheduler::Flags& _flags)
    : ProcessBase(ID::generate(""scheduler"")),
      state(DISCONNECTED),
      contentType(_contentType),
      callbacks {connected, disconnected, received},
      credential(_credential),
      local(false),
      flags(_flags)
  {

....

Mesos::Mesos(
    const string& master,
    ContentType contentType,
    const lambda::function<void()>& connected,
    const lambda::function<void()>& disconnected,
    const lambda::function<void(const queue<Event>&)>& received,
    const Option<Credential>& credential,
    const Option<shared_ptr<MasterDetector>>& detector,
    const mesos::v1::scheduler::Flags& flags)
{
{code}

The {{mesos::v1::scheduler::Flags}} is the class created following the {{src/sched/flags.hpp}} example.  However, I'm not sure if pass the {{Flags}} object is the best idea. I believe that the old api does that because the scheduler driver, as an ""internal"" class, is responsable for that.  The new API {{Mesos}} class is the instanciated by the scheduler itself thereby I had to add the {{mesos::v1::scheduler::Flags}} in the include dir, allowing scheduler to instanciante the class. Is it ok? Should I pass just the flag value in the {{Mesos}} constructor such as master  connection url?","07/Jun/16 00:20;jvanz;[~anandmazumdar] ping?","07/Jun/16 00:40;jvanz;I think that I made a little confusion. I do not need expose the flags class in include dir because the flag will be loaded from env vars. So the scheduler will not define the value itself. ","07/Jun/16 04:19;anandmazumdar;[~jvanz] Sorry, this missed my attention due to MesosCon. As you have remarked, you don't need to expose the {{Flags}} class in the include directory. You should be able to create the class in {{src/scheduler/flags.hpp/cpp}} similar to the driver. Makes sense?","08/Jun/16 01:21;jvanz;https://reviews.apache.org/r/48387/","20/Jun/16 14:40;anandmazumdar;{noformat}
commit 948fe8c7b5b20dda5abd56ad64f4e5b28b8dc750
Author: Jose Guilherme Vanz <guilherme.sft@gmail.com>
Date:   Mon Jun 20 07:15:02 2016 -0700

    Delay before initiating a connection with master.

    The scheduler library has been updated to wait before initiating a
    connection with the master. The maximum amount of time waited by
    the library is defined by the flag: `connection_delay_max`. For
    tests, we default the delay to 0 to speed up the tests.

    Review: https://reviews.apache.org/r/48387/
{noformat}",,,,,,,,,,,,,,,,,,
GET /master/maintenance/schedule/ produces 404.,MESOS-5333,12965425,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,Fixed,alexr,nhandler,nhandler,05/May/16 22:03,25/Jan/18 19:07,29/Oct/20 16:32,25/Jan/18 19:07,,,,,,,,,1.6.0,,,,,,HTTP API,libprocess,,,,0,mesosphere,,,,,,,,"Attempts to make a GET request to /master/maintenance/schedule/ result in a 404. However, if I make a GET request to /master/maintenance/schedule (without the trailing /), it works. My current (untested) theory is that this might be related to the fact that there is also a /master/maintenance/schedule/status endpoint (an endpoint built on top of a functioning endpoint), as requests to /help and /help/ (with and without the trailing slash) produce the same functioning result.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-12-13 15:40:35.703,,,false,MESOS-7201,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 25 19:07:22 UTC 2018,,,,,,,"0|hzzy7l:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 70,Mesosphere Sprint 71,Mesosphere Sprint 72,Mesosphere Sprint 73,,,,,,,,3.0,,,,,,,,,,,"13/Dec/17 15:40;alexr;https://reviews.apache.org/r/64574/","25/Jan/18 19:07;alexr;{noformat}
commit abd1751aa9a305b0648f4a630acbc84e7d837783
Author:     Alexander Rukletsov <alexr@apache.org>
AuthorDate: Wed Dec 13 14:35:42 2017 +0100
Commit:     Alexander Rukletsov <alexr@apache.org>
CommitDate: Thu Jan 25 20:05:07 2018 +0100

    Updated libprocess routing to treat trailing '/' as insignificant.
    
    Prior to this patch, adding a trailing '/' to a valid URL path, e.g.,
    ""/state/"", yielded a 404 response. This patch ensures that two URLs
    which differ only in trailing '/' produce the same result.
    
    This is achieved by (1) trimming a trailing '/' when processing a
    request and (2) disallowing `route()` calls from using `/`.
    
    Review: https://reviews.apache.org/r/64574
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Make `os::close` always catch structured exceptions on Windows,MESOS-5318,12964529,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hausdorff,hausdorff,hausdorff,03/May/16 05:40,31/May/16 02:11,29/Oct/20 16:32,10/May/16 15:44,,,,,,,,,1.0.0,,,,,,stout,,,,,0,mesosphere,windows-mvp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-10 15:16:02.785,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 10 15:44:11 UTC 2016,,,,,,,"0|i2x2xb:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 35,,,,,,,,,,,2.0,,,,,,,,,,,"10/May/16 15:16;jvanremoortere;https://reviews.apache.org/r/46928/","10/May/16 15:44;jvanremoortere;{code}
commit 9aacd202e0286bdfae90f9c28aec7d015aaf9efa
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Tue May 10 11:11:48 2016 -0400

    Windows: Added safety fixes to and tests for `os::close`.
    
    This commit has three goals:
    
      (1) Expand the `os::close` to support closing Windows `HANDLE`
          objects. This will simplify code in many places, but most notably
          (and immediately) subprocess, where sharing code between POSIX and
          Windows implementations is a major priority.
      (2) Make `os::close` safe on Windows. Unlike its POSIX cousins, the
          Windows implementation of `::close` is very picky about which file
          descriptors it will close. If you pass it a `SOCKET` by mistake,
          for example, the C runtime will throw a structured exception.
          Since this could potentially halt the process, and since our core
          socket abstractions (socket.hpp, for example) use `int` to
          represent sockets, it is worth investing in safety here.
      (3) Add `os::close` unit tests to verify the safety requirements.
    
    Review: https://reviews.apache.org/r/46928/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Env `MESOS_SANDBOX` is not set properly for command tasks that changes rootfs.,MESOS-5312,12964006,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,lins05,jieyu,jieyu,29/Apr/16 23:01,04/Jun/16 18:03,29/Oct/20 16:32,03/May/16 05:05,0.28.0,0.28.1,,,,,,,0.28.2,1.0.0,,,,,,,,,,0,mesosphere,,,,,,,,"This is in the context of Mesos containerizer (a.k.a., unified containerizer).

I did a simple test:
{noformat}
sudo sbin/mesos-master --work_dir=/tmp/mesos/master
sudo GLOG_v=1 sbin/mesos-slave --master=10.0.2.15:5050 --isolation=docker/runtime,filesystem/linux --work_dir=/tmp/mesos/slave/ --image_providers=docker --executor_environment_variables=""{}""
sudo bin/mesos-execute --master=10.0.2.15:5050 --name=test --docker_image=alpine --command=""env"" 

MESOS_EXECUTOR_ID=test
SHLVL=1
MESOS_CHECKPOINT=0
MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD=5secs
LIBPROCESS_PORT=0
MESOS_AGENT_ENDPOINT=10.0.2.15:5051
MESOS_SANDBOX=/tmp/mesos/slave/slaves/2d7e44bb-3282-4193-bdc4-eeab9e0943c2-S0/frameworks/1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000/executors/test/runs/bb8dd72c-fb4c-426a-be18-51b0621339f6
MESOS_NATIVE_JAVA_LIBRARY=/home/vagrant/dist/mesos/lib/libmesos-0.29.0.so
MESOS_FRAMEWORK_ID=1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000
MESOS_SLAVE_ID=2d7e44bb-3282-4193-bdc4-eeab9e0943c2-S0
MESOS_NATIVE_LIBRARY=/home/vagrant/dist/mesos/lib/libmesos-0.29.0.so
MESOS_DIRECTORY=/tmp/mesos/slave/slaves/2d7e44bb-3282-4193-bdc4-eeab9e0943c2-S0/frameworks/1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000/executors/test/runs/bb8dd72c-fb4c-426a-be18-51b0621339f6
PWD=/mnt/mesos/sandbox
MESOS_SLAVE_PID=slave(1)@10.0.2.15:5051
{noformat}

`MESOS_SANDBOX` above should be `/mnt/mesos/sandbox`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-01 05:28:42.39,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 03 05:05:19 UTC 2016,,,,,,,"0|i2wzpb:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 34,,,,,,,,,,,2.0,,,,,,,,,,,"01/May/16 05:28;lins05;https://reviews.apache.org/r/46873/","03/May/16 05:05;jieyu;commit 17260204c833c643adf3d8f36ad8a1a606ece809
Author: Shuai Lin <linshuai2012@gmail.com>
Date:   Mon May 2 21:56:47 2016 -0700

    Fixed MESOS_SANDBOX env in unified containerizer.
    
    Review: https://reviews.apache.org/r/46873/",,,,,,,,,,,,,,,,,,,,,,,,,,
Add capabilities support for mesos execute cli.,MESOS-5303,12963682,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,jojy,jojy,28/Apr/16 21:53,20/Oct/16 18:19,29/Oct/20 16:32,20/Oct/16 18:19,,,,,,,,,1.2.0,,,,,,containerization,,,,,0,mesosphere,,,,,,,,Add support for `user` and `capabilities` to execute cli. This will help in testing the `capabilities` feature for unified containerizer.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-10-11 16:12:58.972,,,false,MESOS-4936,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 20 18:19:45 UTC 2016,,,,,,,"0|hzzz7l:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 34,Mesosphere Sprint 35,Mesosphere Sprint 37,Mesosphere Sprint 38,Mesosphere Sprint 39,Mesosphere Sprint 40,Mesosphere Sprint 41,Mesosphere Sprint 42,Mesosphere Sprint 44,Mesosphere Sprint 45,,3.0,,,,,,,,,,,"28/Apr/16 21:54;jojy;https://reviews.apache.org/r/46799/","11/Oct/16 16:12;bbannier;Review: https://reviews.apache.org/r/52741/","20/Oct/16 18:19;jieyu;commit f6d5b883d1b61eb0fd97e60bfb363ac706ab30ed
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Tue Oct 18 10:12:21 2016 -0700

    Added capabilities support to mesos-execute.
    
    Review: https://reviews.apache.org/r/52741/",,,,,,,,,,,,,,,,,,,,,,,,,
Need to add REMOVE semantics to the copy backend,MESOS-5277,12962399,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,avinash.mesos,avinash.mesos,25/Apr/16 21:28,31/May/16 02:11,29/Oct/20 16:32,27/May/16 06:01,,,,,,,,,1.0.0,,,,,,containerization,,,,,0,mesosphere,,,,,,,,"Some Dockerfiles run the `rm` command to remove files from the base image using the ""RUN"" directive in the Dockerfile. An example can be found here:
https://github.com/ngineered/nginx-php-fpm.git

In the final rootfs the removed files should not be present. Presence of these files in the final image can make the container misbehave. For example, the nginx-php-fpm docker image that is referenced tries to remove the default nginx config and replaces it with its own config to point to a different HTML root. If the default nginx config is still present after the building the image, nginx will start pointing to a different HTML root than the one set in the Dockerfile.


Currently the copy backend cannot handle removal of files from intermediate layers. This can cause issues with docker images built using a Dockerfile similar to the one listed here. Hence, we need to add REMOVE semantics to the copy backend.  ",linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-20 20:50:55.122,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 27 06:01:09 UTC 2016,,,,,,,"0|i2wptr:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 35,,,,,,,,,,,5.0,,,,,,,,,,,"20/May/16 20:50;gilbert;https://reviews.apache.org/r/47265/
https://reviews.apache.org/r/47266/","27/May/16 06:01;jieyu;commit 2073bc26fa297925fab9fe249e31ac405fcd136a
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Thu May 26 23:00:25 2016 -0700

    Added test for provisioner to handle whiteout files.
    
    Review: https://reviews.apache.org/r/47935/

commit 477ec7893157368f2138a2f022024f0e28b2510e
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Thu May 26 23:00:21 2016 -0700

    Implemented provisioner removing docker whiteout files.
    
    Review: https://reviews.apache.org/r/47266/",,,,,,,,,,,,,,,,,,,,,,,,,,
add test cases for docker volume driver,MESOS-5266,12962116,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gyliu,gyliu,gyliu,25/Apr/16 00:37,31/May/16 02:12,29/Oct/20 16:32,09/May/16 17:42,,,,,,,,,1.0.0,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-04 19:10:06.481,,,false,MESOS-4355,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 06 17:22:10 UTC 2016,,,,,,,"0|i2wo2v:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 34,,,,,,,,,,,5.0,,,,,,,,,,,"04/May/16 19:10;jieyu;commit aedef94d101c3b1a916f53ef6804034e28f48326
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Wed May 4 10:38:12 2016 -0700

    Added docker volume test ""ROOT_CommandTaskNoRootfsWithVolumes"".
    
    Review: https://reviews.apache.org/r/46140/","05/May/16 18:36;jieyu;commit 521412e5557373be9a66eabd9cb7892d1dcc5ae6
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Thu May 5 11:23:53 2016 -0700

    Added docker volume test CommandTaskNoRootfsFailedWithSameVolumes.
    
    Review: https://reviews.apache.org/r/42028/","05/May/16 19:00;jieyu;commit abf52a9761ab2c15a43d4556ae54c9825af1c999
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Thu May 5 11:47:50 2016 -0700

    Added docker volume test CommandTaskNoRootfsSlaveRecovery.
    
    Review: https://reviews.apache.org/r/38451/

commit 14cde91fd0e1f66952779a88ce2fef010afebd48
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Thu May 5 11:43:49 2016 -0700

    Fixed the typos in docker volume tests.","05/May/16 19:08;jieyu;commit 9e4647723cd193403d01e296cef699e9d06b0593
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Thu May 5 12:03:33 2016 -0700

    Added a docker volume test.
    
    ""CommandTaskNoRootfsSingleVolumeMultipleContainers"".
    
    Review: https://reviews.apache.org/r/44440/","06/May/16 17:22;jieyu;commit 905391ea1c728287ee2b3b23d0fcae6bbafb3e37
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Fri May 6 09:59:09 2016 -0700

    Added a docker volume test.
    
    ""ROOT_INTERNET_CURL_CommandTaskRootfsWithVolumes"".
    
    Review: https://reviews.apache.org/r/39474/",,,,,,,,,,,,,,,,,,,,,,,
Update mesos-execute to support docker volume isolator.,MESOS-5265,12962115,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gyliu,gyliu,gyliu,25/Apr/16 00:37,18/Jun/16 03:14,29/Oct/20 16:32,17/Jun/16 22:07,,,,,,,,,1.0.0,,,,,,,,,,,0,,,,,,,,,The mesos-execute needs to be updated to support docker volume isolator.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-5216,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-17 22:07:19.652,,,false,MESOS-4355,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 17 22:07:19 UTC 2016,,,,,,,"0|i2wo2n:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 37,,,,,,,,,,,3.0,,,,,,,,,,,"25/Apr/16 03:06;gyliu;[~jieyu] , what about adding a new parameter in mesos-execute such as {{--volumes}} option which will specify a path of a JSON file for the volumes, any comments?

{code}
[{
  ""container_path"":""\/tmp\/abc1"",
  ""mode"":""RW"",
  ""source"":
    {
      ""docker_volume"":
        {
          ""driver"":""convoy"",
          ""name"":""dvd1""
        },
        ""type"":""DOCKER_VOLUME""
    }
},
{
  ""container_path"":""\/tmp\/abc2"",
  ""mode"":""RW"",
  ""source"":
    {
      ""docker_volume"":
        {
          ""driver"":""convoy"",
          ""driver_options"":
            {""parameter"":[
              {
                ""key"":""iops"",
                ""value"":""150""
              }
            ]},
            ""name"":""dvd2""
         },
         ""type"":""DOCKER_VOLUME""
    }
}]
{code}","28/Apr/16 13:37;gyliu;https://reviews.apache.org/r/48680/ Stout: Added parse() for JSON::Array.
https://reviews.apache.org/r/46762/ Enabled volume support for mesos-execute.","23/May/16 04:22;gyliu;The docker volume isolator documentation MESOS-5216 will depend on this as I want to use {{mesos-execute}} as an example to test out docker volume isolator.","17/Jun/16 22:07;jieyu;commit 81214c872a0d1dcad34003b4f71432a58c0e2875
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Fri Jun 17 12:00:50 2016 -0700

    Enabled volume support for mesos-execute.
    
    Review: https://reviews.apache.org/r/46762/

commit 5c7be4baf4f79e499f83f630de54155f0a2d3af2
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Fri Jun 17 12:00:47 2016 -0700

    Stout: Added parse() for JSON::Array.
    
    Review: https://reviews.apache.org/r/48680/",,,,,,,,,,,,,,,,,,,,,,,,
Isolator cleanup should not be invoked if they are not prepared yet.,MESOS-5253,12961770,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,22/Apr/16 17:58,04/Jun/16 18:03,29/Oct/20 16:32,22/Apr/16 22:26,0.28.0,0.28.1,,,,,,,0.28.2,1.0.0,,,,,containerization,,,,,0,containerizer,,,,,,,,"If the mesos containerizer destroys a container in PROVISIONING state, isolator cleanup is still called, which is incorrect because there is no isolator prepared yet. 

In this case, there no need to clean up any isolator, call provisioner destroy directly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-5238,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-22 22:26:32.86,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 22 22:26:32 UTC 2016,,,,,,,"0|i2wlzb:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 33,,,,,,,,,,,2.0,,,,,,,,,,,"22/Apr/16 18:21;gilbert;https://reviews.apache.org/r/46577/","22/Apr/16 22:26;jieyu;commit 653eca74f1080f5f55cd5092423506163e65d402
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Fri Apr 22 15:26:04 2016 -0700

    Fixed isolator cleaup issue when destroying a provisioning container.
    
    Review: https://reviews.apache.org/r/46577/",,,,,,,,,,,,,,,,,,,,,,,,,,
Persistent volume DockerContainerizer support assumes proper mount propagation setup on the host.,MESOS-5239,12960571,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,jieyu,jieyu,21/Apr/16 02:28,04/Jun/16 18:02,29/Oct/20 16:32,03/May/16 04:56,0.28.0,0.28.1,,,,,,,0.28.2,1.0.0,,,,,containerization,,,,,0,mesosphere,,,,,,,,"We recently added persistent volume support in DockerContainerizer (MESOS-3413). To understand the problem, we first need to understand how persistent volumes are supported in DockerContainerizer.

To support persistent volumes in DockerContainerizer, we bind mount persistent volumes under a container's sandbox ('container_path' has to be relative for persistent volumes). When the Docker container is launched, since we always add a volume (-v) for the sandbox, the persistent volumes will be bind mounted into the container as well (since Docker does a 'rbind').

The assumption that the above works is that the Docker daemon should see those persistent volume mounts that Mesos mounts on the host mount table. It's not a problem if Docker daemon itself is using the host mount namespace. However, on systemd enabled systems, Docker daemon is running in a separate mount namespace and all mounts in that mount namespace will be marked as slave mounts due to this [patch|https://github.com/docker/docker/commit/eb76cb2301fc883941bc4ca2d9ebc3a486ab8e0a].

So what that means is that: in order for it to work, the parent mount of agent's work_dir should be a shared mount when docker daemon starts. This is typically true on CentOS7, CoreOS as all mounts are shared mounts by default.

However, this causes an issue with the 'filesystem/linux' isolator. To understand why, first I need to show you a typical problem when dealing with shared mounts. Let me explain that using the following commands on a CentOS7 machine:
{noformat}
[root@core-dev run]# cat /proc/self/mountinfo
24 60 0:19 / /run rw,nosuid,nodev shared:22 - tmpfs tmpfs rw,seclabel,mode=755
[root@core-dev run]# mkdir /run/netns
[root@core-dev run]# mount --bind /run/netns /run/netns
[root@core-dev run]# cat /proc/self/mountinfo
24 60 0:19 / /run rw,nosuid,nodev shared:22 - tmpfs tmpfs rw,seclabel,mode=755
121 24 0:19 /netns /run/netns rw,nosuid,nodev shared:22 - tmpfs tmpfs rw,seclabel,mode=755
[root@core-dev run]# ip netns add test
[root@core-dev run]# cat /proc/self/mountinfo
24 60 0:19 / /run rw,nosuid,nodev shared:22 - tmpfs tmpfs rw,seclabel,mode=755
121 24 0:19 /netns /run/netns rw,nosuid,nodev shared:22 - tmpfs tmpfs rw,seclabel,mode=755
162 121 0:3 / /run/netns/test rw,nosuid,nodev,noexec,relatime shared:5 - proc proc rw
163 24 0:3 / /run/netns/test rw,nosuid,nodev,noexec,relatime shared:5 - proc proc rw
{noformat}

As you can see above, there're two entries (/run/netns/test) in the mount table (unexpected). This will confuse some systems sometimes. The reason is because when we create a self bind mount (/run/netns -> /run/netns), the mount will be put into the same shared mount peer group (shared:22) as its parent (/run). Then, when you create another mount underneath that (/run/netns/test), that mount operation will be propagated to all mounts in the same peer group (shared:22), resulting an unexpected additional mount being created.

The reason we need to do a self bind mount in Mesos is that sometimes, we need to make sure some mounts are shared so that it does not get copied when a new mount namespace is created. However, on some systems, mounts are private by default (e.g., Ubuntu 14.04). In those cases, since we cannot change the system mounts, we have to do a self bind mount so that we can set mount propagation to shared. For instance, in filesytem/linux isolator, we do a self bind mount on agent's work_dir.

To avoid the self bind mount pitfall mentioned above, in filesystem/linux isolator, after we created the mount, we do a make-slave + make-shared so that the mount is its own shared mount peer group. In that way, any mounts underneath it will not be propagated back.

However, that operation will break the assumption that the persistent volume DockerContainerizer support makes. As a result, we're seeing problem with persistent volumes in DockerContainerizer when filesystem/linux isolator is turned on.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2386,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 03 04:56:15 UTC 2016,,,,,,,"0|i2wf3j:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 34,,,,,,,,,,,3.0,,,,,,,,,,,"29/Apr/16 22:21;jieyu;The following patch allows the filesystem/linux isolator to skip the bind mount for the agent's work_dir if possible:
https://reviews.apache.org/r/46858/

The above patch will solve this problem on Centos7, Ubuntu 16.04, CoreOS where default mounts are 'shared'.","03/May/16 04:56;jieyu;commit 6b28aab052333e47bc53f66e13e4d78cc1bab3dd
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Apr 29 15:12:47 2016 -0700

    Skipped the bind mounting of the agent's work_dir if possible.
    
    In filesystem/linux isolator, we will need to make sure the mount that
    contains the agent's working directory is a shared mount in its own
    peer group so that cloning the host mount table (e.g., when launching
    a container with a new mount namespace) does not create an extra
    reference to the mounts under the agent's work_dir.
    
    However, if the mount containing the agent's working directory is
    already a shared mount in its own peer group, we don't have to create
    another bind mount for the agent's working directory. This patch
    allows the filesystem/linux isolator to skip the bind mount if
    possible.
    
    Review: https://reviews.apache.org/r/46858",,,,,,,,,,,,,,,,,,,,,,,,,,
CHECK failure in AppcProvisionerIntegrationTest.ROOT_SimpleLinuxImageTest,MESOS-5238,12960548,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Duplicate,gilbert,neilc,neilc,21/Apr/16 00:26,07/Jul/16 22:29,29/Oct/20 16:32,26/Apr/16 01:03,,,,,,,,,,,,,,,,,,,,0,containerizer,flaky,mesosphere,,,,,,"Observed on the Mesosphere internal CI:

{noformat}
[22:56:28]W:     [Step 10/10] F0420 22:56:28.056788   629 containerizer.cpp:1634] Check failed: containers_.contains(containerId)
{noformat}

Complete test log will be attached as a file.","CentOS 7 + SSL, x86-64",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/16 00:27;neilc;5238_check_failure.txt;https://issues.apache.org/jira/secure/attachment/12799871/5238_check_failure.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,2016-04-21 00:32:52.107,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 26 01:03:57 UTC 2016,,,,,,,"0|i2weyf:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 33,,,,,,,,,,,2.0,,,,,,,,,,,"21/Apr/16 00:32;jieyu;[~jojy] Can you take a look?","21/Apr/16 01:03;jieyu;NVM, i think this problem is related to [~gilbert]'s recent change. He'll comment on this ticket for findings.","22/Apr/16 17:52;gilbert;This bug is because of a race in mesos containerizer. From the agent log, there are two containerizer destroy invoked, which should not be allow. It happened because the first time we call the containerizer::destroy, the container state is changed from PROVISIONING to DESTROYING, which is fine. But in destroy, the containerizer has to wait for all provisioner to finish. If the await() is waiting the the second provision(), once the provision() finishes, it invokes prepare, which change the container state back to PREPARING. That is incorrect.

So the race comes from we do not check whether the container is being destroyed when container is being prepared by isolators.","22/Apr/16 18:21;gilbert;https://reviews.apache.org/r/46576/","22/Apr/16 22:27;jieyu;commit 3a5dcfe8b13aeffd4f18760aac3df824414685db
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Fri Apr 22 15:26:00 2016 -0700

    Fixed a mesos containerizer race destroy while preparing.
    
    Review: https://reviews.apache.org/r/46576/","26/Apr/16 01:03;gilbert;The test is fine. This failure results from a race in mesos containerizer, which is tracked/fixed by MESOS-5282.",,,,,,,,,,,,,,,,,,,,,,
Implement HTTP Docker Executor that uses the Executor Library,MESOS-5227,12959757,Bug,Reviewable,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,yongtang,vinodkone,vinodkone,18/Apr/16 19:03,07/Feb/17 18:40,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,1,,,,,,,,,Similar to what we did with the HTTP command executor in MESOS-3558 we should have a HTTP docker executor that can speak the v1 Executor API.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-22 14:04:35.258,,,false,MESOS-4855,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 09 03:55:57 UTC 2017,,,,,,,"0|hzzz1x:",9223372036854775807,,,,,vinodkone,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,"22/Apr/16 14:04;alexr;Hey [~yongtang], are you working on this issue?","22/Apr/16 14:41;yongtang;Hi [~alexr] I just started a couple of days ago. Still looking into how MESOS-3558 handles HTTP executor at the moment.","22/Apr/16 15:03;alexr;[~qianzhang], do you want to help review this effort?","22/Apr/16 16:00;yongtang;Thanks [~alexr]! That will be much appreciated. I will add to the reviewer list when I submit the review request. Thanks.","22/Apr/16 23:09;qianzhang;[~alexr], yeah, I'd like to :-)","25/Apr/16 14:34;alexr;https://reviews.apache.org/r/46615/
https://reviews.apache.org/r/46616/
https://reviews.apache.org/r/46617/","27/Jun/16 05:01;guoger;It would be great if you could put a few words on each review link to summarize.","28/Jun/16 03:30;yongtang;[~guoger] The review request has been updated. Now there is only one RR related to this issue:
https://reviews.apache.org/r/49240/","23/Aug/16 23:33;yongtang;The review request has been updated again. Now the new RR are located:
https://reviews.apache.org/r/51351/
https://reviews.apache.org/r/51352/

Please discard the old ones.","22/Sep/16 10:52;alexr;[~anandmazumdar], are these patches still relevant or we have chose a different approach?","06/Jan/17 19:21;anandmazumdar;[~yongtang] Any updates on this? IIRC, as per our offline discussion last time, the patches were missing unit-tests and you were working on them?","09/Jan/17 03:55;yongtang;[~anandmazumdar] I haven't had much time to work on this issue recently. Will try to see if I could spend some time after I am back from traveling. In the meantime,  don't wait for me if any one is interested in taking over.",,,,,,,,,,,,,,,,
Document docker volume driver isolator.,MESOS-5216,12958650,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gyliu,gilbert,gilbert,13/Apr/16 23:00,17/Jul/17 12:02,29/Oct/20 16:32,15/Jun/16 00:30,,,,,,,,,1.0.0,,,,,,containerization,,,,,0,documentation,mesosphere,,,,,,,"Should include the followings:

1. What features (driver options) are supported in docker volume driver isolator.
2. How to use docker volume driver isolator.
    *related agent flags introduction and usage.
    *isolator dependency clarification (e.g., filesystem/linux).
    *related driver daemon preprocess.
    *volumes pre-specified by users and volume cleanup.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-18 09:44:12.038,,,false,MESOS-4355,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 15 00:30:13 UTC 2016,,,,,,,"0|hzzze6:z",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 35,Mesosphere Sprint 36,Mesosphere Sprint 37,,,,,,,,,5.0,,,,,,,,,,,"18/May/16 09:44;gyliu;https://reviews.apache.org/r/47511/","15/Jun/16 00:30;jieyu;commit 9bd3d5b32b61146d31fb5c7e31ae0049b0b32a35
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Tue Jun 14 17:28:45 2016 -0700

    Added documentation for `docker/volume` isolator.
    
    Review: https://reviews.apache.org/r/47511",,,,,,,,,,,,,,,,,,,,,,,,,,
The filesystem/linux isolator does not set the permissions of the host_path.,MESOS-5187,12958046,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,StephanErb,StephanErb,12/Apr/16 09:10,28/Jul/17 20:06,29/Oct/20 16:32,28/Jul/17 20:06,0.26.0,,,,,,,,1.1.3,1.2.2,1.3.1,1.4.0,,,containerization,,,,,1,mesosphere,volumes,,,,,,,"The {{filesystem/linux}} isolator is not a drop in replacement for the {{filesystem/shared}} isolator. This should be considered before the latter is deprecated.

We are currently using the {{filesystem/shared}} isolator together with the following slave option. This provides us with a private {{/tmp}} and {{/var/tmp}} folder for each task.

{code}
    --default_container_info='{
            ""type"": ""MESOS"",
            ""volumes"": [
                {""host_path"": ""system/tmp"",     ""container_path"": ""/tmp"",        ""mode"": ""RW""},
                {""host_path"": ""system/vartmp"",  ""container_path"": ""/var/tmp"",    ""mode"": ""RW""}
            ]
        }'
{code}

When browsing the Mesos sandbox, one can see the following permissions:
{code}
mode	nlink	uid	gid	size	mtime		
drwxrwxrwx	3	root	root	4 KB	Apr 11 18:16	 tmp	
drwxrwxrwx	2	root	root	4 KB	Apr 11 18:15	 vartmp	
{code}

However, when running with the new {{filesystem/linux}} isolator, the permissions are different:
{code}
mode	nlink	uid	gid	size	mtime		
drwxr-xr-x	 2	root	root	4 KB	Apr 12 10:34	 tmp	
drwxr-xr-x	 2	root	root	4 KB	Apr 12 10:34	 vartmp
{code}

This prevents user code (running as a non-root user) from writing to those folders, i.e. every write attempt fails with permission denied. 

*Context*:
* We are using Apache Aurora. Aurora is running its custom executor as root but then switches to a non-privileged user before running the actual user code. 
* The follow code seems to have enabled our usecase in the existing {{filesystem/shared}} isolator: https://github.com/apache/mesos/blob/4d2b1b793e07a9c90b984ca330a3d7bc9e1404cc/src/slave/containerizer/mesos/isolators/filesystem/shared.cpp#L175-L198 ","Mesos 0.26.0, Apache Aurora 0.12",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-12 18:34:37.565,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 28 20:06:38 UTC 2017,,,,,,,"0|i2vziv:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 60,,,,,,,,,,,3.0,,,,,,,,,,,"12/Apr/16 09:13;StephanErb;/cc [~jieyu] ","12/Apr/16 18:34;idownes;The highlighted code was intended for this quite specific use-case: masking a system directory and inheriting its mode. I agree that the filesystem/linux isolator should support this use-case but suggest that it be made explicit, perhaps by extending the Volume message to include setting the directory mode (different to the existing Volume::Mode) when creating container relative paths. [~jieyu] thoughts?","14/Apr/16 16:50;jieyu;[~idownes] I think there are two problems here:

1) if 'host_path' is relative for a volume, we should change the ownership of that host path in the sandbox to match that of the container sandbox, instead of using the agent uid/gids. This to me is a bug and we should fix that.

2) The mode issue as you mentioned above. If we fix (1), the executor should be able to call chmod itself? Will that be sufficient?","25/Jul/17 23:08;gilbert;https://reviews.apache.org/r/61122/
https://reviews.apache.org/r/61123/","28/Jul/17 20:06;gilbert;commit 8edfbaaf49c0b09ed02bd07334fb65b29d088a40
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Fri Jul 28 12:27:58 2017 -0700

    Added regression test for sandbox volume ownership issue.
    
    Added regression test for sandbox volume ownership issue.
    
    Review: https://reviews.apache.org/r/61123/

commit b5efb9121e523edf344534ba6c5332d6f7190643
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Fri Jul 28 12:27:55 2017 -0700

    Fixed the sandbox volume relative host path ownership.
    
    This bugfix addresses the issue from MESOS-5178. Basically, the
    sandbox volume ownership was not set correctly. This issue can be
    exposed if a framework user is non-root while the agent
    process runs as root. Then, the non-root user does not have
    permissions to write to this volume.
    
    The correct solution should be giving permissions to corresponding
    users by leveraging supplementary groups. But we can still
    introduce a workaround in this patch by changing the ownership
    of this sandbox volume to its sandbox's ownership.
    
    Review: https://reviews.apache.org/r/61122/",,,,,,,,,,,,,,,,,,,,,,,
Registry puller cannot fetch blobs correctly from http Redirect 3xx urls.,MESOS-5172,12957758,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,gilbert,gilbert,gilbert,11/Apr/16 16:23,26/Apr/17 03:24,29/Oct/20 16:32,13/Apr/17 18:06,,,,,,,,,1.1.2,1.2.1,1.3.0,,,,containerization,,,,,0,containerizer,mesosphere,,,,,,,"When the registry puller is pulling a private repository from some private registry (e.g., quay.io), errors may occur when fetching blobs, at which point fetching the manifest of the repo is finished correctly. The error message is `Unexpected HTTP response '400 Bad Request' when trying to download the blob`. This may arise from the logic of fetching blobs, or incorrect format of uri when requesting blobs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-04-11 09:33:22.311,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 18 01:17:07 UTC 2017,,,,,,,"0|hzzzd8:zzzi",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 33,Mesosphere Sprint 37,Mesosphere Sprint 54,,,,,,,,,3.0,,1.1.2,1.2.1,1.3.0,,,,,,,"20/Jun/16 07:07;gilbert;https://reviews.apache.org/r/48917/
https://reviews.apache.org/r/58329/","11/Apr/17 09:33;ipronin;This issue looks related: https://issues.apache.org/jira/browse/MESOS-6561","13/Apr/17 18:06;jieyu;commit f73f29bd3f1399847309fa801f6c3c0a022d57fc
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Thu Apr 13 09:26:08 2017 -0700

    Removed a duplicate unit test 'ROOT_INTERNET_CURL_Normalize'.

    Review: https://reviews.apache.org/r/58410/

commit d1015ffe35dfcd353d7dbbd187244586feddd6a7
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Thu Apr 13 09:26:00 2017 -0700

    Parameterized the existing alpine based test with more registries.

    Now we test alpine image from three registries:
    1. docker hub.
    2. quay.io.
    3. alicloud.

    Review: https://reviews.apache.org/r/58329/

commit a2eaacb0cc2948dc08c498cb09b052ea7b0b3a6e
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Thu Apr 13 09:25:56 2017 -0700

    Fixed docker fetcher 3xx redirect errors by header attached.

    The root cause for this issue is that, in private registry
    like quay.io, layer download request will be redirected to
    storage server in S3. However, the curl command with '-L'
    handles HTTP redirection automatically, in which case HTTP
    headers will be attached to all requests. AmazonS3 server
    will return 400 Bad Request if HTTP Authorization header
    is attached, with 'InvalidArgument' error code. So we need
    to touch the given URL first to add extra logic for HTTP
    redirections.

    Please note that the download() method is changed to be
    recursive since no header should be attached once the
    request get authenticated.

    Review: https://reviews.apache.org/r/48917/","18/Apr/17 01:17;adam-mesos;[~jieyu] Could you please backport these patches to 1.2.x and 1.1.x if we're still targeting them for those patch releases?",,,,,,,,,,,,,,,,,,,,,,,,
Run mesos builds on PowerPC platform in ASF CI,MESOS-5156,12957299,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,,vinodkone,vinodkone,08/Apr/16 18:44,14/Jul/16 02:07,29/Oct/20 16:32,14/Jul/16 02:07,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,"This is the last step to declare official support for PowerPC.

This is currently blocked on ASF INFRA adding PowerPC based Jenkins machines to the ASF CI.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-4312,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 14 02:07:12 UTC 2016,,,,,,,"0|i2vux3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,"14/Jul/16 02:07;vinodkone;The CI job is live now: https://builds.apache.org/view/M-R/view/Mesos/job/Mesos-PPC64LE/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Sandboxes contents should be protected from unauthorized users,MESOS-5153,12957172,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,arojas,arojas,arojas,08/Apr/16 10:35,29/Apr/19 09:26,29/Oct/20 16:32,01/Jun/16 06:43,,,,,,,,,1.0.0,,,,,,agent,security,,,,0,mesosphere,security,,,,,,,"MESOS-4956 introduced authentication support for the sandboxes. However, authentication can only go as far as to tell whether an user is known to mesos or not. An extra additional step is necessary to verify whether the known user is allowed to executed the requested operation on the sandbox (browse, read, download, debug).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-5459,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-27 09:41:24.42,,,false,MESOS-5150,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 07 06:51:56 UTC 2016,,,,,,,"0|hzzzge:y",9223372036854775807,,,,,adam-mesos,,,,,,Mesosphere Sprint 33,Mesosphere Sprint 34,Mesosphere Sprint 35,Mesosphere Sprint 36,,,,,,,,8.0,,,,,,,,,,,"24/May/16 22:10;arojas;[r/47794/|https://reviews.apache.org/r/47794/]: Added authorization support for {{mesos::internal::Files}}.
[r/47795/|https://reviews.apache.org/r/47795/]: Enabled authorization for sandboxes.
","27/May/16 09:41;adam-mesos;commit bcdc1d151a0423593ea39411519165a1b6e900ff
Author: Alexander Rojas <alexander@mesosphere.io>
Date:   Fri May 27 01:00:09 2016 -0700

    Enabled authorization for sandboxes.
    
    Enables authorization of the sandboxes using the callback function
    parameter of `Files::attach()`.
    
    It also adds relevant ACLs and support on the authorizer interface.
    
    Review: https://reviews.apache.org/r/47795/

commit 62150e441540c93e3f7dcbaed98679bf81c14c94
Author: Alexander Rojas <alexander@mesosphere.io>
Date:   Fri May 27 00:49:20 2016 -0700

    Added authorization support for mesos::internal::Files.
    
    Adds an optional parameter to the `mesos::internal::Files::attach()`
    method. The type of this parameter is a callable object which returns
    a future to a boolean and takes as parameter an optional string
    representing a principal name.
    
    The parameter is called, if set, whenever one of the routed endpoints
    of the `Files` object is accessed through HTTP. If the callable object
    returns a false boolean, then processing of the request is aborted
    and a `403 Forbidden` response is returned.
    
    Review: https://reviews.apache.org/r/47794/
","27/May/16 09:42;adam-mesos;Still reviewing: ACCESS_MESOS_LOGS
  https://reviews.apache.org/r/47921/

In addition, we'll need to update the files endpoint help (and autogenerated endpoint docs), and perhaps authorization.md.","30/May/16 08:05;adam-mesos;commit 8443b18489c48996df0a88f90baf260a0fb12176
Author: Alexander Rojas <alexander@mesosphere.io>
Date:   Sun May 29 11:16:03 2016 -0700

    Enabled authorization for Mesos log access.
    
    Uses the authorization primitives in `mesos::internal::Files` to add
    protection of the Mesos logs on both master and agents.
    
    Review: https://reviews.apache.org/r/47921/

All that's left now is the DOCS!","01/Jun/16 06:42;adam-mesos;commit 53b5164bb51ebe850dec5ab19b8382f5c4a59391
Author: Alexander Rojas <alexander@mesosphere.io>
Date:   Tue May 31 23:20:50 2016 -0700

    Added documentation for access_sandboxes and access_mesos_logs acls.
    
    Modifies the file `acls.proto` to take into consideration the added
    authorization actions `access_sandboxes` and `access_mesos_logs`.
    
    Review: https://reviews.apache.org/r/48048/","07/Jun/16 05:36;adam-mesos;commit 66d2a04d38ad7d4ee9ba4478cffccab73290fd90
Author: Alexander Rojas <alexander@mesosphere.io>
Date:   Mon Jun 6 22:32:14 2016 -0700

    Removed trailing space from Files documentation.
    
    A documentation line included a trailing space which caused invalid
    generated markdown docs created through `generate-endpoints-help.py`
    
    Review: https://reviews.apache.org/r/48310/","07/Jun/16 06:51;adam-mesos;commit f29e9e338c1c1429c668f42a4252ca0d87533ad3
Author: Alexander Rojas <alexander@mesosphere.io>
Date:   Mon Jun 6 23:50:47 2016 -0700

    Updated endpoint documentation.
    
    Rerun `generate-endpoint-help.py` after updating the documentation on
    all `master::internal::Files` endpoints.
    
    Review: https://reviews.apache.org/r/48311/",,,,,,,,,,,,,,,,,,,,,
MasterAllocatorTest/1.RebalancedForUpdatedWeights is flaky.,MESOS-5146,12957066,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gradywang,greggomann,greggomann,07/Apr/16 23:54,26/Apr/17 17:01,29/Oct/20 16:32,21/Apr/16 12:29,0.28.0,,,,,,,,1.0.0,,,,,,allocation,test,,,,0,mesosphere,,,,,,,,"Observed on the ASF CI:

{code}
[ RUN      ] MasterAllocatorTest/1.RebalancedForUpdatedWeights
I0407 22:34:10.330394 29278 cluster.cpp:149] Creating default 'local' authorizer
I0407 22:34:10.466182 29278 leveldb.cpp:174] Opened db in 135.608207ms
I0407 22:34:10.516398 29278 leveldb.cpp:181] Compacted db in 50.159558ms
I0407 22:34:10.516464 29278 leveldb.cpp:196] Created db iterator in 34959ns
I0407 22:34:10.516484 29278 leveldb.cpp:202] Seeked to beginning of db in 10195ns
I0407 22:34:10.516496 29278 leveldb.cpp:271] Iterated through 0 keys in the db in 7324ns
I0407 22:34:10.516547 29278 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0407 22:34:10.517277 29298 recover.cpp:447] Starting replica recovery
I0407 22:34:10.517693 29300 recover.cpp:473] Replica is in EMPTY status
I0407 22:34:10.520251 29310 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4775)@172.17.0.3:35855
I0407 22:34:10.520611 29311 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0407 22:34:10.521164 29299 recover.cpp:564] Updating replica status to STARTING
I0407 22:34:10.523435 29298 master.cpp:382] Master f59f9057-a5c7-43e1-b129-96862e640a12 (129e11060069) started on 172.17.0.3:35855
I0407 22:34:10.523473 29298 master.cpp:384] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/3rZY8C/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/3rZY8C/master"" --zk_session_timeout=""10secs""
I0407 22:34:10.523885 29298 master.cpp:433] Master only allowing authenticated frameworks to register
I0407 22:34:10.523901 29298 master.cpp:438] Master only allowing authenticated agents to register
I0407 22:34:10.523913 29298 credentials.hpp:37] Loading credentials for authentication from '/tmp/3rZY8C/credentials'
I0407 22:34:10.524298 29298 master.cpp:480] Using default 'crammd5' authenticator
I0407 22:34:10.524441 29298 master.cpp:551] Using default 'basic' HTTP authenticator
I0407 22:34:10.524564 29298 master.cpp:589] Authorization enabled
I0407 22:34:10.525269 29305 hierarchical.cpp:145] Initialized hierarchical allocator process
I0407 22:34:10.525333 29305 whitelist_watcher.cpp:77] No whitelist given
I0407 22:34:10.527331 29298 master.cpp:1832] The newly elected leader is master@172.17.0.3:35855 with id f59f9057-a5c7-43e1-b129-96862e640a12
I0407 22:34:10.527441 29298 master.cpp:1845] Elected as the leading master!
I0407 22:34:10.527545 29298 master.cpp:1532] Recovering from registrar
I0407 22:34:10.527889 29298 registrar.cpp:331] Recovering registrar
I0407 22:34:10.549734 29299 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 28.25177ms
I0407 22:34:10.549782 29299 replica.cpp:320] Persisted replica status to STARTING
I0407 22:34:10.550010 29299 recover.cpp:473] Replica is in STARTING status
I0407 22:34:10.551352 29299 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (4777)@172.17.0.3:35855
I0407 22:34:10.551676 29299 recover.cpp:193] Received a recover response from a replica in STARTING status
I0407 22:34:10.552315 29308 recover.cpp:564] Updating replica status to VOTING
I0407 22:34:10.574865 29308 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 22.413614ms
I0407 22:34:10.574928 29308 replica.cpp:320] Persisted replica status to VOTING
I0407 22:34:10.575103 29308 recover.cpp:578] Successfully joined the Paxos group
I0407 22:34:10.575346 29308 recover.cpp:462] Recover process terminated
I0407 22:34:10.575913 29308 log.cpp:659] Attempting to start the writer
I0407 22:34:10.577512 29308 replica.cpp:493] Replica received implicit promise request from (4778)@172.17.0.3:35855 with proposal 1
I0407 22:34:10.599984 29308 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 22.453613ms
I0407 22:34:10.600026 29308 replica.cpp:342] Persisted promised to 1
I0407 22:34:10.601773 29304 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0407 22:34:10.603757 29307 replica.cpp:388] Replica received explicit promise request from (4779)@172.17.0.3:35855 for position 0 with proposal 2
I0407 22:34:10.634392 29307 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 30.269987ms
I0407 22:34:10.634829 29307 replica.cpp:712] Persisted action at 0
I0407 22:34:10.637017 29297 replica.cpp:537] Replica received write request for position 0 from (4780)@172.17.0.3:35855
I0407 22:34:10.637099 29297 leveldb.cpp:436] Reading position from leveldb took 52948ns
I0407 22:34:10.676170 29297 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 38.917487ms
I0407 22:34:10.676352 29297 replica.cpp:712] Persisted action at 0
I0407 22:34:10.677564 29306 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0407 22:34:10.717959 29306 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 40.306229ms
I0407 22:34:10.718202 29306 replica.cpp:712] Persisted action at 0
I0407 22:34:10.718399 29306 replica.cpp:697] Replica learned NOP action at position 0
I0407 22:34:10.719883 29306 log.cpp:675] Writer started with ending position 0
I0407 22:34:10.721688 29305 leveldb.cpp:436] Reading position from leveldb took 75934ns
I0407 22:34:10.723640 29306 registrar.cpp:364] Successfully fetched the registry (0B) in 195648us
I0407 22:34:10.723999 29306 registrar.cpp:463] Applied 1 operations in 108099ns; attempting to update the 'registry'
I0407 22:34:10.725077 29311 log.cpp:683] Attempting to append 170 bytes to the log
I0407 22:34:10.725328 29308 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0407 22:34:10.726552 29299 replica.cpp:537] Replica received write request for position 1 from (4781)@172.17.0.3:35855
I0407 22:34:10.759747 29299 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 33.089719ms
I0407 22:34:10.759976 29299 replica.cpp:712] Persisted action at 1
I0407 22:34:10.761739 29299 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0407 22:34:10.801522 29299 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 39.694064ms
I0407 22:34:10.801602 29299 replica.cpp:712] Persisted action at 1
I0407 22:34:10.801638 29299 replica.cpp:697] Replica learned APPEND action at position 1
I0407 22:34:10.803371 29311 registrar.cpp:508] Successfully updated the 'registry' in 79.163904ms
I0407 22:34:10.803829 29311 registrar.cpp:394] Successfully recovered registrar
I0407 22:34:10.804585 29311 master.cpp:1640] Recovered 0 agents from the Registry (131B) ; allowing 10mins for agents to re-register
I0407 22:34:10.805269 29308 log.cpp:702] Attempting to truncate the log to 1
I0407 22:34:10.805721 29310 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0407 22:34:10.805276 29296 hierarchical.cpp:172] Skipping recovery of hierarchical allocator: nothing to recover
I0407 22:34:10.806529 29307 replica.cpp:537] Replica received write request for position 2 from (4782)@172.17.0.3:35855
I0407 22:34:10.843320 29307 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 36.77593ms
I0407 22:34:10.843531 29307 replica.cpp:712] Persisted action at 2
I0407 22:34:10.845369 29311 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0407 22:34:10.885098 29311 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 39.641102ms
I0407 22:34:10.885401 29311 leveldb.cpp:399] Deleting ~1 keys from leveldb took 88701ns
I0407 22:34:10.885745 29311 replica.cpp:712] Persisted action at 2
I0407 22:34:10.885862 29311 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0407 22:34:10.900660 29278 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0407 22:34:10.901793 29278 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0407 22:34:10.905488 29302 slave.cpp:201] Agent started on 111)@172.17.0.3:35855
I0407 22:34:10.905553 29302 slave.cpp:202] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:4096;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa""
I0407 22:34:10.906365 29302 credentials.hpp:86] Loading credential for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/credential'
I0407 22:34:10.906787 29302 slave.cpp:339] Agent using credential for: test-principal
I0407 22:34:10.907202 29302 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/http_credentials'
I0407 22:34:10.907713 29302 slave.cpp:391] Using default 'basic' HTTP authenticator
I0407 22:34:10.908499 29302 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:10.910189 29302 slave.cpp:590] Agent resources: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:10.910362 29302 slave.cpp:598] Agent attributes: [  ]
I0407 22:34:10.910465 29302 slave.cpp:603] Agent hostname: 129e11060069
I0407 22:34:10.913280 29303 state.cpp:57] Recovering state from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/meta'
I0407 22:34:10.914621 29303 status_update_manager.cpp:200] Recovering status update manager
I0407 22:34:10.915226 29303 containerizer.cpp:416] Recovering containerizer
I0407 22:34:10.917246 29301 provisioner.cpp:245] Provisioner recovery complete
I0407 22:34:10.917733 29301 slave.cpp:4784] Finished recovery
I0407 22:34:10.918226 29301 slave.cpp:4956] Querying resource estimator for oversubscribable resources
I0407 22:34:10.918529 29301 slave.cpp:4970] Received oversubscribable resources  from the resource estimator
I0407 22:34:10.918908 29304 slave.cpp:939] New master detected at master@172.17.0.3:35855
I0407 22:34:10.918988 29304 slave.cpp:1002] Authenticating with master master@172.17.0.3:35855
I0407 22:34:10.919098 29301 status_update_manager.cpp:174] Pausing sending status updates
I0407 22:34:10.919309 29304 slave.cpp:1007] Using default CRAM-MD5 authenticatee
I0407 22:34:10.919535 29304 slave.cpp:975] Detecting new master
I0407 22:34:10.919747 29308 authenticatee.cpp:121] Creating new client SASL connection
I0407 22:34:10.920413 29308 master.cpp:5695] Authenticating slave(111)@172.17.0.3:35855
I0407 22:34:10.920650 29308 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(278)@172.17.0.3:35855
I0407 22:34:10.921020 29308 authenticator.cpp:98] Creating new server SASL connection
I0407 22:34:10.921308 29308 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0407 22:34:10.921424 29308 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0407 22:34:10.921596 29308 authenticator.cpp:203] Received SASL authentication start
I0407 22:34:10.921752 29308 authenticator.cpp:325] Authentication requires more steps
I0407 22:34:10.921957 29307 authenticatee.cpp:258] Received SASL authentication step
I0407 22:34:10.922178 29308 authenticator.cpp:231] Received SASL authentication step
I0407 22:34:10.922214 29308 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0407 22:34:10.922229 29308 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0407 22:34:10.922281 29308 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0407 22:34:10.922309 29308 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0407 22:34:10.922322 29308 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:10.922332 29308 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:10.922353 29308 authenticator.cpp:317] Authentication success
I0407 22:34:10.922436 29307 authenticatee.cpp:298] Authentication success
I0407 22:34:10.922587 29308 master.cpp:5725] Successfully authenticated principal 'test-principal' at slave(111)@172.17.0.3:35855
I0407 22:34:10.922668 29299 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(278)@172.17.0.3:35855
I0407 22:34:10.923256 29307 slave.cpp:1072] Successfully authenticated with master master@172.17.0.3:35855
I0407 22:34:10.923429 29307 slave.cpp:1468] Will retry registration in 3.220345ms if necessary
I0407 22:34:10.923707 29302 master.cpp:4406] Registering agent at slave(111)@172.17.0.3:35855 (129e11060069) with id f59f9057-a5c7-43e1-b129-96862e640a12-S0
I0407 22:34:10.924239 29309 registrar.cpp:463] Applied 1 operations in 105794ns; attempting to update the 'registry'
I0407 22:34:10.925787 29309 log.cpp:683] Attempting to append 339 bytes to the log
I0407 22:34:10.926028 29309 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0407 22:34:10.927139 29309 replica.cpp:537] Replica received write request for position 3 from (4797)@172.17.0.3:35855
I0407 22:34:10.929083 29305 slave.cpp:1468] Will retry registration in 39.293556ms if necessary
I0407 22:34:10.929363 29305 master.cpp:4394] Ignoring register agent message from slave(111)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:10.968843 29309 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 41.68025ms
I0407 22:34:10.969005 29309 replica.cpp:712] Persisted action at 3
I0407 22:34:10.969741 29309 slave.cpp:1468] Will retry registration in 54.852242ms if necessary
I0407 22:34:10.970118 29309 master.cpp:4394] Ignoring register agent message from slave(111)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:10.970852 29306 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0407 22:34:11.010634 29306 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 39.680272ms
I0407 22:34:11.010840 29306 replica.cpp:712] Persisted action at 3
I0407 22:34:11.011014 29306 replica.cpp:697] Replica learned APPEND action at position 3
I0407 22:34:11.014020 29306 registrar.cpp:508] Successfully updated the 'registry' in 89.684224ms
I0407 22:34:11.014181 29296 log.cpp:702] Attempting to truncate the log to 3
I0407 22:34:11.014606 29296 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0407 22:34:11.015836 29298 replica.cpp:537] Replica received write request for position 4 from (4798)@172.17.0.3:35855
I0407 22:34:11.016973 29296 master.cpp:4474] Registered agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.017518 29304 hierarchical.cpp:476] Added agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (allocated: )
I0407 22:34:11.017763 29311 slave.cpp:1116] Registered with master master@172.17.0.3:35855; given agent ID f59f9057-a5c7-43e1-b129-96862e640a12-S0
I0407 22:34:11.018362 29311 fetcher.cpp:81] Clearing fetcher cache
I0407 22:34:11.018870 29311 slave.cpp:1139] Checkpointing SlaveInfo to '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/meta/slaves/f59f9057-a5c7-43e1-b129-96862e640a12-S0/slave.info'
I0407 22:34:11.018890 29307 status_update_manager.cpp:181] Resuming sending status updates
I0407 22:34:11.019182 29304 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.019304 29304 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 in 1.077349ms
I0407 22:34:11.019493 29311 slave.cpp:1176] Forwarding total oversubscribed resources 
I0407 22:34:11.019726 29311 slave.cpp:3675] Received ping from slave-observer(112)@172.17.0.3:35855
I0407 22:34:11.019878 29299 master.cpp:4818] Received update of agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069) with total oversubscribed resources 
I0407 22:34:11.020845 29305 hierarchical.cpp:534] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 (129e11060069) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: )
I0407 22:34:11.021005 29305 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.021065 29305 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 in 173907ns
I0407 22:34:11.022289 29278 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0407 22:34:11.023422 29278 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0407 22:34:11.026309 29309 slave.cpp:201] Agent started on 112)@172.17.0.3:35855
I0407 22:34:11.026410 29309 slave.cpp:202] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:4096;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O""
I0407 22:34:11.027070 29309 credentials.hpp:86] Loading credential for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/credential'
I0407 22:34:11.027308 29309 slave.cpp:339] Agent using credential for: test-principal
I0407 22:34:11.027354 29309 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/http_credentials'
I0407 22:34:11.027698 29309 slave.cpp:391] Using default 'basic' HTTP authenticator
I0407 22:34:11.028147 29309 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:11.028854 29309 slave.cpp:590] Agent resources: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.028998 29309 slave.cpp:598] Agent attributes: [  ]
I0407 22:34:11.029064 29309 slave.cpp:603] Agent hostname: 129e11060069
I0407 22:34:11.031188 29309 state.cpp:57] Recovering state from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/meta'
I0407 22:34:11.031844 29300 status_update_manager.cpp:200] Recovering status update manager
I0407 22:34:11.032091 29300 containerizer.cpp:416] Recovering containerizer
I0407 22:34:11.033805 29300 provisioner.cpp:245] Provisioner recovery complete
I0407 22:34:11.034364 29300 slave.cpp:4784] Finished recovery
I0407 22:34:11.061807 29300 slave.cpp:4956] Querying resource estimator for oversubscribable resources
I0407 22:34:11.062371 29300 slave.cpp:939] New master detected at master@172.17.0.3:35855
I0407 22:34:11.062450 29300 slave.cpp:1002] Authenticating with master master@172.17.0.3:35855
I0407 22:34:11.062469 29300 slave.cpp:1007] Using default CRAM-MD5 authenticatee
I0407 22:34:11.062630 29300 slave.cpp:975] Detecting new master
I0407 22:34:11.062737 29300 slave.cpp:4970] Received oversubscribable resources  from the resource estimator
I0407 22:34:11.062820 29300 status_update_manager.cpp:174] Pausing sending status updates
I0407 22:34:11.062952 29300 authenticatee.cpp:121] Creating new client SASL connection
I0407 22:34:11.063413 29300 master.cpp:5695] Authenticating slave(112)@172.17.0.3:35855
I0407 22:34:11.063591 29300 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(279)@172.17.0.3:35855
I0407 22:34:11.063907 29300 authenticator.cpp:98] Creating new server SASL connection
I0407 22:34:11.064159 29300 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0407 22:34:11.064201 29300 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0407 22:34:11.064296 29300 authenticator.cpp:203] Received SASL authentication start
I0407 22:34:11.064363 29300 authenticator.cpp:325] Authentication requires more steps
I0407 22:34:11.064443 29300 authenticatee.cpp:258] Received SASL authentication step
I0407 22:34:11.064537 29300 authenticator.cpp:231] Received SASL authentication step
I0407 22:34:11.064569 29300 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0407 22:34:11.064584 29300 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0407 22:34:11.064640 29300 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0407 22:34:11.064668 29300 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0407 22:34:11.064680 29300 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.064689 29300 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.064708 29300 authenticator.cpp:317] Authentication success
I0407 22:34:11.064856 29300 authenticatee.cpp:298] Authentication success
I0407 22:34:11.064941 29300 master.cpp:5725] Successfully authenticated principal 'test-principal' at slave(112)@172.17.0.3:35855
I0407 22:34:11.065019 29300 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(279)@172.17.0.3:35855
I0407 22:34:11.065431 29305 slave.cpp:1072] Successfully authenticated with master master@172.17.0.3:35855
I0407 22:34:11.065580 29305 slave.cpp:1468] Will retry registration in 14.268351ms if necessary
I0407 22:34:11.065948 29305 master.cpp:4406] Registering agent at slave(112)@172.17.0.3:35855 (129e11060069) with id f59f9057-a5c7-43e1-b129-96862e640a12-S1
I0407 22:34:11.066653 29296 registrar.cpp:463] Applied 1 operations in 190813ns; attempting to update the 'registry'
I0407 22:34:11.075197 29298 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 59.338116ms
I0407 22:34:11.075359 29298 replica.cpp:712] Persisted action at 4
I0407 22:34:11.076177 29301 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0407 22:34:11.080481 29309 slave.cpp:1468] Will retry registration in 23.018984ms if necessary
I0407 22:34:11.080770 29309 master.cpp:4394] Ignoring register agent message from slave(112)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:11.100519 29301 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 24.288152ms
I0407 22:34:11.100792 29301 leveldb.cpp:399] Deleting ~2 keys from leveldb took 98264ns
I0407 22:34:11.100883 29301 replica.cpp:712] Persisted action at 4
I0407 22:34:11.101002 29301 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0407 22:34:11.102180 29309 log.cpp:683] Attempting to append 505 bytes to the log
I0407 22:34:11.102334 29301 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I0407 22:34:11.103551 29309 replica.cpp:537] Replica received write request for position 5 from (4813)@172.17.0.3:35855
I0407 22:34:11.105705 29305 slave.cpp:1468] Will retry registration in 49.972787ms if necessary
I0407 22:34:11.106020 29305 master.cpp:4394] Ignoring register agent message from slave(112)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:11.126212 29309 leveldb.cpp:341] Persisting action (524 bytes) to leveldb took 22.638848ms
I0407 22:34:11.126296 29309 replica.cpp:712] Persisted action at 5
I0407 22:34:11.127374 29305 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I0407 22:34:11.150754 29305 leveldb.cpp:341] Persisting action (526 bytes) to leveldb took 23.376079ms
I0407 22:34:11.150952 29305 replica.cpp:712] Persisted action at 5
I0407 22:34:11.150992 29305 replica.cpp:697] Replica learned APPEND action at position 5
I0407 22:34:11.154031 29305 registrar.cpp:508] Successfully updated the 'registry' in 87.26784ms
I0407 22:34:11.154491 29305 log.cpp:702] Attempting to truncate the log to 5
I0407 22:34:11.154824 29305 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I0407 22:34:11.155413 29308 slave.cpp:3675] Received ping from slave-observer(113)@172.17.0.3:35855
I0407 22:34:11.155467 29303 master.cpp:4474] Registered agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 at slave(112)@172.17.0.3:35855 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.155580 29308 slave.cpp:1116] Registered with master master@172.17.0.3:35855; given agent ID f59f9057-a5c7-43e1-b129-96862e640a12-S1
I0407 22:34:11.155606 29308 fetcher.cpp:81] Clearing fetcher cache
I0407 22:34:11.155856 29304 status_update_manager.cpp:181] Resuming sending status updates
I0407 22:34:11.156281 29308 slave.cpp:1139] Checkpointing SlaveInfo to '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/meta/slaves/f59f9057-a5c7-43e1-b129-96862e640a12-S1/slave.info'
I0407 22:34:11.156661 29304 replica.cpp:537] Replica received write request for position 6 from (4814)@172.17.0.3:35855
I0407 22:34:11.156949 29305 hierarchical.cpp:476] Added agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (allocated: )
I0407 22:34:11.157217 29305 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.157346 29305 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 in 304432ns
I0407 22:34:11.157224 29308 slave.cpp:1176] Forwarding total oversubscribed resources 
I0407 22:34:11.157788 29303 master.cpp:4818] Received update of agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 at slave(112)@172.17.0.3:35855 (129e11060069) with total oversubscribed resources 
I0407 22:34:11.158424 29303 hierarchical.cpp:534] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 (129e11060069) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: )
I0407 22:34:11.158633 29303 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.158699 29303 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 in 178482ns
I0407 22:34:11.162139 29278 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0407 22:34:11.192978 29278 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0407 22:34:11.197527 29307 slave.cpp:201] Agent started on 113)@172.17.0.3:35855
I0407 22:34:11.197581 29307 slave.cpp:202] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:4096;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru""
I0407 22:34:11.198328 29307 credentials.hpp:86] Loading credential for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/credential'
I0407 22:34:11.198562 29307 slave.cpp:339] Agent using credential for: test-principal
I0407 22:34:11.198598 29307 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/http_credentials'
I0407 22:34:11.198884 29307 slave.cpp:391] Using default 'basic' HTTP authenticator
I0407 22:34:11.199286 29307 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:11.199820 29307 slave.cpp:590] Agent resources: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.199905 29307 slave.cpp:598] Agent attributes: [  ]
I0407 22:34:11.199920 29307 slave.cpp:603] Agent hostname: 129e11060069
I0407 22:34:11.201535 29297 state.cpp:57] Recovering state from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/meta'
I0407 22:34:11.201773 29309 status_update_manager.cpp:200] Recovering status update manager
I0407 22:34:11.202081 29307 containerizer.cpp:416] Recovering containerizer
I0407 22:34:11.202180 29304 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 45.487899ms
I0407 22:34:11.202221 29304 replica.cpp:712] Persisted action at 6
I0407 22:34:11.203219 29302 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0
I0407 22:34:11.205412 29301 provisioner.cpp:245] Provisioner recovery complete
I0407 22:34:11.205984 29301 slave.cpp:4784] Finished recovery
I0407 22:34:11.206735 29301 slave.cpp:4956] Querying resource estimator for oversubscribable resources
I0407 22:34:11.207351 29301 slave.cpp:4970] Received oversubscribable resources  from the resource estimator
I0407 22:34:11.207679 29301 slave.cpp:939] New master detected at master@172.17.0.3:35855
I0407 22:34:11.207804 29309 status_update_manager.cpp:174] Pausing sending status updates
I0407 22:34:11.208039 29301 slave.cpp:1002] Authenticating with master master@172.17.0.3:35855
I0407 22:34:11.208072 29301 slave.cpp:1007] Using default CRAM-MD5 authenticatee
I0407 22:34:11.208431 29301 slave.cpp:975] Detecting new master
I0407 22:34:11.208650 29309 authenticatee.cpp:121] Creating new client SASL connection
I0407 22:34:11.208976 29309 master.cpp:5695] Authenticating slave(113)@172.17.0.3:35855
I0407 22:34:11.209081 29307 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(280)@172.17.0.3:35855
I0407 22:34:11.209432 29304 authenticator.cpp:98] Creating new server SASL connection
I0407 22:34:11.209971 29304 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0407 22:34:11.210103 29304 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0407 22:34:11.210382 29304 authenticator.cpp:203] Received SASL authentication start
I0407 22:34:11.210515 29304 authenticator.cpp:325] Authentication requires more steps
I0407 22:34:11.210726 29304 authenticatee.cpp:258] Received SASL authentication step
I0407 22:34:11.210940 29305 authenticator.cpp:231] Received SASL authentication step
I0407 22:34:11.210980 29305 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0407 22:34:11.210997 29305 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0407 22:34:11.211060 29305 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0407 22:34:11.211100 29305 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0407 22:34:11.211175 29305 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.211244 29305 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.211272 29305 authenticator.cpp:317] Authentication success
I0407 22:34:11.211462 29305 authenticatee.cpp:298] Authentication success
I0407 22:34:11.211575 29305 master.cpp:5725] Successfully authenticated principal 'test-principal' at slave(113)@172.17.0.3:35855
I0407 22:34:11.211673 29305 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(280)@172.17.0.3:35855
I0407 22:34:11.212026 29305 slave.cpp:1072] Successfully authenticated with master master@172.17.0.3:35855
I0407 22:34:11.212280 29305 slave.cpp:1468] Will retry registration in 6.415977ms if necessary
I0407 22:34:11.212704 29304 master.cpp:4406] Registering agent at slave(113)@172.17.0.3:35855 (129e11060069) with id f59f9057-a5c7-43e1-b129-96862e640a12-S2
I0407 22:34:11.213373 29311 registrar.cpp:463] Applied 1 operations in 154555ns; attempting to update the 'registry'
I0407 22:34:11.223568 29303 master.cpp:4394] Ignoring register agent message from slave(113)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:11.224171 29300 slave.cpp:1468] Will retry registration in 22.418267ms if necessary
I0407 22:34:11.243433 29302 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 40.20863ms
I0407 22:34:11.243851 29302 leveldb.cpp:399] Deleting ~2 keys from leveldb took 204965ns
I0407 22:34:11.243980 29302 replica.cpp:712] Persisted action at 6
I0407 22:34:11.244148 29302 replica.cpp:697] Replica learned TRUNCATE action at position 6
I0407 22:34:11.245827 29302 log.cpp:683] Attempting to append 671 bytes to the log
I0407 22:34:11.246206 29310 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 7
I0407 22:34:11.247114 29296 replica.cpp:537] Replica received write request for position 7 from (4829)@172.17.0.3:35855
I0407 22:34:11.248457 29304 slave.cpp:1468] Will retry registration in 14.981599ms if necessary
I0407 22:34:11.248837 29302 master.cpp:4394] Ignoring register agent message from slave(113)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:11.265728 29301 slave.cpp:1468] Will retry registration in 117.285894ms if necessary
I0407 22:34:11.266026 29301 master.cpp:4394] Ignoring register agent message from slave(113)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:11.278012 29296 leveldb.cpp:341] Persisting action (690 bytes) to leveldb took 30.789344ms
I0407 22:34:11.278064 29296 replica.cpp:712] Persisted action at 7
I0407 22:34:11.278990 29303 replica.cpp:691] Replica received learned notice for position 7 from @0.0.0.0:0
I0407 22:34:11.337220 29303 leveldb.cpp:341] Persisting action (692 bytes) to leveldb took 58.231676ms
I0407 22:34:11.337312 29303 replica.cpp:712] Persisted action at 7
I0407 22:34:11.337347 29303 replica.cpp:697] Replica learned APPEND action at position 7
I0407 22:34:11.340283 29305 registrar.cpp:508] Successfully updated the 'registry' in 126.71616ms
I0407 22:34:11.340703 29309 log.cpp:702] Attempting to truncate the log to 7
I0407 22:34:11.341044 29309 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 8
I0407 22:34:11.341847 29309 slave.cpp:3675] Received ping from slave-observer(114)@172.17.0.3:35855
I0407 22:34:11.342489 29309 slave.cpp:1116] Registered with master master@172.17.0.3:35855; given agent ID f59f9057-a5c7-43e1-b129-96862e640a12-S2
I0407 22:34:11.342532 29309 fetcher.cpp:81] Clearing fetcher cache
I0407 22:34:11.341804 29303 master.cpp:4474] Registered agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 at slave(113)@172.17.0.3:35855 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.342871 29297 status_update_manager.cpp:181] Resuming sending status updates
I0407 22:34:11.342267 29300 hierarchical.cpp:476] Added agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (allocated: )
I0407 22:34:11.342963 29299 replica.cpp:537] Replica received write request for position 8 from (4830)@172.17.0.3:35855
I0407 22:34:11.343101 29300 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.343178 29300 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 in 242921ns
I0407 22:34:11.342921 29309 slave.cpp:1139] Checkpointing SlaveInfo to '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/meta/slaves/f59f9057-a5c7-43e1-b129-96862e640a12-S2/slave.info'
I0407 22:34:11.343636 29309 slave.cpp:1176] Forwarding total oversubscribed resources 
I0407 22:34:11.343863 29309 master.cpp:4818] Received update of agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 at slave(113)@172.17.0.3:35855 (129e11060069) with total oversubscribed resources 
I0407 22:34:11.344173 29278 sched.cpp:224] Version: 0.29.0
I0407 22:34:11.344425 29309 hierarchical.cpp:534] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 (129e11060069) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: )
I0407 22:34:11.344568 29309 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.344621 29309 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 in 155620ns
I0407 22:34:11.345155 29303 sched.cpp:328] New master detected at master@172.17.0.3:35855
I0407 22:34:11.345387 29303 sched.cpp:384] Authenticating with master master@172.17.0.3:35855
I0407 22:34:11.345479 29303 sched.cpp:391] Using default CRAM-MD5 authenticatee
I0407 22:34:11.346035 29303 authenticatee.cpp:121] Creating new client SASL connection
I0407 22:34:11.346884 29303 master.cpp:5695] Authenticating scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:11.347530 29303 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(281)@172.17.0.3:35855
I0407 22:34:11.349140 29303 authenticator.cpp:98] Creating new server SASL connection
I0407 22:34:11.349580 29303 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0407 22:34:11.349707 29303 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0407 22:34:11.349957 29309 authenticator.cpp:203] Received SASL authentication start
I0407 22:34:11.350040 29309 authenticator.cpp:325] Authentication requires more steps
I0407 22:34:11.350168 29309 authenticatee.cpp:258] Received SASL authentication step
I0407 22:34:11.350275 29309 authenticator.cpp:231] Received SASL authentication step
I0407 22:34:11.350309 29309 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0407 22:34:11.350323 29309 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0407 22:34:11.350375 29309 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0407 22:34:11.350407 29309 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0407 22:34:11.350420 29309 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.350430 29309 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.350450 29309 authenticator.cpp:317] Authentication success
I0407 22:34:11.350550 29303 authenticatee.cpp:298] Authentication success
I0407 22:34:11.350647 29309 master.cpp:5725] Successfully authenticated principal 'test-principal' at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:11.350803 29303 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(281)@172.17.0.3:35855
I0407 22:34:11.350986 29309 sched.cpp:474] Successfully authenticated with master master@172.17.0.3:35855
I0407 22:34:11.351011 29309 sched.cpp:779] Sending SUBSCRIBE call to master@172.17.0.3:35855
I0407 22:34:11.351109 29309 sched.cpp:812] Will retry registration in 82.651114ms if necessary
I0407 22:34:11.351313 29296 master.cpp:2362] Received SUBSCRIBE call for framework 'default' at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:11.351343 29296 master.cpp:1871] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0407 22:34:11.351662 29310 master.cpp:2433] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0407 22:34:11.352442 29311 hierarchical.cpp:267] Added framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:11.353435 29309 sched.cpp:706] Framework registered with f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:11.353519 29309 sched.cpp:720] Scheduler::registered took 66350ns
I0407 22:34:11.355201 29311 hierarchical.cpp:1586] No inverse offers to send out!
I0407 22:34:11.355293 29311 hierarchical.cpp:1142] Performed allocation for 3 agents in 2.836617ms
I0407 22:34:11.356238 29301 master.cpp:5524] Sending 3 offers to framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:11.357260 29311 sched.cpp:876] Scheduler::resourceOffers took 327028ns
I0407 22:34:11.357628 29278 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:11.358330 29278 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:11.358959 29278 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:11.360607 29278 sched.cpp:224] Version: 0.29.0
I0407 22:34:11.361264 29307 sched.cpp:328] New master detected at master@172.17.0.3:35855
I0407 22:34:11.361342 29307 sched.cpp:384] Authenticating with master master@172.17.0.3:35855
I0407 22:34:11.361366 29307 sched.cpp:391] Using default CRAM-MD5 authenticatee
I0407 22:34:11.361670 29307 authenticatee.cpp:121] Creating new client SASL connection
I0407 22:34:11.361959 29307 master.cpp:5695] Authenticating scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:11.362195 29307 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(282)@172.17.0.3:35855
I0407 22:34:11.362535 29311 authenticator.cpp:98] Creating new server SASL connection
I0407 22:34:11.362890 29307 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0407 22:34:11.362926 29307 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0407 22:34:11.363021 29307 authenticator.cpp:203] Received SASL authentication start
I0407 22:34:11.363082 29307 authenticator.cpp:325] Authentication requires more steps
I0407 22:34:11.363199 29311 authenticatee.cpp:258] Received SASL authentication step
I0407 22:34:11.363313 29311 authenticator.cpp:231] Received SASL authentication step
I0407 22:34:11.363406 29311 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0407 22:34:11.363512 29311 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0407 22:34:11.363605 29311 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0407 22:34:11.363651 29311 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0407 22:34:11.363673 29311 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.363685 29311 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.363706 29311 authenticator.cpp:317] Authentication success
I0407 22:34:11.363785 29307 authenticatee.cpp:298] Authentication success
I0407 22:34:11.363858 29297 master.cpp:5725] Successfully authenticated principal 'test-principal' at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:11.363903 29311 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(282)@172.17.0.3:35855
I0407 22:34:11.365274 29297 sched.cpp:474] Successfully authenticated with master master@172.17.0.3:35855
I0407 22:34:11.365301 29297 sched.cpp:779] Sending SUBSCRIBE call to master@172.17.0.3:35855
I0407 22:34:11.365396 29297 sched.cpp:812] Will retry registration in 1.739883809secs if necessary
I0407 22:34:11.365500 29311 master.cpp:2362] Received SUBSCRIBE call for framework 'default' at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:11.365528 29311 master.cpp:1871] Authorizing framework principal 'test-principal' to receive offers for role 'role2'
I0407 22:34:11.365952 29297 master.cpp:2433] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0407 22:34:11.366518 29297 sched.cpp:706] Framework registered with f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:11.366564 29311 hierarchical.cpp:267] Added framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:11.366590 29297 sched.cpp:720] Scheduler::registered took 57363ns
I0407 22:34:11.366768 29311 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.366837 29311 hierarchical.cpp:1586] No inverse offers to send out!
I0407 22:34:11.366914 29311 hierarchical.cpp:1142] Performed allocation for 3 agents in 340908ns
I0407 22:34:11.369886 29309 process.cpp:3165] Handling HTTP event for process 'master' with path: '/master/weights'
I0407 22:34:11.370643 29309 http.cpp:313] HTTP PUT for /master/weights from 172.17.0.3:59397
I0407 22:34:11.370762 29309 weights_handler.cpp:58] Updating weights from request: '[{""role"":""role2"",""weight"":2.0}]'
I0407 22:34:11.370908 29309 weights_handler.cpp:198] Authorizing principal 'test-principal' to update weights for roles '[ role2 ]'
I0407 22:34:11.372067 29306 registrar.cpp:463] Applied 1 operations in 136060ns; attempting to update the 'registry'
I0407 22:34:11.388222 29299 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 45.245469ms
I0407 22:34:11.388381 29299 replica.cpp:712] Persisted action at 8
I0407 22:34:11.389389 29305 replica.cpp:691] Replica received learned notice for position 8 from @0.0.0.0:0
I0407 22:34:11.435415 29305 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 45.918275ms
I0407 22:34:11.435688 29305 leveldb.cpp:399] Deleting ~2 keys from leveldb took 98518ns
I0407 22:34:11.435835 29305 replica.cpp:712] Persisted action at 8
I0407 22:34:11.435956 29305 replica.cpp:697] Replica learned TRUNCATE action at position 8
I0407 22:34:11.437063 29310 log.cpp:683] Attempting to append 691 bytes to the log
I0407 22:34:11.437297 29300 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 9
I0407 22:34:11.437979 29300 replica.cpp:537] Replica received write request for position 9 from (4834)@172.17.0.3:35855
I0407 22:34:11.479363 29300 leveldb.cpp:341] Persisting action (710 bytes) to leveldb took 41.36295ms
I0407 22:34:11.479432 29300 replica.cpp:712] Persisted action at 9
I0407 22:34:11.480434 29296 replica.cpp:691] Replica received learned notice for position 9 from @0.0.0.0:0
I0407 22:34:11.521299 29296 leveldb.cpp:341] Persisting action (712 bytes) to leveldb took 40.855981ms
I0407 22:34:11.521378 29296 replica.cpp:712] Persisted action at 9
I0407 22:34:11.521412 29296 replica.cpp:697] Replica learned APPEND action at position 9
I0407 22:34:11.524554 29304 registrar.cpp:508] Successfully updated the 'registry' in 152.402176ms
I0407 22:34:11.524790 29298 log.cpp:702] Attempting to truncate the log to 9
I0407 22:34:11.524960 29304 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 10
I0407 22:34:11.525243 29298 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.525387 29298 hierarchical.cpp:1586] No inverse offers to send out!
I0407 22:34:11.525538 29298 hierarchical.cpp:1142] Performed allocation for 3 agents in 540681ns
I0407 22:34:11.525856 29296 replica.cpp:537] Replica received write request for position 10 from (4835)@172.17.0.3:35855
I0407 22:34:11.526267 29308 sched.cpp:902] Rescinded offer f59f9057-a5c7-43e1-b129-96862e640a12-O1
I0407 22:34:11.526398 29308 sched.cpp:913] Scheduler::offerRescinded took 54437ns
I0407 22:34:11.526425 29298 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:11.527235 29299 sched.cpp:902] Rescinded offer f59f9057-a5c7-43e1-b129-96862e640a12-O2
I0407 22:34:11.527299 29299 sched.cpp:913] Scheduler::offerRescinded took 29764ns
I0407 22:34:11.527825 29300 sched.cpp:902] Rescinded offer f59f9057-a5c7-43e1-b129-96862e640a12-O0
I0407 22:34:11.527920 29298 hierarchical.cpp:1586] No inverse offers to send out!
I0407 22:34:11.527990 29298 hierarchical.cpp:1142] Performed allocation for 3 agents in 1.481251ms
I0407 22:34:11.528009 29300 sched.cpp:913] Scheduler::offerRescinded took 333035ns
I0407 22:34:11.528591 29298 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:11.529536 29311 master.cpp:5524] Sending 1 offers to framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:11.529846 29298 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:11.530747 29304 sched.cpp:876] Scheduler::resourceOffers took 128400ns
I0407 22:34:11.560456 29296 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 34.585376ms
I0407 22:34:11.560539 29296 replica.cpp:712] Persisted action at 10
I0407 22:34:11.564628 29303 replica.cpp:691] Replica received learned notice for position 10 from @0.0.0.0:0
I0407 22:34:11.601330 29303 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 36.57815ms
I0407 22:34:11.601774 29303 leveldb.cpp:399] Deleting ~2 keys from leveldb took 221499ns
I0407 22:34:11.601899 29303 replica.cpp:712] Persisted action at 10
I0407 22:34:11.602052 29303 replica.cpp:697] Replica learned TRUNCATE action at position 10
I0407 22:34:12.531602 29308 hierarchical.cpp:1586] No inverse offers to send out!
I0407 22:34:12.532578 29308 hierarchical.cpp:1142] Performed allocation for 3 agents in 3.892929ms
I0407 22:34:12.532403 29306 master.cpp:5524] Sending 1 offers to framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
../../src/tests/master_allocator_tests.cpp:1587: Failure
Mock function called more times than expected - returning directly.
    Function call: resourceOffers(0x7fffe87e3370, @0x2adef432e6f0 { 144-byte object <E0-9C 76-EA DE-2A 00-00 00-00 00-00 00-00 00-00 1F-00 00-00 00-00 00-00 90-4B 00-2C DF-2A 00-00 30-6A 00-2C DF-2A 00-00 80-6A 00-2C DF-2A 00-00 20-62 00-2C DF-2A 00-00 60-38 00-2C DF-2A 00-00 ... 04-00 00-00 01-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00> })
         Expected: to be called once
           Actual: called twice - over-saturated and active
I0407 22:34:12.533665 29301 sched.cpp:876] Scheduler::resourceOffers took 250853ns
I0407 22:34:12.533915 29306 master.cpp:5524] Sending 1 offers to framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:12.534454 29306 sched.cpp:876] Scheduler::resourceOffers took 157733ns
../../src/tests/master_allocator_tests.cpp:1629: Failure
Value of: framework2offers.get().size()
  Actual: 1
Expected: 2u
Which is: 2
I0407 22:34:12.534997 29278 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:12.537264 29301 master.cpp:1275] Framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855 disconnected
I0407 22:34:12.537297 29301 master.cpp:2658] Disconnecting framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:12.537330 29301 master.cpp:2682] Deactivating framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
W0407 22:34:12.537849 29301 master.hpp:1822] Master attempted to send message to disconnected framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
W0407 22:34:12.538306 29301 master.hpp:1822] Master attempted to send message to disconnected framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:12.538394 29301 master.cpp:1299] Giving framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855 0ns to failover
I0407 22:34:12.539371 29302 hierarchical.cpp:378] Deactivated framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.540053 29302 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.540732 29302 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.540974 29301 master.cpp:1275] Framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855 disconnected
I0407 22:34:12.541178 29301 master.cpp:2658] Disconnecting framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:12.541292 29301 master.cpp:2682] Deactivating framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:12.541553 29300 hierarchical.cpp:378] Deactivated framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:12.542654 29300 hierarchical.cpp:894] Recovered cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: ) on agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 from framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
W0407 22:34:12.543051 29301 master.hpp:1822] Master attempted to send message to disconnected framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:12.543525 29301 master.cpp:1299] Giving framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855 0ns to failover
I0407 22:34:12.543861 29301 master.cpp:5376] Framework failover timeout, removing framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:12.543959 29301 master.cpp:6109] Removing framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 (default) at scheduler-37080386-2aa8-4592-bf09-8288bd04727a@172.17.0.3:35855
I0407 22:34:12.544445 29301 slave.cpp:2226] Asked to shut down framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 by master@172.17.0.3:35855
W0407 22:34:12.545446 29301 slave.cpp:2241] Cannot shut down unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.544556 29300 slave.cpp:2226] Asked to shut down framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 by master@172.17.0.3:35855
W0407 22:34:12.545661 29300 slave.cpp:2241] Cannot shut down unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.545774 29300 slave.cpp:811] Agent terminating
I0407 22:34:12.544791 29305 hierarchical.cpp:329] Removed framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.545241 29296 master.cpp:5376] Framework failover timeout, removing framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
I0407 22:34:12.544518 29302 slave.cpp:2226] Asked to shut down framework f59f9057-a5c7-43e1-b129-96862e640a12-0001 by master@172.17.0.3:35855
I0407 22:34:12.546140 29296 master.cpp:6109] Removing framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 (default) at scheduler-72944dc9-e3cc-4ebc-bca6-d72c77ad6721@172.17.0.3:35855
W0407 22:34:12.546159 29302 slave.cpp:2241] Cannot shut down unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0001
I0407 22:34:12.546496 29296 master.cpp:1236] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069) disconnected
I0407 22:34:12.546527 29296 master.cpp:2717] Disconnecting agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.546581 29296 master.cpp:2736] Deactivating agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.546752 29296 slave.cpp:2226] Asked to shut down framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 by master@172.17.0.3:35855
W0407 22:34:12.546782 29296 slave.cpp:2241] Cannot shut down unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:12.546844 29296 slave.cpp:2226] Asked to shut down framework f59f9057-a5c7-43e1-b129-96862e640a12-0000 by master@172.17.0.3:35855
W0407 22:34:12.546869 29296 slave.cpp:2241] Cannot shut down unknown framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:12.547111 29296 hierarchical.cpp:329] Removed framework f59f9057-a5c7-43e1-b129-96862e640a12-0000
I0407 22:34:12.547302 29296 hierarchical.cpp:563] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 deactivated
I0407 22:34:12.553478 29278 slave.cpp:811] Agent terminating
I0407 22:34:12.553766 29306 master.cpp:1236] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 at slave(112)@172.17.0.3:35855 (129e11060069) disconnected
I0407 22:34:12.555483 29306 master.cpp:2717] Disconnecting agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 at slave(112)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.555858 29306 master.cpp:2736] Deactivating agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 at slave(112)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.556190 29307 hierarchical.cpp:563] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 deactivated
I0407 22:34:12.559095 29299 slave.cpp:811] Agent terminating
I0407 22:34:12.559301 29300 master.cpp:1236] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 at slave(113)@172.17.0.3:35855 (129e11060069) disconnected
I0407 22:34:12.559327 29300 master.cpp:2717] Disconnecting agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 at slave(113)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.559370 29300 master.cpp:2736] Deactivating agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 at slave(113)@172.17.0.3:35855 (129e11060069)
I0407 22:34:12.559516 29309 hierarchical.cpp:563] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S2 deactivated
I0407 22:34:12.561872 29278 master.cpp:1089] Master terminating
I0407 22:34:12.562566 29304 hierarchical.cpp:508] Removed agent f59f9057-a5c7-43e1-b129-96862e640a12-S2
I0407 22:34:12.562890 29304 hierarchical.cpp:508] Removed agent f59f9057-a5c7-43e1-b129-96862e640a12-S1
I0407 22:34:12.565459 29304 hierarchical.cpp:508] Removed agent f59f9057-a5c7-43e1-b129-96862e640a12-S0
[  FAILED  ] MasterAllocatorTest/1.RebalancedForUpdatedWeights, where TypeParam = mesos::internal::tests::Module<mesos::master::allocator::Allocator, (mesos::internal::tests::ModuleID)6> (2240 ms)
{code}","Ubuntu 14.04 using clang, without libevent or SSL",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-08 00:11:37.843,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 21 12:29:48 UTC 2016,,,,,,,"0|i2vthb:",9223372036854775807,,,,,alexr,,,,,,Mesosphere Sprint 33,,,,,,,,,,,1.0,,,,,,,,,,,"08/Apr/16 00:11;adam-mesos;cc: [~gradywang]","08/Apr/16 07:28;alexr;Looks like a batch allocation sneaked in between recovering resources. We can pause the clock and make sure all resources from rescinded offers are recovered before triggering the next batch allocation.","13/Apr/16 03:22;gradywang;Append the RR: https://reviews.apache.org/r/46135/","21/Apr/16 12:29;alexr;{noformat}
Commit: d778d4502f468336df8094feb9ecde8589e2a86b [d778d45]
Author: Yongqiao Wang <yqwyq@cn.ibm.com>
Date: 21 Apr 2016 13:52:42 CEST
Committer: Alexander Rukletsov <alexr@apache.org>

Fixed flakiness in MasterAllocatorTest.RebalancedForUpdatedWeights.

We make sure all resources from rescinded offers are recovered before
next batch allocation is triggered. This prevents a batch allocation
from sneaking in-between recovering resources.

Review: https://reviews.apache.org/r/46135/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,
Add agent flags for HTTP authorization.,MESOS-5142,12956798,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,nfnt,nfnt,nfnt,07/Apr/16 08:39,29/Apr/19 09:27,29/Oct/20 16:32,27/Apr/16 16:25,,,,,,,,,1.0.0,,,,,,agent,security,,,,0,mesosphere,security,,,,,,,"Flags should be added to the agent to:
1. Enable authorization ({{--authorizers}})
2. Provide ACLs ({{--acls}})",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-5164,MESOS-5273,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-25 17:23:00.061,,,false,MESOS-5150,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 27 16:25:26 UTC 2016,,,,,,,"0|hzzzhy:z",9223372036854775807,,,,,adam-mesos,,,,,,Mesosphere Sprint 32,Mesosphere Sprint 33,Mesosphere Sprint 34,,,,,,,,,2.0,,,,,,,,,,,"08/Apr/16 08:56;nfnt;https://reviews.apache.org/r/45922/
https://reviews.apache.org/r/46203/","25/Apr/16 17:23;adam-mesos;Committed the new --authorizer agent flag (first review above), but not authorization of /flags (2nd review).

commit a3da5811e0de83373f6ef5d98fbe9f72e65de046
Author: Jan Schlicht <jan@mesosphere.io>
Date:   Mon Apr 25 03:57:31 2016 -0700

    Added agent authorization flags.
    
    Review: https://reviews.apache.org/r/45922/","27/Apr/16 16:25;alexr;{noformat}
Commit: 721a414f64dbb90920202e674b87b42a2c7876f9 [721a414]
Author: Jan Schlicht jan@mesosphere.io
Date: 27 Apr 2016 16:26:33 CEST
Committer: Alexander Rukletsov alexr@apache.org
Commit Date: 27 Apr 2016 17:28:25 CEST

Enabled authorization of endpoints on agents.

Add a method that checks with the authorizer whether a GET request
shall be granted. Use this method to authorize '/flags' endpoint.

Review: https://reviews.apache.org/r/46203/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,
Some ProvisionerDockerLocalStoreTest.* are flaky due to tar issue.,MESOS-5139,12956743,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,vinodkone,vinodkone,07/Apr/16 03:45,02/Dec/18 00:15,29/Oct/20 16:32,,0.28.0,1.0.4,1.1.3,1.2.3,1.3.1,1.4.1,1.8.0,,,,,,,,,,,,,0,containerizer,flaky,flaky-test,mesosphere,,,,,"These tests are still occasionally fail as of Mesos 1.5.0-wip:
{code}
ProvisionerDockerLocalStoreTest.LocalStoreTestWithTar
ProvisionerDockerLocalStoreTest.MetadataManagerInitialization
ProvisionerDockerLocalStoreTest.MissingLayer
{code}

Found this on ASF CI while testing 0.28.1-rc2

{code}
[ RUN      ] ProvisionerDockerLocalStoreTest.LocalStoreTestWithTar
E0406 18:29:30.870481   520 shell.hpp:93] Command 'hadoop version 2>&1' failed; this is the output:
sh: 1: hadoop: not found
E0406 18:29:30.870576   520 fetcher.cpp:59] Failed to create URI fetcher plugin 'hadoop': Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
I0406 18:29:30.871052   520 local_puller.cpp:90] Creating local puller with docker registry '/tmp/3l8ZBv/images'
I0406 18:29:30.873325   539 metadata_manager.cpp:159] Looking for image 'abc'
I0406 18:29:30.874438   539 local_puller.cpp:142] Untarring image 'abc' from '/tmp/3l8ZBv/images/abc.tar' to '/tmp/3l8ZBv/store/staging/5tw8bD'
I0406 18:29:30.901916   547 local_puller.cpp:162] The repositories JSON file for image 'abc' is '{""abc"":{""latest"":""456""}}'
I0406 18:29:30.902304   547 local_puller.cpp:290] Extracting layer tar ball '/tmp/3l8ZBv/store/staging/5tw8bD/123/layer.tar to rootfs '/tmp/3l8ZBv/store/staging/5tw8bD/123/rootfs'
I0406 18:29:30.909144   547 local_puller.cpp:290] Extracting layer tar ball '/tmp/3l8ZBv/store/staging/5tw8bD/456/layer.tar to rootfs '/tmp/3l8ZBv/store/staging/5tw8bD/456/rootfs'
../../src/tests/containerizer/provisioner_docker_tests.cpp:183: Failure
(imageInfo).failure(): Collect failed: Subprocess 'tar, tar, -x, -f, /tmp/3l8ZBv/store/staging/5tw8bD/456/layer.tar, -C, /tmp/3l8ZBv/store/staging/5tw8bD/456/rootfs' failed: tar: This does not look like a tar archive
tar: Exiting with failure status due to previous errors

[  FAILED  ] ProvisionerDockerLocalStoreTest.LocalStoreTestWithTar (243 ms)
{code}","Ubuntu 14.04
Ubuntu 16.04",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-3578,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-12 16:16:30.934,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 02 00:15:24 UTC 2018,,,,,,,"0|hzzzhy:zzzv",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 33,,,,,,,,,,,2.0,,,,,,,,,,,"12/Apr/16 16:16;hartem;[~gilbert] please make sure this ticket has a shepherd.","13/Apr/16 20:57;gilbert;Leave a note here, this flaky test cannot be reproduced in 7300+ iterations on my Ubuntu 14.04 VM. ","19/Apr/16 03:23;haosdent@gmail.com;I am not sure whether this is because we use {{tar -czf}} to create tar and use {{tar -xf}} to extract tar or not. We create a gzip compressed tar and extract it in normal way. tar could use gzip to decompress automatically. But I not sure whether it doesn't work in some case.

I think need replace 
{code}
    ASSERT_SOME(os::tar(""."", ""../layer.tar""));
{code}
to {{command::tar}} to keep consistent.
","19/Apr/16 19:03;gilbert;Yes, I am also thinking either use `command::tar` or deprecate this test case.","11/Jul/16 13:29;arojas;It occur again:

{noformat}
[12:04:09] :	 [Step 10/10] [ RUN      ] ProvisionerDockerLocalStoreTest.LocalStoreTestWithTar
[12:04:09]W:	 [Step 10/10] E0711 12:04:09.706239 22891 shell.hpp:106] Command 'hadoop version 2>&1' failed; this is the output:
[12:04:09]W:	 [Step 10/10] sh: 1: hadoop: not found
[12:04:09]W:	 [Step 10/10] I0711 12:04:09.706284 22891 fetcher.cpp:62] Skipping URI fetcher plugin 'hadoop' as it could not be created: Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
[12:04:09]W:	 [Step 10/10] I0711 12:04:09.706362 22891 local_puller.cpp:90] Creating local puller with docker registry '/tmp/w41IML/images'
[12:04:09]W:	 [Step 10/10] I0711 12:04:09.707073 22909 metadata_manager.cpp:167] Looking for image 'abc'
[12:04:09]W:	 [Step 10/10] I0711 12:04:09.707239 22909 local_puller.cpp:142] Untarring image 'abc' from '/tmp/w41IML/images/abc.tar' to '/tmp/w41IML/store/staging/gLXARR'
[12:04:09]W:	 [Step 10/10] I0711 12:04:09.790035 22907 local_puller.cpp:162] The repositories JSON file for image 'abc' is '{""abc"":{""latest"":""456""}}'
[12:04:09]W:	 [Step 10/10] I0711 12:04:09.790181 22907 local_puller.cpp:290] Extracting layer tar ball '/tmp/w41IML/store/staging/gLXARR/123/layer.tar to rootfs '/tmp/w41IML/store/staging/gLXARR/123/rootfs'
[12:04:09]W:	 [Step 10/10] I0711 12:04:09.792733 22907 local_puller.cpp:290] Extracting layer tar ball '/tmp/w41IML/store/staging/gLXARR/456/layer.tar to rootfs '/tmp/w41IML/store/staging/gLXARR/456/rootfs'
[12:04:09] :	 [Step 10/10] ../../src/tests/containerizer/provisioner_docker_tests.cpp:188: Failure
[12:04:09] :	 [Step 10/10] (imageInfo).failure(): Collect failed: Subprocess 'tar, tar, -x, -f, /tmp/w41IML/store/staging/gLXARR/456/layer.tar, -C, /tmp/w41IML/store/staging/gLXARR/456/rootfs' failed: tar: This does not look like a tar archive
[12:04:09] :	 [Step 10/10] tar: Exiting with failure status due to previous errors
[12:04:09] :	 [Step 10/10] 
[12:04:09] :	 [Step 10/10] [  FAILED  ] ProvisionerDockerLocalStoreTest.LocalStoreTestWithTar (202 ms)
[12:04:09] :	 [Step 10/10] [ RUN      ] ProvisionerDockerLocalStoreTest.MetadataManagerInitialization
[12:04:09]W:	 [Step 10/10] E0711 12:04:09.909461 22891 shell.hpp:106] Command 'hadoop version 2>&1' failed; this is the output:
[12:04:09]W:	 [Step 10/10] sh: 1: hadoop: not found
[12:04:09]W:	 [Step 10/10] I0711 12:04:09.909502 22891 fetcher.cpp:62] Skipping URI fetcher plugin 'hadoop' as it could not be created: Failed to create HDFS client: Failed to execute 'hadoop version 2>&1'; the command was either not found or exited with a non-zero exit status: 127
[12:04:09]W:	 [Step 10/10] I0711 12:04:09.909587 22891 local_puller.cpp:90] Creating local puller with docker registry '/tmp/K0CuvY/images'
[12:04:09]W:	 [Step 10/10] I0711 12:04:09.910228 22908 metadata_manager.cpp:167] Looking for image 'abc'
[12:04:09]W:	 [Step 10/10] I0711 12:04:09.910400 22906 local_puller.cpp:142] Untarring image 'abc' from '/tmp/K0CuvY/images/abc.tar' to '/tmp/K0CuvY/store/staging/Kk7Qc5'
[12:04:09]W:	 [Step 10/10] I0711 12:04:09.991353 22908 local_puller.cpp:162] The repositories JSON file for image 'abc' is '{""abc"":{""latest"":""456""}}'
[12:04:09]W:	 [Step 10/10] I0711 12:04:09.991488 22908 local_puller.cpp:290] Extracting layer tar ball '/tmp/K0CuvY/store/staging/Kk7Qc5/123/layer.tar to rootfs '/tmp/K0CuvY/store/staging/Kk7Qc5/123/rootfs'
[12:04:09]W:	 [Step 10/10] I0711 12:04:09.993975 22908 local_puller.cpp:290] Extracting layer tar ball '/tmp/K0CuvY/store/staging/Kk7Qc5/456/layer.tar to rootfs '/tmp/K0CuvY/store/staging/Kk7Qc5/456/rootfs'
[12:04:10] :	 [Step 10/10] ../../src/tests/containerizer/provisioner_docker_tests.cpp:210: Failure
[12:04:10] :	 [Step 10/10] (imageInfo).failure(): Collect failed: Subprocess 'tar, tar, -x, -f, /tmp/K0CuvY/store/staging/Kk7Qc5/456/layer.tar, -C, /tmp/K0CuvY/store/staging/Kk7Qc5/456/rootfs' failed: tar: This does not look like a tar archive
[12:04:10] :	 [Step 10/10] tar: Exiting with failure status due to previous errors
[12:04:10] :	 [Step 10/10] 
[12:04:10] :	 [Step 10/10] [  FAILED  ] ProvisionerDockerLocalStoreTest.MetadataManagerInitialization (201 ms)
{noformat}

My feeling is that this one and MESOS-3578 look as if they were the same issue, should we mark a duplicate?","02/Dec/18 00:15;tillt;The problem persists it seems; observed it on Ubuntu 16.04 (internal CI):
{noformat}
17:05:08 [ RUN ] ProvisionerDockerLocalStoreTest.LocalStoreTestWithTar
17:05:08 sh: 1: hadoop: not found
17:05:08 I1201 17:05:08.887616 5218 fetcher.cpp:66] Skipping URI fetcher plugin 'hadoop' as it could not be created: Failed to create HDFS client: Hadoop client is not available, exit status: 32512
17:05:08 I1201 17:05:08.887815 5218 image_tar_puller.cpp:128] Creating image tar puller with docker registry '/tmp/STXHAi/images'
17:05:08 I1201 17:05:08.888629 31183 metadata_manager.cpp:179] Looking for image 'abc'
17:05:08 I1201 17:05:08.888867 31185 image_tar_puller.cpp:210] Untarring image 'abc' from '/tmp/STXHAi/images/abc.tar' to '/tmp/STXHAi/store/staging/94yLVT'
17:05:09 I1201 17:05:08.988723 31189 image_tar_puller.cpp:231] The repositories JSON file for image 'abc' is '{""abc"":{""latest"":""456""}}'
17:05:09 I1201 17:05:08.988894 31189 image_tar_puller.cpp:375] Extracting layer tar ball '/tmp/STXHAi/store/staging/94yLVT/123/layer.tar to rootfs '/tmp/STXHAi/store/staging/94yLVT/123/rootfs'
17:05:09 I1201 17:05:08.992475 31189 image_tar_puller.cpp:375] Extracting layer tar ball '/tmp/STXHAi/store/staging/94yLVT/456/layer.tar to rootfs '/tmp/STXHAi/store/staging/94yLVT/456/rootfs'
17:05:09 ../../src/tests/containerizer/provisioner_docker_tests.cpp:210: Failure
17:05:09 (imageInfo).failure(): Collect failed: Subprocess 'tar, tar, -x, -f, /tmp/STXHAi/store/staging/94yLVT/456/layer.tar, -C, /tmp/STXHAi/store/staging/94yLVT/456/rootfs' failed: tar: This does not look like a tar archive
17:05:09 tar: Exiting with failure status due to previous errors
17:05:09 
17:05:09 [ FAILED ] ProvisionerDockerLocalStoreTest.LocalStoreTestWithTar (303 ms){noformat}",,,,,,,,,,,,,,,,,,,,,,
PersistentVolumeTest.AccessPersistentVolume is flaky,MESOS-5128,12956385,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,neilc,greggomann,greggomann,06/Apr/16 05:19,26/Apr/17 17:01,29/Oct/20 16:32,06/Apr/16 23:21,0.28.0,,,,,,,,1.0.0,,,,,,test,,,,,0,mesosphere,,,,,,,,"Observed on ASF CI:

{code}
[ RUN      ] DiskResource/PersistentVolumeTest.AccessPersistentVolume/0
I0405 17:29:19.134435 31837 cluster.cpp:139] Creating default 'local' authorizer
I0405 17:29:19.251143 31837 leveldb.cpp:174] Opened db in 116.386403ms
I0405 17:29:19.310050 31837 leveldb.cpp:181] Compacted db in 58.80688ms
I0405 17:29:19.310180 31837 leveldb.cpp:196] Created db iterator in 37145ns
I0405 17:29:19.310199 31837 leveldb.cpp:202] Seeked to beginning of db in 4212ns
I0405 17:29:19.310210 31837 leveldb.cpp:271] Iterated through 0 keys in the db in 410ns
I0405 17:29:19.310279 31837 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0405 17:29:19.311069 31861 recover.cpp:447] Starting replica recovery
I0405 17:29:19.311362 31861 recover.cpp:473] Replica is in EMPTY status
I0405 17:29:19.312641 31861 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (14359)@172.17.0.4:43972
I0405 17:29:19.313045 31860 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0405 17:29:19.313608 31860 recover.cpp:564] Updating replica status to STARTING
I0405 17:29:19.316416 31867 master.cpp:376] Master 9565ff6f-f1b6-4259-8430-690e635c391f (4090d10eba90) started on 172.17.0.4:43972
I0405 17:29:19.316470 31867 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/0A9ELu/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/0A9ELu/master"" --zk_session_timeout=""10secs""
I0405 17:29:19.316938 31867 master.cpp:427] Master only allowing authenticated frameworks to register
I0405 17:29:19.316951 31867 master.cpp:432] Master only allowing authenticated agents to register
I0405 17:29:19.316961 31867 credentials.hpp:37] Loading credentials for authentication from '/tmp/0A9ELu/credentials'
I0405 17:29:19.317402 31867 master.cpp:474] Using default 'crammd5' authenticator
I0405 17:29:19.317643 31867 master.cpp:545] Using default 'basic' HTTP authenticator
I0405 17:29:19.317854 31867 master.cpp:583] Authorization enabled
I0405 17:29:19.318081 31864 whitelist_watcher.cpp:77] No whitelist given
I0405 17:29:19.318079 31861 hierarchical.cpp:144] Initialized hierarchical allocator process
I0405 17:29:19.320838 31864 master.cpp:1826] The newly elected leader is master@172.17.0.4:43972 with id 9565ff6f-f1b6-4259-8430-690e635c391f
I0405 17:29:19.320888 31864 master.cpp:1839] Elected as the leading master!
I0405 17:29:19.320909 31864 master.cpp:1526] Recovering from registrar
I0405 17:29:19.321218 31871 registrar.cpp:331] Recovering registrar
I0405 17:29:19.347045 31860 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 33.164133ms
I0405 17:29:19.347126 31860 replica.cpp:320] Persisted replica status to STARTING
I0405 17:29:19.347611 31869 recover.cpp:473] Replica is in STARTING status
I0405 17:29:19.349215 31871 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (14361)@172.17.0.4:43972
I0405 17:29:19.349653 31870 recover.cpp:193] Received a recover response from a replica in STARTING status
I0405 17:29:19.350236 31866 recover.cpp:564] Updating replica status to VOTING
I0405 17:29:19.388882 31864 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.38299ms
I0405 17:29:19.388993 31864 replica.cpp:320] Persisted replica status to VOTING
I0405 17:29:19.389369 31856 recover.cpp:578] Successfully joined the Paxos group
I0405 17:29:19.389735 31856 recover.cpp:462] Recover process terminated
I0405 17:29:19.390476 31868 log.cpp:659] Attempting to start the writer
I0405 17:29:19.392125 31862 replica.cpp:493] Replica received implicit promise request from (14362)@172.17.0.4:43972 with proposal 1
I0405 17:29:19.430706 31862 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.505062ms
I0405 17:29:19.430816 31862 replica.cpp:342] Persisted promised to 1
I0405 17:29:19.431918 31856 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0405 17:29:19.433725 31861 replica.cpp:388] Replica received explicit promise request from (14363)@172.17.0.4:43972 for position 0 with proposal 2
I0405 17:29:19.472491 31861 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 38.659492ms
I0405 17:29:19.472595 31861 replica.cpp:712] Persisted action at 0
I0405 17:29:19.474556 31864 replica.cpp:537] Replica received write request for position 0 from (14364)@172.17.0.4:43972
I0405 17:29:19.474652 31864 leveldb.cpp:436] Reading position from leveldb took 49423ns
I0405 17:29:19.528175 31864 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 53.443616ms
I0405 17:29:19.528300 31864 replica.cpp:712] Persisted action at 0
I0405 17:29:19.529389 31865 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0405 17:29:19.571137 31865 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 41.676495ms
I0405 17:29:19.571254 31865 replica.cpp:712] Persisted action at 0
I0405 17:29:19.571302 31865 replica.cpp:697] Replica learned NOP action at position 0
I0405 17:29:19.572322 31856 log.cpp:675] Writer started with ending position 0
I0405 17:29:19.574060 31861 leveldb.cpp:436] Reading position from leveldb took 83200ns
I0405 17:29:19.575417 31864 registrar.cpp:364] Successfully fetched the registry (0B) in 0ns
I0405 17:29:19.575565 31864 registrar.cpp:463] Applied 1 operations in 46419ns; attempting to update the 'registry'
I0405 17:29:19.576517 31857 log.cpp:683] Attempting to append 170 bytes to the log
I0405 17:29:19.576849 31857 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0405 17:29:19.578390 31857 replica.cpp:537] Replica received write request for position 1 from (14365)@172.17.0.4:43972
I0405 17:29:19.780277 31857 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 201.808617ms
I0405 17:29:19.780366 31857 replica.cpp:712] Persisted action at 1
I0405 17:29:19.782024 31857 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0405 17:29:19.823770 31857 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 41.667662ms
I0405 17:29:19.823851 31857 replica.cpp:712] Persisted action at 1
I0405 17:29:19.823889 31857 replica.cpp:697] Replica learned APPEND action at position 1
I0405 17:29:19.825701 31867 registrar.cpp:508] Successfully updated the 'registry' in 0ns
I0405 17:29:19.825929 31867 registrar.cpp:394] Successfully recovered registrar
I0405 17:29:19.826015 31857 log.cpp:702] Attempting to truncate the log to 1
I0405 17:29:19.826262 31867 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0405 17:29:19.827647 31867 replica.cpp:537] Replica received write request for position 2 from (14366)@172.17.0.4:43972
I0405 17:29:19.828018 31857 master.cpp:1634] Recovered 0 agents from the Registry (131B) ; allowing 10mins for agents to re-register
I0405 17:29:19.828065 31861 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0405 17:29:19.865555 31867 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 37.822178ms
I0405 17:29:19.865661 31867 replica.cpp:712] Persisted action at 2
I0405 17:29:19.866921 31867 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0405 17:29:19.907341 31867 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 40.356649ms
I0405 17:29:19.907531 31867 leveldb.cpp:399] Deleting ~1 keys from leveldb took 91109ns
I0405 17:29:19.907560 31867 replica.cpp:712] Persisted action at 2
I0405 17:29:19.907599 31867 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0405 17:29:19.923305 31837 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:2048
Trying semicolon-delimited string format instead
I0405 17:29:19.926491 31837 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0405 17:29:19.927836 31837 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0405 17:29:19.932029 31862 slave.cpp:200] Agent started on 441)@172.17.0.4:43972
I0405 17:29:19.932086 31862 slave.cpp:201] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""[{""name"":""cpus"",""role"":""*"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""role"":""*"",""scalar"":{""value"":2048.0},""type"":""SCALAR""},{""name"":""disk"",""role"":""role1"",""scalar"":{""value"":4096.0},""type"":""SCALAR""}]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC""
I0405 17:29:19.932665 31862 credentials.hpp:86] Loading credential for authentication from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/credential'
I0405 17:29:19.932934 31862 slave.cpp:338] Agent using credential for: test-principal
I0405 17:29:19.932968 31862 credentials.hpp:37] Loading credentials for authentication from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/http_credentials'
I0405 17:29:19.933284 31862 slave.cpp:390] Using default 'basic' HTTP authenticator
I0405 17:29:19.934916 31837 sched.cpp:222] Version: 0.29.0
I0405 17:29:19.935566 31862 slave.cpp:589] Agent resources: cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000]
I0405 17:29:19.935664 31862 slave.cpp:597] Agent attributes: [  ]
I0405 17:29:19.935679 31862 slave.cpp:602] Agent hostname: 4090d10eba90
I0405 17:29:19.938390 31864 state.cpp:57] Recovering state from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/meta'
I0405 17:29:19.940608 31869 sched.cpp:326] New master detected at master@172.17.0.4:43972
I0405 17:29:19.940749 31869 sched.cpp:382] Authenticating with master master@172.17.0.4:43972
I0405 17:29:19.940773 31869 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0405 17:29:19.942371 31869 authenticatee.cpp:121] Creating new client SASL connection
I0405 17:29:19.942873 31859 master.cpp:5679] Authenticating scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:19.943156 31859 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(896)@172.17.0.4:43972
I0405 17:29:19.943507 31863 authenticator.cpp:98] Creating new server SASL connection
I0405 17:29:19.943740 31859 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0405 17:29:19.943783 31859 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0405 17:29:19.943892 31859 authenticator.cpp:203] Received SASL authentication start
I0405 17:29:19.943977 31859 authenticator.cpp:325] Authentication requires more steps
I0405 17:29:19.944066 31859 authenticatee.cpp:258] Received SASL authentication step
I0405 17:29:19.944164 31859 authenticator.cpp:231] Received SASL authentication step
I0405 17:29:19.944193 31859 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0405 17:29:19.944206 31859 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0405 17:29:19.944268 31859 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0405 17:29:19.944300 31859 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0405 17:29:19.944313 31859 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.944321 31859 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.944339 31859 authenticator.cpp:317] Authentication success
I0405 17:29:19.944541 31859 authenticatee.cpp:298] Authentication success
I0405 17:29:19.944655 31859 master.cpp:5709] Successfully authenticated principal 'test-principal' at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:19.944737 31859 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(896)@172.17.0.4:43972
I0405 17:29:19.945111 31859 sched.cpp:472] Successfully authenticated with master master@172.17.0.4:43972
I0405 17:29:19.945132 31859 sched.cpp:777] Sending SUBSCRIBE call to master@172.17.0.4:43972
I0405 17:29:19.945591 31859 sched.cpp:810] Will retry registration in 372.80738ms if necessary
I0405 17:29:19.945744 31865 master.cpp:2346] Received SUBSCRIBE call for framework 'default' at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:19.945838 31865 master.cpp:1865] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0405 17:29:19.946194 31865 master.cpp:2417] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0405 17:29:19.946866 31866 hierarchical.cpp:266] Added framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:19.946974 31866 hierarchical.cpp:1490] No resources available to allocate!
I0405 17:29:19.947010 31866 hierarchical.cpp:1585] No inverse offers to send out!
I0405 17:29:19.947054 31865 sched.cpp:704] Framework registered with 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:19.947074 31866 hierarchical.cpp:1141] Performed allocation for 0 agents in 178242ns
I0405 17:29:19.947124 31865 sched.cpp:718] Scheduler::registered took 38907ns
I0405 17:29:19.948712 31866 status_update_manager.cpp:200] Recovering status update manager
I0405 17:29:19.948901 31866 containerizer.cpp:416] Recovering containerizer
I0405 17:29:19.951021 31866 provisioner.cpp:245] Provisioner recovery complete
I0405 17:29:19.951802 31866 slave.cpp:4773] Finished recovery
I0405 17:29:19.952518 31866 slave.cpp:4945] Querying resource estimator for oversubscribable resources
I0405 17:29:19.953248 31866 slave.cpp:928] New master detected at master@172.17.0.4:43972
I0405 17:29:19.953305 31865 status_update_manager.cpp:174] Pausing sending status updates
I0405 17:29:19.953626 31866 slave.cpp:991] Authenticating with master master@172.17.0.4:43972
I0405 17:29:19.953716 31866 slave.cpp:996] Using default CRAM-MD5 authenticatee
I0405 17:29:19.954074 31866 slave.cpp:964] Detecting new master
I0405 17:29:19.954167 31861 authenticatee.cpp:121] Creating new client SASL connection
I0405 17:29:19.954372 31866 slave.cpp:4959] Received oversubscribable resources  from the resource estimator
I0405 17:29:19.954756 31866 master.cpp:5679] Authenticating slave(441)@172.17.0.4:43972
I0405 17:29:19.954944 31861 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(897)@172.17.0.4:43972
I0405 17:29:19.955368 31863 authenticator.cpp:98] Creating new server SASL connection
I0405 17:29:19.955687 31861 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0405 17:29:19.955801 31861 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0405 17:29:19.956075 31861 authenticator.cpp:203] Received SASL authentication start
I0405 17:29:19.956279 31861 authenticator.cpp:325] Authentication requires more steps
I0405 17:29:19.956455 31861 authenticatee.cpp:258] Received SASL authentication step
I0405 17:29:19.956676 31861 authenticator.cpp:231] Received SASL authentication step
I0405 17:29:19.956815 31861 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0405 17:29:19.956907 31861 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0405 17:29:19.957044 31861 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0405 17:29:19.957166 31861 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0405 17:29:19.957264 31861 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.957353 31861 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.957449 31861 authenticator.cpp:317] Authentication success
I0405 17:29:19.957664 31857 authenticatee.cpp:298] Authentication success
I0405 17:29:19.957813 31857 master.cpp:5709] Successfully authenticated principal 'test-principal' at slave(441)@172.17.0.4:43972
I0405 17:29:19.958008 31861 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(897)@172.17.0.4:43972
I0405 17:29:19.958732 31857 slave.cpp:1061] Successfully authenticated with master master@172.17.0.4:43972
I0405 17:29:19.958930 31857 slave.cpp:1457] Will retry registration in 18.568334ms if necessary
I0405 17:29:19.959262 31857 master.cpp:4390] Registering agent at slave(441)@172.17.0.4:43972 (4090d10eba90) with id 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:19.959934 31857 registrar.cpp:463] Applied 1 operations in 99197ns; attempting to update the 'registry'
I0405 17:29:19.961587 31857 log.cpp:683] Attempting to append 343 bytes to the log
I0405 17:29:19.961879 31857 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0405 17:29:19.963135 31857 replica.cpp:537] Replica received write request for position 3 from (14381)@172.17.0.4:43972
I0405 17:29:19.999408 31857 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 36.200109ms
I0405 17:29:19.999512 31857 replica.cpp:712] Persisted action at 3
I0405 17:29:20.001049 31869 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0405 17:29:20.038849 31869 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 37.709507ms
I0405 17:29:20.038930 31869 replica.cpp:712] Persisted action at 3
I0405 17:29:20.038965 31869 replica.cpp:697] Replica learned APPEND action at position 3
I0405 17:29:20.041484 31869 registrar.cpp:508] Successfully updated the 'registry' in 0ns
I0405 17:29:20.041785 31869 log.cpp:702] Attempting to truncate the log to 3
I0405 17:29:20.042364 31859 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0405 17:29:20.043767 31859 replica.cpp:537] Replica received write request for position 4 from (14382)@172.17.0.4:43972
I0405 17:29:20.044585 31869 master.cpp:4458] Registered agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) with cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000]
I0405 17:29:20.044910 31864 slave.cpp:1105] Registered with master master@172.17.0.4:43972; given agent ID 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:20.045075 31864 fetcher.cpp:81] Clearing fetcher cache
I0405 17:29:20.045140 31870 hierarchical.cpp:476] Added agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 (4090d10eba90) with cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000] (allocated: )
I0405 17:29:20.045581 31864 slave.cpp:1128] Checkpointing SlaveInfo to '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/meta/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/slave.info'
I0405 17:29:20.045974 31864 slave.cpp:1165] Forwarding total oversubscribed resources 
I0405 17:29:20.046077 31864 slave.cpp:3664] Received ping from slave-observer(399)@172.17.0.4:43972
I0405 17:29:20.046193 31864 status_update_manager.cpp:181] Resuming sending status updates
I0405 17:29:20.046289 31870 hierarchical.cpp:1585] No inverse offers to send out!
I0405 17:29:20.046370 31870 hierarchical.cpp:1164] Performed allocation for agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 in 1.153377ms
I0405 17:29:20.046499 31864 master.cpp:4802] Received update of agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) with total oversubscribed resources 
I0405 17:29:20.047142 31868 hierarchical.cpp:534] Agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 (4090d10eba90) updated with oversubscribed resources  (total: cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000], allocated: disk(role1):4096; cpus(*):2; mem(*):2048; ports(*):[31000-32000])
I0405 17:29:20.047960 31868 hierarchical.cpp:1490] No resources available to allocate!
I0405 17:29:20.048009 31868 hierarchical.cpp:1585] No inverse offers to send out!
I0405 17:29:20.048065 31868 hierarchical.cpp:1164] Performed allocation for agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 in 866803ns
I0405 17:29:20.048591 31864 master.cpp:5508] Sending 1 offers to framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:20.049188 31860 sched.cpp:874] Scheduler::resourceOffers took 114867ns
I0405 17:29:20.080921 31859 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 37.025538ms
I0405 17:29:20.081001 31859 replica.cpp:712] Persisted action at 4
I0405 17:29:20.082425 31859 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0405 17:29:20.106056 31859 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 23.583037ms
I0405 17:29:20.106205 31859 leveldb.cpp:399] Deleting ~2 keys from leveldb took 76995ns
I0405 17:29:20.106240 31859 replica.cpp:712] Persisted action at 4
I0405 17:29:20.106278 31859 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0405 17:29:20.119488 31837 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:128
Trying semicolon-delimited string format instead
I0405 17:29:20.121356 31859 master.cpp:3288] Processing ACCEPT call for offers: [ 9565ff6f-f1b6-4259-8430-690e635c391f-O0 ] on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:20.121485 31859 master.cpp:3046] Authorizing principal 'test-principal' to create volumes
I0405 17:29:20.121692 31859 master.cpp:2891] Authorizing framework principal 'test-principal' to launch task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 as user 'mesos'
I0405 17:29:20.123877 31871 master.cpp:3617] Applying CREATE operation for volumes disk(role1)[id1:path1]:2048 from framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 to agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.125424 31871 master.cpp:6747] Sending checkpointed resources disk(role1)[id1:path1]:2048 to agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.126397 31856 hierarchical.cpp:656] Updated allocation of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 from disk(role1):4096; cpus(*):2; mem(*):2048; ports(*):[31000-32000] to disk(role1):2048; cpus(*):2; mem(*):2048; ports(*):[31000-32000]; disk(role1)[id1:path1]:2048
I0405 17:29:20.126667 31871 master.hpp:177] Adding task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 with resources cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 (4090d10eba90)
I0405 17:29:20.126875 31871 master.cpp:3773] Launching task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 with resources cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.127390 31856 slave.cpp:2523] Updated checkpointed resources from  to disk(role1)[id1:path1]:2048
I0405 17:29:20.127615 31856 slave.cpp:1497] Got assigned task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.127876 31856 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0405 17:29:20.127841 31871 hierarchical.cpp:893] Recovered disk(role1):2048; cpus(*):1; mem(*):1920; ports(*):[31000-32000] (total: cpus(*):2; mem(*):2048; disk(role1):2048; ports(*):[31000-32000]; disk(role1)[id1:path1]:2048, allocated: disk(role1)[id1:path1]:2048; cpus(*):1; mem(*):128) on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 from framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.127913 31871 hierarchical.cpp:930] Framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 filtered agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 for 5secs
I0405 17:29:20.128667 31856 slave.cpp:1616] Launching task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.128937 31856 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0405 17:29:20.129776 31856 paths.cpp:528] Trying to chown '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09' to user 'mesos'
I0405 17:29:20.145324 31856 slave.cpp:5575] Launching executor 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09'
I0405 17:29:20.146057 31858 containerizer.cpp:675] Starting container 'bc8b48e5-dd32-4283-a1a6-e1988c82ae09' for executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework '9565ff6f-f1b6-4259-8430-690e635c391f-0000'
I0405 17:29:20.146078 31856 slave.cpp:1834] Queuing task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' for executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.146203 31856 slave.cpp:881] Successfully attached file '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09'
I0405 17:29:20.147619 31859 posix.cpp:206] Changing the ownership of the persistent volume at '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/volumes/roles/role1/id1' with uid 1000 and gid 1000
I0405 17:29:20.162421 31859 posix.cpp:250] Adding symlink from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/volumes/roles/role1/id1' to '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09/path1' for persistent volume disk(role1)[id1:path1]:2048 of container bc8b48e5-dd32-4283-a1a6-e1988c82ae09
I0405 17:29:20.172133 31861 launcher.cpp:123] Forked child with pid '7927' for container 'bc8b48e5-dd32-4283-a1a6-e1988c82ae09'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0405 17:29:20.376197  7941 process.cpp:986] libprocess is initialized on 172.17.0.4:50952 with 16 worker threads
I0405 17:29:20.378132  7941 logging.cpp:195] Logging to STDERR
I0405 17:29:20.380861  7941 exec.cpp:150] Version: 0.29.0
I0405 17:29:20.396257  7966 exec.cpp:200] Executor started at: executor(1)@172.17.0.4:50952 with pid 7941
I0405 17:29:20.399426 31860 slave.cpp:2825] Got registration for executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from executor(1)@172.17.0.4:50952
I0405 17:29:20.402995  7966 exec.cpp:225] Executor registered on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:20.403014 31860 slave.cpp:1999] Sending queued task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' to executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 at executor(1)@172.17.0.4:50952
I0405 17:29:20.405624  7966 exec.cpp:237] Executor::registered took 393272ns
I0405 17:29:20.406108  7966 exec.cpp:312] Executor asked to run task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542'
Registered executor on 4090d10eba90
I0405 17:29:20.406708  7966 exec.cpp:321] Executor::launchTask took 568039ns
Starting task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542
Forked command at 7972
sh -c 'echo abc > path1/file'
I0405 17:29:20.411375  7966 exec.cpp:535] Executor sending status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.413156 31857 slave.cpp:3184] Handling status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from executor(1)@172.17.0.4:50952
I0405 17:29:20.415714 31857 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.415788 31857 status_update_manager.cpp:497] Creating StatusUpdate stream for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.416345 31857 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to the agent
I0405 17:29:20.416720 31870 slave.cpp:3582] Forwarding the update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to master@172.17.0.4:43972
I0405 17:29:20.416954 31870 slave.cpp:3476] Status update manager successfully handled status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.416997 31870 slave.cpp:3492] Sending acknowledgement for status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to executor(1)@172.17.0.4:50952
I0405 17:29:20.417505 31870 master.cpp:4947] Status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.417549 31870 master.cpp:4995] Forwarding status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.417724 31870 master.cpp:6608] Updating the state of task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0405 17:29:20.417943  7960 exec.cpp:358] Executor received status update acknowledgement cf4f8fe9-44f2-43ce-8868-b3a09b7298cf for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.418002 31870 sched.cpp:982] Scheduler::statusUpdate took 105225ns
I0405 17:29:20.418623 31870 master.cpp:4102] Processing ACKNOWLEDGE call cf4f8fe9-44f2-43ce-8868-b3a09b7298cf for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:20.419181 31860 status_update_manager.cpp:392] Received status update acknowledgement (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.419816 31860 slave.cpp:2594] Status update manager successfully handled status update acknowledgement (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.513465  7969 exec.cpp:535] Executor sending status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
Command exited with status 0 (pid: 7972)
I0405 17:29:20.515449 31870 slave.cpp:3184] Handling status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from executor(1)@172.17.0.4:50952
I0405 17:29:20.516875 31860 slave.cpp:5885] Terminating task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542
I0405 17:29:20.517496 31867 posix.cpp:156] Removing symlink '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09/path1' for persistent volume disk(role1)[id1:path1]:2048 of container bc8b48e5-dd32-4283-a1a6-e1988c82ae09
I0405 17:29:20.519361 31864 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.519850 31864 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to the agent
I0405 17:29:20.520678 31870 slave.cpp:3582] Forwarding the update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to master@172.17.0.4:43972
I0405 17:29:20.520901 31870 slave.cpp:3476] Status update manager successfully handled status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.520949 31870 slave.cpp:3492] Sending acknowledgement for status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to executor(1)@172.17.0.4:50952
I0405 17:29:20.521550 31864 master.cpp:4947] Status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.521610 31864 master.cpp:4995] Forwarding status update TASK_FINISHED (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.522099 31871 sched.cpp:982] Scheduler::statusUpdate took 102502ns
I0405 17:29:20.522367 31864 master.cpp:6608] Updating the state of task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0405 17:29:20.524288 31871 hierarchical.cpp:1676] Filtered offer with disk(role1):2048; cpus(*):1; mem(*):1920; ports(*):[31000-32000] on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.524379 31871 hierarchical.cpp:1490] No resources available to allocate!
I0405 17:29:20.524451 31871 hierarchical.cpp:1585] No inverse offers to send out!
I0405 17:29:20.524551 31871 hierarchical.cpp:1141] Performed allocation for 1 agents in 961746ns
I0405 17:29:20.525182 31858 hierarchical.cpp:893] Recovered cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 (total: cpus(*):2; mem(*):2048; disk(role1):2048; ports(*):[31000-32000]; disk(role1)[id1:path1]:2048, allocated: ) on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 from framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.525197 31864 master.cpp:4102] Processing ACKNOWLEDGE call 128eb7af-a662-4cbb-9401-125dca38f719 for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:20.525380 31864 master.cpp:6674] Removing task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 with resources cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.526067 31864 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.526425 31864 status_update_manager.cpp:528] Cleaning up status update stream for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.526917 31864 slave.cpp:2594] Status update manager successfully handled status update acknowledgement (UUID: 128eb7af-a662-4cbb-9401-125dca38f719) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.527048 31864 slave.cpp:5926] Completing task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542
I0405 17:29:20.527732  7964 exec.cpp:358] Executor received status update acknowledgement 128eb7af-a662-4cbb-9401-125dca38f719 for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:21.527920 31859 slave.cpp:3710] executor(1)@172.17.0.4:50952 exited
../../src/tests/persistent_volume_tests.cpp:825: Failure
Failed to wait 15secs for offers
I0405 17:29:35.542609 31856 master.cpp:1269] Framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 disconnected
I0405 17:29:35.542811 31856 master.cpp:2642] Disconnecting framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:35.542994 31856 master.cpp:2666] Deactivating framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:35.543349 31860 hierarchical.cpp:378] Deactivated framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:35.543501 31856 master.cpp:1293] Giving framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 0ns to failover
I0405 17:29:35.543903 31868 master.cpp:5360] Framework failover timeout, removing framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:35.543936 31868 master.cpp:6093] Removing framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:35.544337 31861 slave.cpp:2215] Asked to shut down framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 by master@172.17.0.4:43972
I0405 17:29:35.544381 31861 slave.cpp:2240] Shutting down framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:35.544456 31861 slave.cpp:4398] Shutting down executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 at executor(1)@172.17.0.4:50952
I0405 17:29:35.544960 31872 poll_socket.cpp:110] Socket error while connecting
I0405 17:29:35.545013 31872 process.cpp:1650] Failed to send 'mesos.internal.ShutdownExecutorMessage' to '172.17.0.4:50952', connect: Socket error while connecting
E0405 17:29:35.545106 31872 process.cpp:1958] Failed to shutdown socket with fd 27: Transport endpoint is not connected
I0405 17:29:35.545474 31864 hierarchical.cpp:329] Removed framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
../../src/tests/persistent_volume_tests.cpp:819: Failure
Actual function call count doesn't match EXPECT_CALL(sched, resourceOffers(&driver, _))...
         Expected: to be called at least once
           Actual: never called - unsatisfied and active
I0405 17:29:35.558538 31858 containerizer.cpp:1432] Destroying container 'bc8b48e5-dd32-4283-a1a6-e1988c82ae09'
../../src/tests/cluster.cpp:453: Failure
Failed to wait 15secs for wait
I0405 17:29:50.565403 31870 slave.cpp:800] Agent terminating
I0405 17:29:50.565512 31870 slave.cpp:2215] Asked to shut down framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 by @0.0.0.0:0
W0405 17:29:50.565544 31870 slave.cpp:2236] Ignoring shutdown framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 because it is terminating
I0405 17:29:50.574620 31866 master.cpp:1230] Agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) disconnected
I0405 17:29:50.574766 31866 master.cpp:2701] Disconnecting agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:50.575003 31866 master.cpp:2720] Deactivating agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:50.575294 31865 hierarchical.cpp:563] Agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 deactivated
I0405 17:29:50.605787 31837 master.cpp:1083] Master terminating
I0405 17:29:50.606533 31866 hierarchical.cpp:508] Removed agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0
[  FAILED  ] DiskResource/PersistentVolumeTest.AccessPersistentVolume/0, where GetParam() = 0 (31491 ms)
{code}","Ubuntu 14.04 using gcc, with libevent and SSL enabled",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-06 16:32:41.171,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 06 23:21:07 UTC 2016,,,,,,,"0|i2vpaf:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 32,,,,,,,,,,,3.0,,,,,,,,,,,"06/Apr/16 16:32;neilc;Hmm -- this doesn't repro for me in several thousand test iterations on either OSX or a Linux VM. From looking at the logs, it seems the problem is that we expect the following sequence to be valid:

* launch task
* wait for TASK_RUNNING status update to reach scheduler
* wait for TASK_FINISHED status update to reach scheduler
* {{Clock::advance(allocation_interval)}}
* wait for resource offer

When the test case fails, it seems to be because the schedule's {{ACKNOWLEDGE}} of the {{TASK_FINISHED}} hasn't reached the master yet, so the master thinks the resources used by the task aren't available to be offered.","06/Apr/16 16:36;neilc;https://reviews.apache.org/r/45817/
https://reviews.apache.org/r/45818/","06/Apr/16 23:21;jieyu;commit b1dbc63f374174a4b0bb418e1fa21fa76366dffc
Author: Neil Conway <neil.conway@gmail.com>
Date:   Wed Apr 6 16:20:29 2016 -0700

    Fixed flakiness in PersistentVolumeTest.AccessPersistentVolume.
    
    Review: https://reviews.apache.org/r/45818/

commit 365c326f7aa21606af1e6bb18a44e19073fed5d9
Author: Neil Conway <neil.conway@gmail.com>
Date:   Wed Apr 6 16:20:25 2016 -0700

    Cleaned up some comments.
    
    Review: https://reviews.apache.org/r/45817/",,,,,,,,,,,,,,,,,,,,,,,,,
Reset `LIBPROCESS_IP` in `network\cni` isolator.,MESOS-5127,12956379,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,avinash.mesos,avinash.mesos,avinash.mesos,06/Apr/16 04:10,31/May/16 02:12,29/Oct/20 16:32,06/Apr/16 18:42,,,,,,,,,1.0.0,,,,,,containerization,,,,,0,,,,,,,,,"Currently the `LIBPROCESS_IP` environment variable was being set to
    the Agent IP if the environment variable has not be defined by the
    `Framework`. For containers having their own IP address (as with
    containers on CNI networks) this becomes a problem since the command
    executor tries to bind to the `LIBPROCESS_IP` that does not exist in
    its network namespace, and fails. Thus, for containers launched on CNI
    networks the `LIBPROCESS_IP` should not be set, or rather is set to
    ""0.0.0.0"", allowing the container to bind to the IP address provided
    by the CNI network.",Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-06 05:45:02.375,,,false,MESOS-4641,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 06 18:42:04 UTC 2016,,,,,,,"0|i2vp93:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 32,,,,,,,,,,,1.0,,,,,,,,,,,"06/Apr/16 05:45;gilbert;Do we have the same issue in custom executor? Since we have a new network namespace created by CNI isolator, it seems to me that setting the `LIBPROCESS_IP` as 0.0.0.0 should also fix the same case in custom executor.","06/Apr/16 15:50;avinash.mesos;https://reviews.apache.org/r/45801/","06/Apr/16 18:42;jieyu;commit c5b5084894b36c64f89337a6b7db9e6ffe62013b
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Wed Apr 6 11:40:53 2016 -0700

    Reset the `LIBPROCESS_IP` environment variable in `network/cni`.
    
    Currently the `LIBPROCESS_IP` environment variable was being set to the
    Agent IP if the environment variable has not be defined by the
    `Framework`. For containers having their own IP address (as with
    containers on CNI networks) this becomes a problem since the command
    executor tries to bind to the `LIBPROCESS_IP` that does not exist in its
    network namespace, and fails. Thus, for containers launched on CNI
    networks the `LIBPROCESS_IP` should not be set, or rather is set to
    ""0.0.0.0"", allowing the container to bind to the IP address provided by
    the CNI network.
    
    Review: https://reviews.apache.org/r/45801/",,,,,,,,,,,,,,,,,,,,,,,,,
pivot_root is not available on PowerPC,MESOS-5121,12956073,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,chenzhiwei,chenzhiwei,chenzhiwei,05/Apr/16 07:00,31/May/16 02:12,29/Oct/20 16:32,05/Apr/16 17:08,,,,,,,,,1.0.0,,,,,,,,,,,0,,,,,,,,,"When compile on ppc64le, it will through error message: src/linux/fs.cpp:443:2: error: #error ""pivot_root is not available""

The current code logic in src/linux/fs.cpp is:

{code}
#ifdef __NR_pivot_root
  int ret = ::syscall(__NR_pivot_root, newRoot.c_str(), putOld.c_str());
#elif __x86_64__
  // A workaround for systems that have an old glib but have a new
  // kernel. The magic number '155' is the syscall number for
  // 'pivot_root' on the x86_64 architecture, see
  // arch/x86/syscalls/syscall_64.tbl
  int ret = ::syscall(155, newRoot.c_str(), putOld.c_str());
#else
#error ""pivot_root is not available""
#endif
{code}

There is no old glib version and the new kernel version, it will never run code in *#ifdef __NR_pivot_root* condition, and when I build on Ubuntu 16.04(It has the latest linux kernel and glibc), it still can't step into the *#ifdef __NR_pivot_root* condition.

For powerpc case, I added another condition:

{code}
#elif __powerpc__ || __ppc__ || __powerpc64__ || __ppc64__
  // A workaround for powerpc. The magic number '203' is the syscall
  // number for 'pivot_root' on the powerpc architecture, see
  // https://w3challs.com/syscalls/?arch=powerpc_64
  int ret = ::syscall(203, newRoot.c_str(), putOld.c_str());
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-05 17:08:21.96,,,false,MESOS-4312,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 24 10:18:14 UTC 2016,,,,,,,"0|i2vndb:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 32,,,,,,,,,,,1.0,,,,,,,,,,,"05/Apr/16 09:25;chenzhiwei;Review URL: https://reviews.apache.org/r/45734/","05/Apr/16 17:08;vinodkone;commit 7c6fb37280b81e6cf63878efec37b45fb27e4760
Author: Zhiwei Chen <zhiweik@gmail.com>
Date:   Tue Apr 5 10:07:05 2016 -0700

    Fix 'pivot_root is not available' error on powerpc platform.
    
    Review: https://reviews.apache.org/r/45734/
","24/Apr/16 10:18;haosdent@gmail.com;I take a look at [~RobinDong]'s [patch | https://reviews.apache.org/r/34882/diff/1#index_header] for arm64 today. I think the better way to make pivot_root works in different platforms are add this line from his patch

{code}
#include <linux/unistd.h>
{code}

This header file contains the define for {{__NR_pivot_root}}

cc [~janisz]",,,,,,,,,,,,,,,,,,,,,,,,,
Grant access to /dev/nvidiactl and /dev/nvidia-uvm in the Nvidia GPU isolator.,MESOS-5115,12955958,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,klueska,klueska,klueska,04/Apr/16 22:17,26/Apr/17 16:54,29/Oct/20 16:32,06/Apr/16 00:47,,,,,,,,,1.0.0,,,,,,containerization,,,,,0,gpu,,,,,,,," Calls to 'nvidia-smi'  fail inside a container even if access to a GPU has been granted. Moreover, access to /dev/nvidiactl is actually required for a container to do anything useful with a GPU even if it has access to it.
    
We should grant/revoke access to /dev/nvidiactl and /dev/nvidia-uvm as GPUs are added and removed from a container in the Nvidia GPU isolator.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-06 00:47:56.186,,,false,MESOS-4424,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 06 00:47:56 UTC 2016,,,,,,,"0|i2vmnr:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 32,,,,,,,,,,,2.0,,,,,,,,,,,"05/Apr/16 01:24;klueska;https://reviews.apache.org/r/45715/","06/Apr/16 00:47;bmahler;{noformat}
commit 6579bb62d23d55b5e241b6d2c8958cdc26746f47
Author: Kevin Klues <klueska@gmail.com>
Date:   Tue Apr 5 17:38:34 2016 -0700

    Fixed access to /dev/nvidia{ctl,-uvm} in Nvidia GPU isolator.

    Previously, calls to 'nvidia-smi' would fail inside a container even
    if access to a GPU had been granted. Moreover, access to
    /dev/nvidiactl is actually required for a container to do anything
    useful with a GPU even if it has access to it.

    This patch explicitly grants/revokes access to /dev/nvidiactl and
    /dev/nvidia-uvm as GPUs are added and removed from a container in
    the Nvidia GPU isolator.

    Review: https://reviews.apache.org/r/45715/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
`network/cni` isolator crashes when launched without the --network_cni_plugins_dir flag,MESOS-5113,12955909,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,avinash.mesos,avinash.mesos,avinash.mesos,04/Apr/16 20:19,31/May/16 02:11,29/Oct/20 16:32,05/Apr/16 17:35,,,,,,,,,1.0.0,,,,,,containerization,,,,,0,,,,,,,,,"If we start the agent with the --isolation='network/cni' but do not specify the --network_cni_plugins_dir flag, the agent crashes with the following stack dump:
0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
56      ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) bt
#0  0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007ffff23280d8 in __GI_abort () at abort.c:89
#2  0x00007ffff231db86 in __assert_fail_base (fmt=0x7ffff246e830 ""%s%s%s:%u: %s%sAssertion `%s' failed.\n%n"", assertion=assertion@entry=0x451f5c ""isSome()"",
    file=file@entry=0x451f65 ""../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"", line=line@entry=111,
    function=function@entry=0x45294a ""const T &Option<std::basic_string<char> >::get() const & [T = std::basic_string<char>]"") at assert.c:92
#3  0x00007ffff231dc32 in __GI___assert_fail (assertion=0x451f5c ""isSome()"", file=0x451f65 ""../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"", line=111,
    function=0x45294a ""const T &Option<std::basic_string<char> >::get() const & [T = std::basic_string<char>]"") at assert.c:101
#4  0x0000000000432c0d in Option<std::string>::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111
Python Exception <class 'IndexError'> list index out of range:
#5  0x00007ffff63ef7cc in mesos::internal::slave::NetworkCniIsolatorProcess::recover (this=0x6c1e70, states=empty std::list, orphans=...) at ../../src/slave/containerizer/mesos/isolators/network/cni/cni.cpp:331
#6  0x00007ffff60cddd8 in operator() (this=0x7fffc0001e00, process=0x6c1ef8) at ../../3rdparty/libprocess/include/process/dispatch.hpp:239
#7  0x00007ffff60cd972 in std::_Function_handler<void (process::ProcessBase*), process::Future<Nothing> process::dispatch<Nothing, mesos::internal::slave::MesosIsolatorProcess, std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> > const&, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > const&, std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> >, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > >(process::PID<mesos::internal::slave::MesosIsolatorProcess> const&, process::Future<Nothing> (mesos::internal::slave::MesosIsolatorProcess::*)(std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> > const&, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > const&), std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> >, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> >)::{lambda(process::ProcessBase*)#1}>::_M_invoke(std::_Any_data const&, process::ProcessBase*) (__functor=..., __args=0x6c1ef8) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2071
#8  0x00007ffff6a6bf38 in std::function<void (process::ProcessBase*)>::operator()(process::ProcessBase*) const (this=0x7fffc0001d70, __args=0x6c1ef8)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2471
#9  0x00007ffff6a561b4 in process::ProcessBase::visit (this=0x6c1ef8, event=...) at ../../../3rdparty/libprocess/src/process.cpp:3130
#10 0x00007ffff6aac5fe in process::DispatchEvent::visit (this=0x7fffc0001570, visitor=0x6c1ef8) at ../../../3rdparty/libprocess/include/process/event.hpp:161
#11 0x00007ffff55e9c91 in process::ProcessBase::serve (this=0x6c1ef8, event=...) at ../../3rdparty/libprocess/include/process/process.hpp:82
#12 0x00007ffff6a53ed4 in process::ProcessManager::resume (this=0x67cca0, process=0x6c1ef8) at ../../../3rdparty/libprocess/src/process.cpp:2570
#13 0x00007ffff6a5bff5 in operator() (this=0x697d70, joining=...) at ../../../3rdparty/libprocess/src/process.cpp:2218
#14 0x00007ffff6a5bf33 in std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)>::__call<void, , 0ul>(std::tuple<>&&, std::_Index_tuple<0ul>) (this=0x697d70,
    __args=<unknown type in /home/vagrant/mesosphere/mesos/build/src/.libs/libmesos-0.29.0.so, CU 0x45bb552, DIE 0x469efe5>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1295
#15 0x00007ffff6a5bee6 in std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)>::operator()<, void>() (this=0x697d70)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1353
#16 0x00007ffff6a5be95 in std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()>::_M_invoke<>(std::_Index_tuple<>) (this=0x697d70)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1731
#17 0x00007ffff6a5be65 in std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()>::operator()() (this=0x697d70)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1720
#18 0x00007ffff6a5be3c in std::thread::_Impl<std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()> >::_M_run() (this=0x697d58)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/thread:115
#19 0x00007ffff2b98a60 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#20 0x00007ffff26bb182 in start_thread (arg=0x7fffeb92d700) at pthread_create.c:312
#21 0x00007ffff23e847d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
(gdb) frame 4
#4  0x0000000000432c0d in Option<std::string>::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111",linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-05 17:35:17.161,,,false,MESOS-4641,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 05 17:35:17 UTC 2016,,,,,,,"0|i2vmd3:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 32,,,,,,,,,,,1.0,,,,,,,,,,,"04/Apr/16 20:21;avinash.mesos;We are seeing this crash because the `MesosContainerizer` calls recover on all isolators on startup. The `recover` method blindly tries using the `rootDir` without verify if it is set or not. The `rootDir` and `pluginDir` are not set in case the operator does not specify the --network_cni_plugins_dir flag, so we need to check the values of these ""optional"" variables before using this in the `network/cni` isolator. ","05/Apr/16 17:35;jieyu;commit 29dc72c10f71f8548e63684e6f22fa970450a648
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Tue Apr 5 10:34:40 2016 -0700

    Added checks to verify that `rootDir` and `pluginDir` `isSome.
    
    `rootDir` and `pluginDir` can be `None` in case the operator does not
    specify the network cni flags. In this case invoking `recover` on the
    isolator without checking the `rootDir` and `pluginDir` for `isSome`
    will cause the Agent to crash.
    
    Review: https://reviews.apache.org/r/45717/",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix a bug in the Nvidia GPU device isolator that exposes a discrepancy between clang and gcc in 'using' declarations,MESOS-5082,12955543,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,klueska,klueska,klueska,02/Apr/16 21:32,31/May/16 02:12,29/Oct/20 16:32,05/Apr/16 22:21,,,,,,,,,1.0.0,,,,,,,,,,,0,,,,,,,,,"There appears to be a discrepancy between clang and gcc, which allows
clang to accept `using` declarations of the form `using ns_name::name;`
that contain nested classes, structs, and enums after the `name` field
in the declaration (e.g. `using ns_name::name::enum;`).

The language for describing this functionality is ambiguous in the
C++11 specification as referenced here:
http://en.cppreference.com/w/cpp/language/namespace#Using-declarations",gcc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-05 22:21:36.207,,,false,MESOS-4424,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 05 22:21:36 UTC 2016,,,,,,,"0|i2vk3r:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 32,,,,,,,,,,,1.0,,,,,,,,,,,"04/Apr/16 22:04;klueska;https://issues.apache.org/jira/browse/MESOS-5082","05/Apr/16 22:21;bmahler;{noformat}
commit 8cbc04a03d6f7f16627990eb936a770aaeffaddb
Author: Kevin Klues <klueska@gmail.com>
Date:   Tue Apr 5 15:18:15 2016 -0700

    Fixed Nvidia GPU isolator build for gcc.

    There appears to be a discrepancy between clang and gcc, which allows
    clang to accept `using` declarations of the form `using ns_name::name;`
    that contain nested classes, structs, and enums after the `name` field
    in the declaration (e.g. `using ns_name::name::enum;`).

    The language for describing this functionality is ambiguous in the
    C++11 specification as referenced here:
    http://en.cppreference.com/w/cpp/language/namespace#Using-declarations

    This patch fixes a bug where we relied on this functionality in clang
    and had build errors in gcc.

    Review: https://reviews.apache.org/r/45622/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Expose per-role dominant share,MESOS-5058,12954230,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Duplicate,bbannier,bbannier,bbannier,29/Mar/16 08:32,26/Nov/18 09:21,29/Oct/20 16:32,26/Nov/18 09:21,,,,,,,,,,,,,,,allocation,,,,,0,mesosphere,,,,,,,,"A client's dominant share is crucial measure for how likely it is to receive offers in the future. We should expose it in a dedicated allocator metric.

As currently the {{HierarchicalAllocatorProcess}} does work with generic {{Sorters}} which have no notion of DRF share we need to decide whether and where we would need to limit generality in order to expose the innards of the currently used {{DRFSorter}}.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4724,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-4664,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 26 09:21:29 UTC 2018,,,,,,,"0|i2vbzz:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 32,,,,,,,,,,,2.0,,,,,,,,,,,"26/Nov/18 09:21;bbannier;Closing this again. [~mesosphere], why did you reopen this issue?",,,,,,,,,,,,,,,,,,,,,,,,,,,
Authorization Action enum does not support upgrades.,MESOS-5031,12953530,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,yongtang,adam-mesos,adam-mesos,25/Mar/16 07:57,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,1.0.0,,,,,,,,1.0.0,,,,,,,,,,,0,mesosphere,security,,,,,,,"We need to make the Action enum optional in authorization::Request, and add an `UNKNOWN = 0;` enum value. See MESOS-4997 for details.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-25 20:14:14.462,,,false,MESOS-4997,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 22 21:02:19 UTC 2016,,,,,,,"0|hzzzy6:tzm",9223372036854775807,,,,,adam-mesos,,,,,,Mesosphere Sprint 32,,,,,,,,,,,2.0,,,,,,,,,,,"25/Mar/16 20:14;yongtang;Hi [~adam-mesos] I just added a review request:
https://reviews.apache.org/r/45342/
Let me know if there are any issues.","30/Mar/16 20:54;adam-mesos;commit 5a4680d1b9abda1bbcfea497ffe95af16dd71387
Author: Yong Tang <yong.tang.github@outlook.com>
Date:   Wed Mar 30 04:22:29 2016 -0700

    Made the Action enum optional to support upgrades (MESOS-5031).
    
    This fix makes the Action enum in Authorization optional to support
    upgrades. See MESOS-4997 for details.
    
    Review: https://reviews.apache.org/r/45342/","19/Apr/16 00:06;bmahler;[~adam-mesos] [~yongtang] see my comment in the review on preferring an explicit case statement in favor of using 'default': https://reviews.apache.org/r/45342/

See context in MESOS-2664 and MESOS-3754.","19/Apr/16 03:30;yongtang;Hi [~bmahler] I just created a pull request to address the ""default"" issue:

https://reviews.apache.org/r/46364/

Please let me know if there are any issues.

cc [~vinodkone] [~adam-mesos]

","22/Apr/16 21:02;vinodkone;commit 60dcd72c372b393f06f8917266176f9c86af186b
Author: Yong Tang <yong.tang.github@outlook.com>
Date:   Fri Apr 22 13:54:28 2016 -0700

    Fixed handling of UNKNOWN authorization action.
    
    This patch is related to:
    https://reviews.apache.org/r/45342/ (MESOS-5031)
    
    Review: https://reviews.apache.org/r/46470/
",,,,,,,,,,,,,,,,,,,,,,,
Copy provisioner cannot replace directory with symlink,MESOS-5028,12953371,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,chhsia0,zhitao,zhitao,24/Mar/16 20:19,20/Nov/17 09:25,29/Oct/20 16:32,18/Apr/17 06:31,,,,,,,,,1.1.2,1.2.1,1.3.0,,,,containerization,,,,,0,,,,,,,,,"I'm trying to play with the new image provisioner on our custom docker images, but one of layer failed to get copied, possibly due to a dangling symlink.

Error log with Glog_v=1:

{quote}
I0324 05:42:48.926678 15067 copy.cpp:127] Copying layer path '/tmp/mesos/store/docker/layers/5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1/rootfs' to rootfs '/var/lib/mesos/provisioner/containers/5f05be6c-c970-4539-aa64-fd0eef2ec7ae/backends/copy/rootfses/507173f3-e316-48a3-a96e-5fdea9ffe9f6'
E0324 05:42:49.028506 15062 slave.cpp:3773] Container '5f05be6c-c970-4539-aa64-fd0eef2ec7ae' for executor 'test' of framework 75932a89-1514-4011-bafe-beb6a208bb2d-0004 failed to start: Collect failed: Collect failed: Failed to copy layer: cp: cannot overwrite directory ‘/var/lib/mesos/provisioner/containers/5f05be6c-c970-4539-aa64-fd0eef2ec7ae/backends/copy/rootfses/507173f3-e316-48a3-a96e-5fdea9ffe9f6/etc/apt’ with non-directory
{quote}

Content of _/tmp/mesos/store/docker/layers/5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1/rootfs/etc/apt_ points to a non-existing absolute path (cannot provide exact path but it's a result of us trying to mount apt keys into docker container at build time).

I believe what happened is that we executed a script at build time, which contains equivalent of:
{quote}
rm -rf /etc/apt/* && ln -sf /build-mount-point/ /etc/apt
{quote}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-25 15:57:19.133,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 20 09:25:38 UTC 2017,,,,,,,"0|hzzzhy:v",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 32,Mesosphere Sprint 33,,,,,,,,,,3.0,,,,,,,,,,,"25/Mar/16 00:26;zhitao;[~gilbert] and I took a quick, and this is actually caused by the layer trying to replace a directory with a symlink, which is not allowed by `cp -aT` (sorry my previous description was a bit misleading).","25/Mar/16 15:57;gilbert;[~zhitao], thanks for investigating together. :)

Place a note here. The failure comes from cping a symlink to a directory. This situation is possible for some images. I would create a similar image tarball to test/fix it. ","02/Jun/16 15:00;zhitao;Hi [~gilbert] and [~jieyu], any update on this one?","21/Jun/16 22:09;gilbert;Hey [~zhitao], sorry for the delay. I am trying to reproduce this issue. Wish to unblock you asap, but need some input from you:

1. Could you verify {rm -rf /etc/apt/* && ln -sf /build-mount-point/ /etc/apt} is the equivalent command?

2. I used a Dockerfile to generate an image for testing, but seems like it does not match the image you build from a script. Could you take a look at repo:
{noformat}
gilbertsong/alpine:latest
{noformat}

BTW, this is the very simple Dockerfile I used to build it:
{noformat}
FROM registry-1.docker.io/library/alpine
RUN rm -rf /etc/apk/* && ln -sf /build-mouont-point/ /etc/apk
{noformat}
","05/Aug/16 20:09;zhitao;[~gilbert], I managed to reproduce with this Dockerfile:

```
FROM cirros
RUN rm -rf /etc/cirros && ln -sf /tmp /etc/cirros
```
This does not provision in copy backend in my machine.","05/Aug/16 20:17;zhitao;The error looks like this:

/quote
E0805 20:03:32.337234 72361 slave.cpp:4029] Container 'a5633e96-e9b5-4d29-a55e-fd316c28943b' for executor 'test2' of framework 662655e7-1b0a-4873-9307-f908fb96bc00-0000 failed to start: Collect failed: Failed to copy layer: cp: cannot overwrite directory ‘/var/lib/mesos/provisioner/containers/a5633e96-e9b5-4d29-a55e-fd316c28943b/backends/copy/rootfses/b3286fd7-1c43-406e-85d7-170cb385a480/etc/cirros’ with non-directory
/quote","06/Aug/16 00:59;gilbert;[~zhitao], I have been trying manually for a while. Seems like this issue may not be straightforwards. Since the extra layer contains two things:
1. the whiteout file.
2. the symlink file.

Currently we get an error when copying, because we cannot use a non-directory to overwrite a directory. However, even if we can overwrite it finally, the symlink will be deleted by the labeled whiteout file.

Ok, I have one solution for this, by introducing extra whiteout filtering logic into each layers in copy backend. Seems like we may have to do that.","08/Aug/16 16:37;zhitao;One thing I forgot to mention is that I did a {{docker save}} to tar file, and used local store registry option when performing the test. The problematic later I generated does not have a extra whiteout file in such a cast:

/quote
zhitao@zhitao-mesos1:~/mesos/build$ ls -alR /t/layers/90e46350e512b827e8fe73a053ededc13f7eb1bccca96dc8ef86d6a6cd98f29c/rootfs/
/t/layers/90e46350e512b827e8fe73a053ededc13f7eb1bccca96dc8ef86d6a6cd98f29c/rootfs/:
total 12
drwxr-xr-x 3 root root 4096 Aug  8 16:36 .
drwxr-xr-x 3 root root 4096 Aug  8 16:36 ..
drwxrwxr-x 2 root root 4096 Aug  5 20:01 etc

/t/layers/90e46350e512b827e8fe73a053ededc13f7eb1bccca96dc8ef86d6a6cd98f29c/rootfs/etc:
total 8
drwxrwxr-x 2 root root 4096 Aug  5 20:01 .
drwxr-xr-x 3 root root 4096 Aug  8 16:36 ..
lrwxrwxrwx 1 root root    4 Aug  5 20:01 cirros -> /tmp
/quote","13/Apr/17 01:25;chhsia0;Thanks [~zhitao] for your input!","13/Apr/17 01:31;gilbert;https://reviews.apache.org/r/58408/","14/Apr/17 01:25;chhsia0;Unit test: https://reviews.apache.org/r/58443/","18/Apr/17 06:31;jieyu;commit 3c8deedc9a1bce617965c3442713ebdc6691d1ae
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date:   Tue Apr 18 14:18:45 2017 +0800

    Overwriting Symbolic Links with Files in Copy Provisioner.

    When a layer overwrites a symbolic link with a regular file, the link
    must be removed first, otherwise 'cp' would follow the link and
    overwrite the target instead of the link itself.

    Review: https://reviews.apache.org/r/58463/

commit bc12a5835590178112ec0d46bbbcb014ed246f3b
Author: Chun-Hung Hsiao <chhsiao@mesosphere.io>
Date:   Tue Apr 18 14:18:09 2017 +0800

    Overwriting Directories with Files in Copy Provisioner.

    When a layer overwrites a directory with a regular file or symbolic
    link (or vice versa), the old dir/file need to be removed before
    copying the layer into the rootfs. This is processed together with
    whiteout: The copy provisioner find all files to remove, including
    files marked as whiteout and the files described above, and remove
    them before the copy process.

    Review: https://reviews.apache.org/r/58408/","20/Nov/17 09:25;wangqiang8511;I am not sure, I should put it here or not. Seems like I still have similar issue with mesos 1.2.1

I am trying to follow the mesos gpu tutorial (using UCR) but failed with 

```
message: 'Failed to launch container: Collect failed: Failed to remove the entries under the directory labeled as opaque whiteout '/data/mesos/slave/provisioner/containers/d7651be4-5eb9-4973-86e5-e018e207a327/backends/copy/rootfses/d9515065-5806-4a19-82f5-eb80ddb040bf/usr/local/cuda-9.0': No such file or directory'
```

The image I am using is nvida:cuda

The cuda image docker file has the 

```
RUN apt-get update && apt-get install -y --no-install-recommends \
        cuda-cudart-$CUDA_PKG_VERSION && \
    ln -s cuda-9.0 /usr/local/cuda && \
    rm -rf /var/lib/apt/lists/*
```

",,,,,,,,,,,,,,,
MesosContainerizerProvisionerTest.DestroyWhileProvisioning is flaky.,MESOS-5023,12953223,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,gilbert,alexr,alexr,24/Mar/16 14:33,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.28.1,,,,,,containerization,,,,,0,mesosphere,,,,,,,,"Observed on the Apache Jenkins.

{noformat}
[ RUN      ] MesosContainerizerProvisionerTest.ProvisionFailed
I0324 13:38:56.284261  2948 containerizer.cpp:666] Starting container 'test_container' for executor 'executor' of framework ''
I0324 13:38:56.285825  2939 containerizer.cpp:1421] Destroying container 'test_container'
I0324 13:38:56.285854  2939 containerizer.cpp:1424] Waiting for the provisioner to complete for container 'test_container'
[       OK ] MesosContainerizerProvisionerTest.ProvisionFailed (7 ms)
[ RUN      ] MesosContainerizerProvisionerTest.DestroyWhileProvisioning
I0324 13:38:56.291187  2944 containerizer.cpp:666] Starting container 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2' for executor 'executor' of framework ''
I0324 13:38:56.292157  2944 containerizer.cpp:1421] Destroying container 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2'
I0324 13:38:56.292179  2944 containerizer.cpp:1424] Waiting for the provisioner to complete for container 'c2316963-c6cb-4c7f-a3b9-17ca5931e5b2'
F0324 13:38:56.292899  2944 containerizer.cpp:752] Check failed: containers_.contains(containerId)
*** Check failure stack trace: ***
    @     0x2ac9973d0ae4  google::LogMessage::Fail()
    @     0x2ac9973d0a30  google::LogMessage::SendToLog()
    @     0x2ac9973d0432  google::LogMessage::Flush()
    @     0x2ac9973d3346  google::LogMessageFatal::~LogMessageFatal()
    @     0x2ac996af897c  mesos::internal::slave::MesosContainerizerProcess::_launch()
    @     0x2ac996b1f18a  _ZZN7process8dispatchIbN5mesos8internal5slave25MesosContainerizerProcessERKNS1_11ContainerIDERK6OptionINS1_8TaskInfoEERKNS1_12ExecutorInfoERKSsRKS8_ISsERKNS1_7SlaveIDERKNS_3PIDINS3_5SlaveEEEbRKS8_INS3_13ProvisionInfoEES5_SA_SD_SsSI_SL_SQ_bSU_EENS_6FutureIT_EERKNSO_IT0_EEMS10_FSZ_T1_T2_T3_T4_T5_T6_T7_T8_T9_ET10_T11_T12_T13_T14_T15_T16_T17_T18_ENKUlPNS_11ProcessBaseEE_clES1P_
    @     0x2ac996b479d9  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIbN5mesos8internal5slave25MesosContainerizerProcessERKNS5_11ContainerIDERK6OptionINS5_8TaskInfoEERKNS5_12ExecutorInfoERKSsRKSC_ISsERKNS5_7SlaveIDERKNS0_3PIDINS7_5SlaveEEEbRKSC_INS7_13ProvisionInfoEES9_SE_SH_SsSM_SP_SU_bSY_EENS0_6FutureIT_EERKNSS_IT0_EEMS14_FS13_T1_T2_T3_T4_T5_T6_T7_T8_T9_ET10_T11_T12_T13_T14_T15_T16_T17_T18_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x2ac997334fef  std::function<>::operator()()
    @     0x2ac99731b1c7  process::ProcessBase::visit()
    @     0x2ac997321154  process::DispatchEvent::visit()
    @           0x9a699c  process::ProcessBase::serve()
    @     0x2ac9973173c0  process::ProcessManager::resume()
    @     0x2ac99731445a  _ZZN7process14ProcessManager12init_threadsEvENKUlRKSt11atomic_boolE_clES3_
    @     0x2ac997320916  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEE6__callIvIEILm0EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE
    @     0x2ac9973208c6  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEEclIIEvEET0_DpOT_
    @     0x2ac997320858  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEE9_M_invokeIIEEEvSt12_Index_tupleIIXspT_EEE
    @     0x2ac9973207af  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEEclEv
    @     0x2ac997320748  _ZNSt6thread5_ImplISt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS6_EEEvEEE6_M_runEv
    @     0x2ac9989aea60  (unknown)
    @     0x2ac999125182  start_thread
    @     0x2ac99943547d  (unknown)
make[4]: Leaving directory `/mesos/mesos-0.29.0/_build/src'
make[4]: *** [check-local] Aborted
make[3]: *** [check-am] Error 2
make[3]: Leaving directory `/mesos/mesos-0.29.0/_build/src'
make[2]: *** [check] Error 2
make[2]: Leaving directory `/mesos/mesos-0.29.0/_build/src'
make[1]: *** [check-recursive] Error 1
make[1]: Leaving directory `/mesos/mesos-0.29.0/_build'
make: *** [distcheck] Error 1
Build step 'Execute shell' marked build as failure
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-25 19:15:48.35,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 30 00:54:26 UTC 2016,,,,,,,"0|i2v1kl:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 32,,,,,,,,,,,2.0,,,,,,,,,,,"25/Mar/16 19:15;gilbert;This failure comes from a race that the containers_[containerId] is erased accidentally. I am attaching the log which is the same flaky by comment out the check `CHECK(containers_.contains(containerId));` in _launch():

{noformat}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from MesosContainerizerProvisionerTest
[ RUN      ] MesosContainerizerProvisionerTest.DestroyWhileProvisioning
I0325 19:01:02.686094 19729 containerizer.cpp:666] Starting container '63a1b3d5-dd19-453f-9953-439d4126c488' for executor 'executor' of framework ''
I0325 19:01:02.687070 19732 containerizer.cpp:1421] Destroying container '63a1b3d5-dd19-453f-9953-439d4126c488'
I0325 19:01:02.687110 19732 containerizer.cpp:1424] Waiting for the provisioner to complete for container '63a1b3d5-dd19-453f-9953-439d4126c488'
F0325 19:01:02.688181 19731 owned.hpp:110] Check failed: 'get()' Must be non NULL 
*** Check failure stack trace: ***
    @     0x7f00ae6fe84d  google::LogMessage::Fail()
    @     0x7f00ae6fdc2e  google::LogMessage::SendToLog()
    @     0x7f00ae6fe51d  google::LogMessage::Flush()
    @     0x7f00ae701968  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f00adce4df4  google::CheckNotNull<>()
    @     0x7f00adc83760  process::Owned<>::operator->()
    @     0x7f00adc7154d  mesos::internal::slave::MesosContainerizerProcess::_launch()
    @     0x7f00adcec610  _ZZN7process8dispatchIbN5mesos8internal5slave25MesosContainerizerProcessERKNS1_11ContainerIDERK6OptionINS1_8TaskInfoEERKNS1_12ExecutorInfoERKSsRKS8_ISsERKNS1_7SlaveIDERKNS_3PIDINS3_5SlaveEEEbRKS8_INS3_13ProvisionInfoEES5_SA_SD_SsSI_SL_SQ_bSU_EENS_6FutureIT_EERKNSO_IT0_EEMS10_FSZ_T1_T2_T3_T4_T5_T6_T7_T8_T9_ET10_T11_T12_T13_T14_T15_T16_T17_T18_ENKUlPNS_11ProcessBaseEE_clES1P_
    @     0x7f00adcebf32  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIbN5mesos8internal5slave25MesosContainerizerProcessERKNS5_11ContainerIDERK6OptionINS5_8TaskInfoEERKNS5_12ExecutorInfoERKSsRKSC_ISsERKNS5_7SlaveIDERKNS0_3PIDINS7_5SlaveEEEbRKSC_INS7_13ProvisionInfoEES9_SE_SH_SsSM_SP_SU_bSY_EENS0_6FutureIT_EERKNSS_IT0_EEMS14_FS13_T1_T2_T3_T4_T5_T6_T7_T8_T9_ET10_T11_T12_T13_T14_T15_T16_T17_T18_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x7f00ae67c188  std::function<>::operator()()
    @     0x7f00ae666494  process::ProcessBase::visit()
    @     0x7f00ae6bdbde  process::DispatchEvent::visit()
    @           0x857a71  process::ProcessBase::serve()
    @     0x7f00ae6641bd  process::ProcessManager::resume()
    @     0x7f00ae66c325  process::ProcessManager::init_threads()::$_1::operator()()
    @     0x7f00ae66c263  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvE3$_1St17reference_wrapperIKSt11atomic_boolEEE6__callIvJEJLm0EEEET_OSt5tupleIJDpT0_EESt12_Index_tupleIJXspT1_EEE
    @     0x7f00ae66c216  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvE3$_1St17reference_wrapperIKSt11atomic_boolEEEclIJEvEET0_DpOT_
    @     0x7f00ae66c1c5  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvE3$_1St17reference_wrapperIKSt11atomic_boolEEEvEE9_M_invokeIJEEEvSt12_Index_tupleIJXspT_EEE
    @     0x7f00ae66c195  std::_Bind_simple<>::operator()()
    @     0x7f00ae66c16c  std::thread::_Impl<>::_M_run()
    @     0x7f00a9742a60  (unknown)
    @     0x7f00a8f5f182  start_thread
    @     0x7f00a8c8c47d  (unknown)
{noformat}","25/Mar/16 20:56;gilbert;The bug arises from `await()` in destroy(), which creates another future object. Thanks [~anandmazumdar] for investigating it together.

Hey [~klaus1982], sorry to ask that do you mind letting us to resolve this bug? Since it comes from a commit we merged two days ago, I should have it fixed asap. ","25/Mar/16 23:14;klaus1982;sure:)","28/Mar/16 23:49;jieyu;Sorry, I don't get the bug. onAny callbacks registered on the 'future' will be invoked in serial order when the promise is set. I don't understand why this can go out of order. Can someone show more context here?","30/Mar/16 00:54;jieyu;commit c9eece94a5d26502b26d8a90f0cdb52fa038ff6c
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Mar 29 17:51:52 2016 -0700

    Fix container destroy provisioning race.
    
    Review: https://reviews.apache.org/r/45386/",,,,,,,,,,,,,,,,,,,,,,,
Add docker volume driver isolator for Mesos containerizer.,MESOS-5013,12952878,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gyliu,gyliu,gyliu,23/Mar/16 16:00,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,1.0.0,,,,,,,,,,,0,mesosphere,,,,,,,,"The isolator will interact with Docker Volume Driver Plugins to mount and unmount external volumes to container.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-13 21:27:25.743,,,false,MESOS-4355,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 24 18:55:40 UTC 2016,,,,,,,"0|i2v3nz:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 33,,,,,,,,,,,8.0,,,,,,,,,,,"03/Apr/16 10:53;gyliu;https://reviews.apache.org/r/45217/
https://reviews.apache.org/r/45265/
https://reviews.apache.org/r/45270/
https://reviews.apache.org/r/45360/
https://reviews.apache.org/r/45373/
https://reviews.apache.org/r/45370/
https://reviews.apache.org/r/45375/
https://reviews.apache.org/r/45377/","13/Apr/16 21:27;jdef;If this depends upon filesystem/linux isolator at all then we should document that dependency","13/Apr/16 23:01;gilbert;Thanks James, we will keep tracking on it by this JIRA https://issues.apache.org/jira/browse/MESOS-5216","14/Apr/16 21:00;jieyu;commit 6a57889ce9f3591084b7bd71805a0d24c3d9a5df
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Thu Apr 14 14:00:05 2016 -0700

    Implemented docker volume driver isolator interfaces.
    
    Review: https://reviews.apache.org/r/45217/","19/Apr/16 19:00;jieyu;commit fc3c67b2f326fe7798b416daaf3606377bbc0983
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Tue Apr 19 10:35:19 2016 -0700

    Ignored volumes with source in linux filesystem isolator.
    
    Review: https://reviews.apache.org/r/45373/","20/Apr/16 17:46;jieyu;commit d02108fc59d0e6b6e6b2e072e84eb79da370fdc3
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Wed Apr 20 10:04:37 2016 -0700

    Added state protobuf for DockerVolume.
    
    The DockerVolume is used to checkpoint volume information for each
    container. It'll be used during recovery.
    
    Review: https://reviews.apache.org/r/45270/","20/Apr/16 18:58;jieyu;commit 17c954ac1d395192d343eb62409d03e14b851e67
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Wed Apr 20 11:18:53 2016 -0700

    Added paths helper function for docker volume checkpoint.
    
    Review: https://reviews.apache.org/r/46426","20/Apr/16 21:27;jieyu;commit d2d44d5e86cb2b76fcb2025b87031bc47456d035
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Wed Apr 20 12:04:25 2016 -0700

    Added docker volume driver client for mount and unmount.
    
    Review: https://reviews.apache.org/r/45360/","20/Apr/16 21:58;jieyu;commit a823a7285e5e6becb198eef0fc8ae716b9e66126
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Wed Apr 20 14:33:01 2016 -0700

    Implemented create() for docker volume isolator.
    
    Review: https://reviews.apache.org/r/46180/","24/Apr/16 17:27;jieyu;commit e8e65535c8ee105e262662fde95aef2d93b289a5
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Sat Apr 23 22:30:23 2016 -0700

    Implemented prepare() for volume isolator.
    
    Review: https://reviews.apache.org/r/45370/","24/Apr/16 18:47;jieyu;commit e92cc460a7caf220bbb61941bb49e24e739f4b4a
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Sun Apr 24 11:46:29 2016 -0700

    Enabled Sequence mount for prepare() in docker volume isolator.
    
    Review: https://reviews.apache.org/r/44454

commit 65c35fdd5ae83e71c874c09911f58f036e7bfeb6
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Sun Apr 24 11:05:02 2016 -0700

    Implemented recover() for the docker volume isolator.
    
    Review: https://reviews.apache.org/r/45674/

commit 3add364e10ea6dac116bc8102b37f0de9db8bf99
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Sun Apr 24 11:03:36 2016 -0700

    Implemented cleanup() for docker volume isolator.
    
    Review: https://reviews.apache.org/r/45375","24/Apr/16 18:53;jieyu;commit 39a5d9459a862056d658ce0049cc299129172a2d
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Sun Apr 24 11:53:10 2016 -0700

    Enabled sequene for unmount() in docker volume isolator.
    
    Review: https://reviews.apache.org/r/44609","24/Apr/16 18:55;jieyu;commit 8212770ff7142d730383195865bce3487aac5ba0
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Sun Apr 24 11:55:14 2016 -0700

    Plugged in docker volume isolator.
    
    Review: https://reviews.apache.org/r/45265/",,,,,,,,,,,,,,,
MasterTest.MasterLost is flaky,MESOS-5000,12952419,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,bbannier,bbannier,22/Mar/16 12:48,26/Nov/18 09:23,29/Oct/20 16:32,26/Nov/18 09:23,1.0.0,,,,,,,,1.0.0,,,,,,test,,,,,0,flaky,flaky-test,mesosphere,,,,,,"The test {{MasterTest.MasterLost}} and {{ExceptionTest.DisallowSchedulerActionsOnAbort}} fail at least half the time under OS X (clang, not optimized, {{30efac7}}), e.g.,
{code}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from MasterTest
[ RUN      ] MasterTest.MasterLost
*** Aborted at 1458650698 (unix time) try ""date -d @1458650698"" if you are using GNU date ***
PC: @        0x109685fcc mesos::internal::state::State::store()
*** SIGSEGV (@0x0) received by PID 18620 (TID 0x111259000) stack trace: ***
    @     0x7fff850e1f1a _sigtramp
    @        0x108c74eaf boost::uuids::detail::sha1::process_byte_impl()
    @        0x1095fd723 mesos::internal::state::protobuf::State::store<>()
    @        0x1095fbd3e mesos::internal::master::RegistrarProcess::update()
    @        0x1095fcf6c mesos::internal::master::RegistrarProcess::_apply()
    @        0x1096797a0 _ZZN7process8dispatchIbN5mesos8internal6master16RegistrarProcessENS_5OwnedINS3_9OperationEEES7_EENS_6FutureIT_EERKNS_3PIDIT0_EEMSC_FSA_T1_ET2_ENKUlPNS_11ProcessBaseEE_clESL_
    @        0x1096795f0 _ZNSt3__128__invoke_void_return_wrapperIvE6__callIJRZN7process8dispatchIbN5mesos8internal6master16RegistrarProcessENS3_5OwnedINS7_9OperationEEESB_EENS3_6FutureIT_EERKNS3_3PIDIT0_EEMSG_FSE_T1_ET2_EUlPNS3_11ProcessBaseEE_SP_EEEvDpOT_
    @        0x1096792d9 _ZNSt3__110__function6__funcIZN7process8dispatchIbN5mesos8internal6master16RegistrarProcessENS2_5OwnedINS6_9OperationEEESA_EENS2_6FutureIT_EERKNS2_3PIDIT0_EEMSF_FSD_T1_ET2_EUlPNS2_11ProcessBaseEE_NS_9allocatorISP_EEFvSO_EEclEOSO_
    @        0x10b2e9e4c std::__1::function<>::operator()()
    @        0x10b2e9d9c process::ProcessBase::visit()
    @        0x10b31d26e process::DispatchEvent::visit()
    @        0x108ad7d81 process::ProcessBase::serve()
    @        0x10b2e3cb4 process::ProcessManager::resume()
    @        0x10b36c479 process::ProcessManager::init_threads()::$_1::operator()()
    @        0x10b36c0a2 _ZNSt3__114__thread_proxyINS_5tupleIJNS_6__bindIZN7process14ProcessManager12init_threadsEvE3$_1JNS_17reference_wrapperIKNS_6atomicIbEEEEEEEEEEEEPvSD_
    @     0x7fff90eca05a _pthread_body
    @     0x7fff90ec9fd7 _pthread_start
    @     0x7fff90ec73ed thread_start
{code}

Sometimes also {{FaultToleranceTest.SchedulerFailover}} fails with the same stack trace.

I could trace this to the recent refactoring of the test helpers (MESOS-4633, MESOS-4634),
{code}
There are only 'skip'ped commits left to test.
The first bad commit could be any of:
75ca1e6c9fde655c41fdf835aa20c47570d21f10
56e9406763e8514a7557ab3862d2f352a61425d5
b377557c2bfc35c894e87becb47122955540f133
7bf6e4f70131175edd4d6d77ea0dc7692b3e72ae
c7df1d7bcb1604c95800871cc0473c946e5b5d16
951539317525f3afe9490ed098617e5d4563a80a
We cannot bisect more!
{code}

It appears the lifetimes of some objects are still not ordered correctly.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4968,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-06 19:32:29.34,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 26 09:23:31 UTC 2018,,,,,,,"0|i2v0tz:",9223372036854775807,,,,,mcypark,,,,,,Mesosphere Sprint 32,,,,,,,,,,,3.0,,,,,,,,,,,"05/Apr/16 15:52;bbannier;Linking MESOS-4968 as this seems to be caused by the same issue where we use a state ptr after free.","05/Apr/16 17:09;bbannier;Review: https://reviews.apache.org/r/45749/","06/Apr/16 19:32;mcypark;{noformat}
commit af6309f1c0a05937508e4e7f2b495920359602bb
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Wed Apr 6 12:10:35 2016 -0700

    Adjusted lifetimes of member variables.

    This correctly reflects the lifetime dependencies among
    `Master::storage`, `Master::state` and `Master::registrar`.

    Review: https://reviews.apache.org/r/45749/
{noformat}","26/Nov/18 09:23;bbannier;Closing this again. @mesosphere, why did you reopen this issue?",,,,,,,,,,,,,,,,,,,,,,,,
Destroy a container while it's provisioning can lead to leaked provisioned directories.,MESOS-4985,12951645,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,gilbert,jieyu,jieyu,18/Mar/16 23:21,10/Oct/19 01:48,29/Oct/20 16:32,23/Mar/16 22:39,0.28.0,,,,,,,,0.28.1,,,,,,containerization,,,,,0,mesosphere,,,,,,,,"Here is the possible sequence of events:
1) containerizer->launch
2) provisioner->provision is called. it is fetching the image
3) executor registration timed out
4) containerizer->destroy is called
5) container->state is still in PREPARING
6) provisioner->destroy is called

So we can be calling provisioner->destory while provisioner->provision hasn't finished yet. provisioner->destroy might just skip since there's no information about the container yet, and later, provisioner will prepare the root filesystem. This root filesystem will not be destroyed as destroy already finishes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-21 05:52:07.695,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 23 22:39:07 UTC 2016,,,,,,,"0|i2uwiv:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 31,,,,,,,,,,,3.0,,,,,,,,,,,"21/Mar/16 05:52;gilbert;https://reviews.apache.org/r/45092/
https://reviews.apache.org/r/45093/","23/Mar/16 22:39;jieyu;commit 8d3d4707395bf1eee416c8d3fc7a9e5eddf94681
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Mar 23 15:38:33 2016 -0700

    Added mesos containerizer test DestroyWhileProvisioning.
    
    Review: https://reviews.apache.org/r/45093/

commit cb2298ebe7d7dd9ed3c7816011f461848a9d52fa
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Mar 23 15:29:26 2016 -0700

    Fixed containerizer potential race destroy while provisioning.
    
    Review: https://reviews.apache.org/r/45092/",,,,,,,,,,,,,,,,,,,,,,,,,,
MasterTest.SlavesEndpointTwoSlaves is flaky,MESOS-4984,12951630,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,neilc,neilc,18/Mar/16 22:45,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,1.0.0,,,,,,test,,,,,0,flaky-test,mesosphere,tech-debt,,,,,,"Observed on Arch Linux with GCC 6, running in a virtualbox VM:

[ RUN      ] MasterTest.SlavesEndpointTwoSlaves
/mesos-2/src/tests/master_tests.cpp:1710: Failure
Value of: array.get().values.size()
  Actual: 1
Expected: 2u
Which is: 2
[  FAILED  ] MasterTest.SlavesEndpointTwoSlaves (86 ms)

Seems to fail non-deterministically, perhaps more often when there is concurrent CPU load on the machine.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/16 22:54;neilc;slaves_endpoint_flaky_4984_verbose_log.txt;https://issues.apache.org/jira/secure/attachment/12794277/slaves_endpoint_flaky_4984_verbose_log.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,2016-03-22 17:30:39.719,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 24 06:51:13 UTC 2016,,,,,,,"0|i2uwfj:",9223372036854775807,,,,,adam-mesos,,,,,,Mesosphere Sprint 31,,,,,,,,,,,2.0,,,,,,,,,,,"22/Mar/16 17:14;neilc;cc [~arojas] -- this test was added in {{4ed15230}}.","22/Mar/16 17:30;anandmazumdar;Looks like we were missing a pause/settle. I added a trivial patch to fix this: https://reviews.apache.org/r/45166/ . Added you both as reviewers.

Let me find a shepherd willing to commit this.","24/Mar/16 06:51;adam-mesos;commit 40a1626f12fcf6d22fb222793ec64ff022b989b6
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Wed Mar 23 23:46:31 2016 -0700

    Fixed flaky `MasterTest.SlavesEndpointTwoSlaves`.
    
    We were not correctly waiting for the master to register the first
    slave before making a call to the `/slaves` endpoint. There was this
    possible race:
    
    - Slave1 is started.
    - Slave2 is started.
    - Slave2 sends register message to master.
    - Slave2 re-tries the register message.
    - Master registers slave2.
    - Master resends register acknowledgment to slave2.
    - The test thinks both expectations (`FUTURE_PROTOBUF`) have completed.
    - Test calls `/slaves` and sees only 1 slave has registered. Test fails.
    - Master registers slave1.
    
    Reordering the `AWAIT_READY` calls to explicitly wait for `slave1` to
    register first should fix the flakiness.
    
    Review: https://reviews.apache.org/r/45166/
",,,,,,,,,,,,,,,,,,,,,,,,,
Update mesos-execute with Appc changes.,MESOS-4978,12951572,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jojy,jojy,jojy,18/Mar/16 17:36,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,1.0.0,,,,,,containerization,,,,,0,mesosphere,unified-containerizer-mvp,,,,,,,mesos-execute cli application currently does not have support for Appc images. Adding support would make integration tests easier.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-5006,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-23 09:05:38.312,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 08 20:16:01 UTC 2016,,,,,,,"0|hzzzji:zzzzv",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 31,Mesosphere Sprint 32,,,,,,,,,,3.0,,,,,,,,,,,"18/Mar/16 17:37;jojy;https://reviews.apache.org/r/44934","23/Mar/16 09:05;gyliu;[~jieyu] Does it make sense to make this one merged first so that we can also test this out.","08/Apr/16 20:16;jieyu;commit 4e7253e34b4f131f496c1f2a286d1abfe49c41d1
Author: Jojy Varghese <jojy@mesosphere.io>
Date:   Fri Apr 8 12:21:45 2016 -0700

    Updated mesos-execute to add support for Appc.
    
    Review: https://reviews.apache.org/r/44934/",,,,,,,,,,,,,,,,,,,,,,,,,
ContainerLoggerTest.LOGROTATE_RotateInSandbox is flaky,MESOS-4961,12950869,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,kaysoky,kaysoky,kaysoky,16/Mar/16 15:16,31/May/16 02:11,29/Oct/20 16:32,29/Mar/16 20:04,,,,,,,,,1.0.0,,,,,,,,,,,0,flaky,mesosphere,test,,,,,,"The logger subprocesses may exit before we reach the {{waitpid}} in the test.  If this happens, {{waitpid}} will return a {{-1}} as the process no longer exists.

Verbose logs:
{code}
[ RUN      ] ContainerLoggerTest.LOGROTATE_RotateInSandbox
I0316 14:28:51.329337  1242 cluster.cpp:139] Creating default 'local' authorizer
I0316 14:28:51.332823  1242 leveldb.cpp:174] Opened db in 3.079559ms
I0316 14:28:51.333916  1242 leveldb.cpp:181] Compacted db in 1.054247ms
I0316 14:28:51.333979  1242 leveldb.cpp:196] Created db iterator in 21450ns
I0316 14:28:51.334005  1242 leveldb.cpp:202] Seeked to beginning of db in 2205ns
I0316 14:28:51.334025  1242 leveldb.cpp:271] Iterated through 0 keys in the db in 410ns
I0316 14:28:51.334089  1242 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0316 14:28:51.334661  1275 recover.cpp:447] Starting replica recovery
I0316 14:28:51.335044  1275 recover.cpp:473] Replica is in EMPTY status
I0316 14:28:51.336207  1262 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (484)@172.17.0.3:45919
I0316 14:28:51.336730  1270 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0316 14:28:51.337257  1275 recover.cpp:564] Updating replica status to STARTING
I0316 14:28:51.338001  1267 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 537200ns
I0316 14:28:51.338032  1267 replica.cpp:320] Persisted replica status to STARTING
I0316 14:28:51.338183  1261 master.cpp:376] Master c7653f60-33e9-4406-9f62-dc74c906bf83 (2cbb23302fe5) started on 172.17.0.3:45919
I0316 14:28:51.338295  1263 recover.cpp:473] Replica is in STARTING status
I0316 14:28:51.338213  1261 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/XtqwkS/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/XtqwkS/master"" --zk_session_timeout=""10secs""
I0316 14:28:51.338562  1261 master.cpp:423] Master only allowing authenticated frameworks to register
I0316 14:28:51.338572  1261 master.cpp:428] Master only allowing authenticated slaves to register
I0316 14:28:51.338580  1261 credentials.hpp:35] Loading credentials for authentication from '/tmp/XtqwkS/credentials'
I0316 14:28:51.338877  1261 master.cpp:468] Using default 'crammd5' authenticator
I0316 14:28:51.339030  1262 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (485)@172.17.0.3:45919
I0316 14:28:51.339246  1261 master.cpp:537] Using default 'basic' HTTP authenticator
I0316 14:28:51.339393  1261 master.cpp:571] Authorization enabled
I0316 14:28:51.339390  1266 recover.cpp:193] Received a recover response from a replica in STARTING status
I0316 14:28:51.339606  1271 whitelist_watcher.cpp:77] No whitelist given
I0316 14:28:51.339607  1275 hierarchical.cpp:144] Initialized hierarchical allocator process
I0316 14:28:51.340077  1268 recover.cpp:564] Updating replica status to VOTING
I0316 14:28:51.340533  1270 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 331558ns
I0316 14:28:51.340558  1270 replica.cpp:320] Persisted replica status to VOTING
I0316 14:28:51.340672  1270 recover.cpp:578] Successfully joined the Paxos group
I0316 14:28:51.340827  1270 recover.cpp:462] Recover process terminated
I0316 14:28:51.341684  1270 master.cpp:1806] The newly elected leader is master@172.17.0.3:45919 with id c7653f60-33e9-4406-9f62-dc74c906bf83
I0316 14:28:51.341717  1270 master.cpp:1819] Elected as the leading master!
I0316 14:28:51.341740  1270 master.cpp:1508] Recovering from registrar
I0316 14:28:51.341954  1263 registrar.cpp:307] Recovering registrar
I0316 14:28:51.342499  1273 log.cpp:659] Attempting to start the writer
I0316 14:28:51.343616  1266 replica.cpp:493] Replica received implicit promise request from (487)@172.17.0.3:45919 with proposal 1
I0316 14:28:51.344183  1266 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 536941ns
I0316 14:28:51.344208  1266 replica.cpp:342] Persisted promised to 1
I0316 14:28:51.344825  1267 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0316 14:28:51.346009  1276 replica.cpp:388] Replica received explicit promise request from (488)@172.17.0.3:45919 for position 0 with proposal 2
I0316 14:28:51.346371  1276 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 327890ns
I0316 14:28:51.346393  1276 replica.cpp:712] Persisted action at 0
I0316 14:28:51.347363  1267 replica.cpp:537] Replica received write request for position 0 from (489)@172.17.0.3:45919
I0316 14:28:51.347414  1267 leveldb.cpp:436] Reading position from leveldb took 24861ns
I0316 14:28:51.347774  1267 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 323654ns
I0316 14:28:51.347796  1267 replica.cpp:712] Persisted action at 0
I0316 14:28:51.348323  1276 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0316 14:28:51.348714  1276 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 361981ns
I0316 14:28:51.348738  1276 replica.cpp:712] Persisted action at 0
I0316 14:28:51.348760  1276 replica.cpp:697] Replica learned NOP action at position 0
I0316 14:28:51.349318  1274 log.cpp:675] Writer started with ending position 0
I0316 14:28:51.350275  1267 leveldb.cpp:436] Reading position from leveldb took 23849ns
I0316 14:28:51.351171  1271 registrar.cpp:340] Successfully fetched the registry (0B) in 9.173248ms
I0316 14:28:51.351300  1271 registrar.cpp:439] Applied 1 operations in 32119ns; attempting to update the 'registry'
I0316 14:28:51.351989  1272 log.cpp:683] Attempting to append 170 bytes to the log
I0316 14:28:51.352108  1266 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0316 14:28:51.352802  1263 replica.cpp:537] Replica received write request for position 1 from (490)@172.17.0.3:45919
I0316 14:28:51.353313  1263 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 474854ns
I0316 14:28:51.353338  1263 replica.cpp:712] Persisted action at 1
I0316 14:28:51.354101  1273 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0316 14:28:51.354483  1273 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 338210ns
I0316 14:28:51.354507  1273 replica.cpp:712] Persisted action at 1
I0316 14:28:51.354529  1273 replica.cpp:697] Replica learned APPEND action at position 1
I0316 14:28:51.355444  1275 registrar.cpp:484] Successfully updated the 'registry' in 4.084224ms
I0316 14:28:51.355569  1275 registrar.cpp:370] Successfully recovered registrar
I0316 14:28:51.355697  1268 log.cpp:702] Attempting to truncate the log to 1
I0316 14:28:51.355870  1269 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0316 14:28:51.356016  1274 master.cpp:1616] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0316 14:28:51.356032  1272 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0316 14:28:51.356761  1273 replica.cpp:537] Replica received write request for position 2 from (491)@172.17.0.3:45919
I0316 14:28:51.357203  1273 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 406053ns
I0316 14:28:51.357226  1273 replica.cpp:712] Persisted action at 2
I0316 14:28:51.357718  1270 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0316 14:28:51.358093  1270 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 345370ns
I0316 14:28:51.358175  1270 leveldb.cpp:399] Deleting ~1 keys from leveldb took 57us
I0316 14:28:51.358201  1270 replica.cpp:712] Persisted action at 2
I0316 14:28:51.358220  1270 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0316 14:28:51.368399  1242 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0316 14:28:51.406371  1242 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0316 14:28:51.410480  1266 slave.cpp:193] Slave started on 12)@172.17.0.3:45919
I0316 14:28:51.410518  1266 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --container_logger=""org_apache_mesos_LogrotateContainerLogger"" --containerizers=""mesos"" --credential=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy""
I0316 14:28:51.411118  1266 credentials.hpp:83] Loading credential for authentication from '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/credential'
I0316 14:28:51.411381  1266 slave.cpp:324] Slave using credential for: test-principal
I0316 14:28:51.411696  1266 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0316 14:28:51.412075  1266 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0316 14:28:51.412148  1266 slave.cpp:472] Slave attributes: [  ]
I0316 14:28:51.412160  1266 slave.cpp:477] Slave hostname: 2cbb23302fe5
I0316 14:28:51.413516  1263 state.cpp:58] Recovering state from '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/meta'
I0316 14:28:51.413774  1266 status_update_manager.cpp:200] Recovering status update manager
I0316 14:28:51.414029  1276 containerizer.cpp:407] Recovering containerizer
I0316 14:28:51.415222  1269 provisioner.cpp:245] Provisioner recovery complete
I0316 14:28:51.415650  1268 slave.cpp:4565] Finished recovery
I0316 14:28:51.416115  1268 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0316 14:28:51.416365  1268 slave.cpp:796] New master detected at master@172.17.0.3:45919
I0316 14:28:51.416448  1276 status_update_manager.cpp:174] Pausing sending status updates
I0316 14:28:51.416445  1268 slave.cpp:859] Authenticating with master master@172.17.0.3:45919
I0316 14:28:51.416522  1268 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0316 14:28:51.416671  1268 slave.cpp:832] Detecting new master
I0316 14:28:51.416731  1275 authenticatee.cpp:121] Creating new client SASL connection
I0316 14:28:51.416807  1268 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0316 14:28:51.417006  1263 master.cpp:5659] Authenticating slave(12)@172.17.0.3:45919
I0316 14:28:51.417103  1262 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(38)@172.17.0.3:45919
I0316 14:28:51.417348  1273 authenticator.cpp:98] Creating new server SASL connection
I0316 14:28:51.417548  1266 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0316 14:28:51.417582  1266 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0316 14:28:51.417696  1264 authenticator.cpp:203] Received SASL authentication start
I0316 14:28:51.417753  1264 authenticator.cpp:325] Authentication requires more steps
I0316 14:28:51.417948  1265 authenticatee.cpp:258] Received SASL authentication step
I0316 14:28:51.418107  1267 authenticator.cpp:231] Received SASL authentication step
I0316 14:28:51.418159  1267 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0316 14:28:51.418180  1267 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0316 14:28:51.418233  1267 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0316 14:28:51.418270  1267 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0316 14:28:51.418289  1267 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.418300  1267 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.418323  1267 authenticator.cpp:317] Authentication success
I0316 14:28:51.418414  1264 authenticatee.cpp:298] Authentication success
I0316 14:28:51.418473  1269 master.cpp:5689] Successfully authenticated principal 'test-principal' at slave(12)@172.17.0.3:45919
I0316 14:28:51.418514  1275 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(38)@172.17.0.3:45919
I0316 14:28:51.418781  1276 slave.cpp:927] Successfully authenticated with master master@172.17.0.3:45919
I0316 14:28:51.418937  1276 slave.cpp:1321] Will retry registration in 1.983001ms if necessary
I0316 14:28:51.419108  1262 master.cpp:4370] Registering slave at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with id c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:51.419643  1266 registrar.cpp:439] Applied 1 operations in 75642ns; attempting to update the 'registry'
I0316 14:28:51.420670  1272 log.cpp:683] Attempting to append 339 bytes to the log
I0316 14:28:51.420820  1269 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0316 14:28:51.421495  1270 slave.cpp:1321] Will retry registration in 1.437257ms if necessary
I0316 14:28:51.421716  1275 master.cpp:4358] Ignoring register slave message from slave(12)@172.17.0.3:45919 (2cbb23302fe5) as admission is already in progress
I0316 14:28:51.422107  1267 replica.cpp:537] Replica received write request for position 3 from (505)@172.17.0.3:45919
I0316 14:28:51.423033  1267 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 762815ns
I0316 14:28:51.423066  1267 replica.cpp:712] Persisted action at 3
I0316 14:28:51.424069  1267 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0316 14:28:51.424232  1264 slave.cpp:1321] Will retry registration in 66.01292ms if necessary
I0316 14:28:51.424342  1269 master.cpp:4358] Ignoring register slave message from slave(12)@172.17.0.3:45919 (2cbb23302fe5) as admission is already in progress
I0316 14:28:51.424686  1267 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 574743ns
I0316 14:28:51.424757  1267 replica.cpp:712] Persisted action at 3
I0316 14:28:51.424792  1267 replica.cpp:697] Replica learned APPEND action at position 3
I0316 14:28:51.426441  1272 registrar.cpp:484] Successfully updated the 'registry' in 6.721024ms
I0316 14:28:51.426677  1262 log.cpp:702] Attempting to truncate the log to 3
I0316 14:28:51.426808  1264 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0316 14:28:51.427584  1261 slave.cpp:3482] Received ping from slave-observer(11)@172.17.0.3:45919
I0316 14:28:51.428213  1262 hierarchical.cpp:473] Added slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 (2cbb23302fe5) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0316 14:28:51.427865  1266 master.cpp:4438] Registered slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0316 14:28:51.428270  1267 slave.cpp:971] Registered with master master@172.17.0.3:45919; given slave ID c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:51.428412  1265 replica.cpp:537] Replica received write request for position 4 from (506)@172.17.0.3:45919
I0316 14:28:51.428443  1267 fetcher.cpp:81] Clearing fetcher cache
I0316 14:28:51.428503  1262 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:51.428535  1262 hierarchical.cpp:1150] Performed allocation for slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 in 205421ns
I0316 14:28:51.428750  1273 status_update_manager.cpp:181] Resuming sending status updates
I0316 14:28:51.429157  1265 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 695258ns
I0316 14:28:51.429225  1267 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/meta/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/slave.info'
I0316 14:28:51.429275  1265 replica.cpp:712] Persisted action at 4
I0316 14:28:51.429759  1267 slave.cpp:1030] Forwarding total oversubscribed resources 
I0316 14:28:51.430055  1265 master.cpp:4782] Received update of slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with total oversubscribed resources 
I0316 14:28:51.430614  1271 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0316 14:28:51.430891  1242 sched.cpp:222] Version: 0.29.0
I0316 14:28:51.431043  1265 hierarchical.cpp:531] Slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 (2cbb23302fe5) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0316 14:28:51.431236  1271 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 536892ns
I0316 14:28:51.431267  1265 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:51.431584  1271 leveldb.cpp:399] Deleting ~2 keys from leveldb took 66904ns
I0316 14:28:51.431538  1273 sched.cpp:326] New master detected at master@172.17.0.3:45919
I0316 14:28:51.431622  1271 replica.cpp:712] Persisted action at 4
I0316 14:28:51.431623  1265 hierarchical.cpp:1150] Performed allocation for slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 in 518588ns
I0316 14:28:51.431660  1271 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0316 14:28:51.431711  1273 sched.cpp:382] Authenticating with master master@172.17.0.3:45919
I0316 14:28:51.431737  1273 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0316 14:28:51.431982  1266 authenticatee.cpp:121] Creating new client SASL connection
I0316 14:28:51.432369  1261 master.cpp:5659] Authenticating scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.432509  1263 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(39)@172.17.0.3:45919
I0316 14:28:51.432868  1267 authenticator.cpp:98] Creating new server SASL connection
I0316 14:28:51.433135  1276 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0316 14:28:51.433233  1276 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0316 14:28:51.433423  1276 authenticator.cpp:203] Received SASL authentication start
I0316 14:28:51.433502  1276 authenticator.cpp:325] Authentication requires more steps
I0316 14:28:51.433606  1274 authenticatee.cpp:258] Received SASL authentication step
I0316 14:28:51.433744  1273 authenticator.cpp:231] Received SASL authentication step
I0316 14:28:51.433785  1273 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0316 14:28:51.433801  1273 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0316 14:28:51.433861  1273 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0316 14:28:51.433897  1273 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0316 14:28:51.433912  1273 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.433924  1273 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.433944  1273 authenticator.cpp:317] Authentication success
I0316 14:28:51.434037  1274 authenticatee.cpp:298] Authentication success
I0316 14:28:51.434108  1268 master.cpp:5689] Successfully authenticated principal 'test-principal' at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.434211  1272 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(39)@172.17.0.3:45919
I0316 14:28:51.434512  1274 sched.cpp:471] Successfully authenticated with master master@172.17.0.3:45919
I0316 14:28:51.434535  1274 sched.cpp:776] Sending SUBSCRIBE call to master@172.17.0.3:45919
I0316 14:28:51.434648  1274 sched.cpp:809] Will retry registration in 356.547014ms if necessary
I0316 14:28:51.434819  1266 master.cpp:2326] Received SUBSCRIBE call for framework 'default' at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.434905  1266 master.cpp:1845] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0316 14:28:51.435464  1265 master.cpp:2397] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0316 14:28:51.435979  1269 hierarchical.cpp:265] Added framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.436213  1272 sched.cpp:703] Framework registered with c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.436316  1272 sched.cpp:717] Scheduler::registered took 73782ns
I0316 14:28:51.436928  1269 hierarchical.cpp:1548] No inverse offers to send out!
I0316 14:28:51.436978  1269 hierarchical.cpp:1130] Performed allocation for 1 slaves in 970638ns
I0316 14:28:51.437278  1272 master.cpp:5488] Sending 1 offers to framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.437782  1262 sched.cpp:873] Scheduler::resourceOffers took 129952ns
I0316 14:28:51.440006  1274 master.cpp:3268] Processing ACCEPT call for offers: [ c7653f60-33e9-4406-9f62-dc74c906bf83-O0 ] on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.440094  1274 master.cpp:2871] Authorizing framework principal 'test-principal' to launch task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 as user 'mesos'
I0316 14:28:51.442152  1274 master.hpp:177] Adding task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 (2cbb23302fe5)
I0316 14:28:51.442348  1274 master.cpp:3753] Launching task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:51.442749  1265 slave.cpp:1361] Got assigned task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.443006  1265 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0316 14:28:51.443624  1265 slave.cpp:1480] Launching task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.443730  1265 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0316 14:28:51.444629  1265 paths.cpp:528] Trying to chown '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621' to user 'mesos'
I0316 14:28:51.449493  1265 slave.cpp:5367] Launching executor 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:51.450256  1261 containerizer.cpp:666] Starting container '6e2770ca-32d3-47ad-b4fe-7d9f26489621' for executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework 'c7653f60-33e9-4406-9f62-dc74c906bf83-0000'
I0316 14:28:51.450299  1265 slave.cpp:1698] Queuing task '864698ee-117b-4b95-b8d7-4c3ec6e0b917' for executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.450428  1265 slave.cpp:749] Successfully attached file '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:51.459421  1268 launcher.cpp:147] Forked child with pid '1453' for container '6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:51.613296  1274 slave.cpp:2643] Got registration for executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from executor(1)@172.17.0.3:56062
I0316 14:28:51.615416  1271 slave.cpp:1863] Sending queued task '864698ee-117b-4b95-b8d7-4c3ec6e0b917' to executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 at executor(1)@172.17.0.3:56062
I0316 14:28:51.622187  1272 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from executor(1)@172.17.0.3:56062
I0316 14:28:51.623610  1275 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.623646  1275 status_update_manager.cpp:497] Creating StatusUpdate stream for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.624053  1275 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to the slave
I0316 14:28:51.624423  1274 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to master@172.17.0.3:45919
I0316 14:28:51.624621  1274 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.624677  1274 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to executor(1)@172.17.0.3:56062
I0316 14:28:51.624836  1270 master.cpp:4927] Status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:51.624881  1270 master.cpp:4975] Forwarding status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.625077  1270 master.cpp:6588] Updating the state of task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0316 14:28:51.625355  1269 sched.cpp:981] Scheduler::statusUpdate took 141149ns
I0316 14:28:51.625671  1266 master.cpp:4082] Processing ACKNOWLEDGE call aee0de1c-8acd-46eb-8723-d26cd203228f for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919 on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:51.625977  1267 status_update_manager.cpp:392] Received status update acknowledgement (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.626369  1265 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:52.340801  1266 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:52.340884  1266 hierarchical.cpp:1548] No inverse offers to send out!
I0316 14:28:52.340922  1266 hierarchical.cpp:1130] Performed allocation for 1 slaves in 350313ns
I0316 14:28:53.342003  1263 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:53.342077  1263 hierarchical.cpp:1548] No inverse offers to send out!
I0316 14:28:53.342110  1263 hierarchical.cpp:1130] Performed allocation for 1 slaves in 332715ns
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0316 14:28:53.619144  1451 process.cpp:986] libprocess is initialized on 172.17.0.3:40885 for 16 cpus
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0316 14:28:53.790701  1452 process.cpp:986] libprocess is initialized on 172.17.0.3:50144 for 16 cpus
I0316 14:28:53.939643  1268 slave.cpp:3002] Handling status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from executor(1)@172.17.0.3:56062
I0316 14:28:53.940950  1267 slave.cpp:5677] Terminating task 864698ee-117b-4b95-b8d7-4c3ec6e0b917
I0316 14:28:53.942181  1275 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.942358  1275 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to the slave
I0316 14:28:53.942715  1265 slave.cpp:3400] Forwarding the update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to master@172.17.0.3:45919
I0316 14:28:53.942919  1265 slave.cpp:3294] Status update manager successfully handled status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.942961  1265 slave.cpp:3310] Sending acknowledgement for status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to executor(1)@172.17.0.3:56062
I0316 14:28:53.943159  1273 master.cpp:4927] Status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:53.943218  1273 master.cpp:4975] Forwarding status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.943392  1273 master.cpp:6588] Updating the state of task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0316 14:28:53.944248  1275 sched.cpp:981] Scheduler::statusUpdate took 172957ns
I0316 14:28:53.944351  1262 hierarchical.cpp:890] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 from framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.944548  1242 sched.cpp:1903] Asked to stop the driver
I0316 14:28:53.944672  1275 sched.cpp:1143] Stopping framework 'c7653f60-33e9-4406-9f62-dc74c906bf83-0000'
I0316 14:28:53.944736  1263 master.cpp:4082] Processing ACKNOWLEDGE call a873c6e2-442e-439e-a13f-54bb19df1881 for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919 on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:53.944795  1263 master.cpp:6654] Removing task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:53.945226  1263 master.cpp:6061] Processing TEARDOWN call for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:53.945253  1263 master.cpp:6073] Removing framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:53.945324  1275 status_update_manager.cpp:392] Received status update acknowledgement (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.945412  1274 hierarchical.cpp:375] Deactivated framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.945462  1276 slave.cpp:2079] Asked to shut down framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 by master@172.17.0.3:45919
I0316 14:28:53.945579  1276 slave.cpp:2104] Shutting down framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.945669  1276 slave.cpp:4198] Shutting down executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 at executor(1)@172.17.0.3:56062
I0316 14:28:53.945714  1275 status_update_manager.cpp:528] Cleaning up status update stream for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.945818  1274 hierarchical.cpp:326] Removed framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.946151  1265 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.946213  1265 slave.cpp:5718] Completing task 864698ee-117b-4b95-b8d7-4c3ec6e0b917
I0316 14:28:54.343000  1263 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:54.343056  1263 hierarchical.cpp:1130] Performed allocation for 1 slaves in 213036ns
I0316 14:28:54.943627  1261 slave.cpp:3528] executor(1)@172.17.0.3:56062 exited
I0316 14:28:54.944002  1274 containerizer.cpp:1608] Executor for container '6e2770ca-32d3-47ad-b4fe-7d9f26489621' has exited
I0316 14:28:54.944205  1274 containerizer.cpp:1392] Destroying container '6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:54.949076  1276 provisioner.cpp:306] Ignoring destroy request for unknown container 6e2770ca-32d3-47ad-b4fe-7d9f26489621
I0316 14:28:54.949502  1276 slave.cpp:3886] Executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 exited with status 0
I0316 14:28:54.949556  1276 slave.cpp:3990] Cleaning up executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 at executor(1)@172.17.0.3:56062
I0316 14:28:54.949807  1270 gc.cpp:55] Scheduling '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621' for gc 6.99998900785778days in the future
I0316 14:28:54.949931  1276 slave.cpp:4078] Cleaning up framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:54.950188  1276 status_update_manager.cpp:282] Closing status update streams for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:54.950196  1270 gc.cpp:55] Scheduling '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917' for gc 6.99998900606519days in the future
I0316 14:28:54.950458  1270 gc.cpp:55] Scheduling '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000' for gc 6.99998900418963days in the future
../../src/tests/container_logger_tests.cpp:461: Failure
Value of: waitpid(pstree.process.pid, __null, 0)
  Actual: -1
Expected: pstree.process.pid
Which is: 1453
I0316 14:28:54.952739  1264 slave.cpp:668] Slave terminating
I0316 14:28:54.952980  1275 master.cpp:1212] Slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) disconnected
I0316 14:28:54.953069  1275 master.cpp:2681] Disconnecting slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:54.953172  1275 master.cpp:2700] Deactivating slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:54.953404  1269 hierarchical.cpp:560] Slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 deactivated
I0316 14:28:54.957495  1274 master.cpp:1065] Master terminating
I0316 14:28:54.958026  1276 hierarchical.cpp:505] Removed slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0
[  FAILED  ] ContainerLoggerTest.LOGROTATE_RotateInSandbox (3635 ms)
{code}",Seen on ASF CI (Ubuntu 14 + GCC),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-29 20:04:45.333,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 29 20:04:45 UTC 2016,,,,,,,"0|i2urqf:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 32,,,,,,,,,,,1.0,,,,,,,,,,,"22/Mar/16 17:54;kaysoky;Pinpointed the problem here: https://reviews.apache.org/r/45051/","29/Mar/16 20:04;vinodkone;commit e8a1a578fdf85f9edf90eb7af30e55fe957c8eba
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Tue Mar 29 13:03:23 2016 -0700

    Fixed flakiness in  ContainerLoggerTest.LOGROTATE_RotateInSandbox.
    
    This removes a race between the test and the global `ReapProcess`, which were
    originally both calling `waitpid` on the logger subprocesses.  The test now
    defers the reaping to the `ReapProcess` and just waits for all logger
    subprocesses to exit naturally via `os::exists(pid)`.
    
    Since the `ReapProcess` reaps on an interval, a `Clock::advance` was inserted to
    skip the reap delay.
    
    Review: https://reviews.apache.org/r/45051/
",,,,,,,,,,,,,,,,,,,,,,,,,,
Implement reconnect funtionality in the scheduler library.,MESOS-4950,12950649,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,anandmazumdar,anandmazumdar,15/Mar/16 20:29,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,1.0.0,,,,,,,,,,,0,mesosphere,,,,,,,,"Currently, there is no way for the schedulers to force a reconnection attempt with the master using the scheduler library {{src/scheduler/scheduler.cpp}}. It is specifically useful in scenarios where there is a one way network partition with the master. Due to this, the scheduler has not received any {{HEARTBEAT}} events from the master. In this case, the scheduler might want to force a reconnection attempt with the master instead of relying on the {{disconnected}} callback.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-5016,,,MESOS-4630,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-30 19:02:47.133,,,false,MESOS-3302,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 30 19:02:47 UTC 2016,,,,,,,"0|hzzzy6:txi",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 32,,,,,,,,,,,3.0,,,,,,,,,,,"29/Mar/16 00:19;anandmazumdar;Review chain: https://reviews.apache.org/r/45408/","30/Mar/16 19:02;vinodkone;commit c6d6182223816607038b76f57ece2e1453f82d8b
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Wed Mar 30 12:00:35 2016 -0700

    Documented when to invoke `send` using the scheduler library.
    
    Trivial change to add a comment to invoke `send` only after
    receiving the `connected` callback. If not, all calls would
    be dropped while disconnected.
    
    Review: https://reviews.apache.org/r/45410/

commit bae4f13f7ad9f35202866b398b7bdd278f6d3fea
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Wed Mar 30 12:00:31 2016 -0700

    Added test for `reconnect` functionality.
    
    This change adds a trivial test for testing the `reconnect`
    method on the scheduler library.
    
    Review: https://reviews.apache.org/r/45409/

commit 140c82a41b1353fd838f19f2eaa5bb8198d704fb
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Wed Mar 30 12:00:27 2016 -0700

    Introduced a `reconnect` method on the scheduler library.
    
    This change adds a `reconnect` method that allows schedulers
    to force a reconnection with the master. This is useful when
    the scheduler detects that there is a one-way partition with
    the master (e.g., lack of `HEARTBEAT` events) and wants to
    trigger a reconnection rather than relying on the `disconnected`
    callback.
    
    Review: https://reviews.apache.org/r/45408/
",,,,,,,,,,,,,,,,,,,,,,,,,,
Docker runtime isolator tests may cause disk issue.,MESOS-4942,12950121,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,15/Mar/16 00:15,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,1.0.0,,,,,,containerization,,,,,0,containerizer,mesosphere,,,,,,,"Currently slave working directory is used as docker store dir and archive dir, which is problematic. Because slave work dir is exactly `environment->mkdtemp()`, it will get cleaned up until the end of the whole test. But the runtime isolator local puller tests cp the host's rootfs, which size is relatively big. Cleanup has to be done by each test tear down. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-15 21:42:18.115,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 15 21:42:18 UTC 2016,,,,,,,"0|i2un4f:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 31,,,,,,,,,,,2.0,,,,,,,,,,,"15/Mar/16 00:22;gilbert;https://reviews.apache.org/r/44753/","15/Mar/16 21:42;jieyu;commit a0c225418df53e714fa76cc3037bb8ccec213a7c
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Mar 15 14:41:48 2016 -0700

    Fixed runtime isolator tests out of disk issue.
    
    Review: https://reviews.apache.org/r/44753/",,,,,,,,,,,,,,,,,,,,,,,,,,
"Setup proper /etc/hostname, /etc/hosts and /etc/resolv.conf for containers in network/cni isolator.",MESOS-4922,12949638,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,avinash.mesos,qianzhang,qianzhang,13/Mar/16 00:58,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,1.0.0,,,,,,containerization,,,,,0,mesosphere,,,,,,,,"The network/cni isolator needs to properly setup /etc/hostname and /etc/hosts for the container with a hostname (e.g., randomly generated) and the assigned IP returned by CNI plugin.
We should consider the following cases:
1) container is using host filesystem
2) container is using a different filesystem
3) custom executor and command executor",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-10 19:01:50.086,,,false,MESOS-4641,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 14 16:19:40 UTC 2016,,,,,,,"0|hzzzim:zi",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 32,Mesosphere Sprint 33,,,,,,,,,,5.0,,,,,,,,,,,"10/Apr/16 19:01;avinash.mesos;https://reviews.apache.org/r/45953/
https://reviews.apache.org/r/45954/
https://reviews.apache.org/r/45955/
https://reviews.apache.org/r/45956/
https://reviews.apache.org/r/45983/","11/Apr/16 16:51;jieyu;commit 00141e4a56a81525fec1f86f2b212dcbc04e3a8c
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Mon Apr 11 09:51:16 2016 -0700

    Adding a stout interface for `sethostname` system call in linux.
    
    Review: https://reviews.apache.org/r/45953/","12/Apr/16 18:34;jieyu;commit 2d75778557d3d29291c4ec98d00d6784010be769
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Tue Apr 12 09:02:39 2016 -0700

    Added `subcommand` to `network/cni` isolator.
    
    The `subcommand` allow configuring the container hostname and setting up
    network files in the container such as /etc/hosts, /etc/hostname,
    /etc/resolv.conf. This will allow for correct name to IP resolution
    within the container network namespace.
    
    Review: https://reviews.apache.org/r/45954/","14/Apr/16 00:44;jieyu;commit 0845ec04395faeb05a518a81c89c87b726dc8711
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Wed Apr 13 17:15:21 2016 -0700

    Added the `_isolate` method.
    
    Once the `isolate` is successful, the `_isolate` method calls out the
    `mesos-cni-helper` to setup the /etc/hosts, /etc/hostname and
    /etc/resolv.conf for the container.
    
    Review: https://reviews.apache.org/r/45956/","14/Apr/16 16:19;jieyu;commit 3f745624c6e98a31050531324da444298ae9a66a
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Thu Apr 14 09:14:34 2016 -0700

    Added CNI helper subcommand to `mesos-containerizer`.
    
    This will be used by the `network/cni` isolator to setup hostname and
    various network files within the container UTS and mnt namespace.
    
    Review: https://reviews.apache.org/r/45955/",,,,,,,,,,,,,,,,,,,,,,,
LinuxFilesystemIsolatorTest.ROOT_MultipleContainers fails.,MESOS-4912,12948829,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,bernd-mesos,bernd-mesos,10/Mar/16 15:37,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,0.28.0,,,,,,,,1.0.0,,,,,,containerization,,,,,0,mesosphere,,,,,,,,"Observed on our CI:
{noformat}
[09:34:15] :	 [Step 11/11] [ RUN      ] LinuxFilesystemIsolatorTest.ROOT_MultipleContainers
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.906719  2357 linux.cpp:81] Making '/tmp/MLVLnv' a shared mount
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.923548  2357 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.924705  2376 containerizer.cpp:666] Starting container 'da610f7f-a709-4de8-94d3-74f4a520619b' for executor 'test_executor1' of framework ''
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.925355  2371 provisioner.cpp:285] Provisioning image rootfs '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:19]W:	 [Step 11/11] I0309 09:34:19.925881  2377 copy.cpp:127] Copying layer path '/tmp/MLVLnv/test_image1' to rootfs '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0'
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.835127  2376 linux.cpp:355] Bind mounting work directory from '/tmp/MLVLnv/slaves/test_slave/frameworks/executors/test_executor1/runs/da610f7f-a709-4de8-94d3-74f4a520619b' to '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.835392  2376 linux.cpp:683] Changing the ownership of the persistent volume at '/tmp/MLVLnv/volumes/roles/test_role/persistent_volume_id' with uid 0 and gid 0
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.840425  2376 linux.cpp:723] Mounting '/tmp/MLVLnv/volumes/roles/test_role/persistent_volume_id' to '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox/volume' for persistent volume disk(test_role)[persistent_volume_id:volume]:32 of container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.843878  2374 linux_launcher.cpp:304] Cloning child process with flags = CLONE_NEWNS
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.848302  2371 containerizer.cpp:666] Starting container 'fe4729c5-1e63-4cc6-a2e3-fe5006ffe087' for executor 'test_executor2' of framework ''
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.848758  2371 containerizer.cpp:1392] Destroying container 'da610f7f-a709-4de8-94d3-74f4a520619b'
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.848865  2373 provisioner.cpp:285] Provisioning image rootfs '/tmp/MLVLnv/provisioner/containers/fe4729c5-1e63-4cc6-a2e3-fe5006ffe087/backends/copy/rootfses/518b2464-43dd-47b0-9648-e78aedde6917' for container fe4729c5-1e63-4cc6-a2e3-fe5006ffe087
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.849449  2375 copy.cpp:127] Copying layer path '/tmp/MLVLnv/test_image2' to rootfs '/tmp/MLVLnv/provisioner/containers/fe4729c5-1e63-4cc6-a2e3-fe5006ffe087/backends/copy/rootfses/518b2464-43dd-47b0-9648-e78aedde6917'
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.854038  2374 cgroups.cpp:2427] Freezing cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.856693  2372 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b after 2.608128ms
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.859237  2377 cgroups.cpp:2445] Thawing cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.861454  2377 cgroups.cpp:1438] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos/da610f7f-a709-4de8-94d3-74f4a520619b after 2176us
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.934608  2378 containerizer.cpp:1608] Executor for container 'da610f7f-a709-4de8-94d3-74f4a520619b' has exited
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.937692  2372 linux.cpp:798] Unmounting volume '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox/volume' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.937742  2372 linux.cpp:817] Unmounting sandbox/work directory '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0/mnt/mesos/sandbox' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:30]W:	 [Step 11/11] I0309 09:34:30.938129  2375 provisioner.cpp:330] Destroying container rootfs at '/tmp/MLVLnv/provisioner/containers/da610f7f-a709-4de8-94d3-74f4a520619b/backends/copy/rootfses/0d7e047a-50f1-490b-bb58-00e9c49628d0' for container da610f7f-a709-4de8-94d3-74f4a520619b
[09:34:45] :	 [Step 11/11] ../../src/tests/containerizer/filesystem_isolator_tests.cpp:1318: Failure
[09:34:45] :	 [Step 11/11] Failed to wait 15secs for wait1
[09:34:48] :	 [Step 11/11] [  FAILED  ] LinuxFilesystemIsolatorTest.ROOT_MultipleContainers (32341 ms)
{noformat}","CenOS 7, SSL",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-5192,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-15 21:26:17.991,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 16 00:23:03 UTC 2016,,,,,,,"0|i2uh3j:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 31,,,,,,,,,,,3.0,,,,,,,,,,,"15/Mar/16 21:26;gilbert;https://reviews.apache.org/r/44867/","16/Mar/16 00:23;jieyu;commit 3d7feed6348e952f76924786195a595747453e74
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Mar 15 17:22:39 2016 -0700

    Fixed linux fs isolator multi containers test.
    
    Review: https://reviews.apache.org/r/44867/",,,,,,,,,,,,,,,,,,,,,,,,,,
Executor driver does not respect executor shutdown grace period.,MESOS-4911,12948824,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,alexr,alexr,alexr,10/Mar/16 15:22,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,1.0.0,,,,,,,,,,,0,mesosphere,,,,,,,,"Executor shutdown grace period, configured on the agent, is
propagated to executors via the `MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD`
environment variable. The executor driver must use this timeout to delay
the hard shutdown of the related executor.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-4919,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 19 10:35:38 UTC 2016,,,,,,,"0|i2uh2f:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 31,,,,,,,,,,,1.0,,,,,,,,,,,"15/Mar/16 14:28;alexr;https://reviews.apache.org/r/44654/","19/Mar/16 10:35;alexr;{noformat}
Commit: 630b12ad86c31ad21d536e068ee18f8722d79c48 [630b12a]
Author: Alexander Rukletsov <rukletsov@gmail.com>
Date: 18 Mar 2016 21:36:26 CET
Committer: Benjamin Mahler <bmahler@apache.org>
Commit Date: 18 Mar 2016 21:39:42 CET

Removed hard-coded executor shutdown grace period in executor driver.

The executor shutdown grace period, which configured on the agent, is
propagated to executors via the `MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD`
environment variable. The executor driver uses this timeout to delay
the forcible kill of the executor process group.

Review: https://reviews.apache.org/r/44654/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Allow multiple loads of module manifests,MESOS-4903,12948534,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,karya,karya,karya,09/Mar/16 20:46,29/Apr/19 09:26,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.28.0,,,,,,modules,,,,,0,mesosphere,,,,,,,,"The ModuleManager::load() is designed to be called exactly once during a process lifetime. This works well for Master/Agent environments. However, it can fail in Scheduler environments. For example, a single Scheduler binary might implement multiple scheduler drivers causing multiple calls to ModuleManager::load() leading to a failure.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4915,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-10 02:47:35.788,,,false,MESOS-1384,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 11 10:31:13 UTC 2016,,,,,,,"0|i2uf9z:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 30,,,,,,,,,,,3.0,,,,,,,,,,,"10/Mar/16 02:47;vinodkone;why is this targeted for 0.28.0?","11/Mar/16 01:35;karya;Here is the review request: https://reviews.apache.org/r/44694/","11/Mar/16 10:31;karya;{code}
commit 5ea02aea22122a833fb133f0a6e7c085fcd5c01c
Author: Kapil Arya <kapil@mesosphere.io>
Date:   Thu Mar 10 21:50:21 2016 -0500

    Enabled multiple calls to ModuleManager::load().
    
    As long as the module manifest(s) being loaded don't conflict with the
    already loaded manifests, the module manager will allow multiple calls
    to `load()`.
    
    Review: https://reviews.apache.org/r/44694
{code}",,,,,,,,,,,,,,,,,,,,,,,,,
Default cmd is executed as an incorrect command.,MESOS-4888,12947795,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,07/Mar/16 21:16,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.28.0,,,,,,containerization,,,,,0,containerizer,mesosphere,,,,,,,"When mesos containerizer launch a container using a docker image, which only container default Cmd. The executable command is is a incorrect sequence. For example:

If an image default entrypoint is null, cmd is ""sh"", user defines shell=false, value is none, and arguments as [-c, echo 'hello world']. The executable command is `[sh, -c, echo 'hello world', sh]`, which is incorrect. It should be `[sh, sh, -c, echo 'hello world']` instead.

This problem is only exposed for the case: sh=0, value=0, argv=1, entrypoint=0, cmd=1. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4915,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 09 17:58:47 UTC 2016,,,,,,,"0|i2uapr:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 30,,,,,,,,,,,2.0,,,,,,,,,,,"08/Mar/16 01:24;gilbert;https://reviews.apache.org/r/44468/
","09/Mar/16 17:58;gilbert;commit e42f740ccb655c0478a3002c0b6fa90c1144f41c
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Mon Mar 7 17:32:10 2016 -0800

    Fixed the logic for default docker cmd case.
    
    Review: https://reviews.apache.org/r/44468/",,,,,,,,,,,,,,,,,,,,,,,,,,
"Mesos containerizer can't handle top level docker image like ""alpine"" (must use ""library/alpine"")",MESOS-4877,12947531,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,lins05,lins05,06/Mar/16 16:44,22/Mar/16 16:20,29/Oct/20 16:32,22/Mar/16 16:20,0.27.0,0.27.1,,,,,,,0.28.1,,,,,,containerization,docker,,,,0,,,,,,,,,"This can be demonstrated with the {{mesos-execute}} command:

# Docker containerizer with image {{alpine}}: success
{code}
sudo ./build/src/mesos-execute --docker_image=alpine --containerizer=docker --name=just-a-test --command=""sleep 1000"" --master=localhost:5050
{code}
# Mesos containerizer with image {{alpine}}: failure
{code}
sudo ./build/src/mesos-execute --docker_image=alpine --containerizer=mesos --name=just-a-test --command=""sleep 1000"" --master=localhost:5050
{code}
# Mesos containerizer with image {{library/alpine}}: success
{code}
sudo ./build/src/mesos-execute --docker_image=library/alpine --containerizer=mesos --name=just-a-test --command=""sleep 1000"" --master=localhost:5050
{code}

In the slave logs:

{code}
ea-4460-83
9c-838da86af34c-0007'
I0306 16:32:41.418269  3403 metadata_manager.cpp:159] Looking for image 'alpine:latest'
I0306 16:32:41.418699  3403 registry_puller.cpp:194] Pulling image 'alpine:latest' from 'docker-manifest://registry-1.docker.io:443alpine?latest#https' to '/tmp/mesos-test
/store/docker/staging/ka7MlQ'
E0306 16:32:43.098131  3400 slave.cpp:3773] Container '4bf9132d-9a57-4baa-a78c-e7164e93ace6' for executor 'just-a-test' of framework 4f055c6f-1bea-4460-839c-838da86af34c-0
007 failed to start: Collect failed: Unexpected HTTP response '401 Unauthorized
{code}

curl command executed:

{code}
$ sudo sysdig -A -p ""*%evt.time %proc.cmdline"" evt.type=execve and proc.name=curl                                                                   16:42:53.198998042 curl -s -S -L -D - https://registry-1.docker.io:443/v2/alpine/manifests/latest
16:42:53.784958541 curl -s -S -L -D - https://auth.docker.io/token?service=registry.docker.io&scope=repository:alpine:pull
16:42:54.294192024 curl -s -S -L -D - -H Authorization: Bearer eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCIsIng1YyI6WyJNSUlDTHpDQ0FkU2dBd0lCQWdJQkFEQUtCZ2dxaGtqT1BRUURBakJHTVVRd1FnWURWUVFERXp0Uk5Gb3pPa2RYTjBrNldGUlFSRHBJVFRSUk9rOVVWRmc2TmtGRlF6cFNUVE5ET2tGU01rTTZUMFkzTnpwQ1ZrVkJPa2xHUlVrNlExazFTekFlRncweE5UQTJNalV4T1RVMU5EWmFGdzB4TmpBMk1qUXhPVFUxTkRaYU1FWXhSREJDQmdOVkJBTVRPMGhHU1UwNldGZFZWam8yUVZkSU9sWlpUVEk2TTFnMVREcFNWREkxT2s5VFNrbzZTMVExUmpwWVRsSklPbFJMTmtnNlMxUkxOanBCUVV0VU1Ga3dFd1lIS29aSXpqMENBUVlJS29aSXpqMERBUWNEUWdBRXl2UzIvdEI3T3JlMkVxcGRDeFdtS1NqV1N2VmJ2TWUrWGVFTUNVMDByQjI0akNiUVhreFdmOSs0MUxQMlZNQ29BK0RMRkIwVjBGZGdwajlOWU5rL2pxT0JzakNCcnpBT0JnTlZIUThCQWY4RUJBTUNBSUF3RHdZRFZSMGxCQWd3QmdZRVZSMGxBREJFQmdOVkhRNEVQUVE3U0VaSlRUcFlWMVZXT2paQlYwZzZWbGxOTWpveldEVk1PbEpVTWpVNlQxTktTanBMVkRWR09saE9Va2c2VkVzMlNEcExWRXMyT2tGQlMxUXdSZ1lEVlIwakJEOHdQWUE3VVRSYU16cEhWemRKT2xoVVVFUTZTRTAwVVRwUFZGUllPalpCUlVNNlVrMHpRenBCVWpKRE9rOUdOemM2UWxaRlFUcEpSa1ZKT2tOWk5Vc3dDZ1lJS29aSXpqMEVBd0lEU1FBd1JnSWhBTXZiT2h4cHhrTktqSDRhMFBNS0lFdXRmTjZtRDFvMWs4ZEJOVGxuWVFudkFpRUF0YVJGSGJSR2o4ZlVSSzZ4UVJHRURvQm1ZZ3dZelR3Z3BMaGJBZzNOUmFvPSJdfQ.eyJhY2Nlc3MiOltdLCJhdWQiOiJyZWdpc3RyeS5kb2NrZXIuaW8iLCJleHAiOjE0NTcyODI4NzQsImlhdCI6MTQ1NzI4MjU3NCwiaXNzIjoiYXV0aC5kb2NrZXIuaW8iLCJqdGkiOiJaOGtyNXZXNEJMWkNIRS1IcVJIaCIsIm5iZiI6MTQ1NzI4MjU3NCwic3ViIjoiIn0.C2wtJq_P-m0buPARhmQjDfh6ztIAhcvgN3tfWIZEClSgXlVQ_sAQXAALNZKwAQL2Chj7NpHX--0GW-aeL_28Aw https://registry-1.docker.io:443/v2/alpine/manifests/latest
{code}

Also got the same result with {{ubuntu}} docker image.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-06 23:23:25.392,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 22 16:20:10 UTC 2016,,,,,,,"0|i2uljp:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 31,,,,,,,,,,,3.0,,,,,,,,,,,"06/Mar/16 17:03;lins05;I've tried to send the requests to docker registry by hand:

# First Get the token: 
{code}
curl 'https://auth.docker.io/token?service=registry.docker.io&scope=repository:alpine:pull'
{code}
# Then request the manifest from the registry:
{code}
curl -H 'Authorization: Bearer <token>'  'https://auth.docker.io/token?service=registry.docker.io&scope=repository:alpine:pull'
{code}

It would fail with status code 401.  In the response header, there is a message {{error=""insufficient_scope""}}:

{code}
Www-Authenticate: Bearer realm=""https://auth.docker.io/token"",service=""registry.docker.io"",scope=""repository:alpine:pull"",error=""insufficient_scope""
{code}","06/Mar/16 17:22;lins05;I think we can just add a {{library/}} prefix to the image name if both of the following conditions are true:

* the image name doesn't have a slash in the {{ImageReference.repository}} part, and
* the image host is empty or the same as the one specified in {{--docker_registry}}

This is how docker engine handles the image name: https://github.com/docker/docker/blob/v1.10.2/reference/reference.go#L171-L173","06/Mar/16 23:23;gyliu;{quota}
I think we can just add a library/ prefix to the image name if both of the following conditions are true:
the image name doesn't have a slash in the ImageReference.repository part, and
the image host is empty or the same as the one specified in --docker_registry
{quota}

This solution only fit into the case when end user using docker remote registry but cannot handle the case of local registry as end user may not always have {{library/}} as prefix for a image, what about clarify this in the document?","07/Mar/16 04:05;lins05;Yeah, we need to think about what to do if the user is using a local registry. But I still think this need to fixed on mesos side because this could cause confusion for the users. He may ask: I can ""ubuntu"" image when using docker containerizer (or using docker command line), why do I have to change the name to ""library/ubuntu"" when using the mesos containerizer?","07/Mar/16 05:19;gyliu;Yes, that's make sense. Do you know how docker is handling such case with a local registry? does it still add a prefix or just fail for such case.","13/Mar/16 05:14;gyliu;There is a patch chain here https://reviews.apache.org/r/44672/","17/Mar/16 16:36;gilbert;Sorry [~lins05], I addressed those TODOs before I saw this JIRA. Would you mind to take those patches?

https://reviews.apache.org/r/44672/","19/Mar/16 06:49;lins05;Never mind! I'll reassign this ticket you.","21/Mar/16 06:46;gilbert;Thanks, [~lins05]!","21/Mar/16 06:48;gilbert;Re-post all reviewable patches here:
https://reviews.apache.org/r/44672/
https://reviews.apache.org/r/44673/","22/Mar/16 16:20;jieyu;commit e951a9aac9f78c84ddad43536098ce4e323aef2b
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Mar 22 09:17:20 2016 -0700

    Added test for registry puller normalize.
    
    Review: https://reviews.apache.org/r/44673/

commit e56d1a84af4d5ca97f5b41c701c23637d3d0fd65
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Mar 22 09:15:50 2016 -0700

    Added normalize method to registry puller.
    
    Review: https://reviews.apache.org/r/44672/",,,,,,,,,,,,,,,,,
Fix rmdir for windows,MESOS-4836,12946008,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hausdorff,vinodkone,vinodkone,02/Mar/16 01:45,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.28.0,,,,,,,,,,,0,mesosphere,windows,windows-mvp,,,,,,This is due to a bug in MESOS-4415 that landed for 0.27.0.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-02 07:49:52.15,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 02 07:58:23 UTC 2016,,,,,,,"0|i2u08f:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 30,,,,,,,,,,,1.0,,,,,,,,,,,"02/Mar/16 07:49;jvanremoortere;https://reviews.apache.org/r/43907/
https://reviews.apache.org/r/43908/","02/Mar/16 07:58;jvanremoortere;{code}
commit acd2f5e6fbc51d7a0397eec0e626e670842b0f84
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Tue Mar 1 23:53:14 2016 -0800

    Added rmdir tests target.

commit c55ca2941b0119c13764bccdebcea46119e69e4e
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Tue Mar 1 23:29:31 2016 -0800

    Stout:[2/2] Added significant test coverage of `os::rmdir`.
    
    Review: https://reviews.apache.org/r/43908/

commit fbe1f37f65fd9f1d4f2c30a3cfd7a50df92ccc2c
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Tue Mar 1 23:29:21 2016 -0800

    Stout:[1/2] Fixed error reporting bug in `os::rmdir`.
    
    Review: https://reviews.apache.org/r/43907/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
CgroupsAnyHierarchyWithFreezerTest.ROOT_CGROUPS_DestroyTracedProcess is flaky,MESOS-4835,12946004,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Won't Fix,,kaysoky,kaysoky,02/Mar/16 01:22,24/Jan/19 20:20,29/Oct/20 16:32,24/Jan/19 20:20,,,,,,,,,,,,,,,,,,,,0,flaky,mesosphere,test,,,,,,"Verbose logs: 
{code}
[ RUN      ] CgroupsAnyHierarchyWithFreezerTest.ROOT_CGROUPS_DestroyTracedProcess
I0302 00:43:14.127846 11755 cgroups.cpp:2427] Freezing cgroup /sys/fs/cgroup/freezer/mesos_test
I0302 00:43:14.267411 11758 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos_test after 139.46496ms
I0302 00:43:14.409395 11751 cgroups.cpp:2445] Thawing cgroup /sys/fs/cgroup/freezer/mesos_test
I0302 00:43:14.551304 11751 cgroups.cpp:1438] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos_test after 141.811968ms
../../src/tests/containerizer/cgroups_tests.cpp:949: Failure
Value of: ::waitpid(pid, &status, 0)
  Actual: 23809
Expected: -1
../../src/tests/containerizer/cgroups_tests.cpp:950: Failure
Value of: (*__errno_location ())
  Actual: 0
Expected: 10
[  FAILED  ] CgroupsAnyHierarchyWithFreezerTest.ROOT_CGROUPS_DestroyTracedProcess (1055 ms)
{code}","Seen on Ubuntu 15 & Debian 8, GCC 4.9",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-01-24 20:20:06.168,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 24 20:20:06 UTC 2019,,,,,,,"0|hzzzkn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,"24/Jan/19 20:20;vinodkone;Resolving stale issue. Please re-open if the issue persists.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Bind docker runtime isolator with docker image provider.,MESOS-4830,12945924,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,01/Mar/16 19:43,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.28.0,,,,,,containerization,,,,,0,containerizer,mesosphere,,,,,,,"If image provider is specified as `docker` but docker/runtime is not set, it would be not meaningful, because of no executables. A check should be added to make sure docker runtime isolator is on if using docker as image provider.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-01 21:16:09.858,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 01 21:16:09 UTC 2016,,,,,,,"0|i2tzpr:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 30,,,,,,,,,,,1.0,,,,,,,,,,,"01/Mar/16 19:50;gilbert;https://reviews.apache.org/r/44221/","01/Mar/16 21:16;jieyu;commit c7dea5888b4f5194e7d1c6abe0f379ab229e3039
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Mar 1 13:15:23 2016 -0800

    Added docker runtime isolator check if image providers include docker.
    
    Review: https://reviews.apache.org/r/44221/",,,,,,,,,,,,,,,,,,,,,,,,,,
"""filesystem/linux"" isolator does not unmount orphaned persistent volumes",MESOS-4824,12945688,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,kaysoky,kaysoky,kaysoky,01/Mar/16 00:28,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,0.24.0,0.25.0,0.26.0,0.27.0,,,,,0.28.0,,,,,,containerization,,,,,1,containerizer,mesosphere,persistent-volumes,,,,,,"A persistent volume can be orphaned when:
# A framework registers with checkpointing enabled.
# The framework starts a task + a persistent volume.
# The agent exits.  The task continues to run.
# Something wipes the agent's {{meta}} directory.  This removes the checkpointed framework info from the agent.
# The agent comes back and recovers.  The framework for the task is not found, so the task is considered orphaned now.

The agent currently does not unmount the persistent volume, saying (with {{GLOG_v=1}}) 
{code}
I0229 23:55:42.078940  5635 linux.cpp:711] Ignoring cleanup request for unknown container: a35189d3-85d5-4d02-b568-67f675b6dc97
{code}

Test implemented here: https://reviews.apache.org/r/44122/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-1554,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 02 15:43:09 UTC 2016,,,,,,,"0|i2ty9b:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 30,,,,,,,,,,,2.0,,,,,,,,,,,"02/Mar/16 01:08;kaysoky;|| Review || Summary ||
| https://reviews.apache.org/r/44122/ | Two new tests |
| https://reviews.apache.org/r/44196/ | Fix for orphans |","02/Mar/16 15:43;kaysoky;{code}
commit e3cb9fcabeaf8031e12243467d7d2b0eb7f2c546
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Tue Mar 1 21:57:16 2016 -0800

    Added tests for recovering orphaned persistent volumes.
    
    Based on
    DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes.
    This tests orphaned persistent volumes and the MesosContainerizer, with
    and without rootfs.
    
    Review: https://reviews.apache.org/r/44122/
{code}
{code}
commit f5c28102baf309365392b00dd69d94af776ae53d
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Tue Mar 1 21:57:48 2016 -0800

    Fixed MesosContainerizer orphaned persistent volume recovery.
    
    Adds extra mount-table checking logic specifically for orphaned
    persistent volumes that can be safely cleaned up.  This includes ""known""
    orphans (i.e. containers detected via the `Launcher`).
    
    Also adds some extra helpers in `slave::paths`.
    
    Review: https://reviews.apache.org/r/44196/
{code}
{code}
commit 2de2e5791a6c119e26e9e0bc35bdea4b2e54bbec
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Tue Mar 1 23:18:19 2016 -0800

    Removed a redundant orphan recovery test for persistent volumes.
    
    The test in LinuxFilesystemIsolatorTest uses the command task. The
    command executor itself does not change filesystem root. So these two
    tests are essentially testing the same thing. Remove one of them. Added
    a TODO about testing the scenario that the executor itself is changing
    filesystem root.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos fails to escape command health checks,MESOS-4812,12945534,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,akagup,lloesche,lloesche,29/Feb/16 17:46,26/Nov/18 12:48,29/Oct/20 16:32,26/Nov/18 12:48,0.25.0,,,,,,,,,,,,,,,,,,,4,health-check,mesosphere,tech-debt,,,,,,"As described in https://github.com/mesosphere/marathon/issues/3333
I would like to run a command health check
{noformat}
/bin/bash -c ""</dev/tcp/$HOST/$PORT0""
{noformat}

The health check fails because Mesos, while running the command inside double quotes of a sh -c """" doesn't escape the double quotes in the command.

If I escape the double quotes myself the command health check succeeds. But this would mean that the user needs intimate knowledge of how Mesos executes his commands which can't be right.

I was told this is not a Marathon but a Mesos issue so am opening this JIRA. I don't know if this only affects the command health check.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/16 16:35;haosdent@gmail.com;health_task.gif;https://issues.apache.org/jira/secure/attachment/12799279/health_task.gif",,,,,1.0,,,,,,,,,,,,,,,,,,,,2016-04-17 05:41:08.761,,,false,MESOS-5916,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 26 07:37:32 UTC 2018,,,,,,,"0|hzzyak:t",9223372036854775807,,,,,alexr,,,,,,Mesosphere Sprint 65,Mesosphere Sprint 66,,,,,,,,,,5.0,,,,,,,,,,,"17/Apr/16 05:41;haosdent@gmail.com;Hi, do you use MesosContainerizer or DockerContainerizer?  [~lloesche]

As I checked by [HealthCheckTest.HealthyTaskShellEscape|https://reviews.apache.org/r/46306/diff/1#index_header]

All below ways could pass:

{code}
  // First way.
  command.set_shell(true);
  command.set_value(""bash -c \""</dev/tcp/$HOST/$PORT0\"""");
{code}

{code}
  // Second way.
  command.set_shell(true);
  command.set_value(""</dev/tcp/$HOST/$PORT0"");
{code}

{code}
  // Third way.
  command.set_shell(false);
  command.set_value(""bash"");
  command.add_arguments(""bash"");
  command.add_arguments(""-c"");
  command.add_arguments(""</dev/tcp/$HOST/$PORT0"");
{code}

And I am going to set up a marathon to test e2e.","17/Apr/16 06:29;haosdent@gmail.com;My marathon task json
{code}
{
  ""app"": {
    ""id"": ""/test-health"",
    ""cmd"": ""sleep 200"",
    ""args"": null,
    ""user"": null,
    ""env"": {},
    ""instances"": 1,
    ""cpus"": 1,
    ""mem"": 128,
    ""disk"": 0,
    ""executor"": """",
    ""constraints"": [],
    ""uris"": [],
    ""fetch"": [],
    ""storeUrls"": [],
    ""ports"": [
      10000
    ],
    ""portDefinitions"": [
      {
        ""port"": 10000,
        ""protocol"": ""tcp"",
        ""labels"": {}
      }
    ],
    ""requirePorts"": false,
    ""backoffSeconds"": 1,
    ""backoffFactor"": 1.15,
    ""maxLaunchDelaySeconds"": 3600,
    ""container"": null,
    ""healthChecks"": [
      {
        ""protocol"": ""COMMAND"",
        ""command"": {
          ""value"": ""bash -c \""</dev/tcp/www.google.com/80\""""
        },
        ""gracePeriodSeconds"": 300,
        ""intervalSeconds"": 60,
        ""timeoutSeconds"": 20,
        ""maxConsecutiveFailures"": 3,
        ""ignoreHttp1xx"": false
      }
    ],
    ""readinessChecks"": [],
    ""dependencies"": [],
    ""upgradeStrategy"": {
      ""minimumHealthCapacity"": 1,
      ""maximumOverCapacity"": 1
    },
    ""labels"": {},
    ""acceptedResourceRoles"": null,
    ""ipAddress"": null,
    ""version"": ""2016-04-17T06:24:40.457Z"",
    ""residency"": null,
    ""versionInfo"": {
      ""lastScalingAt"": ""2016-04-17T06:24:40.457Z"",
      ""lastConfigChangeAt"": ""2016-04-17T06:24:40.457Z""
    },
    ""tasksStaged"": 0,
    ""tasksRunning"": 1,
    ""tasksHealthy"": 1,
    ""tasksUnhealthy"": 0,
    ""deployments"": [],
    ""tasks"": [
      {
        ""id"": ""test-health.1108b4a6-0465-11e6-9af4-0242d20a0294"",
        ""slaveId"": ""7f8c8ce6-45b2-4e63-a5c4-a88e08af12ed-S0"",
        ""host"": ""127.0.0.1"",
        ""startedAt"": ""2016-04-17T06:24:40.770Z"",
        ""stagedAt"": ""2016-04-17T06:24:40.537Z"",
        ""ports"": [
          31619
        ],
        ""version"": ""2016-04-17T06:24:40.457Z"",
        ""ipAddresses"": [
          {
            ""ipAddress"": ""127.0.0.1"",
            ""protocol"": ""IPv4""
          }
        ],
        ""appId"": ""/test-health"",
        ""healthCheckResults"": [
          {
            ""alive"": true,
            ""consecutiveFailures"": 0,
            ""firstSuccess"": ""2016-04-17T06:24:40.924Z"",
            ""lastFailure"": null,
            ""lastSuccess"": ""2016-04-17T06:24:40.924Z"",
            ""lastFailureCause"": null,
            ""taskId"": ""test-health.1108b4a6-0465-11e6-9af4-0242d20a0294""
          }
        ]
      }
    ]
  }
}
{code}

Which could pass health check in Mesos
{code}
Registered executor on 127.0.0.1
Starting task test-health.1108b4a6-0465-11e6-9af4-0242d20a0294
sh -c 'sleep 200'
Forked command at 27046
Launching health check process: /home/haosdent/mesos/build/src/mesos-health-check --executor=(1)@127.0.0.1:39822 --health_check_json={""command"":{""shell"":true,""value"":""bash -c \""<\/dev\/tcp\/www.google.com\/80\""""},""consecutive_failures"":3,""delay_seconds"":0.0,""grace_period_seconds"":300.0,""interval_seconds"":60.0,""timeout_seconds"":20.0} --task_id=test-health.1108b4a6-0465-11e6-9af4-0242d20a0294
Health check process launched at pid: 27047
Received task health update, healthy: true
{code}","18/Apr/16 12:28;lloesche;Hi, thanks for looking into this! I know how to make the health check pass. In the Github Link above I explained how I worked around the problem which is essentially your first solution.

The second solution is broken for systems that don't use bash for /bin/sh as /dev/tcp is a bash only thing.

Anyways finding some workaround is not the problem. This issue is about Mesos (or Marathon) doing the wrong thing, imho. Why would the user need to know about the details of the implementation to get a valid shell command executed?

Like why would you expect the user to read the Mesos source code to find out his command is executed inside a
{noformat}
/bin/sh -c """"
{noformat}
and that's why he has to escape double quotes in his own command before sending it to Mesos. That seems unreasonable to me.
","18/Apr/16 14:25;haosdent@gmail.com;[~lloesche] Sorry for my unclear explain. I mean I could not reproduce the problem you mentioned in github link. As you see, I use
{code}
    ""healthChecks"": [
        ...
        ""command"": {
          ""value"": ""bash -c \""</dev/tcp/www.google.com/80\""""
        },
    ...
{code}
in marathon task definition. Which you mentioned didn't work in github link.

And 
{code}
  // First way.
  command.set_shell(true);
  command.set_value(""bash -c \""</dev/tcp/$HOST/$PORT0\"""");
{code}
is equal to 
{code}
{
      ""protocol"": ""COMMAND"",
      ""command"": { ""value"": ""/bin/bash -c \""</dev/tcp/$HOST/$PORT0\"""" }
    }
{code}
the one you mentioned in github link. Because in {{""...""}}, you have to add slash if you want to put {{""}}.

{code}
  {
    ""protocol"": ""COMMAND"",
    ""command"": { ""value"": ""/bin/bash -c \\\""</dev/tcp/$HOST/$PORT0\\\"""" }
  }
{code}
is equal to
{code}
  command.set_shell(true);
  command.set_value(""bash -c \\\""</dev/tcp/$HOST/$PORT0\\\"""");
{code}

However, this is not work for me. I use MesosContainerizer and marathon master branch, would you double check this again?","18/Apr/16 14:43;lloesche;I think you're mixing up syntax. What you wrote was
{noformat}
command.set_value(""bash -c \""</dev/tcp/$HOST/$PORT0\"""");
{noformat}
which is a direct call to set_value().

Whereas what I wrote is
{noformat}
    {
      ""protocol"": ""COMMAND"",
      ""command"": { ""value"": ""/bin/bash -c \""</dev/tcp/$HOST/$PORT0\"""" }
    }
{noformat}
as part of a JSON file! Remember that the JSON data must also be escaped.
The escaped double quote \"" will be replaced by the JSON parser.

So the command that's being send to Mesos ends up being:
{noformat}
/bin/bash -c ""</dev/tcp/$HOST/$PORT0""
{noformat}
which is a perfectly valid shell command but will not execute successfully.

What you have to send to Mesos is
{noformat}
/bin/bash -c \""</dev/tcp/$HOST/$PORT0\""
{noformat}
because Mesos will take that string and put a /bin/sh -c """" around it.

To do that the JSON file that you're sending to Marathon has to look like this:
{noformat}
  {
    ""protocol"": ""COMMAND"",
    ""command"": { ""value"": ""/bin/bash -c \\\""</dev/tcp/$HOST/$PORT0\\\"""" }
  }
{noformat}

So here I'm escaping the backslash as well as the double quotes inside the JSON string so that I'm left with one backslash and double quotes after the JSON parser has processed the data. That can then be send to Mesos and successfully executed.

But as you can see from this conversation this gets confusing really quickly even for people working with it daily. So how can we expect our users to work with it.
","18/Apr/16 16:21;haosdent@gmail.com;{quote}
What you have to send to Mesos is
/bin/bash -c \""</dev/tcp/$HOST/$PORT0\""
{quote}

Actually this is not true. You know I use C++ here, and I have to add \, just like why you need add \ in json
{code}
command.set_value(""bash -c \""</dev/tcp/$HOST/$PORT0\"""");
{code}
If I don't add \, compile would failed. Because
{code}
command.set_value(""bash -c ""</dev/tcp/$HOST/$PORT0"""");
{code}
is illegal C++ sytnax.

As you see, 
{code}
""bash -c \""</dev/tcp/$HOST/$PORT0\""""
{code}
would become the string
{code}
bash -c </dev/tcp/$HOST/$PORT0
{code}
in C++.

Anyway, do you mind try my task definition in your side?
{code}
{
	""id"": ""/test-health"",
	""cmd"": ""sleep 200"",
	""healthChecks"": [{
		""protocol"": ""COMMAND"",
		""command"": {
			""value"": ""bash -c \""</dev/tcp/www.google.com/80\""""
		},
		""gracePeriodSeconds"": 300,
		""intervalSeconds"": 60,
		""timeoutSeconds"": 20,
		""maxConsecutiveFailures"": 3,
		""ignoreHttp1xx"": false
	}]
}
{code}

I sure it would running correctly in my marathon. If you not convenience at this, I would like to send you a video to prove this.","18/Apr/16 16:37;haosdent@gmail.com;The gif a bit large although I have already compress it.
!https://issues.apache.org/jira/secure/attachment/12799279/health_task.gif!

It show my steps in marathon I mentioned above.  ","01/May/16 06:08;haosdent@gmail.com;[~lloesche] I closed this because can not reproduce, free feel to reopen it if you can reproduce it. Thanks a lot!","07/Dec/16 04:18;haosdent@gmail.com;Reopen this ticket and wait for the details from [~lloesche] about how to reproduce it.","07/Dec/16 14:13;haosdent@gmail.com;Thanks [~lloesche]'s help. I could reproduce by this application definition
{code}
{
  ""id"": ""/test"",
  ""cmd"": null,
  ""cpus"": 1,
  ""mem"": 128,
  ""disk"": 0,
  ""instances"": 1,
  ""executor"": null,
  ""fetch"": null,
  ""constraints"": null,
  ""acceptedResourceRoles"": null,
  ""user"": null,
  ""container"": {
    ""docker"": {
      ""image"": ""nginx"",
      ""forcePullImage"": false,
      ""privileged"": false,
      ""portMappings"": [
        {
          ""containerPort"": 80,
          ""protocol"": ""tcp""
        }
      ],
      ""network"": ""BRIDGE""
    }
  },
  ""labels"": null,
  ""healthChecks"": [
    {
      ""protocol"": ""COMMAND"",
      ""command"": {
        ""value"": ""bash -c \""</dev/tcp/$HOST/$PORT0\""""
      }
    }
  ],
  ""env"": null
}
{code}


It is because we warp the health check command with {{docker exec}} blindly. 
{code}
      // Wrap the original health check command in `docker exec`.
      const CommandInfo& command = healthCheck.command();

      vector<string> commandArguments;
      commandArguments.push_back(docker->getPath());
      commandArguments.push_back(""exec"");
      commandArguments.push_back(containerName);

      if (command.shell()) {
        commandArguments.push_back(""sh"");
        commandArguments.push_back(""-c"");
        commandArguments.push_back(""\"""");
        commandArguments.push_back(command.value());
        commandArguments.push_back(""\"""");
      } else {
        commandArguments.push_back(command.value());

        foreach (const string& argument, command.arguments()) {
          commandArguments.push_back(argument);
        }
      }

      healthCheck.mutable_command()->set_shell(true); <-- Cause problem.
      healthCheck.mutable_command()->clear_arguments();
      healthCheck.mutable_command()->set_value(
          strings::join("" "", commandArguments)); <-- Cause problem.
{code}

Then it would generate the health check command 
{code}
sh -c 'docker exec mesos-ce13aa71-ebba-4361-b6dd-8d4ce57ea4ab-S9.566f6c77-a6c9-46e0-bc40-5fe95a1aa9ae sh -c "" bash -c ""</dev/tcp/$HOST/$PORT"" ""'
{code}

This leads to the error
{code}
sh: 1: cannot open /dev/tcp/127.0.0.1/80 : No such file
{code}","18/Dec/16 17:29;haosdent@gmail.com;Patch: https://reviews.apache.org/r/54846/","08/Feb/17 14:19;jdef;would like to see some traction on this. several issues have been reported against Marathon, the latest of which is https://github.com/mesosphere/marathon/issues/5136","17/Jul/17 22:53;xds2000;any update?","20/Jul/17 10:45;lloesche;[~alexr] do you have cycles to look into this?","31/Jul/17 13:35;alexr;[~haosdent@gmail.com], could you please rebase this?","18/Sep/17 12:31;abudnik;Reworked Haosdent's patch:
https://reviews.apache.org/r/62381/","04/Oct/17 10:52;abudnik;I have closed [/r/62381|https://reviews.apache.org/r/62381/], for details see comment in discard reason.","26/Feb/18 07:37;andschwa;{noformat}
commit 3b6d58cfb
Author: Akash Gupta <akash-gupta@hotmail.com>
Date:   Sat Feb 24 12:10:11 2018 -0800

    Fixed quoting bug in Docker executor command checks.

    The docker executor was wrapping the `docker exec` command with
    `sh -c """"`, which was causing problems due to quoting, especially on
    Windows. Now, the command health check simply runs `docker exec`
    without any wrapping.

    Review: https://reviews.apache.org/r/65419/

commit 8098ac239
Author: Akash Gupta <akash-gupta@hotmail.com>
Date:   Sun Feb 25 13:41:15 2018 -0800

    Added test to check quotes in Docker executor health check.

    Added `ROOT_DOCKER_DockerHealthyTaskWithQuotedCommand` to make sure that
    the Docker executor properly deals with quotes in the command health
    check to ensure that MESOS-4812 is fixed.

    Review: https://reviews.apache.org/r/65728/
{noformat}",,,,,,,,,,
IOTest.BufferedRead writes to the current directory,MESOS-4807,12945379,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,Fixed,yongtang,bbannier,bbannier,29/Feb/16 08:38,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,1.0.0,,,,,,libprocess,test,,,,0,mesosphere,newbie,parallel-tests,,,,,,"libprocess's {{IOTest.BufferedRead}} writes to the current directory. This is bad for a number of reasons, e.g.,

* should the test fail data might be leaked to random locations,
* the test cannot be executed from a write-only directory, or
* executing the same test in parallel would race on the existence of the created file, and show bogus behavior.

The test should probably be executed from a temporary directory, e.g., via stout's {{TemporaryDirectoryTest}} fixture.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-04 07:28:15.393,,,false,MESOS-4809,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 15 20:21:09 UTC 2016,,,,,,,"0|i2twdb:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 31,,,,,,,,,,,1.0,,,,,,,,,,,"04/Mar/16 07:28;yongtang;Added Review Request:
https://reviews.apache.org/r/44380/","15/Mar/16 20:21;jvanremoortere;{code}
commit 6202ab1562c4c9355f31553b5d22dd8db137ec91
Author: Yong Tang <yong.tang.github@outlook.com>
Date:   Tue Mar 15 12:57:35 2016 -0700

    Fixed IOTest.BufferedRead to write to a temporary directory.
    
    This commit changes IOTest.BufferedRead so that this specific
    test (not all IOTest) could be executed from temporary
    directories via TemporaryDirectoryTest fixture (MESOS-4807).
    
    Review: https://reviews.apache.org/r/44380/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
LevelDBStateTests write to the current directory,MESOS-4806,12945377,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,bbannier,bbannier,29/Feb/16 08:36,26/Mar/18 08:15,29/Oct/20 16:32,02/Mar/16 17:21,,,,,,,,,0.28.0,,,,,,test,,,,,0,newbie,parallel-tests,,,,,,,"All {{LevelDBStateTest}} tests write to the current directory. This is bad for a number of reasons, e.g.,

* should the test fail data might be leaked to random locations,
* the test cannot be executed from a write-only directory, or
* executing tests from the same suite in parallel (e.g., with {{gtest-parallel}} would race on the existence of the created files, and show bogus behavior.

The tests should probably be executed from a temporary directory, e.g., via stout's {{TemporaryDirectoryTest}} fixture.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4042,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-02 17:21:27.717,,,false,MESOS-4809,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 02 17:21:27 UTC 2016,,,,,,,"0|i2twcv:",9223372036854775807,,,,,tillt,,,,,,Mesosphere Sprint 30,,,,,,,,,,,2.0,,,,,,,,,,,"29/Feb/16 09:51;bbannier;Review: https://reviews.apache.org/r/44163/","02/Mar/16 17:21;tillt;{noformat}
commit effce8ca660957dcdc4e2fee74f4c38f27b9b2cd
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Wed Mar 2 16:46:21 2016 +0100

    Used temporary directory for fixture creating output files.

    Review: https://reviews.apache.org/r/44163/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Executor env variables should not be leaked to the command task.,MESOS-4781,12944703,Bug,Reviewable,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,gilbert,gilbert,25/Feb/16 23:03,01/Jul/19 17:32,29/Oct/20 16:32,,,,,,,,,,,,,,,,containerization,,,,,0,mesosphere,,,,,,,,"Currently, command task inherits the env variables of the command executor. This is less ideal because the command executor environment variables include some Mesos internal env variables like MESOS_XXX and LIBPROCESS_XXX. Also, this behavior does not match what Docker containerizer does. We should construct the env variables from scratch for the command task, rather than relying on inheriting the env variables from the command executor.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-26 18:36:55.272,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 01 17:32:32 UTC 2019,,,,,,,"0|hzzze3:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 30,Mesosphere Sprint 31,Mesosphere Sprint 32,Mesosphere Sprint 33,Mesosphere Sprint 34,Mesosphere Sprint 35,Mesosphere Sprint 36,,,,,3.0,,,,,,,,,,,"26/Feb/16 18:36;vinodkone;s/inhibit/inherit/ ?","26/Feb/16 18:41;gilbert;Yeah, thanks.","14/Mar/16 17:34;gilbert;https://reviews.apache.org/r/44499/
https://reviews.apache.org/r/44500/","06/Feb/17 22:32;jieyu;One exception maybe MESOS_SANDBOX. Also, we need to consider those task that depends on libprocess (e.g., running framework as a command task). People might already depend on this behavior. We need to have a deprecation cycle for that.","06/Feb/17 22:38;adam-mesos;I guess we've already discussed LIBPROCESS_IP inheritance in MESOS-3740","08/Feb/18 20:58;jieyu;[~gilbert], are you still working on this?","01/Jul/19 17:32;gilbert;[~jieyu], sorry I missed your comment. No, not actively let me unassign for now",,,,,,,,,,,,,,,,,,,,,
MasterMaintenanceTest.InverseOffers is flaky,MESOS-4768,12944005,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,kaysoky,kaysoky,kaysoky,24/Feb/16 22:57,26/Apr/17 17:01,29/Oct/20 16:32,25/Feb/16 07:34,0.28.0,,,,,,,,0.28.0,,,,,,test,,,,,0,mesosphere,test,,,,,,,"[MESOS-4169] significantly sped up this test, but also surfaced some more flakiness.  This can be fixed in the same way as [MESOS-4059].

Verbose logs from ASF Centos7 build:
{code}
[ RUN      ] MasterMaintenanceTest.InverseOffers
I0224 22:35:53.714018  1948 leveldb.cpp:174] Opened db in 2.034387ms
I0224 22:35:53.714663  1948 leveldb.cpp:181] Compacted db in 608839ns
I0224 22:35:53.714709  1948 leveldb.cpp:196] Created db iterator in 19043ns
I0224 22:35:53.714844  1948 leveldb.cpp:202] Seeked to beginning of db in 2330ns
I0224 22:35:53.714956  1948 leveldb.cpp:271] Iterated through 0 keys in the db in 518ns
I0224 22:35:53.715092  1948 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0224 22:35:53.715646  1968 recover.cpp:447] Starting replica recovery
I0224 22:35:53.715915  1981 recover.cpp:473] Replica is in EMPTY status
I0224 22:35:53.717067  1972 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4533)@172.17.0.1:36678
I0224 22:35:53.717445  1981 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0224 22:35:53.717888  1978 recover.cpp:564] Updating replica status to STARTING
I0224 22:35:53.718585  1979 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 525061ns
I0224 22:35:53.718618  1979 replica.cpp:320] Persisted replica status to STARTING
I0224 22:35:53.718827  1982 recover.cpp:473] Replica is in STARTING status
I0224 22:35:53.719728  1969 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (4534)@172.17.0.1:36678
I0224 22:35:53.719974  1971 recover.cpp:193] Received a recover response from a replica in STARTING status
I0224 22:35:53.720369  1970 recover.cpp:564] Updating replica status to VOTING
I0224 22:35:53.720789  1982 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 322308ns
I0224 22:35:53.720823  1982 replica.cpp:320] Persisted replica status to VOTING
I0224 22:35:53.720968  1982 recover.cpp:578] Successfully joined the Paxos group
I0224 22:35:53.721101  1982 recover.cpp:462] Recover process terminated
I0224 22:35:53.721698  1982 master.cpp:376] Master aab18b61-7811-4c43-a672-d1a63818c880 (4db5fa128d2d) started on 172.17.0.1:36678
I0224 22:35:53.721719  1982 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/MjbcWP/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/MjbcWP/master"" --zk_session_timeout=""10secs""
I0224 22:35:53.722039  1982 master.cpp:425] Master allowing unauthenticated frameworks to register
I0224 22:35:53.722053  1982 master.cpp:428] Master only allowing authenticated slaves to register
I0224 22:35:53.722061  1982 credentials.hpp:35] Loading credentials for authentication from '/tmp/MjbcWP/credentials'
I0224 22:35:53.722394  1982 master.cpp:468] Using default 'crammd5' authenticator
I0224 22:35:53.722525  1982 master.cpp:537] Using default 'basic' HTTP authenticator
I0224 22:35:53.722661  1982 master.cpp:571] Authorization enabled
I0224 22:35:53.722813  1968 hierarchical.cpp:144] Initialized hierarchical allocator process
I0224 22:35:53.722846  1980 whitelist_watcher.cpp:77] No whitelist given
I0224 22:35:53.724957  1977 master.cpp:1712] The newly elected leader is master@172.17.0.1:36678 with id aab18b61-7811-4c43-a672-d1a63818c880
I0224 22:35:53.725000  1977 master.cpp:1725] Elected as the leading master!
I0224 22:35:53.725023  1977 master.cpp:1470] Recovering from registrar
I0224 22:35:53.725306  1967 registrar.cpp:307] Recovering registrar
I0224 22:35:53.725808  1977 log.cpp:659] Attempting to start the writer
I0224 22:35:53.727145  1973 replica.cpp:493] Replica received implicit promise request from (4536)@172.17.0.1:36678 with proposal 1
I0224 22:35:53.727728  1973 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 424560ns
I0224 22:35:53.727828  1973 replica.cpp:342] Persisted promised to 1
I0224 22:35:53.729080  1973 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0224 22:35:53.731009  1979 replica.cpp:388] Replica received explicit promise request from (4537)@172.17.0.1:36678 for position 0 with proposal 2
I0224 22:35:53.731580  1979 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 478479ns
I0224 22:35:53.731613  1979 replica.cpp:712] Persisted action at 0
I0224 22:35:53.734354  1979 replica.cpp:537] Replica received write request for position 0 from (4538)@172.17.0.1:36678
I0224 22:35:53.734485  1979 leveldb.cpp:436] Reading position from leveldb took 60879ns
I0224 22:35:53.735877  1979 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.324061ms
I0224 22:35:53.735930  1979 replica.cpp:712] Persisted action at 0
I0224 22:35:53.737061  1970 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0224 22:35:53.738881  1970 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.772814ms
I0224 22:35:53.738939  1970 replica.cpp:712] Persisted action at 0
I0224 22:35:53.738975  1970 replica.cpp:697] Replica learned NOP action at position 0
I0224 22:35:53.740136  1976 log.cpp:675] Writer started with ending position 0
I0224 22:35:53.741750  1976 leveldb.cpp:436] Reading position from leveldb took 74863ns
I0224 22:35:53.743479  1976 registrar.cpp:340] Successfully fetched the registry (0B) in 18.11968ms
I0224 22:35:53.743755  1976 registrar.cpp:439] Applied 1 operations in 56670ns; attempting to update the 'registry'
I0224 22:35:53.745604  1978 log.cpp:683] Attempting to append 170 bytes to the log
I0224 22:35:53.745905  1977 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0224 22:35:53.746968  1981 replica.cpp:537] Replica received write request for position 1 from (4539)@172.17.0.1:36678
I0224 22:35:53.747480  1981 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 456947ns
I0224 22:35:53.747609  1981 replica.cpp:712] Persisted action at 1
I0224 22:35:53.750448  1981 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0224 22:35:53.751158  1981 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 535163ns
I0224 22:35:53.751258  1981 replica.cpp:712] Persisted action at 1
I0224 22:35:53.751389  1981 replica.cpp:697] Replica learned APPEND action at position 1
I0224 22:35:53.753149  1979 registrar.cpp:484] Successfully updated the 'registry' in 9.228032ms
I0224 22:35:53.753324  1979 registrar.cpp:370] Successfully recovered registrar
I0224 22:35:53.753593  1979 log.cpp:702] Attempting to truncate the log to 1
I0224 22:35:53.753805  1979 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0224 22:35:53.754055  1981 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0224 22:35:53.754349  1979 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0224 22:35:53.755764  1977 replica.cpp:537] Replica received write request for position 2 from (4540)@172.17.0.1:36678
I0224 22:35:53.756459  1977 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 488559ns
I0224 22:35:53.756561  1977 replica.cpp:712] Persisted action at 2
I0224 22:35:53.757932  1972 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0224 22:35:53.758400  1972 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 343827ns
I0224 22:35:53.758539  1972 leveldb.cpp:399] Deleting ~1 keys from leveldb took 34231ns
I0224 22:35:53.758658  1972 replica.cpp:712] Persisted action at 2
I0224 22:35:53.758782  1972 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0224 22:35:53.778059  1978 slave.cpp:193] Slave started on 115)@172.17.0.1:36678
I0224 22:35:53.778105  1978 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname=""maintenance-host"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF""
I0224 22:35:53.778609  1978 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/credential'
I0224 22:35:53.779175  1978 slave.cpp:324] Slave using credential for: test-principal
I0224 22:35:53.779520  1978 resources.cpp:576] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0224 22:35:53.780192  1978 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0224 22:35:53.780362  1978 slave.cpp:472] Slave attributes: [  ]
I0224 22:35:53.780483  1978 slave.cpp:477] Slave hostname: maintenance-host
I0224 22:35:53.782126  1967 state.cpp:58] Recovering state from '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/meta'
I0224 22:35:53.782892  1969 status_update_manager.cpp:200] Recovering status update manager
I0224 22:35:53.783242  1969 slave.cpp:4565] Finished recovery
I0224 22:35:53.784001  1969 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0224 22:35:53.784678  1969 slave.cpp:796] New master detected at master@172.17.0.1:36678
I0224 22:35:53.784874  1967 status_update_manager.cpp:174] Pausing sending status updates
I0224 22:35:53.784808  1969 slave.cpp:859] Authenticating with master master@172.17.0.1:36678
I0224 22:35:53.784945  1969 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0224 22:35:53.785181  1969 slave.cpp:832] Detecting new master
I0224 22:35:53.785326  1969 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0224 22:35:53.785557  1969 authenticatee.cpp:121] Creating new client SASL connection
I0224 22:35:53.786227  1969 master.cpp:5526] Authenticating slave(115)@172.17.0.1:36678
I0224 22:35:53.786492  1969 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(298)@172.17.0.1:36678
I0224 22:35:53.786962  1969 authenticator.cpp:98] Creating new server SASL connection
I0224 22:35:53.787274  1969 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0224 22:35:53.787308  1969 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0224 22:35:53.787400  1969 authenticator.cpp:203] Received SASL authentication start
I0224 22:35:53.787470  1969 authenticator.cpp:325] Authentication requires more steps
I0224 22:35:53.787884  1972 authenticatee.cpp:258] Received SASL authentication step
I0224 22:35:53.787992  1972 authenticator.cpp:231] Received SASL authentication step
I0224 22:35:53.788027  1972 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4db5fa128d2d' server FQDN: '4db5fa128d2d' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0224 22:35:53.788040  1972 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0224 22:35:53.788090  1972 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0224 22:35:53.788122  1972 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4db5fa128d2d' server FQDN: '4db5fa128d2d' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0224 22:35:53.788136  1972 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0224 22:35:53.788146  1972 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0224 22:35:53.788164  1972 authenticator.cpp:317] Authentication success
I0224 22:35:53.788331  1972 authenticatee.cpp:298] Authentication success
I0224 22:35:53.788439  1972 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(115)@172.17.0.1:36678
I0224 22:35:53.788529  1972 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(298)@172.17.0.1:36678
I0224 22:35:53.788988  1972 slave.cpp:927] Successfully authenticated with master master@172.17.0.1:36678
I0224 22:35:53.789139  1972 slave.cpp:1321] Will retry registration in 1.535786ms if necessary
I0224 22:35:53.789515  1972 master.cpp:4240] Registering slave at slave(115)@172.17.0.1:36678 (maintenance-host) with id aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.790577  1972 registrar.cpp:439] Applied 1 operations in 78745ns; attempting to update the 'registry'
I0224 22:35:53.791128  1971 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/maintenance/schedule'
I0224 22:35:53.791877  1971 http.cpp:501] HTTP POST for /master/maintenance/schedule from 172.17.0.1:45095
I0224 22:35:53.793313  1972 log.cpp:683] Attempting to append 343 bytes to the log
I0224 22:35:53.793586  1972 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0224 22:35:53.794533  1971 replica.cpp:537] Replica received write request for position 3 from (4547)@172.17.0.1:36678
I0224 22:35:53.794862  1971 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 283614ns
I0224 22:35:53.794893  1971 replica.cpp:712] Persisted action at 3
I0224 22:35:53.796646  1979 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0224 22:35:53.797102  1972 slave.cpp:1321] Will retry registration in 17.198963ms if necessary
I0224 22:35:53.797186  1979 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 498502ns
I0224 22:35:53.797230  1979 replica.cpp:712] Persisted action at 3
I0224 22:35:53.797260  1979 replica.cpp:697] Replica learned APPEND action at position 3
I0224 22:35:53.797417  1972 master.cpp:4228] Ignoring register slave message from slave(115)@172.17.0.1:36678 (maintenance-host) as admission is already in progress
I0224 22:35:53.799119  1978 registrar.cpp:484] Successfully updated the 'registry' in 8.45824ms
I0224 22:35:53.799613  1978 registrar.cpp:439] Applied 1 operations in 176193ns; attempting to update the 'registry'
I0224 22:35:53.800472  1972 master.cpp:4308] Registered slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0224 22:35:53.800623  1978 log.cpp:702] Attempting to truncate the log to 3
I0224 22:35:53.801255  1969 hierarchical.cpp:473] Added slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0224 22:35:53.801301  1978 slave.cpp:971] Registered with master master@172.17.0.1:36678; given slave ID aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.801331  1978 fetcher.cpp:81] Clearing fetcher cache
I0224 22:35:53.801431  1969 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.801466  1969 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 162751ns
I0224 22:35:53.801532  1969 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0224 22:35:53.801867  1978 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/meta/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/slave.info'
I0224 22:35:53.801877  1969 status_update_manager.cpp:181] Resuming sending status updates
I0224 22:35:53.802898  1977 replica.cpp:537] Replica received write request for position 4 from (4548)@172.17.0.1:36678
I0224 22:35:53.803252  1978 slave.cpp:1030] Forwarding total oversubscribed resources 
I0224 22:35:53.803640  1970 master.cpp:4649] Received update of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) with total oversubscribed resources 
I0224 22:35:53.803858  1977 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 912626ns
I0224 22:35:53.803889  1977 replica.cpp:712] Persisted action at 4
I0224 22:35:53.804144  1978 slave.cpp:3482] Received ping from slave-observer(117)@172.17.0.1:36678
I0224 22:35:53.804535  1971 hierarchical.cpp:531] Slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0224 22:35:53.804684  1971 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.804714  1971 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 131453ns
I0224 22:35:53.805541  1967 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0224 22:35:53.805941  1967 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 366444ns
I0224 22:35:53.806015  1967 leveldb.cpp:399] Deleting ~2 keys from leveldb took 42808ns
I0224 22:35:53.806041  1967 replica.cpp:712] Persisted action at 4
I0224 22:35:53.806066  1967 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0224 22:35:53.807355  1978 log.cpp:683] Attempting to append 465 bytes to the log
I0224 22:35:53.807551  1978 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I0224 22:35:53.809638  1979 replica.cpp:537] Replica received write request for position 5 from (4549)@172.17.0.1:36678
I0224 22:35:53.810858  1979 leveldb.cpp:341] Persisting action (484 bytes) to leveldb took 1.167663ms
I0224 22:35:53.810904  1979 replica.cpp:712] Persisted action at 5
I0224 22:35:53.811997  1979 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I0224 22:35:53.812348  1979 leveldb.cpp:341] Persisting action (486 bytes) to leveldb took 318928ns
I0224 22:35:53.812376  1979 replica.cpp:712] Persisted action at 5
I0224 22:35:53.812397  1979 replica.cpp:697] Replica learned APPEND action at position 5
I0224 22:35:53.815132  1973 registrar.cpp:484] Successfully updated the 'registry' in 15.437312ms
I0224 22:35:53.815491  1976 log.cpp:702] Attempting to truncate the log to 5
I0224 22:35:53.815610  1973 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I0224 22:35:53.815661  1968 master.cpp:4705] Updating unavailability of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host), starting at 2410.99235909694weeks
I0224 22:35:53.815845  1968 master.cpp:4705] Updating unavailability of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host), starting at 2410.99235909694weeks
I0224 22:35:53.816069  1975 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.816103  1975 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 175822ns
I0224 22:35:53.816272  1975 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.816303  1975 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 110913ns
I0224 22:35:53.817291  1972 replica.cpp:537] Replica received write request for position 6 from (4550)@172.17.0.1:36678
I0224 22:35:53.817908  1972 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 576032ns
I0224 22:35:53.817932  1972 replica.cpp:712] Persisted action at 6
I0224 22:35:53.818686  1980 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0
I0224 22:35:53.819021  1980 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 305298ns
I0224 22:35:53.819095  1980 leveldb.cpp:399] Deleting ~2 keys from leveldb took 44332ns
I0224 22:35:53.819120  1980 replica.cpp:712] Persisted action at 6
I0224 22:35:53.819162  1980 replica.cpp:697] Replica learned TRUNCATE action at position 6
I0224 22:35:53.820662  1967 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/maintenance/status'
I0224 22:35:53.821190  1976 http.cpp:501] HTTP GET for /master/maintenance/status from 172.17.0.1:45096
I0224 22:35:53.823709  1948 scheduler.cpp:154] Version: 0.28.0
I0224 22:35:53.824424  1972 scheduler.cpp:236] New master detected at master@172.17.0.1:36678
I0224 22:35:53.825402  1982 scheduler.cpp:298] Sending SUBSCRIBE call to master@172.17.0.1:36678
I0224 22:35:53.827201  1978 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.827636  1978 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45097
I0224 22:35:53.827922  1978 master.cpp:1974] Received subscription request for HTTP framework 'default'
I0224 22:35:53.827991  1978 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0224 22:35:53.828418  1982 master.cpp:2065] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I0224 22:35:53.828943  1968 hierarchical.cpp:265] Added framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.829124  1982 master.hpp:1657] Sending heartbeat to aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.829987  1968 hierarchical.cpp:1127] Performed allocation for 1 slaves in 1.011356ms
I0224 22:35:53.830204  1982 master.cpp:5355] Sending 1 offers to framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
I0224 22:35:53.830801  1982 master.cpp:5445] Sending 1 inverse offers to framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
I0224 22:35:53.831132  1969 scheduler.cpp:457] Enqueuing event SUBSCRIBED received from master@172.17.0.1:36678
I0224 22:35:53.832396  1968 scheduler.cpp:457] Enqueuing event HEARTBEAT received from master@172.17.0.1:36678
I0224 22:35:53.833050  1976 master_maintenance_tests.cpp:177] Ignoring HEARTBEAT event
I0224 22:35:53.833256  1979 scheduler.cpp:457] Enqueuing event OFFERS received from master@172.17.0.1:36678
I0224 22:35:53.833775  1979 scheduler.cpp:457] Enqueuing event OFFERS received from master@172.17.0.1:36678
I0224 22:35:53.835662  1980 scheduler.cpp:298] Sending ACCEPT call to master@172.17.0.1:36678
I0224 22:35:53.837591  1967 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.838021  1967 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45098
I0224 22:35:53.838851  1967 master.cpp:3138] Processing ACCEPT call for offers: [ aab18b61-7811-4c43-a672-d1a63818c880-O0 ] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) for framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
I0224 22:35:53.838946  1967 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 90bcae0c-9d40-40b7-9537-dae7e83479f6 as user 'mesos'
W0224 22:35:53.841048  1967 validation.cpp:404] Executor default for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0224 22:35:53.841101  1967 validation.cpp:416] Executor default for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0224 22:35:53.841624  1967 master.hpp:176] Adding task 90bcae0c-9d40-40b7-9537-dae7e83479f6 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host)
I0224 22:35:53.842157  1967 master.cpp:3623] Launching task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default) with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host)
I0224 22:35:53.842571  1980 slave.cpp:1361] Got assigned task 90bcae0c-9d40-40b7-9537-dae7e83479f6 for framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.843122  1980 slave.cpp:1480] Launching task 90bcae0c-9d40-40b7-9537-dae7e83479f6 for framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.843718  1980 paths.cpp:474] Trying to chown '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159' to user 'mesos'
I0224 22:35:53.852052  1980 slave.cpp:5367] Launching executor default of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 with resources  in work directory '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159'
I0224 22:35:53.854452  1980 exec.cpp:143] Version: 0.28.0
I0224 22:35:53.854812  1967 exec.cpp:193] Executor started at: executor(47)@172.17.0.1:36678 with pid 1948
I0224 22:35:53.855108  1980 slave.cpp:1698] Queuing task '90bcae0c-9d40-40b7-9537-dae7e83479f6' for executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.855264  1980 slave.cpp:749] Successfully attached file '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159'
I0224 22:35:53.855362  1980 slave.cpp:2643] Got registration for executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from executor(47)@172.17.0.1:36678
I0224 22:35:53.855785  1974 exec.cpp:217] Executor registered on slave aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.855857  1974 exec.cpp:229] Executor::registered took 42512ns
I0224 22:35:53.856391  1980 slave.cpp:1863] Sending queued task '90bcae0c-9d40-40b7-9537-dae7e83479f6' to executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 at executor(47)@172.17.0.1:36678
I0224 22:35:53.856720  1974 exec.cpp:304] Executor asked to run task '90bcae0c-9d40-40b7-9537-dae7e83479f6'
I0224 22:35:53.856812  1974 exec.cpp:313] Executor::launchTask took 65703ns
I0224 22:35:53.856922  1974 exec.cpp:526] Executor sending status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.857378  1980 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from executor(47)@172.17.0.1:36678
I0224 22:35:53.858175  1980 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.858222  1980 status_update_manager.cpp:497] Creating StatusUpdate stream for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.858687  1980 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to the slave
I0224 22:35:53.859210  1980 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to master@172.17.0.1:36678
I0224 22:35:53.859390  1980 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.859436  1980 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to executor(47)@172.17.0.1:36678
I0224 22:35:53.859663  1980 exec.cpp:350] Executor received status update acknowledgement 249b169a-6b5f-4776-95c8-c897ba6b3f0b for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.859657  1967 master.cpp:4794] Status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host)
I0224 22:35:53.859851  1967 master.cpp:4842] Forwarding status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.860587  1967 master.cpp:6450] Updating the state of task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0224 22:35:53.862711  1967 scheduler.cpp:457] Enqueuing event UPDATE received from master@172.17.0.1:36678
I0224 22:35:53.866711  1976 scheduler.cpp:298] Sending ACKNOWLEDGE call to master@172.17.0.1:36678
I0224 22:35:53.870667  1972 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.871269  1972 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45099
I0224 22:35:53.871459  1972 master.cpp:3952] Processing ACKNOWLEDGE call 249b169a-6b5f-4776-95c8-c897ba6b3f0b for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default) on slave aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.872184  1972 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.872537  1972 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.874407  1975 scheduler.cpp:298] Sending DECLINE call to master@172.17.0.1:36678
I0224 22:35:53.877537  1979 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.877795  1979 hierarchical.cpp:1127] Performed allocation for 1 slaves in 482441ns
I0224 22:35:53.878082  1981 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.878675  1978 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45100
I0224 22:35:53.878931  1978 master.cpp:3675] Processing DECLINE call for offers: [ aab18b61-7811-4c43-a672-d1a63818c880-O1 ] for framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
../../src/tests/master_maintenance_tests.cpp:1222: Failure
Failed to wait 15secs for event
I0224 22:36:08.881649  1948 master.cpp:1027] Master terminating
W0224 22:36:08.881925  1948 master.cpp:6502] Removing task 90bcae0c-9d40-40b7-9537-dae7e83479f6 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) in non-terminal state TASK_RUNNING
I0224 22:36:08.882961  1948 master.cpp:6545] Removing executor 'default' with resources  of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host)
I0224 22:36:08.884789  1969 hierarchical.cpp:505] Removed slave aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:36:08.887261  1969 hierarchical.cpp:326] Removed framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.916983  1976 slave.cpp:3528] master@172.17.0.1:36678 exited
W0224 22:36:08.917191  1976 slave.cpp:3531] Master disconnected! Waiting for a new master to be elected
I0224 22:36:08.934546  1975 slave.cpp:3528] executor(47)@172.17.0.1:36678 exited
I0224 22:36:08.934806  1974 slave.cpp:3886] Executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 exited with status 0
I0224 22:36:08.935024  1974 slave.cpp:3002] Handling status update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from @0.0.0.0:0
I0224 22:36:08.935505  1974 slave.cpp:5677] Terminating task 90bcae0c-9d40-40b7-9537-dae7e83479f6
I0224 22:36:08.936190  1967 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.936368  1967 status_update_manager.cpp:374] Forwarding update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to the slave
I0224 22:36:08.936606  1974 slave.cpp:3400] Forwarding the update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to master@172.17.0.1:36678
I0224 22:36:08.936779  1974 slave.cpp:3294] Status update manager successfully handled status update TASK_FAILED (UUID: 77d415df-58bd-4cf5-9c49-6106691d9599) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.955370  1967 slave.cpp:668] Slave terminating
I0224 22:36:08.955499  1967 slave.cpp:2079] Asked to shut down framework aab18b61-7811-4c43-a672-d1a63818c880-0000 by @0.0.0.0:0
I0224 22:36:08.955538  1967 slave.cpp:2104] Shutting down framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.955606  1967 slave.cpp:3990] Cleaning up executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 at executor(47)@172.17.0.1:36678
I0224 22:36:08.956053  1967 slave.cpp:4078] Cleaning up framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.956327  1967 gc.cpp:54] Scheduling '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159' for gc 1.00002336880296weeks in the future
I0224 22:36:08.956495  1973 status_update_manager.cpp:282] Closing status update streams for framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.956524  1967 gc.cpp:54] Scheduling '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default' for gc 1.00002336880296weeks in the future
I0224 22:36:08.956549  1973 status_update_manager.cpp:528] Cleaning up status update stream for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:36:08.956619  1967 gc.cpp:54] Scheduling '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000' for gc 1.00002336880296weeks in the future
[  FAILED  ] MasterMaintenanceTest.InverseOffers (15258 ms)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-25 07:34:12.119,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 25 07:34:12 UTC 2016,,,,,,,"0|i2tnvr:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 29,,,,,,,,,,,1.0,,,,,,,,,,,"25/Feb/16 00:27;kaysoky;Fix: https://reviews.apache.org/r/43971/","25/Feb/16 07:34;jvanremoortere;{code}
commit 82ff5ca906af52ce6ce509d14cc30c7c18ae576e
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Wed Feb 24 23:15:56 2016 -0800

    Fixed flakiness in MasterMaintenanceTest.InverseOffers.
    
    The accept/decline of the inverse offers in the test could arrive after
    the clock is paused and advanced in the test.  This meant that the test
    would trigger an allocation and then idle (because the clock is paused).
    This is the same problem we fixed in
    https://issues.apache.org/jira/browse/MESOS-4059 .
    
    Review: https://reviews.apache.org/r/43971/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos containerizer should get uid/gids before pivot_root.,MESOS-4757,12943037,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,jieyu,jieyu,24/Feb/16 18:47,10/Jul/17 19:33,29/Oct/20 16:32,10/Jul/17 19:33,,,,,,,,,1.0.0,,,,,,,,,,,0,,,,,,,,,"Currently, we call os::su(user) after pivot_root. This is problematic because /etc/passwd and /etc/group might be missing in container's root filesystem. We should instead, get the uid/gids before pivot_root, and call setuid/setgroups after pivot_root.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-29 00:43:26.383,,,false,MESOS-2386,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 10 19:33:43 UTC 2017,,,,,,,"0|i2thwn:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 29,,,,,,,,,,,3.0,,,,,,,,,,,"29/Feb/16 00:09;jieyu;https://reviews.apache.org/r/44150
https://reviews.apache.org/r/44151
https://reviews.apache.org/r/44152
https://reviews.apache.org/r/44153","29/Feb/16 00:43;jamespeach;I think this is a problematic approach. Switching credentials tends to be a bit subtle on many systems and it doesn't easily decompose into separate operations.

For example, BSD requires (or assumes) that the first {{setgroups(2)}} element is the primary GID. {{NGROUPS_MAX}} is a dynamic parameter on many systems. In Darwin, {{setgroups(2)}} just primes the kernel credential cache, but only if you call the {{initgroups}} system call afterwards.

I suggest that a more reliable approach is to keep doing a full credential switch before the {{pivot_root}}, but retain enough capabilities to be able to enter the chroot afterwards.","29/Feb/16 01:27;jieyu;I am not familiar with BSD, is there a way to retain capabilities to do pivot_root when switching the credentials?","29/Feb/16 01:31;jieyu;[~jamespeach] Can you also give me a pointer to the 'setgroups' problem you mentioned on Darwin?","29/Feb/16 02:15;jieyu;BTW, I tested my patch on OSX (EL Capitan, 10.11.3), and it works fine.

{noformat}
$ sudo sbin/mesos-master --work_dir=/tmp/mesos/master
$ sudo GLOG_v=1 sbin/mesos-slave --master=10.0.1.26:5050 --work_dir=/tmp/mesos/slave --executor_environment_variables=""{}""
$ bin/mesos-execute --master=10.0.1.26:5050 --name=test --command=""id"" # under my name 'jie'
Registered executor on 10.0.1.26
Starting task test
sh -c 'id'
Forked command at 86930
uid=501(jie) gid=20(staff) groups=20(staff),701(com.apple.sharepoint.group.1),12(everyone),61(localaccounts),79(_appserverusr),80(admin),81(_appserveradm),98(_lpadmin),33(_appstore),100(_lpoperator),204(_developer),395(com.apple.access_ftp),398(com.apple.access_screensharing),399(com.apple.access_ssh)
Command exited with status 0 (pid: 86930)
Shutting down
Sending SIGTERM to process tree at pid 86930
Sent SIGTERM to the following process trees:
[ 

]
$ id
uid=501(jie) gid=20(staff) groups=20(staff),701(com.apple.sharepoint.group.1),12(everyone),61(localaccounts),79(_appserverusr),80(admin),81(_appserveradm),98(_lpadmin),33(_appstore),100(_lpoperator),204(_developer),395(com.apple.access_ftp),398(com.apple.access_screensharing),399(com.apple.access_ssh)
{noformat}

","29/Feb/16 19:27;idownes;IMHO this is incorrect and highlights the inconsistent relationship we have between the host and the container environments, mostly attributable to our history of running in the host context. Ideally, the container should be completely independent of the host configuration! It should not be resolving user/group names to uids/gids using the host's database. That is making huge assumptions about consistent configuration across a cluster -- and an external system to maintain it -- that are unnecessary and undesirable.

I suggest something like the following behavior when container images are used:
# If a job specifies a user and group name then the container image *must* include the necessary user and group database files and must resolve the names to ids. If not, then it fails.
# Support the job specifying uid and gid(s) directly.
# Also support picking the user and gid off a file in the image (I think appc supports this?).

If a container image is not used then fallback to the current (and terrible) behavior of using the host's databases.

Thoughts?","29/Feb/16 19:48;jieyu;[~idownes] My main concern is about the sandbox. Currently, sandbox is prepared by the agent (thus using the agent's host database) when chown happens and we bind mount that directory to the container. Without user namespace, I don't know if using the container database is desired or not. ","29/Feb/16 21:54;wangcong;Appc already fixes this by: https://github.com/appc/spec/pull/315/files . Mesos could take the similar approach.","29/Feb/16 23:06;idownes;I skimmed the pull request and it looks reasonable.

[~jieyu] Then we should change the the ownership of the sandbox to match? There doesn't have to be a mapping in the user/group database to set ownership:
{noformat}
[1500][idownes:~]$ touch foo
[1500][idownes:~]$ sudo chown 1234 foo
[1500][idownes:~]$ cat /etc/passwd | grep 1234
[1500][idownes:~]$ stat -f ""%N: %u"" foo
foo: 1234
{noformat}","29/Feb/16 23:15;jamespeach;This only works because you have < 16 groups.","29/Feb/16 23:19;jieyu;OK, ic. Maybe I can just use a large enough number (e.g., 65536)? I think getting this number for sysconf is the right way. I can easily change that.

I guess we need a boarder discussion on whether we should do something like this or not (per your email reply and Ian's comment).
","29/Feb/16 23:23;jieyu;[~idownes] I don't we think we should change the owner/group of the sandbox according to information from a container which can be malicious. What do you think?","29/Feb/16 23:31;jamespeach;That would work for Linux and BSD I think, but not for Darwin. I recommend against providing low-level APIs like {{setgroups}}. It's really easy to get this wrong with APIs at this level.","24/May/16 21:39;gilbert;[~idownes], Kevin proposed a solution for host user -> container user around two months ago via mailing list. Could you take a look at it to see whether it may break your cases? Thanks! :)

https://docs.google.com/document/d/1ENNJKyPrqqm8OsYV8-dDoHTiRmqtuVbcdzNWj1nURsQ/edit#heading=h.j9cu8f69ljik","10/Jul/17 19:33;gilbert;commit 6b8134a9637b08243ee89fa02b4092fe479c5947
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jun 15 16:06:00 2016 -0700

    Modified os user test for user switching.
    
    This test verifies 'setuid', 'setgid', 'getgrouplist' and 'setgroups'
    work properly on both linux and os x.
    
    Review: https://reviews.apache.org/r/48669/

commit e002a5d020c19537538dd3c8d793ee3fa6148132
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jun 15 16:05:57 2016 -0700

    Added stout functions to get and set supplementary groud ids.
    
    Review: https://reviews.apache.org/r/47667/

commit fe43ca8061b7bb848f1cb77afa746eb26f9d0494
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jun 15 16:05:54 2016 -0700

    Added stout functions to set uid and gid.
    
    Review: https://reviews.apache.org/r/47666/

commit 0eabdf7a54704de902b5dc12195f0f368491f169
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Jun 15 16:05:51 2016 -0700

    Implemented image user support in docker runtime isolator.
    
    Review: https://reviews.apache.org/r/47664/",,,,,,,,,,,,,
"The ""executors"" field is exposed under a backwards incompatible schema.",MESOS-4754,12942875,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,mcypark,mcypark,mcypark,24/Feb/16 08:49,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.24.2,0.25.1,0.26.1,0.27.2,,,master,,,,,0,mesosphere,,,,,,,,"In 0.26.0, the master's {{/state}} endpoint generated the following:

{code}
{
  /* ... */
  ""frameworks"": [
    {
      /* ... */
      ""executors"": [
        {
          ""command"": {
            ""argv"": [],
            ""uris"": [],
            ""value"": ""/Users/mpark/Projects/mesos/build/opt/src/long-lived-executor""
          },
          ""executor_id"": ""default"",
          ""framework_id"": ""0ea528a9-64ba-417f-98ea-9c4b8d418db6-0000"",
          ""name"": ""Long Lived Executor (C++)"",
          ""resources"": {
            ""cpus"": 0,
            ""disk"": 0,
            ""mem"": 0
          },
          ""slave_id"": ""8a513678-03a1-4cb5-9279-c3c0c591f1d8-S0""
        }
      ],
      /* ... */
    }
  ]
  /* ... */
}
{code}

In 0.27.1, the {{ExecutorInfo}} is mistakenly exposed in the raw protobuf schema:

{code}
{
  /* ... */
  ""frameworks"": [
    {
      /* ... */
      ""executors"": [
        {
          ""command"": {
            ""shell"": true,
            ""value"": ""/Users/mpark/Projects/mesos/build/opt/src/long-lived-executor""
          },
          ""executor_id"": {
            ""value"": ""default""
          },
          ""framework_id"": {
            ""value"": ""368a5a49-480b-41f6-a13b-24a69c92a72e-0000""
          },
          ""name"": ""Long Lived Executor (C++)"",
          ""slave_id"": ""8a513678-03a1-4cb5-9279-c3c0c591f1d8-S0"",
          ""source"": ""cpp_long_lived_framework""
        }
      ],
      /* ... */
    }
  ]
  /* ... */
}
{code}

This is a backwards incompatible API change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-4235,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 28 02:46:46 UTC 2016,,,,,,,"0|i2tgwn:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 29,,,,,,,,,,,2.0,,,,,,,,,,,"24/Feb/16 08:59;mcypark;The issue here is that even though {{src/common/http.cpp}} has a definition of
{{void json(JSON::ObjectWriter* writer, const ExecutorInfo& executorInfo);}},
its declaration is missing from {{src/common/http.hpp}}.

We would have liked this to cause a compiler error, but it didn't because of the generic {{json}} function for protobuf messages:
{{inline void json(ObjectWriter* writer, const google::protobuf::Message& message)}}, which can jsonify {{ExecutorInfo}} using the protobuf schema.

The resolution will be the following:
  1. Add the missing declaration of {{void json(JSON::ObjectWriter* writer, const ExecutorInfo& executorInfo);}} to {{src/common/http.hpp}}
  2. Make the generic {{json}} function that handles protobuf messages to require explicit opt-in.

{code}
-    writer->field(""cgroup_info"", status.cgroup_info());
+    writer->field(""cgroup_info"", JSON::Protobuf(status.cgroup_info()));
{code}","24/Feb/16 10:33;mcypark;1. Add the missing declaration of {{void json(JSON::ObjectWriter* writer, const ExecutorInfo& executorInfo);}} to {{src/common/http.hpp}}:
https://reviews.apache.org/r/43937/

2. Make the generic json function that handles protobuf messages to require explicit opt-in.
https://reviews.apache.org/r/43938/
https://reviews.apache.org/r/43939/
","25/Feb/16 07:18;mcypark;{noformat}
commit d99c778de22954c0b3f7089be45ef250386fccd1
Author: Michael Park <mpark@apache.org>
Date:   Wed Feb 24 22:35:39 2016 -0800

    Added missing `json` declaration for `ExecutorInfo`.

    Review: https://reviews.apache.org/r/43937/
{noformat}","28/Feb/16 02:46;mcypark;{noformat}
commit b91a740fb3c6deca4b8f1ab6a7d83decf144b5fb
Author: Michael Park <mpark@apache.org>
Date:   Sat Feb 27 18:12:13 2016 -0800

    Required jsonifying of generic protobuf to be explicit opt-in [mesos].

    Review: https://reviews.apache.org/r/43939/
{noformat}
{noformat}
commit 19945044e46b8512df5cd08469941b4d4da429a7
Author: Michael Park <mpark@apache.org>
Date:   Sat Feb 27 18:12:00 2016 -0800

    Required jsonifying of generic protobuf to be explicit opt-in [stout].

    Review: https://reviews.apache.org/r/43938/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,
ContainerLoggerTest.MesosContainerizerRecover cannot be executed in isolation,MESOS-4747,12941516,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,bbannier,bbannier,23/Feb/16 19:40,26/Nov/18 09:26,29/Oct/20 16:32,26/Nov/18 09:26,,,,,,,,,0.28.0,,,,,,test,,,,,0,mesosphere,,,,,,,,"Some cleanup of spawned processes is missing in {{ContainerLoggerTest.MesosContainerizerRecover}} so that when the test is run in isolation the global teardown might find lingering processes.
{code}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from ContainerLoggerTest
[ RUN      ] ContainerLoggerTest.MesosContainerizerRecover
[       OK ] ContainerLoggerTest.MesosContainerizerRecover (13 ms)
[----------] 1 test from ContainerLoggerTest (13 ms total)

[----------] Global test environment tear-down
../../src/tests/environment.cpp:728: Failure
Failed
Tests completed with child processes remaining:
-+- 7112 /SOME/PATH/src/mesos/build/src/.libs/mesos-tests --gtest_filter=ContainerLoggerTest.MesosContainerizerRecover
 \--- 7130 (sh)
[==========] 1 test from 1 test case ran. (23 ms total)
[  PASSED  ] 1 test.
[  FAILED  ] 0 tests, listed below:

 0 FAILED TESTS
{code}

Observered on OS X with clang-trunk and an unoptimized build.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-23 19:44:12.852,,,false,MESOS-4809,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 24 08:02:39 UTC 2016,,,,,,,"0|i2t8in:",9223372036854775807,,,,,adam-mesos,,,,,,Mesosphere Sprint 29,,,,,,,,,,,1.0,,,,,,,,,,,"23/Feb/16 19:42;bbannier;Review: https://reviews.apache.org/r/43893/","23/Feb/16 19:44;kaysoky;Apparently, the {{exit 0}} subprocess doesn't exit fast enough :)

Technically, we should have the same problem in the next test down {{ROOT_DOCKER_ContainerizerRecover}}.","24/Feb/16 08:02;adam-mesos;commit 0786e6636576e12f5168d0923cfbe000403eabea
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Wed Feb 24 00:00:54 2016 -0800

    Made sure that spawned processes terminated before leaving tests.
    
    Review: https://reviews.apache.org/r/43893/",,,,,,,,,,,,,,,,,,,,,,,,,
DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes fails on CentOS 6,MESOS-4736,12941144,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Won't Fix,kaysoky,kaysoky,kaysoky,22/Feb/16 19:16,14/Oct/17 23:13,29/Oct/20 16:32,06/Mar/17 19:01,0.28.0,,,,,,,,,,,,,,,,,,,0,flaky,mesosphere,test,,,,,,"This test passes consistently on other OS's, but fails consistently on CentOS 6.

Verbose logs from test failure:
{code}
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes
I0222 18:16:12.327957 26681 leveldb.cpp:174] Opened db in 7.466102ms
I0222 18:16:12.330528 26681 leveldb.cpp:181] Compacted db in 2.540139ms
I0222 18:16:12.330580 26681 leveldb.cpp:196] Created db iterator in 16908ns
I0222 18:16:12.330592 26681 leveldb.cpp:202] Seeked to beginning of db in 1403ns
I0222 18:16:12.330600 26681 leveldb.cpp:271] Iterated through 0 keys in the db in 315ns
I0222 18:16:12.330634 26681 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0222 18:16:12.331082 26698 recover.cpp:447] Starting replica recovery
I0222 18:16:12.331289 26698 recover.cpp:473] Replica is in EMPTY status
I0222 18:16:12.332162 26703 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (13761)@172.30.2.148:35274
I0222 18:16:12.332701 26701 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0222 18:16:12.333230 26699 recover.cpp:564] Updating replica status to STARTING
I0222 18:16:12.334102 26698 master.cpp:376] Master 652149b4-3932-4d8b-ba6f-8c9d9045be70 (ip-172-30-2-148.mesosphere.io) started on 172.30.2.148:35274
I0222 18:16:12.334116 26698 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/QEhLBS/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/QEhLBS/master"" --zk_session_timeout=""10secs""
I0222 18:16:12.334354 26698 master.cpp:423] Master only allowing authenticated frameworks to register
I0222 18:16:12.334363 26698 master.cpp:428] Master only allowing authenticated slaves to register
I0222 18:16:12.334369 26698 credentials.hpp:35] Loading credentials for authentication from '/tmp/QEhLBS/credentials'
I0222 18:16:12.335366 26698 master.cpp:468] Using default 'crammd5' authenticator
I0222 18:16:12.335492 26698 master.cpp:537] Using default 'basic' HTTP authenticator
I0222 18:16:12.335623 26698 master.cpp:571] Authorization enabled
I0222 18:16:12.335752 26703 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.314693ms
I0222 18:16:12.335769 26700 whitelist_watcher.cpp:77] No whitelist given
I0222 18:16:12.335778 26703 replica.cpp:320] Persisted replica status to STARTING
I0222 18:16:12.335821 26697 hierarchical.cpp:144] Initialized hierarchical allocator process
I0222 18:16:12.335965 26701 recover.cpp:473] Replica is in STARTING status
I0222 18:16:12.336771 26703 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (13763)@172.30.2.148:35274
I0222 18:16:12.337191 26696 recover.cpp:193] Received a recover response from a replica in STARTING status
I0222 18:16:12.337635 26700 recover.cpp:564] Updating replica status to VOTING
I0222 18:16:12.337671 26703 master.cpp:1712] The newly elected leader is master@172.30.2.148:35274 with id 652149b4-3932-4d8b-ba6f-8c9d9045be70
I0222 18:16:12.337698 26703 master.cpp:1725] Elected as the leading master!
I0222 18:16:12.337713 26703 master.cpp:1470] Recovering from registrar
I0222 18:16:12.337828 26696 registrar.cpp:307] Recovering registrar
I0222 18:16:12.339972 26702 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.06039ms
I0222 18:16:12.339994 26702 replica.cpp:320] Persisted replica status to VOTING
I0222 18:16:12.340082 26700 recover.cpp:578] Successfully joined the Paxos group
I0222 18:16:12.340267 26700 recover.cpp:462] Recover process terminated
I0222 18:16:12.340591 26699 log.cpp:659] Attempting to start the writer
I0222 18:16:12.341594 26698 replica.cpp:493] Replica received implicit promise request from (13764)@172.30.2.148:35274 with proposal 1
I0222 18:16:12.343598 26698 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.97941ms
I0222 18:16:12.343619 26698 replica.cpp:342] Persisted promised to 1
I0222 18:16:12.344182 26698 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0222 18:16:12.345285 26702 replica.cpp:388] Replica received explicit promise request from (13765)@172.30.2.148:35274 for position 0 with proposal 2
I0222 18:16:12.347275 26702 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.960198ms
I0222 18:16:12.347296 26702 replica.cpp:712] Persisted action at 0
I0222 18:16:12.348201 26703 replica.cpp:537] Replica received write request for position 0 from (13766)@172.30.2.148:35274
I0222 18:16:12.348247 26703 leveldb.cpp:436] Reading position from leveldb took 21399ns
I0222 18:16:12.350667 26703 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 2.39166ms
I0222 18:16:12.350690 26703 replica.cpp:712] Persisted action at 0
I0222 18:16:12.351191 26696 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0222 18:16:12.353152 26696 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.935798ms
I0222 18:16:12.353173 26696 replica.cpp:712] Persisted action at 0
I0222 18:16:12.353188 26696 replica.cpp:697] Replica learned NOP action at position 0
I0222 18:16:12.353639 26696 log.cpp:675] Writer started with ending position 0
I0222 18:16:12.354508 26697 leveldb.cpp:436] Reading position from leveldb took 25625ns
I0222 18:16:12.355274 26696 registrar.cpp:340] Successfully fetched the registry (0B) in 17.406976ms
I0222 18:16:12.355357 26696 registrar.cpp:439] Applied 1 operations in 20977ns; attempting to update the 'registry'
I0222 18:16:12.355929 26697 log.cpp:683] Attempting to append 210 bytes to the log
I0222 18:16:12.356032 26703 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0222 18:16:12.356657 26698 replica.cpp:537] Replica received write request for position 1 from (13767)@172.30.2.148:35274
I0222 18:16:12.358566 26698 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 1.881945ms
I0222 18:16:12.358588 26698 replica.cpp:712] Persisted action at 1
I0222 18:16:12.359081 26697 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0222 18:16:12.361002 26697 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.894331ms
I0222 18:16:12.361023 26697 replica.cpp:712] Persisted action at 1
I0222 18:16:12.361038 26697 replica.cpp:697] Replica learned APPEND action at position 1
I0222 18:16:12.361883 26697 registrar.cpp:484] Successfully updated the 'registry' in 6.482944ms
I0222 18:16:12.361981 26697 registrar.cpp:370] Successfully recovered registrar
I0222 18:16:12.362052 26701 log.cpp:702] Attempting to truncate the log to 1
I0222 18:16:12.362167 26703 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0222 18:16:12.362421 26696 master.cpp:1522] Recovered 0 slaves from the Registry (171B) ; allowing 10mins for slaves to re-register
I0222 18:16:12.362447 26698 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0222 18:16:12.362911 26701 replica.cpp:537] Replica received write request for position 2 from (13768)@172.30.2.148:35274
I0222 18:16:12.364760 26701 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.819954ms
I0222 18:16:12.364783 26701 replica.cpp:712] Persisted action at 2
I0222 18:16:12.365384 26697 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0222 18:16:12.367961 26697 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.55143ms
I0222 18:16:12.368015 26697 leveldb.cpp:399] Deleting ~1 keys from leveldb took 28196ns
I0222 18:16:12.368028 26697 replica.cpp:712] Persisted action at 2
I0222 18:16:12.368044 26697 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0222 18:16:12.376824 26703 slave.cpp:193] Slave started on 396)@172.30.2.148:35274
I0222 18:16:12.376838 26703 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpu:2;mem:2048;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1""
I0222 18:16:12.377109 26703 credentials.hpp:83] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/credential'
I0222 18:16:12.377300 26703 slave.cpp:324] Slave using credential for: test-principal
I0222 18:16:12.377439 26703 resources.cpp:576] Parsing resources as JSON failed: cpu:2;mem:2048;disk(role1):2048
Trying semicolon-delimited string format instead
I0222 18:16:12.377804 26703 slave.cpp:464] Slave resources: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000]
I0222 18:16:12.377881 26703 slave.cpp:472] Slave attributes: [  ]
I0222 18:16:12.377889 26703 slave.cpp:477] Slave hostname: ip-172-30-2-148.mesosphere.io
I0222 18:16:12.378779 26701 state.cpp:58] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/meta'
I0222 18:16:12.379092 26697 status_update_manager.cpp:200] Recovering status update manager
I0222 18:16:12.379156 26681 sched.cpp:222] Version: 0.28.0
I0222 18:16:12.379250 26697 docker.cpp:722] Recovering Docker containers
I0222 18:16:12.379421 26703 slave.cpp:4565] Finished recovery
I0222 18:16:12.379627 26700 sched.cpp:326] New master detected at master@172.30.2.148:35274
I0222 18:16:12.379735 26703 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0222 18:16:12.379765 26700 sched.cpp:382] Authenticating with master master@172.30.2.148:35274
I0222 18:16:12.379781 26700 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0222 18:16:12.379964 26696 status_update_manager.cpp:174] Pausing sending status updates
I0222 18:16:12.379992 26702 authenticatee.cpp:121] Creating new client SASL connection
I0222 18:16:12.380030 26697 slave.cpp:796] New master detected at master@172.30.2.148:35274
I0222 18:16:12.380106 26697 slave.cpp:859] Authenticating with master master@172.30.2.148:35274
I0222 18:16:12.380127 26697 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0222 18:16:12.380188 26699 master.cpp:5526] Authenticating scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.380269 26700 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(832)@172.30.2.148:35274
I0222 18:16:12.380280 26698 authenticatee.cpp:121] Creating new client SASL connection
I0222 18:16:12.380307 26697 slave.cpp:832] Detecting new master
I0222 18:16:12.380450 26697 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0222 18:16:12.380452 26699 master.cpp:5526] Authenticating slave(396)@172.30.2.148:35274
I0222 18:16:12.380506 26698 authenticator.cpp:98] Creating new server SASL connection
I0222 18:16:12.380540 26697 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(833)@172.30.2.148:35274
I0222 18:16:12.380635 26700 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0222 18:16:12.380659 26700 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0222 18:16:12.380762 26700 authenticator.cpp:203] Received SASL authentication start
I0222 18:16:12.380765 26701 authenticator.cpp:98] Creating new server SASL connection
I0222 18:16:12.380843 26700 authenticator.cpp:325] Authentication requires more steps
I0222 18:16:12.380911 26698 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0222 18:16:12.380931 26702 authenticatee.cpp:258] Received SASL authentication step
I0222 18:16:12.380936 26698 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0222 18:16:12.381036 26702 authenticator.cpp:231] Received SASL authentication step
I0222 18:16:12.381052 26698 authenticator.cpp:203] Received SASL authentication start
I0222 18:16:12.381062 26702 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0222 18:16:12.381072 26702 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0222 18:16:12.381104 26702 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0222 18:16:12.381104 26698 authenticator.cpp:325] Authentication requires more steps
I0222 18:16:12.381134 26702 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0222 18:16:12.381142 26702 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381147 26702 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381162 26702 authenticator.cpp:317] Authentication success
I0222 18:16:12.381184 26698 authenticatee.cpp:258] Received SASL authentication step
I0222 18:16:12.381247 26699 authenticatee.cpp:298] Authentication success
I0222 18:16:12.381283 26696 authenticator.cpp:231] Received SASL authentication step
I0222 18:16:12.381311 26696 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0222 18:16:12.381325 26696 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0222 18:16:12.381319 26701 master.cpp:5556] Successfully authenticated principal 'test-principal' at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.381345 26700 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(832)@172.30.2.148:35274
I0222 18:16:12.381361 26696 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0222 18:16:12.381397 26696 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0222 18:16:12.381413 26696 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381422 26696 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381441 26696 authenticator.cpp:317] Authentication success
I0222 18:16:12.381548 26698 sched.cpp:471] Successfully authenticated with master master@172.30.2.148:35274
I0222 18:16:12.381563 26698 sched.cpp:776] Sending SUBSCRIBE call to master@172.30.2.148:35274
I0222 18:16:12.381634 26700 authenticatee.cpp:298] Authentication success
I0222 18:16:12.381660 26698 sched.cpp:809] Will retry registration in 770.60771ms if necessary
I0222 18:16:12.381675 26697 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(396)@172.30.2.148:35274
I0222 18:16:12.381734 26702 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(833)@172.30.2.148:35274
I0222 18:16:12.381811 26697 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.381882 26697 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0222 18:16:12.382004 26698 slave.cpp:927] Successfully authenticated with master master@172.30.2.148:35274
I0222 18:16:12.382123 26698 slave.cpp:1321] Will retry registration in 8.1941ms if necessary
I0222 18:16:12.382282 26701 master.cpp:4240] Registering slave at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) with id 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:12.382482 26701 master.cpp:2351] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0222 18:16:12.382612 26703 registrar.cpp:439] Applied 1 operations in 46327ns; attempting to update the 'registry'
I0222 18:16:12.382829 26699 hierarchical.cpp:265] Added framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.382910 26699 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:12.382915 26701 sched.cpp:703] Framework registered with 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.382936 26699 hierarchical.cpp:1529] No inverse offers to send out!
I0222 18:16:12.382953 26699 hierarchical.cpp:1127] Performed allocation for 0 slaves in 89949ns
I0222 18:16:12.382982 26701 sched.cpp:717] Scheduler::registered took 46498ns
I0222 18:16:12.383536 26698 log.cpp:683] Attempting to append 423 bytes to the log
I0222 18:16:12.383628 26699 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0222 18:16:12.384196 26700 replica.cpp:537] Replica received write request for position 3 from (13775)@172.30.2.148:35274
I0222 18:16:12.386602 26700 leveldb.cpp:341] Persisting action (442 bytes) to leveldb took 2.377119ms
I0222 18:16:12.386625 26700 replica.cpp:712] Persisted action at 3
I0222 18:16:12.387104 26698 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0222 18:16:12.389159 26698 leveldb.cpp:341] Persisting action (444 bytes) to leveldb took 2.032301ms
I0222 18:16:12.389181 26698 replica.cpp:712] Persisted action at 3
I0222 18:16:12.389196 26698 replica.cpp:697] Replica learned APPEND action at position 3
I0222 18:16:12.390281 26698 registrar.cpp:484] Successfully updated the 'registry' in 7.619072ms
I0222 18:16:12.390444 26702 log.cpp:702] Attempting to truncate the log to 3
I0222 18:16:12.390569 26701 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0222 18:16:12.390904 26701 slave.cpp:3482] Received ping from slave-observer(364)@172.30.2.148:35274
I0222 18:16:12.391054 26700 master.cpp:4308] Registered slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) with cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000]
I0222 18:16:12.391144 26703 slave.cpp:971] Registered with master master@172.30.2.148:35274; given slave ID 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:12.391168 26703 fetcher.cpp:81] Clearing fetcher cache
I0222 18:16:12.391238 26700 replica.cpp:537] Replica received write request for position 4 from (13776)@172.30.2.148:35274
I0222 18:16:12.391263 26701 status_update_manager.cpp:181] Resuming sending status updates
I0222 18:16:12.391304 26697 hierarchical.cpp:473] Added slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 (ip-172-30-2-148.mesosphere.io) with cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000] (allocated: )
I0222 18:16:12.391388 26703 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/meta/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/slave.info'
I0222 18:16:12.391636 26703 slave.cpp:1030] Forwarding total oversubscribed resources 
I0222 18:16:12.391772 26699 master.cpp:4649] Received update of slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) with total oversubscribed resources 
I0222 18:16:12.392011 26697 hierarchical.cpp:1529] No inverse offers to send out!
I0222 18:16:12.392053 26697 hierarchical.cpp:1147] Performed allocation for slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 in 708377ns
I0222 18:16:12.392307 26703 master.cpp:5355] Sending 1 offers to framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.392374 26697 hierarchical.cpp:531] Slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 (ip-172-30-2-148.mesosphere.io) updated with oversubscribed resources  (total: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000], allocated: disk(role1):2048; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000])
I0222 18:16:12.392500 26697 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:12.392531 26697 hierarchical.cpp:1529] No inverse offers to send out!
I0222 18:16:12.392556 26697 hierarchical.cpp:1147] Performed allocation for slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 in 136779ns
I0222 18:16:12.392704 26701 sched.cpp:873] Scheduler::resourceOffers took 94330ns
I0222 18:16:12.393086 26681 resources.cpp:576] Parsing resources as JSON failed: cpus:1;mem:64;
Trying semicolon-delimited string format instead
I0222 18:16:12.393600 26700 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 2.326382ms
I0222 18:16:12.393625 26700 replica.cpp:712] Persisted action at 4
I0222 18:16:12.394162 26696 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0222 18:16:12.394533 26701 master.cpp:3138] Processing ACCEPT call for offers: [ 652149b4-3932-4d8b-ba6f-8c9d9045be70-O0 ] on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.394567 26701 master.cpp:2926] Authorizing principal 'test-principal' to create volumes
I0222 18:16:12.394628 26701 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 1 as user 'root'
I0222 18:16:12.395519 26701 master.cpp:3467] Applying CREATE operation for volumes disk(role1)[id1:path1]:64 from framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 to slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.395808 26701 master.cpp:6589] Sending checkpointed resources disk(role1)[id1:path1]:64 to slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.396316 26696 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.130659ms
I0222 18:16:12.396317 26703 slave.cpp:2341] Updated checkpointed resources from  to disk(role1)[id1:path1]:64
I0222 18:16:12.396368 26696 leveldb.cpp:399] Deleting ~2 keys from leveldb took 30004ns
I0222 18:16:12.396381 26696 replica.cpp:712] Persisted action at 4
I0222 18:16:12.396397 26696 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0222 18:16:12.396533 26701 master.hpp:176] Adding task 1 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.396680 26701 master.cpp:3623] Launching task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.397009 26696 slave.cpp:1361] Got assigned task 1 for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.397143 26696 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0222 18:16:12.397306 26699 hierarchical.cpp:653] Updated allocation of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 from disk(role1):2048; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000] to disk(role1):1984; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64
I0222 18:16:12.397625 26699 hierarchical.cpp:892] Recovered disk(role1):1984; cpu(*):2; mem(*):1984; cpus(*):7; ports(*):[31000-32000] (total: cpu(*):2; mem(*):2048; disk(role1):1984; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64, allocated: disk(role1)[id1:path1]:64; cpus(*):1; mem(*):64) on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 from framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.397857 26696 slave.cpp:1480] Launching task 1 for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.397943 26696 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0222 18:16:12.398560 26696 paths.cpp:474] Trying to chown '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc' to user 'root'
I0222 18:16:12.403491 26696 slave.cpp:5367] Launching executor 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:12.404115 26696 slave.cpp:1698] Queuing task '1' for executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.405709 26696 slave.cpp:749] Successfully attached file '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:12.408308 26697 docker.cpp:1019] Starting container '207172a3-0ebd-4faa-946b-75a829fc75fc' for task '1' (and executor '1') of framework '652149b4-3932-4d8b-ba6f-8c9d9045be70-0000'
I0222 18:16:12.408592 26697 docker.cpp:1053] Running docker -H unix:///var/run/docker.sock inspect alpine:latest
I0222 18:16:12.520663 26702 docker.cpp:390] Docker pull alpine completed
I0222 18:16:12.520853 26702 docker.cpp:479] Changing the ownership of the persistent volume at '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/volumes/roles/role1/id1' with uid 0 and gid 0
I0222 18:16:12.524782 26702 docker.cpp:500] Mounting '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/volumes/roles/role1/id1' to '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc/path1' for persistent volume disk(role1)[id1:path1]:64 of container 207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:12.580834 26700 slave.cpp:2643] Got registration for executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from executor(1)@172.30.2.148:56026
I0222 18:16:12.581961 26699 docker.cpp:1299] Ignoring updating container '207172a3-0ebd-4faa-946b-75a829fc75fc' with resources passed to update is identical to existing resources
I0222 18:16:12.582307 26698 slave.cpp:1863] Sending queued task '1' to executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 at executor(1)@172.30.2.148:56026
I0222 18:16:13.295573 26703 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from executor(1)@172.30.2.148:56026
I0222 18:16:13.295940 26703 slave.cpp:3002] Handling status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from executor(1)@172.30.2.148:56026
I0222 18:16:13.296381 26701 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.296422 26701 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.296422 26703 slave.cpp:5677] Terminating task 1
I0222 18:16:13.296839 26701 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to the slave
I0222 18:16:13.296902 26702 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:13.299427 26699 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to master@172.30.2.148:35274
I0222 18:16:13.299921 26699 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.299969 26699 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to executor(1)@172.30.2.148:56026
I0222 18:16:13.300130 26696 master.cpp:4794] Status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:13.300176 26696 master.cpp:4842] Forwarding status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.300375 26696 master.cpp:6450] Updating the state of task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (latest state: TASK_FINISHED, status update state: TASK_RUNNING)
I0222 18:16:13.300765 26703 sched.cpp:981] Scheduler::statusUpdate took 164263ns
I0222 18:16:13.300962 26700 hierarchical.cpp:892] Recovered cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 (total: cpu(*):2; mem(*):2048; disk(role1):1984; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64, allocated: ) on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 from framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.301178 26699 master.cpp:3952] Processing ACKNOWLEDGE call 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0 for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:13.301450 26699 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.301697 26701 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.327133 26697 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.327280 26697 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to the slave
I0222 18:16:13.327481 26696 slave.cpp:3400] Forwarding the update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to master@172.30.2.148:35274
I0222 18:16:13.327621 26696 slave.cpp:3294] Status update manager successfully handled status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.327679 26696 slave.cpp:3310] Sending acknowledgement for status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to executor(1)@172.30.2.148:56026
I0222 18:16:13.327800 26698 master.cpp:4794] Status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:13.327850 26698 master.cpp:4842] Forwarding status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.327977 26698 master.cpp:6450] Updating the state of task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (latest state: TASK_FINISHED, status update state: TASK_FINISHED)
I0222 18:16:13.328248 26699 sched.cpp:981] Scheduler::statusUpdate took 100279ns
I0222 18:16:13.328588 26700 master.cpp:3952] Processing ACKNOWLEDGE call ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:13.328662 26681 sched.cpp:1903] Asked to stop the driver
I0222 18:16:13.328630 26700 master.cpp:6516] Removing task 1 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:13.328747 26697 sched.cpp:1143] Stopping framework '652149b4-3932-4d8b-ba6f-8c9d9045be70-0000'
I0222 18:16:13.329064 26696 status_update_manager.cpp:392] Received status update acknowledgement (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329069 26700 master.cpp:5926] Processing TEARDOWN call for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:13.329100 26700 master.cpp:5938] Removing framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:13.329200 26696 status_update_manager.cpp:528] Cleaning up status update stream for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329218 26703 hierarchical.cpp:375] Deactivated framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329309 26697 slave.cpp:2079] Asked to shut down framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 by master@172.30.2.148:35274
I0222 18:16:13.329346 26697 slave.cpp:2104] Shutting down framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329418 26697 slave.cpp:4198] Shutting down executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 at executor(1)@172.30.2.148:56026
I0222 18:16:13.329578 26699 hierarchical.cpp:326] Removed framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329684 26697 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.329733 26697 slave.cpp:5718] Completing task 1
I0222 18:16:13.337236 26703 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:13.337266 26703 hierarchical.cpp:1127] Performed allocation for 1 slaves in 153077ns
I0222 18:16:14.297827 26702 slave.cpp:3528] executor(1)@172.30.2.148:56026 exited
I0222 18:16:14.332489 26697 docker.cpp:1915] Executor for container '207172a3-0ebd-4faa-946b-75a829fc75fc' has exited
I0222 18:16:14.332512 26697 docker.cpp:1679] Destroying container '207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:14.332600 26697 docker.cpp:1807] Running docker stop on container '207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:14.333111 26697 docker.cpp:908] Unmounting volume for container '207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:14.333288 26700 slave.cpp:3886] Executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 exited with status 0
I0222 18:16:14.333340 26700 slave.cpp:3990] Cleaning up executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 at executor(1)@172.30.2.148:56026
I0222 18:16:14.333603 26703 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc' for gc 6.99999614056593days in the future
I0222 18:16:14.333669 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:14.333704 26700 slave.cpp:4078] Cleaning up framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:14.333726 26703 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1' for gc 6.99999613825185days in the future
I0222 18:16:14.336545 26703 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000' for gc 6.9999961115763days in the future
I0222 18:16:14.336699 26701 status_update_manager.cpp:282] Closing status update streams for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:14.338240 26699 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:14.338270 26699 hierarchical.cpp:1127] Performed allocation for 1 slaves in 191822ns
I0222 18:16:14.635416 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:14.940042 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:15.245256 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:15.339015 26697 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:15.339053 26697 hierarchical.cpp:1127] Performed allocation for 1 slaves in 265093ns
I0222 18:16:15.549804 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:15.854646 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:16.159210 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:16.339910 26698 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:16.339951 26698 hierarchical.cpp:1127] Performed allocation for 1 slaves in 255857ns
I0222 18:16:16.463809 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:16.768708 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:17.073479 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:17.340798 26696 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:17.340864 26696 hierarchical.cpp:1127] Performed allocation for 1 slaves in 260467ns
I0222 18:16:17.377902 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:17.683398 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:17.988231 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:18.292505 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:18.330112 26700 slave.cpp:4231] Framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 seems to have exited. Ignoring shutdown timeout for executor '1'
I0222 18:16:18.341600 26702 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:18.341634 26702 hierarchical.cpp:1127] Performed allocation for 1 slaves in 252012ns
I0222 18:16:18.596279 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:18.901157 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:19.204834 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:19.342326 26699 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:19.342358 26699 hierarchical.cpp:1127] Performed allocation for 1 slaves in 186829ns
I0222 18:16:19.508533 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:19.812255 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:20.116345 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:20.343556 26698 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:20.343588 26698 hierarchical.cpp:1127] Performed allocation for 1 slaves in 194704ns
I0222 18:16:20.420814 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:20.724819 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:21.029549 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:21.334319 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:21.344859 26702 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:21.344892 26702 hierarchical.cpp:1127] Performed allocation for 1 slaves in 241099ns
I0222 18:16:21.638164 26681 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
../../src/tests/containerizer/docker_containerizer_tests.cpp:1434: Failure
os::read(path::join(volumePath, ""file"")): Failed to open file '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/volumes/roles/role1/id1/file': No such file or directory
I0222 18:16:21.943008 26703 master.cpp:1027] Master terminating
I0222 18:16:21.943635 26696 hierarchical.cpp:505] Removed slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:21.943989 26702 slave.cpp:3528] master@172.30.2.148:35274 exited
W0222 18:16:21.944016 26702 slave.cpp:3531] Master disconnected! Waiting for a new master to be elected
I0222 18:16:21.948807 26699 slave.cpp:668] Slave terminating
I0222 18:16:21.951902 26681 docker.cpp:885] Running docker -H unix:///var/run/docker.sock ps -a
I0222 18:16:22.044273 26698 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:22.148877 26681 docker.cpp:727] Running docker -H unix:///var/run/docker.sock rm -f -v 422bfef31d51d2d3d2aafcf49b3e502654354bd98a98b076f4089b9a8e274d05
[  FAILED  ] DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes (10535 ms)
{code}",Centos6 + GCC 4.9 on AWS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-06 18:19:49.736,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 06 19:01:56 UTC 2017,,,,,,,"0|i2t67z:",9223372036854775807,,,,,tnachen,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,"23/Feb/16 17:56;kaysoky;This seems to be a problem with how Docker mounts volumes specified via {{-v <host path>:<container path>}}.  This must do a recursive bind mount for the persistent volume to show up correctly.  (It appears to only do a normal bind mount.)

Centos6 only supports up to Docker 1.7.1.  I'll try downgrading another OS (Centos7) to Docker 1.7.1 and see if the same failure exists.","06/Mar/17 18:19;tillt;[~kaysoky] do you have any update here?","06/Mar/17 19:01;kaysoky;Looks like I forgot to update this ticket.  The follow-up ticket is here: MESOS-4794

Basically, don't expect docker to work on CentOS 6, because docker doesn't support CentOS 6.  We should filter this test out on any CI's.",,,,,,,,,,,,,,,,,,,,,,,,,
"Make Stout configuration modular and consumable by downstream (e.g., libprocess and agent)",MESOS-4703,12940045,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hausdorff,hausdorff,hausdorff,18/Feb/16 03:50,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.28.0,,,,,,stout,,,,,0,cmake,mesosphere,,,,,,,"Stout configuration is replicated in at least 3 configuration files -- stout itself, libprocess, and agent. More will follow in the future.

We should make a StoutConfigure.cmake that can be included by any package downstream.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-01 18:42:50.253,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 01 18:43:19 UTC 2016,,,,,,,"0|i2szfz:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 30,,,,,,,,,,,1.0,,,,,,,,,,,"01/Mar/16 18:42;jvanremoortere;https://reviews.apache.org/r/43697/
https://reviews.apache.org/r/43698/
https://reviews.apache.org/r/43699/","01/Mar/16 18:43;jvanremoortere;{code}
commit 4194e395d31411416e3e9b0f7bb30d6845a2b8e8
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Mon Feb 29 15:31:26 2016 -0800

    CMake:[3/3] Used Stout config script in agent build.
    
    Review: https://reviews.apache.org/r/43699/

commit 684fce00f04b26f2c17a47063c332281b4c147c3
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Mon Feb 29 15:31:19 2016 -0800

    CMake:[2/3] Used new Stout config script in libprocess 3rdparty build.
    
    Review: https://reviews.apache.org/r/43698/

commit 86191221cc3cf51ba5cc7ebde9903bc34bb0512a
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Mon Feb 29 15:31:12 2016 -0800

    CMake:[1/3] Moved Stout configuration to its own file.
    
    Review: https://reviews.apache.org/r/43697/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
SlaveTest.StateEndpoint is flaky,MESOS-4695,12939913,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,kaysoky,kaysoky,17/Feb/16 19:55,26/Nov/18 09:28,29/Oct/20 16:32,26/Nov/18 09:28,,,,,,,,,1.2.0,,,,,,test,,,,,0,flaky,mesosphere,test,,,,,,"{code}
[ RUN      ] SlaveTest.StateEndpoint
../../src/tests/slave_tests.cpp:1220: Failure
Value of: state.values[""start_time""].as<JSON::Number>().as<int>()
  Actual: 1458159086
Expected: static_cast<int>(Clock::now().secs())
  Which is: 1458159085
[  FAILED  ] SlaveTest.StateEndpoint (193 ms)
{code}

Even though this test does {{Clock::pause()}} before starting the agent, there's a possibility that a numified-stringified double to not equal itself, even after rounding to the nearest int.","Ubuntu 15.10, gcc 4.9",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6837,MESOS-7029,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-11-30 12:42:13.164,,,false,MESOS-4018,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 09 14:31:01 UTC 2016,,,,,,,"0|i2symn:",9223372036854775807,,,,,tillt,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,"30/Nov/16 12:42;bbannier;Review: https://reviews.apache.org/r/54205/","09/Dec/16 14:31;tillt;{noformat}
commit 595c929f2816b713b4c36ce1bd23a7767afe8135
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Fri Dec 9 14:33:36 2016 +0100

    Improved equality check in SlaveTest.StateEndpoint.

    This test was checking two floating point values for equality by only
    examining their integer part. Since one of the values went through
    a number of conversions, even its integer part could have changed for
    certain starting values.

    This patch introduces an epsilon into the comparison to account for
    possible changes in the integer part.

    Review: https://reviews.apache.org/r/54205/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Create base docker image for test suite.,MESOS-4684,12939422,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,16/Feb/16 07:15,31/May/16 02:11,29/Oct/20 16:32,14/Mar/16 17:21,,,,,,,,,1.0.0,,,,,,containerization,,,,,0,containerizer,,,,,,,,"This should be widely used for unified containerizer testing. Should basically include:

*at least one layer.
*repositories.

For each layer:
*root file system as a layer tar ball.
*docker image json (manifest).
*docker version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-14 17:21:29.826,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 14 17:21:29 UTC 2016,,,,,,,"0|hzzzm6:zx",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 29,Mesosphere Sprint 30,,,,,,,,,,3.0,,,,,,,,,,,"24/Feb/16 20:21;gilbert;https://reviews.apache.org/r/43956/","14/Mar/16 17:21;jieyu;commit 5c7b56d9169de69c23e46767ab81a2e2591d8d88
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Fri Mar 11 17:07:24 2016 -0800

    Created base docker image for test suite.
    
    Review: https://reviews.apache.org/r/43956/",,,,,,,,,,,,,,,,,,,,,,,,,,
Document docker runtime isolator.,MESOS-4683,12939421,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,16/Feb/16 07:07,02/Mar/16 02:28,29/Oct/20 16:32,02/Mar/16 02:28,,,,,,,,,0.28.0,,,,,,documentation,,,,,0,containerizer,documentation,,,,,,,"Should include the following information:

*What features are currently supported in docker runtime isolator.
*How to use the docker runtime isolator (user manual).
*Compare the different semantics v.s. docker containerizer, and explain why.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-02 02:28:08.793,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 02 02:28:08 UTC 2016,,,,,,,"0|hzzzm6:zv",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 29,Mesosphere Sprint 30,,,,,,,,,,2.0,,,,,,,,,,,"02/Mar/16 02:28;jieyu;commit 4093229e9d5fa3b3d772a6f82f7b5b603fd51317
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Tue Mar 1 18:25:29 2016 -0800

    Documented the docker runtime isolator.
    
    Review: https://reviews.apache.org/r/44189/",,,,,,,,,,,,,,,,,,,,,,,,,,,
ROOT_DOCKER_Logs is flaky.,MESOS-4676,12939325,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,kaysoky,bernd-mesos,bernd-mesos,15/Feb/16 17:36,03/Mar/16 22:51,29/Oct/20 16:32,25/Feb/16 20:40,0.27.0,,,,,,,,0.28.0,,,,,,,,,,,0,flaky,mesosphere,test,,,,,,"{noformat}
[18:06:25][Step 8/8] [ RUN      ] DockerContainerizerTest.ROOT_DOCKER_Logs
[18:06:25][Step 8/8] I0215 17:06:25.256103  1740 leveldb.cpp:174] Opened db in 6.548327ms
[18:06:25][Step 8/8] I0215 17:06:25.258002  1740 leveldb.cpp:181] Compacted db in 1.837816ms
[18:06:25][Step 8/8] I0215 17:06:25.258059  1740 leveldb.cpp:196] Created db iterator in 22044ns
[18:06:25][Step 8/8] I0215 17:06:25.258076  1740 leveldb.cpp:202] Seeked to beginning of db in 2347ns
[18:06:25][Step 8/8] I0215 17:06:25.258091  1740 leveldb.cpp:271] Iterated through 0 keys in the db in 571ns
[18:06:25][Step 8/8] I0215 17:06:25.258152  1740 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[18:06:25][Step 8/8] I0215 17:06:25.258936  1758 recover.cpp:447] Starting replica recovery
[18:06:25][Step 8/8] I0215 17:06:25.259177  1758 recover.cpp:473] Replica is in EMPTY status
[18:06:25][Step 8/8] I0215 17:06:25.260327  1757 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (13608)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.260545  1758 recover.cpp:193] Received a recover response from a replica in EMPTY status
[18:06:25][Step 8/8] I0215 17:06:25.261065  1757 master.cpp:376] Master 112363e2-c680-4946-8fee-d0626ed8b21e (ip-172-30-2-239.mesosphere.io) started on 172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.261209  1761 recover.cpp:564] Updating replica status to STARTING
[18:06:25][Step 8/8] I0215 17:06:25.261086  1757 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/HncLLj/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/HncLLj/master"" --zk_session_timeout=""10secs""
[18:06:25][Step 8/8] I0215 17:06:25.261446  1757 master.cpp:423] Master only allowing authenticated frameworks to register
[18:06:25][Step 8/8] I0215 17:06:25.261456  1757 master.cpp:428] Master only allowing authenticated slaves to register
[18:06:25][Step 8/8] I0215 17:06:25.261462  1757 credentials.hpp:35] Loading credentials for authentication from '/tmp/HncLLj/credentials'
[18:06:25][Step 8/8] I0215 17:06:25.261723  1757 master.cpp:468] Using default 'crammd5' authenticator
[18:06:25][Step 8/8] I0215 17:06:25.261855  1757 master.cpp:537] Using default 'basic' HTTP authenticator
[18:06:25][Step 8/8] I0215 17:06:25.262022  1757 master.cpp:571] Authorization enabled
[18:06:25][Step 8/8] I0215 17:06:25.262177  1755 hierarchical.cpp:144] Initialized hierarchical allocator process
[18:06:25][Step 8/8] I0215 17:06:25.262177  1758 whitelist_watcher.cpp:77] No whitelist given
[18:06:25][Step 8/8] I0215 17:06:25.262899  1760 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.517992ms
[18:06:25][Step 8/8] I0215 17:06:25.262924  1760 replica.cpp:320] Persisted replica status to STARTING
[18:06:25][Step 8/8] I0215 17:06:25.263144  1754 recover.cpp:473] Replica is in STARTING status
[18:06:25][Step 8/8] I0215 17:06:25.264010  1757 master.cpp:1712] The newly elected leader is master@172.30.2.239:39785 with id 112363e2-c680-4946-8fee-d0626ed8b21e
[18:06:25][Step 8/8] I0215 17:06:25.264044  1757 master.cpp:1725] Elected as the leading master!
[18:06:25][Step 8/8] I0215 17:06:25.264061  1757 master.cpp:1470] Recovering from registrar
[18:06:25][Step 8/8] I0215 17:06:25.264117  1760 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (13610)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.264197  1758 registrar.cpp:307] Recovering registrar
[18:06:25][Step 8/8] I0215 17:06:25.264827  1756 recover.cpp:193] Received a recover response from a replica in STARTING status
[18:06:25][Step 8/8] I0215 17:06:25.265219  1757 recover.cpp:564] Updating replica status to VOTING
[18:06:25][Step 8/8] I0215 17:06:25.267302  1754 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.887739ms
[18:06:25][Step 8/8] I0215 17:06:25.267326  1754 replica.cpp:320] Persisted replica status to VOTING
[18:06:25][Step 8/8] I0215 17:06:25.267453  1759 recover.cpp:578] Successfully joined the Paxos group
[18:06:25][Step 8/8] I0215 17:06:25.267632  1759 recover.cpp:462] Recover process terminated
[18:06:25][Step 8/8] I0215 17:06:25.268007  1757 log.cpp:659] Attempting to start the writer
[18:06:25][Step 8/8] I0215 17:06:25.269055  1759 replica.cpp:493] Replica received implicit promise request from (13611)@172.30.2.239:39785 with proposal 1
[18:06:25][Step 8/8] I0215 17:06:25.270488  1759 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.406068ms
[18:06:25][Step 8/8] I0215 17:06:25.270511  1759 replica.cpp:342] Persisted promised to 1
[18:06:25][Step 8/8] I0215 17:06:25.271078  1761 coordinator.cpp:238] Coordinator attempting to fill missing positions
[18:06:25][Step 8/8] I0215 17:06:25.272146  1756 replica.cpp:388] Replica received explicit promise request from (13612)@172.30.2.239:39785 for position 0 with proposal 2
[18:06:25][Step 8/8] I0215 17:06:25.273478  1756 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.297217ms
[18:06:25][Step 8/8] I0215 17:06:25.273500  1756 replica.cpp:712] Persisted action at 0
[18:06:25][Step 8/8] I0215 17:06:25.274355  1757 replica.cpp:537] Replica received write request for position 0 from (13613)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.274405  1757 leveldb.cpp:436] Reading position from leveldb took 25294ns
[18:06:25][Step 8/8] I0215 17:06:25.275800  1757 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.362978ms
[18:06:25][Step 8/8] I0215 17:06:25.275823  1757 replica.cpp:712] Persisted action at 0
[18:06:25][Step 8/8] I0215 17:06:25.276348  1755 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.277765  1755 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.391531ms
[18:06:25][Step 8/8] I0215 17:06:25.277788  1755 replica.cpp:712] Persisted action at 0
[18:06:25][Step 8/8] I0215 17:06:25.277802  1755 replica.cpp:697] Replica learned NOP action at position 0
[18:06:25][Step 8/8] I0215 17:06:25.278336  1754 log.cpp:675] Writer started with ending position 0
[18:06:25][Step 8/8] I0215 17:06:25.279371  1755 leveldb.cpp:436] Reading position from leveldb took 29214ns
[18:06:25][Step 8/8] I0215 17:06:25.280272  1758 registrar.cpp:340] Successfully fetched the registry (0B) in 16.02688ms
[18:06:25][Step 8/8] I0215 17:06:25.280385  1758 registrar.cpp:439] Applied 1 operations in 31040ns; attempting to update the 'registry'
[18:06:25][Step 8/8] I0215 17:06:25.281054  1755 log.cpp:683] Attempting to append 210 bytes to the log
[18:06:25][Step 8/8] I0215 17:06:25.281165  1757 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[18:06:25][Step 8/8] I0215 17:06:25.281780  1757 replica.cpp:537] Replica received write request for position 1 from (13614)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.283159  1757 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 1.348041ms
[18:06:25][Step 8/8] I0215 17:06:25.283184  1757 replica.cpp:712] Persisted action at 1
[18:06:25][Step 8/8] I0215 17:06:25.283695  1759 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.285059  1759 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.334577ms
[18:06:25][Step 8/8] I0215 17:06:25.285084  1759 replica.cpp:712] Persisted action at 1
[18:06:25][Step 8/8] I0215 17:06:25.285099  1759 replica.cpp:697] Replica learned APPEND action at position 1
[18:06:25][Step 8/8] I0215 17:06:25.285910  1758 registrar.cpp:484] Successfully updated the 'registry' in 5.46816ms
[18:06:25][Step 8/8] I0215 17:06:25.286043  1758 registrar.cpp:370] Successfully recovered registrar
[18:06:25][Step 8/8] I0215 17:06:25.286121  1755 log.cpp:702] Attempting to truncate the log to 1
[18:06:25][Step 8/8] I0215 17:06:25.286301  1756 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[18:06:25][Step 8/8] I0215 17:06:25.286478  1759 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
[18:06:25][Step 8/8] I0215 17:06:25.286476  1754 master.cpp:1522] Recovered 0 slaves from the Registry (171B) ; allowing 10mins for slaves to re-register
[18:06:25][Step 8/8] I0215 17:06:25.287137  1755 replica.cpp:537] Replica received write request for position 2 from (13615)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.289104  1755 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.938609ms
[18:06:25][Step 8/8] I0215 17:06:25.289127  1755 replica.cpp:712] Persisted action at 2
[18:06:25][Step 8/8] I0215 17:06:25.289667  1759 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.290956  1759 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.256421ms
[18:06:25][Step 8/8] I0215 17:06:25.291007  1759 leveldb.cpp:399] Deleting ~1 keys from leveldb took 28064ns
[18:06:25][Step 8/8] I0215 17:06:25.291021  1759 replica.cpp:712] Persisted action at 2
[18:06:25][Step 8/8] I0215 17:06:25.291038  1759 replica.cpp:697] Replica learned TRUNCATE action at position 2
[18:06:25][Step 8/8] I0215 17:06:25.300550  1760 slave.cpp:193] Slave started on 393)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.300573  1760 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N""
[18:06:25][Step 8/8] I0215 17:06:25.300868  1760 credentials.hpp:83] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/credential'
[18:06:25][Step 8/8] I0215 17:06:25.301030  1760 slave.cpp:324] Slave using credential for: test-principal
[18:06:25][Step 8/8] I0215 17:06:25.301180  1760 resources.cpp:576] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
[18:06:25][Step 8/8] Trying semicolon-delimited string format instead
[18:06:25][Step 8/8] I0215 17:06:25.301553  1760 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[18:06:25][Step 8/8] I0215 17:06:25.301609  1760 slave.cpp:472] Slave attributes: [  ]
[18:06:25][Step 8/8] I0215 17:06:25.301620  1760 slave.cpp:477] Slave hostname: ip-172-30-2-239.mesosphere.io
[18:06:25][Step 8/8] I0215 17:06:25.302417  1757 state.cpp:58] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/meta'
[18:06:25][Step 8/8] I0215 17:06:25.302515  1740 sched.cpp:222] Version: 0.28.0
[18:06:25][Step 8/8] I0215 17:06:25.302772  1755 status_update_manager.cpp:200] Recovering status update manager
[18:06:25][Step 8/8] I0215 17:06:25.302956  1758 docker.cpp:559] Recovering Docker containers
[18:06:25][Step 8/8] I0215 17:06:25.303050  1761 sched.cpp:326] New master detected at master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303133  1754 slave.cpp:4565] Finished recovery
[18:06:25][Step 8/8] I0215 17:06:25.303154  1761 sched.cpp:382] Authenticating with master master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303169  1761 sched.cpp:389] Using default CRAM-MD5 authenticatee
[18:06:25][Step 8/8] I0215 17:06:25.303364  1759 authenticatee.cpp:121] Creating new client SASL connection
[18:06:25][Step 8/8] I0215 17:06:25.303467  1754 slave.cpp:4737] Querying resource estimator for oversubscribable resources
[18:06:25][Step 8/8] I0215 17:06:25.303668  1756 master.cpp:5523] Authenticating scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303707  1760 status_update_manager.cpp:174] Pausing sending status updates
[18:06:25][Step 8/8] I0215 17:06:25.303707  1754 slave.cpp:796] New master detected at master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303767  1755 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(829)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303791  1754 slave.cpp:859] Authenticating with master master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303805  1754 slave.cpp:864] Using default CRAM-MD5 authenticatee
[18:06:25][Step 8/8] I0215 17:06:25.303956  1754 slave.cpp:832] Detecting new master
[18:06:25][Step 8/8] I0215 17:06:25.303971  1761 authenticatee.cpp:121] Creating new client SASL connection
[18:06:25][Step 8/8] I0215 17:06:25.303984  1760 authenticator.cpp:98] Creating new server SASL connection
[18:06:25][Step 8/8] I0215 17:06:25.304131  1754 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
[18:06:25][Step 8/8] I0215 17:06:25.304275  1757 master.cpp:5523] Authenticating slave(393)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.304344  1754 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
[18:06:25][Step 8/8] I0215 17:06:25.304369  1754 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
[18:06:25][Step 8/8] I0215 17:06:25.304373  1761 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(830)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.304440  1757 authenticator.cpp:203] Received SASL authentication start
[18:06:25][Step 8/8] I0215 17:06:25.304491  1757 authenticator.cpp:325] Authentication requires more steps
[18:06:25][Step 8/8] I0215 17:06:25.304548  1754 authenticator.cpp:98] Creating new server SASL connection
[18:06:25][Step 8/8] I0215 17:06:25.304582  1761 authenticatee.cpp:258] Received SASL authentication step
[18:06:25][Step 8/8] I0215 17:06:25.304688  1761 authenticator.cpp:231] Received SASL authentication step
[18:06:25][Step 8/8] I0215 17:06:25.304714  1761 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[18:06:25][Step 8/8] I0215 17:06:25.304723  1761 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[18:06:25][Step 8/8] I0215 17:06:25.304767  1761 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[18:06:25][Step 8/8] I0215 17:06:25.304805  1761 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[18:06:25][Step 8/8] I0215 17:06:25.304817  1761 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[18:06:25][Step 8/8] I0215 17:06:25.304824  1761 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[18:06:25][Step 8/8] I0215 17:06:25.304836  1761 authenticator.cpp:317] Authentication success
[18:06:25][Step 8/8] I0215 17:06:25.304841  1758 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
[18:06:25][Step 8/8] I0215 17:06:25.304870  1758 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
[18:06:25][Step 8/8] I0215 17:06:25.304909  1757 authenticatee.cpp:298] Authentication success
[18:06:25][Step 8/8] I0215 17:06:25.304983  1756 authenticator.cpp:203] Received SASL authentication start
[18:06:25][Step 8/8] I0215 17:06:25.305033  1756 authenticator.cpp:325] Authentication requires more steps
[18:06:25][Step 8/8] I0215 17:06:25.305042  1759 master.cpp:5553] Successfully authenticated principal 'test-principal' at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305071  1755 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(829)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305124  1756 authenticatee.cpp:258] Received SASL authentication step
[18:06:25][Step 8/8] I0215 17:06:25.305222  1758 sched.cpp:471] Successfully authenticated with master master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305246  1758 sched.cpp:776] Sending SUBSCRIBE call to master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305286  1760 authenticator.cpp:231] Received SASL authentication step
[18:06:25][Step 8/8] I0215 17:06:25.305310  1760 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[18:06:25][Step 8/8] I0215 17:06:25.305318  1760 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[18:06:25][Step 8/8] I0215 17:06:25.305344  1760 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[18:06:25][Step 8/8] I0215 17:06:25.305363  1758 sched.cpp:809] Will retry registration in 1.888777185secs if necessary
[18:06:25][Step 8/8] I0215 17:06:25.305379  1760 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[18:06:25][Step 8/8] I0215 17:06:25.305397  1760 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[18:06:25][Step 8/8] I0215 17:06:25.305408  1760 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[18:06:25][Step 8/8] I0215 17:06:25.305426  1760 authenticator.cpp:317] Authentication success
[18:06:25][Step 8/8] I0215 17:06:25.305466  1761 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305506  1756 authenticatee.cpp:298] Authentication success
[18:06:25][Step 8/8] I0215 17:06:25.305534  1761 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
[18:06:25][Step 8/8] I0215 17:06:25.305625  1755 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(830)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305701  1761 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave(393)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305831  1758 slave.cpp:927] Successfully authenticated with master master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305902  1757 master.cpp:2351] Subscribing framework default with checkpointing disabled and capabilities [  ]
[18:06:25][Step 8/8] I0215 17:06:25.305953  1758 slave.cpp:1321] Will retry registration in 1.941456ms if necessary
[18:06:25][Step 8/8] I0215 17:06:25.306280  1761 hierarchical.cpp:265] Added framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.306352  1759 sched.cpp:703] Framework registered with 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.306363  1761 hierarchical.cpp:1403] No resources available to allocate!
[18:06:25][Step 8/8] I0215 17:06:25.306401  1761 hierarchical.cpp:1498] No inverse offers to send out!
[18:06:25][Step 8/8] I0215 17:06:25.306432  1761 hierarchical.cpp:1096] Performed allocation for 0 slaves in 126082ns
[18:06:25][Step 8/8] I0215 17:06:25.306447  1759 sched.cpp:717] Scheduler::registered took 67960ns
[18:06:25][Step 8/8] I0215 17:06:25.306437  1757 master.cpp:4237] Registering slave at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) with id 112363e2-c680-4946-8fee-d0626ed8b21e-S0
[18:06:25][Step 8/8] I0215 17:06:25.306884  1759 registrar.cpp:439] Applied 1 operations in 63175ns; attempting to update the 'registry'
[18:06:25][Step 8/8] I0215 17:06:25.307592  1756 log.cpp:683] Attempting to append 396 bytes to the log
[18:06:25][Step 8/8] I0215 17:06:25.307724  1760 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[18:06:25][Step 8/8] I0215 17:06:25.308398  1760 replica.cpp:537] Replica received write request for position 3 from (13622)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.308473  1755 slave.cpp:1321] Will retry registration in 37.671741ms if necessary
[18:06:25][Step 8/8] I0215 17:06:25.308627  1758 master.cpp:4225] Ignoring register slave message from slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) as admission is already in progress
[18:06:25][Step 8/8] I0215 17:06:25.310000  1760 leveldb.cpp:341] Persisting action (415 bytes) to leveldb took 1.556814ms
[18:06:25][Step 8/8] I0215 17:06:25.310025  1760 replica.cpp:712] Persisted action at 3
[18:06:25][Step 8/8] I0215 17:06:25.310541  1755 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.311928  1755 leveldb.cpp:341] Persisting action (417 bytes) to leveldb took 1.357404ms
[18:06:25][Step 8/8] I0215 17:06:25.311950  1755 replica.cpp:712] Persisted action at 3
[18:06:25][Step 8/8] I0215 17:06:25.311966  1755 replica.cpp:697] Replica learned APPEND action at position 3
[18:06:25][Step 8/8] I0215 17:06:25.313117  1755 registrar.cpp:484] Successfully updated the 'registry' in 6.16704ms
[18:06:25][Step 8/8] I0215 17:06:25.313297  1758 log.cpp:702] Attempting to truncate the log to 3
[18:06:25][Step 8/8] I0215 17:06:25.313391  1755 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[18:06:25][Step 8/8] I0215 17:06:25.313807  1761 slave.cpp:3482] Received ping from slave-observer(360)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.313946  1754 master.cpp:4305] Registered slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[18:06:25][Step 8/8] I0215 17:06:25.314067  1756 slave.cpp:971] Registered with master master@172.30.2.239:39785; given slave ID 112363e2-c680-4946-8fee-d0626ed8b21e-S0
[18:06:25][Step 8/8] I0215 17:06:25.314095  1756 fetcher.cpp:81] Clearing fetcher cache
[18:06:25][Step 8/8] I0215 17:06:25.314102  1760 replica.cpp:537] Replica received write request for position 4 from (13623)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.314164  1758 hierarchical.cpp:473] Added slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 (ip-172-30-2-239.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[18:06:25][Step 8/8] I0215 17:06:25.314219  1761 status_update_manager.cpp:181] Resuming sending status updates
[18:06:25][Step 8/8] I0215 17:06:25.314370  1756 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/meta/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/slave.info'
[18:06:25][Step 8/8] I0215 17:06:25.314579  1756 slave.cpp:1030] Forwarding total oversubscribed resources 
[18:06:25][Step 8/8] I0215 17:06:25.314707  1756 master.cpp:4646] Received update of slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) with total oversubscribed resources 
[18:06:25][Step 8/8] I0215 17:06:25.314818  1758 hierarchical.cpp:1498] No inverse offers to send out!
[18:06:25][Step 8/8] I0215 17:06:25.314848  1758 hierarchical.cpp:1116] Performed allocation for slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 in 654176ns
[18:06:25][Step 8/8] I0215 17:06:25.315137  1758 hierarchical.cpp:531] Slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 (ip-172-30-2-239.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[18:06:25][Step 8/8] I0215 17:06:25.315217  1756 master.cpp:5352] Sending 1 offers to framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.315238  1758 hierarchical.cpp:1403] No resources available to allocate!
[18:06:25][Step 8/8] I0215 17:06:25.315268  1758 hierarchical.cpp:1498] No inverse offers to send out!
[18:06:25][Step 8/8] I0215 17:06:25.315285  1758 hierarchical.cpp:1116] Performed allocation for slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 in 118646ns
[18:06:25][Step 8/8] I0215 17:06:25.315635  1755 sched.cpp:873] Scheduler::resourceOffers took 99802ns
[18:06:25][Step 8/8] I0215 17:06:25.317126  1755 master.cpp:3138] Processing ACCEPT call for offers: [ 112363e2-c680-4946-8fee-d0626ed8b21e-O0 ] on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.317163  1755 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 1 as user 'root'
[18:06:25][Step 8/8] I0215 17:06:25.317229  1760 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 3.089068ms
[18:06:25][Step 8/8] I0215 17:06:25.317261  1760 replica.cpp:712] Persisted action at 4
[18:06:25][Step 8/8] I0215 17:06:25.317845  1759 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.318722  1755 master.hpp:176] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 (ip-172-30-2-239.mesosphere.io)
[18:06:25][Step 8/8] I0215 17:06:25.318886  1755 master.cpp:3623] Launching task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io)
[18:06:25][Step 8/8] I0215 17:06:25.319195  1757 slave.cpp:1361] Got assigned task 1 for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.319305  1759 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.430044ms
[18:06:25][Step 8/8] I0215 17:06:25.319349  1757 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
[18:06:25][Step 8/8] Trying semicolon-delimited string format instead
[18:06:25][Step 8/8] I0215 17:06:25.319363  1759 leveldb.cpp:399] Deleting ~2 keys from leveldb took 34738ns
[18:06:25][Step 8/8] I0215 17:06:25.319380  1759 replica.cpp:712] Persisted action at 4
[18:06:25][Step 8/8] I0215 17:06:25.319396  1759 replica.cpp:697] Replica learned TRUNCATE action at position 4
[18:06:25][Step 8/8] I0215 17:06:25.320034  1757 slave.cpp:1480] Launching task 1 for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.320127  1757 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
[18:06:25][Step 8/8] Trying semicolon-delimited string format instead
[18:06:25][Step 8/8] I0215 17:06:25.320725  1757 paths.cpp:474] Trying to chown '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08' to user 'root'
[18:06:25][Step 8/8] I0215 17:06:25.325739  1757 slave.cpp:5351] Launching executor 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08'
[18:06:25][Step 8/8] I0215 17:06:25.326493  1757 slave.cpp:1698] Queuing task '1' for executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.326633  1757 slave.cpp:749] Successfully attached file '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08'
[18:06:25][Step 8/8] I0215 17:06:25.331328  1761 docker.cpp:803] Starting container '4f58cd28-7ac7-4960-b3e3-8d28918e6d08' for task '1' (and executor '1') of framework '112363e2-c680-4946-8fee-d0626ed8b21e-0000'
[18:06:25][Step 8/8] I0215 17:06:25.331699  1761 docker.cpp:1053] Running docker -H unix:///var/run/docker.sock inspect alpine:latest
[18:06:25][Step 8/8] I0215 17:06:25.449668  1758 docker.cpp:384] Docker pull alpine completed
[18:06:25][Step 8/8] I0215 17:06:25.511905  1760 slave.cpp:2643] Got registration for executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from executor(1)@172.30.2.239:38602
[18:06:25][Step 8/8] I0215 17:06:25.513098  1759 docker.cpp:1077] Ignoring updating container '4f58cd28-7ac7-4960-b3e3-8d28918e6d08' with resources passed to update is identical to existing resources
[18:06:25][Step 8/8] I0215 17:06:25.513494  1756 slave.cpp:1863] Sending queued task '1' to executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 at executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] 2016-02-15 17:06:25,981:1740(0x7f870b7fe700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36716] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
[18:06:26][Step 8/8] I0215 17:06:26.227973  1757 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.228302  1757 slave.cpp:3002] Handling status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.228734  1754 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.228790  1754 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.228837  1757 slave.cpp:5661] Terminating task 1
[18:06:26][Step 8/8] I0215 17:06:26.229243  1754 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to the slave
[18:06:26][Step 8/8] I0215 17:06:26.229346  1755 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-112363e2-c680-4946-8fee-d0626ed8b21e-S0.4f58cd28-7ac7-4960-b3e3-8d28918e6d08
[18:06:26][Step 8/8] I0215 17:06:26.232147  1758 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to master@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.232383  1758 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.232419  1758 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.232631  1759 master.cpp:4791] Status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io)
[18:06:26][Step 8/8] I0215 17:06:26.232681  1759 master.cpp:4839] Forwarding status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.232911  1759 master.cpp:6447] Updating the state of task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (latest state: TASK_FAILED, status update state: TASK_RUNNING)
[18:06:26][Step 8/8] I0215 17:06:26.233170  1756 sched.cpp:981] Scheduler::statusUpdate took 100304ns
[18:06:26][Step 8/8] I0215 17:06:26.233613  1754 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 from framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.233642  1759 master.cpp:3949] Processing ACKNOWLEDGE call b0f4bfb3-0f3c-4b03-843d-b5af82a902a3 for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785 on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0
[18:06:26][Step 8/8] I0215 17:06:26.233944  1759 status_update_manager.cpp:392] Received status update acknowledgement (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.234294  1761 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.264482  1759 hierarchical.cpp:1498] No inverse offers to send out!
[18:06:26][Step 8/8] I0215 17:06:26.264554  1759 hierarchical.cpp:1096] Performed allocation for 1 slaves in 1.210209ms
[18:06:26][Step 8/8] I0215 17:06:26.264837  1757 master.cpp:5352] Sending 1 offers to framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.265275  1760 sched.cpp:873] Scheduler::resourceOffers took 26245ns
[18:06:26][Step 8/8] I0215 17:06:26.357859  1756 status_update_manager.cpp:320] Received status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.358085  1756 status_update_manager.cpp:374] Forwarding update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to the slave
[18:06:26][Step 8/8] I0215 17:06:26.358330  1758 slave.cpp:3400] Forwarding the update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to master@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.358554  1758 slave.cpp:3294] Status update manager successfully handled status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.358594  1758 slave.cpp:3310] Sending acknowledgement for status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 to executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.358687  1761 master.cpp:4791] Status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io)
[18:06:26][Step 8/8] I0215 17:06:26.358724  1761 master.cpp:4839] Forwarding status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.358921  1761 master.cpp:6447] Updating the state of task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
[18:06:26][Step 8/8] ../../src/tests/containerizer/docker_containerizer_tests.cpp:1269: Failure
[18:06:26][Step 8/8] I0215 17:06:26.359128  1754 sched.cpp:981] Scheduler::statusUpdate took 108209ns
[18:06:26][Step 8/8] Value of: statusFinished.get().state()
[18:06:26][Step 8/8] I0215 17:06:26.359400  1759 master.cpp:3949] Processing ACKNOWLEDGE call 05810f46-10e7-4d50-a83d-d05bf79dd8e2 for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785 on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0
[18:06:26][Step 8/8]   Actual: TASK_FAILED
[18:06:26][Step 8/8] Expected: TASK_FINISHED
[18:06:26][Step 8/8] ../../src/tests/containerizer/docker_containerizer_tests.cpp:1278: Failure
[18:06:26][Step 8/8] I0215 17:06:26.359470  1759 master.cpp:6513] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io)
[18:06:26][Step 8/8] Value of: containsLine(lines, ""err"" + uuid)
[18:06:26][Step 8/8]   Actual: false
[18:06:26][Step 8/8] I0215 17:06:26.359928  1761 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] Expected: true
[18:06:26][Step 8/8] I0215 17:06:26.359931  1740 sched.cpp:1903] Asked to stop the driver
[18:06:26][Step 8/8] ../../src/tests/containerizer/docker_containerizer_tests.cpp:1286: Failure
[18:06:26][Step 8/8] I0215 17:06:26.360031  1759 sched.cpp:1143] Stopping framework '112363e2-c680-4946-8fee-d0626ed8b21e-0000'
[18:06:26][Step 8/8] Value of: containsLine(lines, ""out"" + uuid)
[18:06:26][Step 8/8] I0215 17:06:26.360080  1761 status_update_manager.cpp:528] Cleaning up status update stream for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8]   Actual: false
[18:06:26][Step 8/8] Expected: true
[18:06:26][Step 8/8] I0215 17:06:26.360213  1760 master.cpp:5923] Processing TEARDOWN call for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.360239  1760 master.cpp:5935] Removing framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.360481  1755 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.360522  1755 slave.cpp:5702] Completing task 1
[18:06:26][Step 8/8] I0215 17:06:26.360539  1756 hierarchical.cpp:375] Deactivated framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.360586  1755 slave.cpp:2079] Asked to shut down framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 by master@172.30.2.239:39785
[18:06:26][Step 8/8] I0215 17:06:26.360610  1755 slave.cpp:2104] Shutting down framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.360666  1755 slave.cpp:4198] Shutting down executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 at executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.361110  1761 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 from framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.361395  1756 hierarchical.cpp:326] Removed framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.361397  1760 master.cpp:1027] Master terminating
[18:06:26][Step 8/8] I0215 17:06:26.361800  1755 hierarchical.cpp:505] Removed slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0
[18:06:26][Step 8/8] I0215 17:06:26.362174  1756 slave.cpp:3528] master@172.30.2.239:39785 exited
[18:06:26][Step 8/8] W0215 17:06:26.362200  1756 slave.cpp:3531] Master disconnected! Waiting for a new master to be elected
[18:06:26][Step 8/8] I0215 17:06:26.367146  1758 docker.cpp:1455] Destroying container '4f58cd28-7ac7-4960-b3e3-8d28918e6d08'
[18:06:26][Step 8/8] I0215 17:06:26.367168  1758 docker.cpp:1515] Sending SIGTERM to executor with pid: 10194
[18:06:26][Step 8/8] I0215 17:06:26.383013  1758 docker.cpp:1557] Running docker stop on container '4f58cd28-7ac7-4960-b3e3-8d28918e6d08'
[18:06:26][Step 8/8] I0215 17:06:26.384843  1757 slave.cpp:3528] executor(1)@172.30.2.239:38602 exited
[18:06:26][Step 8/8] I0215 17:06:26.458639  1761 docker.cpp:1654] Executor for container '4f58cd28-7ac7-4960-b3e3-8d28918e6d08' has exited
[18:06:26][Step 8/8] I0215 17:06:26.458806  1757 slave.cpp:3886] Executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 terminated with signal Terminated
[18:06:26][Step 8/8] I0215 17:06:26.458852  1757 slave.cpp:3990] Cleaning up executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 at executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.459166  1754 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08' for gc 6.99999468745481days in the future
[18:06:26][Step 8/8] I0215 17:06:26.459262  1757 slave.cpp:4078] Cleaning up framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.459336  1754 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1' for gc 6.99999468505778days in the future
[18:06:26][Step 8/8] I0215 17:06:26.459372  1761 status_update_manager.cpp:282] Closing status update streams for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.459481  1754 gc.cpp:54] Scheduling '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000' for gc 6.99999468310222days in the future
[18:06:26][Step 8/8] I0215 17:06:26.459915  1759 slave.cpp:668] Slave terminating
[18:06:26][Step 8/8] I0215 17:06:26.463074  1740 docker.cpp:885] Running docker -H unix:///var/run/docker.sock ps -a
[18:06:26][Step 8/8] I0215 17:06:26.560154  1760 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-112363e2-c680-4946-8fee-d0626ed8b21e-S0.4f58cd28-7ac7-4960-b3e3-8d28918e6d08
[18:06:26][Step 8/8] I0215 17:06:26.666368  1740 docker.cpp:727] Running docker -H unix:///var/run/docker.sock rm -f -v c7f89c245f256c03222551178f6a0c26413bb91dfb3c843bff837b365d7c7432
[18:06:26][Step 8/8] [  FAILED  ] DockerContainerizerTest.ROOT_DOCKER_Logs (1519 ms)
{noformat}",CentOS 7 with SSL.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-17 18:06:10.275,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 25 20:39:57 UTC 2016,,,,,,,"0|i2sv0f:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 29,,,,,,,,,,,2.0,,,,,,,,,,,"17/Feb/16 18:06;haosdent@gmail.com;I think this is related to docker issue. When I try to reproduce this, I saw {{Unrecognized input header}} in stderr.
{code}
I0218 01:44:26.620074 17540 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-d2bdb09d-f546-4c5a-9385-628ebce457d9-S0.5978ddad-425c-4641-a5e8-2a62d6c45753
Unrecognized input header
{code}

And stdout looks weird.
{code}
Starting task 1
^B^@^@^@^@^@^@(errba84f3ab-0f60-4747-8451-56d82Shutting down
{code}","23/Feb/16 00:38;kaysoky;Confirmed that this is a docker issue.  I fished out a command string from a failed test with {{GLOG_v=1}}, then ran it independently repeatedly.

On Ubuntu12 (another place we're seeing the failure):
{code}
sh -c 'while true; do 
  docker -H unix:///var/run/docker.sock run --cpu-shares 2048 --memory 1073741824 \
  -e MESOS_SANDBOX=/mnt/mesos/sandbox \
  -e MESOS_CONTAINER_NAME=mesos-3672e44c-2c92-48d5-825e-e8475227ad88-S0.bdb7f52c-5d3e-46f9-b676-4e693fb0d1f2 \
  -v /tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_vSwYXT/slaves/3672e44c-2c92-48d5-825e-e8475227ad88-S0/frameworks/3672e44c-2c92-48d5-825e-e8475227ad88-0000/executors/1/runs/bdb7f52c-5d3e-46f9-b676-4e693fb0d1f2:/mnt/mesos/sandbox \
  --net host \
  --entrypoint /bin/sh \
  alpine -c ""echo outd5d895af-0c86-41bc-9f27-037ab12d8035 ; echo errd5d895af-0c86-41bc-9f27-037ab12d8035 1>&2""; 
done' 2>&1 | grep -v \
  -e ""^outd5d895af-0c86-41bc-9f27-037ab12d8035$"" \
  -e ""^errd5d895af-0c86-41bc-9f27-037ab12d8035$"" \
  -e ""^WARNING: Your kernel does not support swap limit capabilities, memory limited without swap.$""
{code}

After about an hour (don't know exactly how many iterations), got the following output:
{code}
(outd5d895af-0c86-41bc-9f27-037abUnrecognized input header
(outd5d895af-0c86-41bc-9f27-037abUnrecognized input header
(errd5d895af-0c86-41bc-9f27-037abUnrecognized input header
(errd5d895af-0c86-41bc-9f27-037abUnrecognized input header
...
{code}","24/Feb/16 20:08;kaysoky;Based on the linked issue (""Bug report for Docker 1.9.1 on Fedora""), it looks like docker has some sort of race when the containerized process writes to both stdout & stderr at the same time.

To mitigate the test hitting this:
* Try separating the two {{echo}} commands.
* Try using the {{unbuffer}} utility. i.e. {{unbuffer echo foo; unbuffer echo bar 1>&2}}.  See https://github.com/docker/docker/issues/1385","25/Feb/16 02:01;kaysoky;Approach #2: https://reviews.apache.org/r/43963/","25/Feb/16 02:45;haosdent@gmail.com;I also find this issue https://github.com/docker/docker/issues/19950 It said should be 
{quote}
This error is from stdcopy package which muxes stdout/stderr streams. It seems like now it writes something weird; I think it can also be golang version change.
{quote}
And I could reproduce through the example code in the issue https://gist.github.com/dpiddy/0c460a8bb297ee19a7a0

Verify that add {{-t}} when {{docker run}} also could avoid this problem.","25/Feb/16 20:39;kaysoky;{code}
commit d0d4d5a64e8aa17f0bc364060d98690b49037550
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Thu Feb 25 12:22:07 2016 +0100

    Fixed flakiness in DockerContainerizerTest.ROOT_DOCKER_Logs.
    
    Adds the `unbuffer` utility in front of each `echo` in the test.
    Since Docker appears to handle simultaneous stdout/stderr in a
    non-robust fashion, this mitigates the amount of overlap the two
    streams will have in the test.
    
    Review: https://reviews.apache.org/r/43963/
{code}",,,,,,,,,,,,,,,,,,,,,,
Cannot disable systemd support,MESOS-4675,12939324,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jvanremoortere,jvanremoortere,jvanremoortere,15/Feb/16 17:35,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,0.25.0,0.26.0,0.27.0,,,,,,0.27.1,0.28.0,,,,,,,,,,0,mesosphere,systemd,,,,,,,"On certain platforms the systemd init system is available, but not used.
Not being able to disable the mesos systemd integration on these platforms makes it hard to operate using a different init / monit system.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-6046,,,,,,,,"07/Jun/16 17:11;haosdent@gmail.com;MESOS-4675_0_25_0.patch;https://issues.apache.org/jira/secure/attachment/12808715/MESOS-4675_0_25_0.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,2016-06-07 17:11:49.692,,,false,MESOS-3007,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 08 05:24:00 UTC 2016,,,,,,,"0|i2sv07:",9223372036854775807,,,,,benjaminhindman,,,,,,Mesosphere Sprint 28,,,,,,,,,,,1.0,,,,,,,,,,,"15/Feb/16 17:38;jvanremoortere;https://reviews.apache.org/r/43582/","15/Feb/16 18:57;jvanremoortere;{code}
commit 0c53a34e35c2bd345251281c21a5932448c34683
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Mon Feb 15 12:30:27 2016 -0500

    Added flag to disable systemd support.
    
    Review: https://reviews.apache.org/r/43582
{code}","07/Jun/16 17:11;haosdent@gmail.com;Add the simple backport patch for mesos 0.25. Use `--systemd_enable_support=false` to disable systemd when start Mesos agent. ","08/Jun/16 05:24;xds2000;cool",,,,,,,,,,,,,,,,,,,,,,,,
Status updates from executor can be forwarded out of order by the Agent.,MESOS-4671,12938990,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,avinash.mesos,anandmazumdar,anandmazumdar,13/Feb/16 01:59,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,0.28.0,,,,,,,,0.28.0,,,,,,containerization,HTTP API,,,,0,mesosphere,,,,,,,,"Previously, all status update messages from the executor were forwarded by the agent to the master in the order that they had been received. 

However, that seems to be no longer valid due to a recently introduced change in the agent:

{code}
// Before sending update, we need to retrieve the container status.
  containerizer->status(executor->containerId)
    .onAny(defer(self(),
                 &Slave::_statusUpdate,
                 update,
                 pid,
                 executor->id,
                 lambda::_1));
{code}

This can sometimes lead to status updates being sent out of order depending on the order the {{Future}} is fulfilled from the call to {{status(...)}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4661,,,,,,MESOS-4661,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-15 00:53:02.49,,,false,MESOS-4343,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 17 23:07:03 UTC 2016,,,,,,,"0|i2ssz3:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 29,,,,,,,,,,,1.0,,,,,,,,,,,"13/Feb/16 02:02;anandmazumdar;cc: [~avinash@mesosphere.io], [~jieyu]","15/Feb/16 00:53;avinash.mesos;From the logs attached to MESOS-4661 it does look like the status updates for TASK_RUNNING and TASK_FINISHED got inverted. Here are the relevant logs:
I0212 00:23:11.639288   731 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: b0810058-a1c0-4918-b699-61c16eb0f46a) for task 1 of framework 6508f198-e145-4d76-844f-0460dc5d7d39-0000
I0212 00:23:11.639622   731 slave.cpp:3002] Handling status update TASK_FINISHED (UUID: 4674114a-c022-4945-ac98-52c0fae325e5) for task 1 of framework 6508f198-e145-4d76-844f-0460dc5d7d39-0000
I0212 00:23:11.641832   729 slave.cpp:5661] Terminating task 1
I0212 00:23:11.643185   721 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: 4674114a-c022-4945-ac98-52c0fae325e5) for task 1 of framework 6508f198-e145-4d76-844f-0460dc5d7d39-0000
I0212 00:23:11.643405  7480 executor.cpp:588] Enqueuing event SUBSCRIBED received from http://172.17.0.2:57200/slave/api/v1/executor
I0212 00:23:11.643427   721 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 6508f198-e145-4d76-844f-0460dc5d7d39-0000
I0212 00:23:11.644057   721 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_FINISHED (UUID: 4674114a-c022-4945-ac98-52c0fae325e5) for task 1 of framework 6508f198-e145-4d76-844f-0460dc5d7d39-0000
Received a SUBSCRIBED event


Since `dispatch` maintains the causality of events in a `process`, as long as the messages go through the same set of `processes` we should never see a re-ordering of events. However, the implementation of the status method in `MesosContainerizer` uses `await` to collect all the `ContainerStatus` `Futures` from the isolators, before completing the `Promise` given to the agent. The `await` method internally launches a `process` to wait for these futures. Thus, due to the use of `await` multiple `StatusUpdate` messages might end up being processed by a different set of `libprocess` thread causing a race.","15/Feb/16 03:05;klaus1982;In {{MesosContainerizerProcess}}, it'll also handle {{status}} in order; is it possible to return {{FINISHED}} before {{RUNNING}} for {{StatusUpdate}}?","15/Feb/16 03:28;avinash.mesos;Hi [~klaus1982],
 Sorry didn't understand your comment?","16/Feb/16 05:50;klaus1982;[~avinash@mesosphere.io], when we call {{containerizer->status(...)}}, it'll dispatch event to {{MesosContainerizerProcess}}; it's also handled in order although it used {{await}}. Does there any case that make {{FINISHED}} status return firstly? One possibility is that: both {{containerizer->status()}} finished between the two {{await}}:
{code}
await() for RUN failed (futures are pending) ==> both containerizer->status() finished ==> await() for FIN successfully (futures are ready)
{code}","17/Feb/16 23:07;jieyu;commit b45b9df715aa191722f22e57a15de3e6fbf22a4b
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Wed Feb 17 15:04:59 2016 -0800

    Searialized invocation of `await` in the status method.
    
    Review: https://reviews.apache.org/r/43673/

commit 95a8fc34057795a04354e562e4f4f995a311c1e6
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Wed Feb 17 15:04:45 2016 -0800

    Introduced `Sequence` in container.
    
    The `Sequence` will be used to serialize the invocation for status
    requests from isolators for a given container.
    
    Review: https://reviews.apache.org/r/43672/",,,,,,,,,,,,,,,,,,,,,,
`cgroup_info` not being exposed in state.json when ComposingContainerizer is used.,MESOS-4670,12938965,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,avinash.mesos,avinash.mesos,avinash.mesos,12/Feb/16 23:15,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.28.0,,,,,,,,,,,0,mesosphere,,,,,,,,"The ComposingContainerizer currently does not have a `status` method. This results in no `ContainerStatus` being updated in the agent, when uses `ComposingContainerizer` to launch containers. This would specifically happen when the agent is launched with `--containerizer=docker,mesos`",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-4343,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 18 14:35:03 UTC 2016,,,,,,,"0|i2sstj:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 29,,,,,,,,,,,1.0,,,,,,,,,,,"15/Feb/16 19:57;avinash.mesos;https://reviews.apache.org/r/43564/
https://reviews.apache.org/r/43565/
https://reviews.apache.org/r/43566/","18/Feb/16 14:35;avinash.mesos;commit 74c503658489293ed78c8abc493c36472431303f
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Tue Feb 16 15:50:13 2016 -0800

    Implemented `status` method in `ComposingContainerizer`.

    This method will be used by the agent to retrieve `ContainerStatus` from
    the `Containerizer`, that was responsible for launching the container.

    Review: https://reviews.apache.org/r/43565/
",,,,,,,,,,,,,,,,,,,,,,,,,,
Add common compression utility,MESOS-4669,12938943,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jojy,jojy,jojy,12/Feb/16 22:10,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.28.0,,,,,,containerization,,,,,0,mesosphere,unified-containerizer-mvp,,,,,,,We need GZIP uncompress utility for Appc image fetching functionality. The images are tar + gzip'ed and they needs to be first uncompressed so that we can compute sha 512 checksum on it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-25 16:54:39.202,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 25 16:54:39 UTC 2016,,,,,,,"0|hzzzqj:r",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 29,,,,,,,,,,,2.0,,,,,,,,,,,"12/Feb/16 22:11;jojy;https://reviews.apache.org/r/43546/","25/Feb/16 16:54;jieyu;commit 906566db499ce2c66a4d8044025ddb94131ead8b
Author: Jojy Varghese <jojy@mesosphere.io>
Date:   Wed Feb 17 11:33:20 2016 -0800

    Added common compression utilities.
    
    Added support for GZIP compress and uncompress.
    
    Review: https://reviews.apache.org/r/43546/",,,,,,,,,,,,,,,,,,,,,,,,,,
Logrotate container logger can die with agent unit on systemd.,MESOS-4640,12938233,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jvanremoortere,jvanremoortere,jvanremoortere,10/Feb/16 16:47,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,0.27.0,,,,,,,,0.27.1,0.28.0,,,,,,,,,,0,mesosphere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-3007,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 10 17:22:37 UTC 2016,,,,,,,"0|i2sob3:",9223372036854775807,,,,,benjaminhindman,,,,,,Mesosphere Sprint 28,,,,,,,,,,,1.0,,0.27.1,0.28.0,,,,,,,,"10/Feb/16 16:55;jvanremoortere;https://reviews.apache.org/r/43308/","10/Feb/16 17:22;jvanremoortere;{code}
commit 50d85172e68f25ebfa1e86611fb4a333b0ecc8f9
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Fri Feb 5 14:52:18 2016 -0500

    Extended life of logrotate processes on systemd.
    
    Review: https://reviews.apache.org/r/43308
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Posix process executor can die with agent unit on systemd.,MESOS-4639,12938232,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jvanremoortere,jvanremoortere,jvanremoortere,10/Feb/16 16:46,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,0.25.0,0.26.0,0.27.0,,,,,,0.25.1,0.26.1,0.27.1,0.28.0,,,,,,,,0,mesosphere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-3007,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 10 17:23:02 UTC 2016,,,,,,,"0|i2soav:",9223372036854775807,,,,,benjaminhindman,,,,,,Mesosphere Sprint 28,,,,,,,,,,,1.0,,0.27.1,0.28.0,,,,,,,,"10/Feb/16 16:55;jvanremoortere;https://reviews.apache.org/r/43309/","10/Feb/16 17:23;jvanremoortere;{code}
commit 2940f1fd448b7cb53fb7dc73cc3f0024cad3c669
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Fri Feb 5 16:34:07 2016 -0500

    Extended life of process based posix exector on systemd.
    
    Review: https://reviews.apache.org/r/43309
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Docker process executor can die with agent unit on systemd.,MESOS-4637,12938230,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jvanremoortere,jvanremoortere,jvanremoortere,10/Feb/16 16:45,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,0.25.0,0.26.0,0.27.0,,,,,,0.25.1,0.26.1,0.27.1,0.28.0,,,docker,,,,,0,mesosphere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-3007,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 10 17:21:43 UTC 2016,,,,,,,"0|i2soaf:",9223372036854775807,,,,,benjaminhindman,,,,,,Mesosphere Sprint 28,,,,,,,,,,,1.0,,0.27.1,0.28.0,,,,,,,,"10/Feb/16 16:55;jvanremoortere;https://reviews.apache.org/r/43307/","10/Feb/16 17:21;jvanremoortere;{code}
commit 435a147cabff20f96acebbce65b047afba22e102
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Fri Feb 5 14:00:01 2016 -0500

    Extended life of process based docker executor on systemd.
    
    Review: https://reviews.apache.org/r/43307
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Tests will dereference stack allocated master objects upon assertion/expectation failure.,MESOS-4634,12938029,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,kaysoky,kaysoky,kaysoky,09/Feb/16 23:04,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,1.0.0,,,,,,,,,,,0,flaky,mesosphere,tech-debt,test,,,,,"Tests that use the {{StartMaster}} test helper are generally fragile when the test fails an assert/expect in the middle of the test.  This is because the {{StartMaster}} helper takes raw pointer arguments, which may be stack-allocated.

In case of an assert failure, the test immediately exits (destroying stack allocated objects) and proceeds onto test cleanup.  The test cleanup may dereference some of these destroyed objects, leading to a test crash like:
{code}
[18:27:36][Step 8/8] F0204 18:27:35.981302 23085 logging.cpp:64] RAW: Pure virtual method called
[18:27:36][Step 8/8]     @     0x7f7077055e1c  google::LogMessage::Fail()
[18:27:36][Step 8/8]     @     0x7f707705ba6f  google::RawLog__()
[18:27:36][Step 8/8]     @     0x7f70760f76c9  __cxa_pure_virtual
[18:27:36][Step 8/8]     @           0xa9423c  mesos::internal::tests::Cluster::Slaves::shutdown()
[18:27:36][Step 8/8]     @          0x1074e45  mesos::internal::tests::MesosTest::ShutdownSlaves()
[18:27:36][Step 8/8]     @          0x1074de4  mesos::internal::tests::MesosTest::Shutdown()
[18:27:36][Step 8/8]     @          0x1070ec7  mesos::internal::tests::MesosTest::TearDown()
{code}

The {{StartMaster}} helper should take {{shared_ptr}} arguments instead.
This also means that we can remove the {{Shutdown}} helper from most of these tests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4633,,,,,,MESOS-4075,,,,MESOS-5000,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 16 22:20:36 UTC 2016,,,,,,,"0|hzzzl2:i",9223372036854775807,,,,,mcypark,,,,,,Mesosphere Sprint 28,Mesosphere Sprint 29,Mesosphere Sprint 30,Mesosphere Sprint 31,,,,,,,,5.0,,,,,,,,,,,"16/Feb/16 22:20;kaysoky;Note: As it turns out, this refactor is difficult to separate from the refactor in [MESOS-4634].  The patches in [MESOS-4634] also include this fix.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests will dereference stack allocated agent objects upon assertion/expectation failure.,MESOS-4633,12938019,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,kaysoky,kaysoky,kaysoky,09/Feb/16 22:28,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,1.0.0,,,,,,,,,,,0,flaky,mesosphere,tech-debt,test,,,,,"Tests that use the {{StartSlave}} test helper are generally fragile when the test fails an assert/expect in the middle of the test.  This is because the {{StartSlave}} helper takes raw pointer arguments, which may be stack-allocated.

In case of an assert failure, the test immediately exits (destroying stack allocated objects) and proceeds onto test cleanup.  The test cleanup may dereference some of these destroyed objects, leading to a test crash like:
{code}
[18:27:36][Step 8/8] F0204 18:27:35.981302 23085 logging.cpp:64] RAW: Pure virtual method called
[18:27:36][Step 8/8]     @     0x7f7077055e1c  google::LogMessage::Fail()
[18:27:36][Step 8/8]     @     0x7f707705ba6f  google::RawLog__()
[18:27:36][Step 8/8]     @     0x7f70760f76c9  __cxa_pure_virtual
[18:27:36][Step 8/8]     @           0xa9423c  mesos::internal::tests::Cluster::Slaves::shutdown()
[18:27:36][Step 8/8]     @          0x1074e45  mesos::internal::tests::MesosTest::ShutdownSlaves()
[18:27:36][Step 8/8]     @          0x1074de4  mesos::internal::tests::MesosTest::Shutdown()
[18:27:36][Step 8/8]     @          0x1070ec7  mesos::internal::tests::MesosTest::TearDown()
{code}

The {{StartSlave}} helper should take {{shared_ptr}} arguments instead.
This also means that we can remove the {{Shutdown}} helper from most of these tests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4604,,,,,,,,,MESOS-4075,,,,MESOS-5000,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-11 08:42:10.955,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 16 13:23:16 UTC 2016,,,,,,,"0|hzzzl2:",9223372036854775807,,,,,mcypark,,,,,,Mesosphere Sprint 28,Mesosphere Sprint 29,Mesosphere Sprint 30,Mesosphere Sprint 31,,,,,,,,5.0,,,,,,,,,,,"10/Feb/16 23:09;kaysoky;|| Review || Summary ||
| https://reviews.apache.org/r/43434/ | Change to {{Option}} |
|| Discarded below | (decided to take a different approach) |
| https://reviews.apache.org/r/43435/ | Change to {{StartSlave}} helper |
| https://reviews.apache.org/r/43436/ | Change to {{TestContainerizer}} |
| https://reviews.apache.org/r/43437/
https://reviews.apache.org/r/43438/
https://reviews.apache.org/r/43439/
https://reviews.apache.org/r/43440/
https://reviews.apache.org/r/43441/
https://reviews.apache.org/r/43442/
https://reviews.apache.org/r/43444/
https://reviews.apache.org/r/43445/
https://reviews.apache.org/r/43446/
https://reviews.apache.org/r/43447/
https://reviews.apache.org/r/43448/ | Tons and tons of test changes |","11/Feb/16 08:42;mcypark;{noformat}
commit 28917d657a69ee4731375ba54fe67d13816198ff
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Wed Feb 10 16:56:35 2016 -0800

    Constrain `Option`'s forwarding constructor to constructible types.

    Review: https://reviews.apache.org/r/43434/
{noformat}","16/Feb/16 20:10;kaysoky;|| Reviews || Summary ||
| https://reviews.apache.org/r/43613/ | Refactor of {{cluster::}} helpers |
| https://reviews.apache.org/r/43614/ | Change {{StartSlave}}/{{StartMaster}} helpers |
| https://reviews.apache.org/r/43629/
https://reviews.apache.org/r/43630/ | Outlier test changes |
| https://reviews.apache.org/r/43615/ | Tons of similar test changes |","16/Mar/16 13:23;mcypark;{noformat}
commit 951539317525f3afe9490ed098617e5d4563a80a
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Wed Mar 16 08:19:17 2016 -0400

    Update test suite to use the reworked MesosTest helpers.

    Includes the following changes:

    * Added the `<process/owned.hpp>` header where appropriate.
    * Added the namespace `using process::Owned;` where appropriate.
    * Generally replaced `Try<PID<Master>>` with
      `Try<Owned<cluster::Master>>`, and `Try<PID<Slave>>` with
      `Try<Owned<cluster::Slave>>`.
    * Added the (now required) `MasterDetector` argument to all slaves.
      Before, this was fetched from the first master in `Cluster`.
    * Removed `Shutdown();` from all tests.
    * Replaced `Stop(...)` with the appropriate master/slave
      destruction calls.
    * Wrap various slave objects in `Owned`
      (i.e. containerizers, isolators, launchers, etc).
    * Replace `CHECK` in tests with `ASSERT`.

    Review: https://reviews.apache.org/r/43615/
{noformat}
{noformat}
commit c7df1d7bcb1604c95800871cc0473c946e5b5d16
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Wed Mar 16 08:18:19 2016 -0400

    Especially updated scheduler tests to use the updated MesosTest helpers.

    Continuation of https://reviews.apache.org/r/43615/ with re-ordering of
    some local variables due to the order of destruction.

    Review: https://reviews.apache.org/r/43630/
{noformat}
{noformat}
commit 56e9406763e8514a7557ab3862d2f352a61425d5
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Wed Mar 16 08:17:46 2016 -0400

    Especially updated tests to use the updated MesosTest helpers.

    Continuation of https://reviews.apache.org/r/43615/ with a slightly
    different pattern.

    Review: https://reviews.apache.org/r/43629/
{noformat}
{noformat}
commit 75ca1e6c9fde655c41fdf835aa20c47570d21f10
Author: Michael Park <mpark@apache.org>
Date:   Wed Mar 16 08:14:58 2016 -0400

    Fixed formatting issues in `src/tests/mesos.{hpp,cpp}`.
{noformat}
{noformat}
commit b377557c2bfc35c894e87becb47122955540f133
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Tue Mar 15 18:51:09 2016 -0400

    Refactor MesosTest and remove cleanup logic.

    Updates `StartMaster` and `StartSlave` test helpers to use the reworked
    `cluster` helpers. Removes all `MesosTest` cleanup logic and as well as
    the helpers that accept a `MockExecutor` pointer.

    Review: https://reviews.apache.org/r/4361
{noformat}
{noformat}
commit 7bf6e4f70131175edd4d6d77ea0dc7692b3e72ae
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Tue Mar 15 18:51:06 2016 -0400

    Refactor cluster test helpers into self-contained objects.

    Major rewrite of the `tests/cluster` helpers. This strongly ties
    the scope of test objects to the test body.

    Changes the `Cluster` class into two RAII objects
    (`Master` and `Slave`). The `Slave` object performs cleanup originally
    found in `cluster::Slave::stop`. `cluster::Master::start` and
    `cluster::Slave::start` were changed to factory methods.

    Review: https://reviews.apache.org/r/43613/4/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,
ContainerLoggerTest.DefaultToSandbox is flaky,MESOS-4615,12937224,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,kaysoky,greggomann,greggomann,06/Feb/16 01:43,26/Apr/17 17:01,29/Oct/20 16:32,17/Feb/16 15:33,0.27.0,,,,,,,,0.28.0,,,,,,test,,,,,0,flaky-test,logger,mesosphere,,,,,,"Just saw this failure on the ASF CI:

{code}
[ RUN      ] ContainerLoggerTest.DefaultToSandbox
I0206 01:25:03.766458  2824 leveldb.cpp:174] Opened db in 72.979786ms
I0206 01:25:03.811712  2824 leveldb.cpp:181] Compacted db in 45.162067ms
I0206 01:25:03.811810  2824 leveldb.cpp:196] Created db iterator in 26090ns
I0206 01:25:03.811828  2824 leveldb.cpp:202] Seeked to beginning of db in 3173ns
I0206 01:25:03.811839  2824 leveldb.cpp:271] Iterated through 0 keys in the db in 497ns
I0206 01:25:03.811900  2824 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0206 01:25:03.812785  2849 recover.cpp:447] Starting replica recovery
I0206 01:25:03.813043  2849 recover.cpp:473] Replica is in EMPTY status
I0206 01:25:03.814668  2854 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (371)@172.17.0.8:37843
I0206 01:25:03.815210  2849 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0206 01:25:03.815732  2854 recover.cpp:564] Updating replica status to STARTING
I0206 01:25:03.819664  2857 master.cpp:376] Master 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de (74ef606c4063) started on 172.17.0.8:37843
I0206 01:25:03.819703  2857 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/h5vu5I/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/h5vu5I/master"" --zk_session_timeout=""10secs""
I0206 01:25:03.820241  2857 master.cpp:423] Master only allowing authenticated frameworks to register
I0206 01:25:03.820257  2857 master.cpp:428] Master only allowing authenticated slaves to register
I0206 01:25:03.820269  2857 credentials.hpp:35] Loading credentials for authentication from '/tmp/h5vu5I/credentials'
I0206 01:25:03.821110  2857 master.cpp:468] Using default 'crammd5' authenticator
I0206 01:25:03.821311  2857 master.cpp:537] Using default 'basic' HTTP authenticator
I0206 01:25:03.821636  2857 master.cpp:571] Authorization enabled
I0206 01:25:03.821979  2846 hierarchical.cpp:144] Initialized hierarchical allocator process
I0206 01:25:03.822057  2846 whitelist_watcher.cpp:77] No whitelist given
I0206 01:25:03.825460  2847 master.cpp:1712] The newly elected leader is master@172.17.0.8:37843 with id 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de
I0206 01:25:03.825512  2847 master.cpp:1725] Elected as the leading master!
I0206 01:25:03.825533  2847 master.cpp:1470] Recovering from registrar
I0206 01:25:03.825835  2847 registrar.cpp:307] Recovering registrar
I0206 01:25:03.848212  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 32.226093ms
I0206 01:25:03.848299  2854 replica.cpp:320] Persisted replica status to STARTING
I0206 01:25:03.848702  2854 recover.cpp:473] Replica is in STARTING status
I0206 01:25:03.850728  2858 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (373)@172.17.0.8:37843
I0206 01:25:03.851230  2854 recover.cpp:193] Received a recover response from a replica in STARTING status
I0206 01:25:03.852018  2854 recover.cpp:564] Updating replica status to VOTING
I0206 01:25:03.881681  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.184163ms
I0206 01:25:03.881772  2854 replica.cpp:320] Persisted replica status to VOTING
I0206 01:25:03.882058  2854 recover.cpp:578] Successfully joined the Paxos group
I0206 01:25:03.882258  2854 recover.cpp:462] Recover process terminated
I0206 01:25:03.883076  2854 log.cpp:659] Attempting to start the writer
I0206 01:25:03.885040  2854 replica.cpp:493] Replica received implicit promise request from (374)@172.17.0.8:37843 with proposal 1
I0206 01:25:03.915132  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.980589ms
I0206 01:25:03.915215  2854 replica.cpp:342] Persisted promised to 1
I0206 01:25:03.916038  2856 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0206 01:25:03.917659  2856 replica.cpp:388] Replica received explicit promise request from (375)@172.17.0.8:37843 for position 0 with proposal 2
I0206 01:25:03.948698  2856 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 30.974607ms
I0206 01:25:03.948786  2856 replica.cpp:712] Persisted action at 0
I0206 01:25:03.950920  2849 replica.cpp:537] Replica received write request for position 0 from (376)@172.17.0.8:37843
I0206 01:25:03.951011  2849 leveldb.cpp:436] Reading position from leveldb took 44263ns
I0206 01:25:03.982026  2849 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 30.947321ms
I0206 01:25:03.982225  2849 replica.cpp:712] Persisted action at 0
I0206 01:25:03.983867  2849 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0206 01:25:04.015499  2849 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 30.957888ms
I0206 01:25:04.015591  2849 replica.cpp:712] Persisted action at 0
I0206 01:25:04.015682  2849 replica.cpp:697] Replica learned NOP action at position 0
I0206 01:25:04.016666  2849 log.cpp:675] Writer started with ending position 0
I0206 01:25:04.017881  2855 leveldb.cpp:436] Reading position from leveldb took 56779ns
I0206 01:25:04.018934  2852 registrar.cpp:340] Successfully fetched the registry (0B) in 193.048064ms
I0206 01:25:04.019076  2852 registrar.cpp:439] Applied 1 operations in 38180ns; attempting to update the 'registry'
I0206 01:25:04.020100  2844 log.cpp:683] Attempting to append 170 bytes to the log
I0206 01:25:04.020288  2855 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0206 01:25:04.021323  2844 replica.cpp:537] Replica received write request for position 1 from (377)@172.17.0.8:37843
I0206 01:25:04.054726  2844 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 33.309419ms
I0206 01:25:04.054818  2844 replica.cpp:712] Persisted action at 1
I0206 01:25:04.055933  2844 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0206 01:25:04.088142  2844 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 32.116643ms
I0206 01:25:04.088230  2844 replica.cpp:712] Persisted action at 1
I0206 01:25:04.088265  2844 replica.cpp:697] Replica learned APPEND action at position 1
I0206 01:25:04.090070  2856 registrar.cpp:484] Successfully updated the 'registry' in 70.90816ms
I0206 01:25:04.090338  2851 log.cpp:702] Attempting to truncate the log to 1
I0206 01:25:04.090358  2856 registrar.cpp:370] Successfully recovered registrar
I0206 01:25:04.090507  2847 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0206 01:25:04.090867  2858 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0206 01:25:04.091449  2858 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0206 01:25:04.092280  2857 replica.cpp:537] Replica received write request for position 2 from (378)@172.17.0.8:37843
I0206 01:25:04.125702  2857 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 33.192265ms
I0206 01:25:04.125804  2857 replica.cpp:712] Persisted action at 2
I0206 01:25:04.127400  2857 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0206 01:25:04.157727  2857 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 30.268594ms
I0206 01:25:04.157905  2857 leveldb.cpp:399] Deleting ~1 keys from leveldb took 88436ns
I0206 01:25:04.157941  2857 replica.cpp:712] Persisted action at 2
I0206 01:25:04.157984  2857 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0206 01:25:04.166174  2824 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0206 01:25:04.166954  2824 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I0206 01:25:04.172008  2844 slave.cpp:193] Slave started on 9)@172.17.0.8:37843
I0206 01:25:04.172046  2844 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw""
I0206 01:25:04.172569  2844 credentials.hpp:83] Loading credential for authentication from '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/credential'
I0206 01:25:04.172886  2844 slave.cpp:324] Slave using credential for: test-principal
I0206 01:25:04.173141  2844 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0206 01:25:04.173620  2844 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 01:25:04.173686  2844 slave.cpp:472] Slave attributes: [  ]
I0206 01:25:04.173702  2844 slave.cpp:477] Slave hostname: 74ef606c4063
I0206 01:25:04.174816  2847 state.cpp:58] Recovering state from '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/meta'
I0206 01:25:04.175441  2847 status_update_manager.cpp:200] Recovering status update manager
I0206 01:25:04.175678  2858 containerizer.cpp:397] Recovering containerizer
I0206 01:25:04.177573  2858 provisioner.cpp:245] Provisioner recovery complete
I0206 01:25:04.178231  2847 slave.cpp:4496] Finished recovery
I0206 01:25:04.178834  2847 slave.cpp:4668] Querying resource estimator for oversubscribable resources
I0206 01:25:04.179405  2847 slave.cpp:796] New master detected at master@172.17.0.8:37843
I0206 01:25:04.179500  2847 slave.cpp:859] Authenticating with master master@172.17.0.8:37843
I0206 01:25:04.179525  2847 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0206 01:25:04.179656  2858 status_update_manager.cpp:174] Pausing sending status updates
I0206 01:25:04.179798  2847 slave.cpp:832] Detecting new master
I0206 01:25:04.179891  2852 authenticatee.cpp:121] Creating new client SASL connection
I0206 01:25:04.179916  2847 slave.cpp:4682] Received oversubscribable resources  from the resource estimator
I0206 01:25:04.180286  2847 master.cpp:5523] Authenticating slave(9)@172.17.0.8:37843
I0206 01:25:04.180569  2847 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(32)@172.17.0.8:37843
I0206 01:25:04.181000  2847 authenticator.cpp:98] Creating new server SASL connection
I0206 01:25:04.181315  2847 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 01:25:04.181387  2847 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 01:25:04.181562  2847 authenticator.cpp:203] Received SASL authentication start
I0206 01:25:04.181648  2847 authenticator.cpp:325] Authentication requires more steps
I0206 01:25:04.181843  2847 authenticatee.cpp:258] Received SASL authentication step
I0206 01:25:04.182034  2853 authenticator.cpp:231] Received SASL authentication step
I0206 01:25:04.182071  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 01:25:04.182093  2853 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 01:25:04.182145  2853 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 01:25:04.182173  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 01:25:04.182185  2853 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.182193  2853 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.182211  2853 authenticator.cpp:317] Authentication success
I0206 01:25:04.182333  2849 authenticatee.cpp:298] Authentication success
I0206 01:25:04.182422  2853 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave(9)@172.17.0.8:37843
I0206 01:25:04.182510  2853 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(32)@172.17.0.8:37843
I0206 01:25:04.182945  2849 slave.cpp:927] Successfully authenticated with master master@172.17.0.8:37843
I0206 01:25:04.183178  2849 slave.cpp:1321] Will retry registration in 9.87937ms if necessary
I0206 01:25:04.183466  2852 master.cpp:4237] Registering slave at slave(9)@172.17.0.8:37843 (74ef606c4063) with id 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0
I0206 01:25:04.184039  2845 registrar.cpp:439] Applied 1 operations in 89453ns; attempting to update the 'registry'
I0206 01:25:04.185288  2856 log.cpp:683] Attempting to append 339 bytes to the log
I0206 01:25:04.185672  2850 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0206 01:25:04.186674  2846 replica.cpp:537] Replica received write request for position 3 from (392)@172.17.0.8:37843
I0206 01:25:04.195863  2856 slave.cpp:1321] Will retry registration in 11.038094ms if necessary
I0206 01:25:04.196233  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.208094  2856 slave.cpp:1321] Will retry registration in 27.881223ms if necessary
I0206 01:25:04.208472  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.216698  2846 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 29.961291ms
I0206 01:25:04.216789  2846 replica.cpp:712] Persisted action at 3
I0206 01:25:04.218246  2845 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0206 01:25:04.237861  2846 slave.cpp:1321] Will retry registration in 1.006941ms if necessary
I0206 01:25:04.238221  2846 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.239858  2856 slave.cpp:1321] Will retry registration in 167.305686ms if necessary
I0206 01:25:04.240044  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.241482  2845 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 23.193162ms
I0206 01:25:04.241524  2845 replica.cpp:712] Persisted action at 3
I0206 01:25:04.241557  2845 replica.cpp:697] Replica learned APPEND action at position 3
I0206 01:25:04.243746  2844 registrar.cpp:484] Successfully updated the 'registry' in 59.587072ms
I0206 01:25:04.244210  2857 log.cpp:702] Attempting to truncate the log to 3
I0206 01:25:04.244344  2845 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0206 01:25:04.244597  2856 master.cpp:4305] Registered slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 01:25:04.244746  2843 slave.cpp:3436] Received ping from slave-observer(8)@172.17.0.8:37843
I0206 01:25:04.244976  2845 hierarchical.cpp:473] Added slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0206 01:25:04.245072  2843 slave.cpp:971] Registered with master master@172.17.0.8:37843; given slave ID 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0
I0206 01:25:04.245121  2843 fetcher.cpp:81] Clearing fetcher cache
I0206 01:25:04.245146  2845 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:04.245178  2845 hierarchical.cpp:1116] Performed allocation for slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 in 159744ns
I0206 01:25:04.245465  2846 status_update_manager.cpp:181] Resuming sending status updates
I0206 01:25:04.245776  2843 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/meta/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/slave.info'
I0206 01:25:04.245745  2846 replica.cpp:537] Replica received write request for position 4 from (393)@172.17.0.8:37843
I0206 01:25:04.246273  2843 slave.cpp:1030] Forwarding total oversubscribed resources 
I0206 01:25:04.246507  2850 master.cpp:4646] Received update of slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) with total oversubscribed resources 
I0206 01:25:04.247180  2824 sched.cpp:222] Version: 0.28.0
I0206 01:25:04.247155  2850 hierarchical.cpp:531] Slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0206 01:25:04.247357  2850 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:04.247406  2850 hierarchical.cpp:1116] Performed allocation for slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 in 183250ns
I0206 01:25:04.247938  2854 sched.cpp:326] New master detected at master@172.17.0.8:37843
I0206 01:25:04.248157  2854 sched.cpp:382] Authenticating with master master@172.17.0.8:37843
I0206 01:25:04.248265  2854 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0206 01:25:04.248769  2854 authenticatee.cpp:121] Creating new client SASL connection
I0206 01:25:04.249311  2854 master.cpp:5523] Authenticating scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.249646  2854 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(33)@172.17.0.8:37843
I0206 01:25:04.250114  2854 authenticator.cpp:98] Creating new server SASL connection
I0206 01:25:04.250453  2854 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 01:25:04.250525  2854 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 01:25:04.250814  2853 authenticator.cpp:203] Received SASL authentication start
I0206 01:25:04.250881  2853 authenticator.cpp:325] Authentication requires more steps
I0206 01:25:04.250982  2853 authenticatee.cpp:258] Received SASL authentication step
I0206 01:25:04.251092  2853 authenticator.cpp:231] Received SASL authentication step
I0206 01:25:04.251128  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 01:25:04.251144  2853 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 01:25:04.251200  2853 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 01:25:04.251242  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 01:25:04.251260  2853 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.251269  2853 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.251288  2853 authenticator.cpp:317] Authentication success
I0206 01:25:04.251471  2853 authenticatee.cpp:298] Authentication success
I0206 01:25:04.251574  2853 master.cpp:5553] Successfully authenticated principal 'test-principal' at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.251669  2851 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(33)@172.17.0.8:37843
I0206 01:25:04.252162  2854 sched.cpp:471] Successfully authenticated with master master@172.17.0.8:37843
I0206 01:25:04.252188  2854 sched.cpp:776] Sending SUBSCRIBE call to master@172.17.0.8:37843
I0206 01:25:04.252286  2854 sched.cpp:809] Will retry registration in 1.575999657secs if necessary
I0206 01:25:04.252583  2853 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.252694  2853 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0206 01:25:04.253110  2853 master.cpp:2351] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0206 01:25:04.253703  2843 hierarchical.cpp:265] Added framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.255300  2843 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:04.255367  2843 hierarchical.cpp:1096] Performed allocation for 1 slaves in 1.621522ms
I0206 01:25:04.255820  2844 sched.cpp:703] Framework registered with 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.256006  2844 sched.cpp:717] Scheduler::registered took 105156ns
I0206 01:25:04.256572  2853 master.cpp:5352] Sending 1 offers to framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.257524  2853 sched.cpp:873] Scheduler::resourceOffers took 173470ns
I0206 01:25:04.260818  2855 master.cpp:3138] Processing ACCEPT call for offers: [ 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-O0 ] on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.260968  2855 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 0e7267ed-c5ed-4914-9042-5970b2aaec1c as user 'mesos'
I0206 01:25:04.264458  2844 master.hpp:176] Adding task 0e7267ed-c5ed-4914-9042-5970b2aaec1c with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063)
I0206 01:25:04.264796  2844 master.cpp:3623] Launching task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063)
I0206 01:25:04.265341  2855 slave.cpp:1361] Got assigned task 0e7267ed-c5ed-4914-9042-5970b2aaec1c for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.265941  2855 resources.cpp:564] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0206 01:25:04.267323  2855 slave.cpp:1480] Launching task 0e7267ed-c5ed-4914-9042-5970b2aaec1c for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.267627  2855 resources.cpp:564] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0206 01:25:04.268705  2855 paths.cpp:474] Trying to chown '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7' to user 'mesos'
I0206 01:25:04.274116  2855 slave.cpp:5282] Launching executor 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7'
I0206 01:25:04.275185  2844 containerizer.cpp:656] Starting container '5c952202-44cf-427a-8452-0f501140a4b7' for executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework '914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000'
I0206 01:25:04.275311  2846 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 29.403837ms
I0206 01:25:04.275390  2846 replica.cpp:712] Persisted action at 4
I0206 01:25:04.275511  2855 slave.cpp:1698] Queuing task '0e7267ed-c5ed-4914-9042-5970b2aaec1c' for executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.275832  2855 slave.cpp:749] Successfully attached file '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7'
I0206 01:25:04.276707  2855 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0206 01:25:04.284708  2844 launcher.cpp:132] Forked child with pid '2872' for container '5c952202-44cf-427a-8452-0f501140a4b7'
I0206 01:25:04.301365  2855 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 24.497489ms
I0206 01:25:04.301528  2855 leveldb.cpp:399] Deleting ~2 keys from leveldb took 92156ns
I0206 01:25:04.301563  2855 replica.cpp:712] Persisted action at 4
I0206 01:25:04.301640  2855 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0206 01:25:04.823314  2854 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:04.823387  2854 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:04.823420  2854 hierarchical.cpp:1096] Performed allocation for 1 slaves in 327509ns
I0206 01:25:05.825943  2850 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:05.826027  2850 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:05.826066  2850 hierarchical.cpp:1096] Performed allocation for 1 slaves in 362856ns
I0206 01:25:06.827154  2857 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:06.827235  2857 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:06.827275  2857 hierarchical.cpp:1096] Performed allocation for 1 slaves in 328221ns
I0206 01:25:07.828547  2843 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:07.828753  2843 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:07.828907  2843 hierarchical.cpp:1096] Performed allocation for 1 slaves in 624979ns
I0206 01:25:08.829737  2855 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:08.829918  2855 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:08.830070  2855 hierarchical.cpp:1096] Performed allocation for 1 slaves in 596793ns
I0206 01:25:09.831233  2856 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:09.831316  2856 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:09.831352  2856 hierarchical.cpp:1096] Performed allocation for 1 slaves in 353864ns
I0206 01:25:10.832953  2849 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:10.833307  2849 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:10.833411  2849 hierarchical.cpp:1096] Performed allocation for 1 slaves in 731864ns
I0206 01:25:11.834967  2847 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:11.835149  2847 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:11.835294  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 586988ns
I0206 01:25:12.174247  2853 slave.cpp:2643] Got registration for executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 from executor(1)@172.17.0.8:43659
I0206 01:25:12.179061  2844 slave.cpp:1863] Sending queued task '0e7267ed-c5ed-4914-9042-5970b2aaec1c' to executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 at executor(1)@172.17.0.8:43659
I0206 01:25:12.194753  2858 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 from executor(1)@172.17.0.8:43659
I0206 01:25:12.195852  2858 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.196094  2858 status_update_manager.cpp:497] Creating StatusUpdate stream for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.197000  2858 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 to the slave
I0206 01:25:12.197739  2855 slave.cpp:3354] Forwarding the update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 to master@172.17.0.8:37843
I0206 01:25:12.198442  2855 master.cpp:4791] Status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 from slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063)
I0206 01:25:12.198673  2855 master.cpp:4839] Forwarding status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.199038  2855 master.cpp:6447] Updating the state of task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0206 01:25:12.199581  2854 sched.cpp:981] Scheduler::statusUpdate took 159022ns
I0206 01:25:12.200568  2854 master.cpp:3949] Processing ACKNOWLEDGE call 9d924a5b-76ab-4886-8091-7af3428ff179 for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843 on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0
I0206 01:25:12.201513  2858 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
../../src/tests/container_logger_tests.cpp:350: Failure
Value of: strings::contains(stdout.get(), ""Hello World!"")
  Actual: false
Expected: true
I0206 01:25:12.201702  2824 sched.cpp:1903] Asked to stop the driver
I0206 01:25:12.202831  2848 sched.cpp:1143] Stopping framework '914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000'
I0206 01:25:12.203284  2848 master.cpp:5923] Processing TEARDOWN call for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:12.203321  2848 master.cpp:5935] Removing framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:12.201762  2854 slave.cpp:3248] Status update manager successfully handled status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.203384  2854 slave.cpp:3264] Sending acknowledgement for status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 to executor(1)@172.17.0.8:43659
I0206 01:25:12.204712  2843 hierarchical.cpp:375] Deactivated framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.204953  2848 master.cpp:6447] Updating the state of task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I0206 01:25:12.205885  2854 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.206082  2854 slave.cpp:2079] Asked to shut down framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 by master@172.17.0.8:37843
I0206 01:25:12.206125  2854 slave.cpp:2104] Shutting down framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.206331  2854 slave.cpp:4129] Shutting down executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 at executor(1)@172.17.0.8:43659
I0206 01:25:12.206408  2843 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 from framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.207352  2848 master.cpp:6513] Removing task 0e7267ed-c5ed-4914-9042-5970b2aaec1c with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063)
I0206 01:25:12.208258  2848 master.cpp:1027] Master terminating
I0206 01:25:12.208703  2857 hierarchical.cpp:326] Removed framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.209658  2857 hierarchical.cpp:505] Removed slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0
I0206 01:25:12.212208  2848 slave.cpp:3482] master@172.17.0.8:37843 exited
W0206 01:25:12.212261  2848 slave.cpp:3485] Master disconnected! Waiting for a new master to be elected
I0206 01:25:12.224596  2854 containerizer.cpp:1318] Destroying container '5c952202-44cf-427a-8452-0f501140a4b7'
I0206 01:25:12.241466  2852 slave.cpp:3482] executor(1)@172.17.0.8:43659 exited
I0206 01:25:12.250931  2856 containerizer.cpp:1534] Executor for container '5c952202-44cf-427a-8452-0f501140a4b7' has exited
I0206 01:25:12.253350  2850 provisioner.cpp:306] Ignoring destroy request for unknown container 5c952202-44cf-427a-8452-0f501140a4b7
I0206 01:25:12.253885  2850 slave.cpp:3817] Executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 terminated with signal Killed
I0206 01:25:12.254125  2850 slave.cpp:3921] Cleaning up executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 at executor(1)@172.17.0.8:43659
I0206 01:25:12.254545  2847 gc.cpp:54] Scheduling '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7' for gc 6.99999705530074days in the future
I0206 01:25:12.254803  2850 slave.cpp:4009] Cleaning up framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.254822  2847 gc.cpp:54] Scheduling '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c' for gc 6.99999705202667days in the future
I0206 01:25:12.255084  2857 status_update_manager.cpp:282] Closing status update streams for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.255143  2856 gc.cpp:54] Scheduling '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000' for gc 6.99999704808days in the future
I0206 01:25:12.255190  2857 status_update_manager.cpp:528] Cleaning up status update stream for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.255192  2850 slave.cpp:668] Slave terminating
[  FAILED  ] ContainerLoggerTest.DefaultToSandbox (8566 ms)
{code}","CentOS 7, gcc, libevent & SSL enabled",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-08 17:30:30.432,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 17 15:33:19 UTC 2016,,,,,,,"0|hzzzn2:zzzzr",9223372036854775807,,,,,bernd-mesos,,,,,,Mesosphere Sprint 28,Mesosphere Sprint 29,,,,,,,,,,1.0,,,,,,,,,,,"08/Feb/16 17:30;kaysoky;Review: https://reviews.apache.org/r/42842/","17/Feb/16 15:33;bernd-mesos;commit 250439f5b81127f9f7d69ed2d8ab5853ab9d6225
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Wed Feb 17 16:15:30 2016 +0100

    Fixed flakiness in ContainerLoggerTest.DefaultToSandbox.
    
    The test needs to wait for the task to finish before examining
    the resulting files.
    
    Review: https://reviews.apache.org/r/42842/",,,,,,,,,,,,,,,,,,,,,,,,,,
SlaveRecoveryTest/0.CleanupHTTPExecutor is flaky,MESOS-4614,12937220,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,greggomann,greggomann,06/Feb/16 00:56,26/Apr/17 17:01,29/Oct/20 16:32,08/Feb/16 23:35,0.27.0,,,,,,,,0.28.0,,,,,,agent,HTTP API,test,,,0,flaky-test,mesosphere,,,,,,,"Just saw this failure on the ASF CI:

{code}
[ RUN      ] SlaveRecoveryTest/0.CleanupHTTPExecutor
I0206 00:22:44.791671  2824 leveldb.cpp:174] Opened db in 2.539372ms
I0206 00:22:44.792459  2824 leveldb.cpp:181] Compacted db in 740473ns
I0206 00:22:44.792510  2824 leveldb.cpp:196] Created db iterator in 24164ns
I0206 00:22:44.792532  2824 leveldb.cpp:202] Seeked to beginning of db in 1831ns
I0206 00:22:44.792548  2824 leveldb.cpp:271] Iterated through 0 keys in the db in 342ns
I0206 00:22:44.792605  2824 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0206 00:22:44.793256  2847 recover.cpp:447] Starting replica recovery
I0206 00:22:44.793480  2847 recover.cpp:473] Replica is in EMPTY status
I0206 00:22:44.794538  2847 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (9472)@172.17.0.2:43484
I0206 00:22:44.795040  2848 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0206 00:22:44.795644  2848 recover.cpp:564] Updating replica status to STARTING
I0206 00:22:44.796519  2850 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 752810ns
I0206 00:22:44.796545  2850 replica.cpp:320] Persisted replica status to STARTING
I0206 00:22:44.796725  2848 recover.cpp:473] Replica is in STARTING status
I0206 00:22:44.797828  2857 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (9473)@172.17.0.2:43484
I0206 00:22:44.798355  2850 recover.cpp:193] Received a recover response from a replica in STARTING status
I0206 00:22:44.799193  2850 recover.cpp:564] Updating replica status to VOTING
I0206 00:22:44.799583  2855 master.cpp:376] Master 0b206a40-a9c3-4d44-a5bd-8032d60a32ca (6632562f1ade) started on 172.17.0.2:43484
I0206 00:22:44.799609  2855 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/n2FxQV/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/n2FxQV/master"" --zk_session_timeout=""10secs""
I0206 00:22:44.799991  2855 master.cpp:423] Master only allowing authenticated frameworks to register
I0206 00:22:44.800009  2855 master.cpp:428] Master only allowing authenticated slaves to register
I0206 00:22:44.800020  2855 credentials.hpp:35] Loading credentials for authentication from '/tmp/n2FxQV/credentials'
I0206 00:22:44.800245  2850 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 679345ns
I0206 00:22:44.800370  2850 replica.cpp:320] Persisted replica status to VOTING
I0206 00:22:44.800397  2855 master.cpp:468] Using default 'crammd5' authenticator
I0206 00:22:44.800693  2855 master.cpp:537] Using default 'basic' HTTP authenticator
I0206 00:22:44.800815  2855 master.cpp:571] Authorization enabled
I0206 00:22:44.801216  2850 recover.cpp:578] Successfully joined the Paxos group
I0206 00:22:44.801604  2850 recover.cpp:462] Recover process terminated
I0206 00:22:44.801759  2856 whitelist_watcher.cpp:77] No whitelist given
I0206 00:22:44.801725  2847 hierarchical.cpp:144] Initialized hierarchical allocator process
I0206 00:22:44.803982  2855 master.cpp:1712] The newly elected leader is master@172.17.0.2:43484 with id 0b206a40-a9c3-4d44-a5bd-8032d60a32ca
I0206 00:22:44.804026  2855 master.cpp:1725] Elected as the leading master!
I0206 00:22:44.804059  2855 master.cpp:1470] Recovering from registrar
I0206 00:22:44.804424  2855 registrar.cpp:307] Recovering registrar
I0206 00:22:44.805202  2855 log.cpp:659] Attempting to start the writer
I0206 00:22:44.806782  2856 replica.cpp:493] Replica received implicit promise request from (9475)@172.17.0.2:43484 with proposal 1
I0206 00:22:44.807368  2856 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 547939ns
I0206 00:22:44.807395  2856 replica.cpp:342] Persisted promised to 1
I0206 00:22:44.808375  2856 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0206 00:22:44.809460  2848 replica.cpp:388] Replica received explicit promise request from (9476)@172.17.0.2:43484 for position 0 with proposal 2
I0206 00:22:44.809929  2848 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 427561ns
I0206 00:22:44.809967  2848 replica.cpp:712] Persisted action at 0
I0206 00:22:44.811035  2850 replica.cpp:537] Replica received write request for position 0 from (9477)@172.17.0.2:43484
I0206 00:22:44.811149  2850 leveldb.cpp:436] Reading position from leveldb took 36452ns
I0206 00:22:44.811532  2850 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 318924ns
I0206 00:22:44.811615  2850 replica.cpp:712] Persisted action at 0
I0206 00:22:44.812532  2850 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0206 00:22:44.813117  2850 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 476530ns
I0206 00:22:44.813143  2850 replica.cpp:712] Persisted action at 0
I0206 00:22:44.813166  2850 replica.cpp:697] Replica learned NOP action at position 0
I0206 00:22:44.813984  2848 log.cpp:675] Writer started with ending position 0
I0206 00:22:44.815549  2848 leveldb.cpp:436] Reading position from leveldb took 31800ns
I0206 00:22:44.817061  2848 registrar.cpp:340] Successfully fetched the registry (0B) in 12.591104ms
I0206 00:22:44.817319  2848 registrar.cpp:439] Applied 1 operations in 63480ns; attempting to update the 'registry'
I0206 00:22:44.818780  2845 log.cpp:683] Attempting to append 170 bytes to the log
I0206 00:22:44.818981  2845 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0206 00:22:44.819941  2845 replica.cpp:537] Replica received write request for position 1 from (9478)@172.17.0.2:43484
I0206 00:22:44.820582  2845 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 600949ns
I0206 00:22:44.820608  2845 replica.cpp:712] Persisted action at 1
I0206 00:22:44.821552  2845 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0206 00:22:44.821934  2845 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 352813ns
I0206 00:22:44.821960  2845 replica.cpp:712] Persisted action at 1
I0206 00:22:44.821979  2845 replica.cpp:697] Replica learned APPEND action at position 1
I0206 00:22:44.823447  2845 registrar.cpp:484] Successfully updated the 'registry' in 5.987072ms
I0206 00:22:44.823580  2845 registrar.cpp:370] Successfully recovered registrar
I0206 00:22:44.823833  2845 log.cpp:702] Attempting to truncate the log to 1
I0206 00:22:44.824203  2845 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0206 00:22:44.824291  2845 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0206 00:22:44.824645  2845 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0206 00:22:44.825222  2850 replica.cpp:537] Replica received write request for position 2 from (9479)@172.17.0.2:43484
I0206 00:22:44.825742  2850 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 481617ns
I0206 00:22:44.825772  2850 replica.cpp:712] Persisted action at 2
I0206 00:22:44.826748  2852 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0206 00:22:44.827368  2852 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 588591ns
I0206 00:22:44.827432  2852 leveldb.cpp:399] Deleting ~1 keys from leveldb took 33059ns
I0206 00:22:44.827450  2852 replica.cpp:712] Persisted action at 2
I0206 00:22:44.827468  2852 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0206 00:22:44.838011  2824 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0206 00:22:44.838873  2824 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I0206 00:22:44.843785  2857 slave.cpp:193] Slave started on 172.17.0.2:43484
I0206 00:22:44.843819  2857 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw""
I0206 00:22:44.844292  2857 credentials.hpp:83] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/credential'
I0206 00:22:44.844518  2857 slave.cpp:324] Slave using credential for: test-principal
I0206 00:22:44.844696  2857 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0206 00:22:44.845243  2857 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 00:22:44.845326  2857 slave.cpp:472] Slave attributes: [  ]
I0206 00:22:44.845342  2857 slave.cpp:477] Slave hostname: 6632562f1ade
I0206 00:22:44.845953  2824 sched.cpp:222] Version: 0.28.0
I0206 00:22:44.846853  2848 sched.cpp:326] New master detected at master@172.17.0.2:43484
I0206 00:22:44.846936  2848 sched.cpp:382] Authenticating with master master@172.17.0.2:43484
I0206 00:22:44.846958  2848 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0206 00:22:44.847692  2858 state.cpp:58] Recovering state from '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta'
I0206 00:22:44.848108  2850 status_update_manager.cpp:200] Recovering status update manager
I0206 00:22:44.848325  2852 containerizer.cpp:397] Recovering containerizer
I0206 00:22:44.848603  2845 authenticatee.cpp:121] Creating new client SASL connection
I0206 00:22:44.849719  2845 master.cpp:5523] Authenticating scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.850052  2852 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(662)@172.17.0.2:43484
I0206 00:22:44.850227  2854 provisioner.cpp:245] Provisioner recovery complete
I0206 00:22:44.850410  2852 authenticator.cpp:98] Creating new server SASL connection
I0206 00:22:44.850692  2852 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 00:22:44.850720  2852 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 00:22:44.850805  2852 authenticator.cpp:203] Received SASL authentication start
I0206 00:22:44.850862  2852 authenticator.cpp:325] Authentication requires more steps
I0206 00:22:44.850939  2852 authenticatee.cpp:258] Received SASL authentication step
I0206 00:22:44.851027  2852 authenticator.cpp:231] Received SASL authentication step
I0206 00:22:44.851052  2852 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 00:22:44.851063  2852 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 00:22:44.851102  2852 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 00:22:44.851121  2852 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 00:22:44.851130  2852 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.851136  2852 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.851150  2852 authenticator.cpp:317] Authentication success
I0206 00:22:44.851219  2850 authenticatee.cpp:298] Authentication success
I0206 00:22:44.851310  2850 master.cpp:5553] Successfully authenticated principal 'test-principal' at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.851485  2849 slave.cpp:4496] Finished recovery
I0206 00:22:44.852154  2843 sched.cpp:471] Successfully authenticated with master master@172.17.0.2:43484
I0206 00:22:44.852175  2843 sched.cpp:776] Sending SUBSCRIBE call to master@172.17.0.2:43484
I0206 00:22:44.852262  2843 sched.cpp:809] Will retry registration in 939.183679ms if necessary
I0206 00:22:44.852375  2844 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.852448  2844 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0206 00:22:44.852699  2852 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(662)@172.17.0.2:43484
I0206 00:22:44.852782  2844 master.cpp:2351] Subscribing framework default with checkpointing enabled and capabilities [  ]
I0206 00:22:44.853056  2849 slave.cpp:4668] Querying resource estimator for oversubscribable resources
I0206 00:22:44.853421  2856 hierarchical.cpp:265] Added framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.853513  2856 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:44.853582  2844 sched.cpp:703] Framework registered with 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.853613  2852 slave.cpp:4682] Received oversubscribable resources  from the resource estimator
I0206 00:22:44.853663  2844 sched.cpp:717] Scheduler::registered took 53762ns
I0206 00:22:44.853899  2843 slave.cpp:796] New master detected at master@172.17.0.2:43484
I0206 00:22:44.853955  2854 status_update_manager.cpp:174] Pausing sending status updates
I0206 00:22:44.853997  2856 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:44.853960  2843 slave.cpp:859] Authenticating with master master@172.17.0.2:43484
I0206 00:22:44.854035  2843 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0206 00:22:44.854030  2856 hierarchical.cpp:1096] Performed allocation for 0 slaves in 581355ns
I0206 00:22:44.854182  2843 slave.cpp:832] Detecting new master
I0206 00:22:44.854277  2854 authenticatee.cpp:121] Creating new client SASL connection
I0206 00:22:44.854517  2843 master.cpp:5523] Authenticating slave@172.17.0.2:43484
I0206 00:22:44.854603  2854 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(663)@172.17.0.2:43484
I0206 00:22:44.854836  2855 authenticator.cpp:98] Creating new server SASL connection
I0206 00:22:44.855013  2852 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 00:22:44.855044  2852 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 00:22:44.855139  2855 authenticator.cpp:203] Received SASL authentication start
I0206 00:22:44.855186  2855 authenticator.cpp:325] Authentication requires more steps
I0206 00:22:44.855263  2855 authenticatee.cpp:258] Received SASL authentication step
I0206 00:22:44.855352  2855 authenticator.cpp:231] Received SASL authentication step
I0206 00:22:44.855381  2855 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 00:22:44.855389  2855 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 00:22:44.855419  2855 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 00:22:44.855438  2855 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 00:22:44.855448  2855 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.855453  2855 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.855464  2855 authenticator.cpp:317] Authentication success
I0206 00:22:44.855540  2851 authenticatee.cpp:298] Authentication success
I0206 00:22:44.855721  2851 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(663)@172.17.0.2:43484
I0206 00:22:44.855832  2852 slave.cpp:927] Successfully authenticated with master master@172.17.0.2:43484
I0206 00:22:44.855615  2855 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave@172.17.0.2:43484
I0206 00:22:44.855973  2852 slave.cpp:1321] Will retry registration in 9.327708ms if necessary
I0206 00:22:44.856145  2854 master.cpp:4237] Registering slave at slave@172.17.0.2:43484 (6632562f1ade) with id 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0
I0206 00:22:44.856598  2851 registrar.cpp:439] Applied 1 operations in 59112ns; attempting to update the 'registry'
I0206 00:22:44.857403  2851 log.cpp:683] Attempting to append 339 bytes to the log
I0206 00:22:44.857525  2855 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0206 00:22:44.858482  2844 replica.cpp:537] Replica received write request for position 3 from (9493)@172.17.0.2:43484
I0206 00:22:44.858755  2844 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 228484ns
I0206 00:22:44.858855  2844 replica.cpp:712] Persisted action at 3
I0206 00:22:44.859751  2852 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0206 00:22:44.860332  2852 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 549638ns
I0206 00:22:44.860358  2852 replica.cpp:712] Persisted action at 3
I0206 00:22:44.860411  2852 replica.cpp:697] Replica learned APPEND action at position 3
I0206 00:22:44.862709  2856 registrar.cpp:484] Successfully updated the 'registry' in 6.020864ms
I0206 00:22:44.863106  2850 log.cpp:702] Attempting to truncate the log to 3
I0206 00:22:44.863358  2850 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0206 00:22:44.864321  2850 slave.cpp:3436] Received ping from slave-observer(288)@172.17.0.2:43484
I0206 00:22:44.864706  2849 hierarchical.cpp:473] Added slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0206 00:22:44.864716  2843 replica.cpp:537] Replica received write request for position 4 from (9494)@172.17.0.2:43484
I0206 00:22:44.865309  2843 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 410199ns
I0206 00:22:44.865337  2843 replica.cpp:712] Persisted action at 4
I0206 00:22:44.866092  2849 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:44.866132  2848 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0206 00:22:44.866137  2849 hierarchical.cpp:1116] Performed allocation for slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 in 1.30657ms
I0206 00:22:44.866497  2856 master.cpp:4305] Registered slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 00:22:44.866564  2843 slave.cpp:1321] Will retry registration in 32.803438ms if necessary
I0206 00:22:44.866690  2843 slave.cpp:971] Registered with master master@172.17.0.2:43484; given slave ID 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0
I0206 00:22:44.866716  2843 fetcher.cpp:81] Clearing fetcher cache
I0206 00:22:44.867066  2856 master.cpp:5352] Sending 1 offers to framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.867105  2843 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/slave.info'
I0206 00:22:44.867347  2856 master.cpp:4207] Slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) already registered, resending acknowledgement
I0206 00:22:44.867441  2856 status_update_manager.cpp:181] Resuming sending status updates
I0206 00:22:44.867465  2843 slave.cpp:1030] Forwarding total oversubscribed resources 
W0206 00:22:44.867547  2843 slave.cpp:1016] Already registered with master master@172.17.0.2:43484
I0206 00:22:44.867574  2843 slave.cpp:1030] Forwarding total oversubscribed resources 
I0206 00:22:44.867710  2843 master.cpp:4646] Received update of slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) with total oversubscribed resources 
I0206 00:22:44.867951  2856 sched.cpp:873] Scheduler::resourceOffers took 133371ns
I0206 00:22:44.867961  2843 master.cpp:4646] Received update of slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) with total oversubscribed resources 
I0206 00:22:44.868484  2856 hierarchical.cpp:531] Slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0206 00:22:44.868599  2848 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.418545ms
I0206 00:22:44.868700  2848 leveldb.cpp:399] Deleting ~2 keys from leveldb took 54053ns
I0206 00:22:44.868751  2848 replica.cpp:712] Persisted action at 4
I0206 00:22:44.868811  2848 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0206 00:22:44.869241  2856 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:44.869287  2856 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:44.869321  2856 hierarchical.cpp:1116] Performed allocation for slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 in 782848ns
I0206 00:22:44.869840  2856 hierarchical.cpp:531] Slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0206 00:22:44.869985  2856 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:44.870028  2856 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:44.870053  2856 hierarchical.cpp:1116] Performed allocation for slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 in 160104ns
I0206 00:22:44.871824  2853 master.cpp:3138] Processing ACCEPT call for offers: [ 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-O0 ] on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.871868  2853 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 1 as user 'mesos'
W0206 00:22:44.873613  2843 validation.cpp:404] Executor http for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0206 00:22:44.873667  2843 validation.cpp:416] Executor http for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0206 00:22:44.874035  2843 master.hpp:176] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade)
I0206 00:22:44.874223  2843 master.cpp:3623] Launching task 1 of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade)
I0206 00:22:44.874802  2843 slave.cpp:1361] Got assigned task 1 for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.874966  2843 slave.cpp:5202] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/framework.info'
I0206 00:22:44.875440  2843 slave.cpp:5213] Checkpointing framework pid 'scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484' to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/framework.pid'
I0206 00:22:44.876106  2843 slave.cpp:1480] Launching task 1 for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.876644  2843 paths.cpp:474] Trying to chown '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17' to user 'mesos'
I0206 00:22:44.884089  2843 slave.cpp:5654] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/executor.info'
I0206 00:22:44.900928  2843 slave.cpp:5282] Launching executor http of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 with resources  in work directory '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17'
I0206 00:22:44.901449  2853 containerizer.cpp:656] Starting container 'fd4649a4-1c82-4eda-b663-b568b6110d17' for executor 'http' of framework '0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000'
I0206 00:22:44.901561  2843 slave.cpp:5677] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17/tasks/1/task.info'
I0206 00:22:44.902060  2843 slave.cpp:1698] Queuing task '1' for executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.902207  2843 slave.cpp:749] Successfully attached file '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17'
I0206 00:22:44.907027  2850 launcher.cpp:132] Forked child with pid '8875' for container 'fd4649a4-1c82-4eda-b663-b568b6110d17'
I0206 00:22:44.907229  2850 containerizer.cpp:1094] Checkpointing executor's forked pid 8875 to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17/pids/forked.pid'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0206 00:22:45.080060  8875 process.cpp:991] libprocess is initialized on 172.17.0.2:49724 for 16 cpus
I0206 00:22:45.082499  8875 logging.cpp:193] Logging to STDERR
I0206 00:22:45.082862  8875 executor.cpp:172] Version: 0.28.0
I0206 00:22:45.087201  8903 executor.cpp:316] Connected with the agent
I0206 00:22:45.802878  2858 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:45.802969  2858 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:45.803014  2858 hierarchical.cpp:1096] Performed allocation for 1 slaves in 424120ns
2016-02-06 00:22:45,982:2824(0x7fd8c5ffb700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
W0206 00:22:46.588022  2854 group.cpp:503] Timed out waiting to connect to ZooKeeper. Forcing ZooKeeper session (sessionId=0) expiration
I0206 00:22:46.588969  2854 group.cpp:519] ZooKeeper session expired
2016-02-06 00:22:46,589:2824(0x7fd9fefd1700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0

2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@716: Client environment:host.name=6632562f1ade
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@724: Client environment:os.arch=3.13.0-36-lowlatency
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@725: Client environment:os.version=#63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@741: Client environment:user.home=/home/mesos
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@753: Client environment:user.dir=/tmp/n2FxQV
2016-02-06 00:22:46,590:2824(0x7fda03fdb700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:40712 sessionTimeout=10000 watcher=0x7fda10e9e520 sessionId=0 sessionPasswd=<null> context=0x7fd9d401bc10 flags=0
2016-02-06 00:22:46,590:2824(0x7fd8c67fc700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0206 00:22:46.804400  2844 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:46.804481  2844 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:46.804514  2844 hierarchical.cpp:1096] Performed allocation for 1 slaves in 347954ns
I0206 00:22:47.805842  2847 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:47.805934  2847 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:47.805980  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 415449ns
I0206 00:22:48.807723  2851 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:48.807814  2851 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:48.807857  2851 hierarchical.cpp:1096] Performed allocation for 1 slaves in 442104ns
I0206 00:22:49.808733  2848 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:49.808816  2848 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:49.808856  2848 hierarchical.cpp:1096] Performed allocation for 1 slaves in 384959ns
2016-02-06 00:22:49,926:2824(0x7fd8c67fc700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0206 00:22:50.810307  2847 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:50.810400  2847 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:50.810443  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 389572ns
I0206 00:22:51.811586  2849 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:51.811681  2849 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:51.811722  2849 hierarchical.cpp:1096] Performed allocation for 1 slaves in 404450ns
I0206 00:22:52.812860  2851 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:52.812944  2851 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:52.812981  2851 hierarchical.cpp:1096] Performed allocation for 1 slaves in 359671ns
2016-02-06 00:22:53,263:2824(0x7fd8c67fc700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0206 00:22:53.814512  2847 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:53.814599  2847 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:53.814651  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 386669ns
I0206 00:22:54.815238  2852 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:54.815321  2852 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:54.815356  2852 hierarchical.cpp:1096] Performed allocation for 1 slaves in 376235ns
I0206 00:22:55.816453  2846 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:55.816550  2846 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:55.816596  2846 hierarchical.cpp:1096] Performed allocation for 1 slaves in 416350ns
W0206 00:22:56.592408  2849 group.cpp:503] Timed out waiting to connect to ZooKeeper. Forcing ZooKeeper session (sessionId=0) expiration
I0206 00:22:56.593480  2849 group.cpp:519] ZooKeeper session expired
2016-02-06 00:22:56,593:2824(0x7fda017d6700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0

2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@716: Client environment:host.name=6632562f1ade
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@724: Client environment:os.arch=3.13.0-36-lowlatency
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@725: Client environment:os.version=#63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@741: Client environment:user.home=/home/mesos
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@log_env@753: Client environment:user.dir=/tmp/n2FxQV
2016-02-06 00:22:56,594:2824(0x7fda007d4700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:40712 sessionTimeout=10000 watcher=0x7fda10e9e520 sessionId=0 sessionPasswd=<null> context=0x7fd9e401f350 flags=0
2016-02-06 00:22:56,595:2824(0x7fd8c5ffb700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0206 00:22:56.817683  2848 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:56.817766  2848 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:56.817803  2848 hierarchical.cpp:1096] Performed allocation for 1 slaves in 374115ns
I0206 00:22:57.818447  2844 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:57.818526  2844 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:57.818562  2844 hierarchical.cpp:1096] Performed allocation for 1 slaves in 344545ns
I0206 00:22:58.819828  2851 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:58.819914  2851 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:58.819957  2851 hierarchical.cpp:1096] Performed allocation for 1 slaves in 376948ns
I0206 00:22:59.820874  2848 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:59.820957  2848 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:59.820991  2848 hierarchical.cpp:1096] Performed allocation for 1 slaves in 344192ns
I0206 00:22:59.854698  2845 slave.cpp:4668] Querying resource estimator for oversubscribable resources
I0206 00:22:59.854991  2845 slave.cpp:4682] Received oversubscribable resources  from the resource estimator
I0206 00:22:59.864612  2857 slave.cpp:3436] Received ping from slave-observer(288)@172.17.0.2:43484
../../src/tests/slave_recovery_tests.cpp:1105: Failure
Failed to wait 15secs for updateCall1
I0206 00:22:59.876358  2852 master.cpp:1213] Framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 disconnected
I0206 00:22:59.876410  2852 master.cpp:2576] Disconnecting framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:59.876456  2852 master.cpp:2600] Deactivating framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:59.876569  2852 master.cpp:1237] Giving framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 0ns to failover
I0206 00:22:59.876981  2844 hierarchical.cpp:375] Deactivated framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.877049  2844 master.cpp:5204] Framework failover timeout, removing framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:59.877075  2844 master.cpp:5935] Removing framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:59.877276  2844 master.cpp:6447] Updating the state of task 1 of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I0206 00:22:59.878051  2844 master.cpp:6513] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade)
I0206 00:22:59.878433  2844 master.cpp:6542] Removing executor 'http' with resources  of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade)
I0206 00:22:59.878667  2852 slave.cpp:2079] Asked to shut down framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 by master@172.17.0.2:43484
I0206 00:22:59.878733  2852 slave.cpp:2104] Shutting down framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.878806  2852 slave.cpp:4129] Shutting down executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
W0206 00:22:59.878834  2852 slave.hpp:655] Unable to send event to executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000: unknown connection type
I0206 00:22:59.879550  2844 master.cpp:1027] Master terminating
I0206 00:22:59.879703  2854 hierarchical.cpp:892] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 from framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.879947  2854 hierarchical.cpp:326] Removed framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.880306  2854 hierarchical.cpp:505] Removed slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0
I0206 00:22:59.880666  2852 slave.cpp:3482] master@172.17.0.2:43484 exited
W0206 00:22:59.880695  2852 slave.cpp:3485] Master disconnected! Waiting for a new master to be elected
I0206 00:22:59.885498  2857 containerizer.cpp:1318] Destroying container 'fd4649a4-1c82-4eda-b663-b568b6110d17'
I0206 00:22:59.904532  2858 containerizer.cpp:1534] Executor for container 'fd4649a4-1c82-4eda-b663-b568b6110d17' has exited
I0206 00:22:59.907024  2858 provisioner.cpp:306] Ignoring destroy request for unknown container fd4649a4-1c82-4eda-b663-b568b6110d17
I0206 00:22:59.907428  2858 slave.cpp:3817] Executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 terminated with signal Killed
I0206 00:22:59.907538  2858 slave.cpp:3921] Cleaning up executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.908213  2858 slave.cpp:4009] Cleaning up framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.908555  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17' for gc 6.99998949252444days in the future
I0206 00:22:59.908720  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http' for gc 6.99998949082074days in the future
I0206 00:22:59.908807  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17' for gc 6.99998948980444days in the future
I0206 00:22:59.908927  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http' for gc 6.99998948890074days in the future
I0206 00:22:59.909009  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000' for gc 6.99998948710518days in the future
I0206 00:22:59.909121  2858 gc.cpp:54] Scheduling '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000' for gc 6.99998948630815days in the future
I0206 00:22:59.909211  2858 status_update_manager.cpp:282] Closing status update streams for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:59.910423  2853 slave.cpp:668] Slave terminating
../../3rdparty/libprocess/include/process/gmock.hpp:425: Failure
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const HttpEvent&>()))...
    Expected args: union http matcher (72-byte object <D0-11 44-12 DA-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00>, UPDATE, 1-byte object <1B>, 16-byte object <1B-F4 34-01 00-00 00-00 00-00 00-00 DA-7F 00-00>)
         Expected: to be called once
           Actual: never called - unsatisfied and active
../../3rdparty/libprocess/include/process/gmock.hpp:425: Failure
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const HttpEvent&>()))...
    Expected args: union http matcher (72-byte object <D0-11 44-12 DA-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00>, UPDATE, 1-byte object <1B>, 16-byte object <1B-F4 34-01 00-00 00-00 00-00 00-00 DA-7F 00-00>)
         Expected: to be called once
           Actual: never called - unsatisfied and active
[  FAILED  ] SlaveRecoveryTest/0.CleanupHTTPExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer (15126 ms)
{code}","CentOS 7, gcc, libevent & SSL enabled",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-06 01:01:04.109,,,false,MESOS-4793,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 08 23:35:10 UTC 2016,,,,,,,"0|i2sia7:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 28,,,,,,,,,,,3.0,,,,,,,,,,,"06/Feb/16 01:01;anandmazumdar;The executor did not even send the {{Subscribe}} call after it connected with the agent. 

This is similar to the behavior that we have been observing with another flaky test in {{MESOS-3273}} in which the example test framework does not send the initial {{SUBSCRIBE}} call.","06/Feb/16 17:52;anandmazumdar;Patch: https://reviews.apache.org/r/43285/","08/Feb/16 23:35;vinodkone;commit 1af86559c3cf47159aa00c289208bbec50fd6df9
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Mon Feb 8 15:32:22 2016 -0800

    Fixed flakiness in SlaveRecoveryTest/0.CleanupHTTPExecutor.
    
    This change fixes the flakiness in this test. The issue was a race between the
    `connected` callback being called before we did `process::spawn` to start the
    process.
    
    The details of the race that lead to the failure are as follows:
    - We started the executor library inside the constructor of `TestExecutor`. The
      callback function did `process::defer(self(), &Self::connected)`
    - The `connected` callback can be invoked by the Executor library before we got
      a chance to actually invoke `process::spawn` on the `TestExecutor` process
    itself. This in can turn lead to the `dispatch` being silently dropped.
    
    This change now starts the library inside the `initialize` function that is
    gurranteed to be called after `process::spawn` is invoked.
    
    Review: https://reviews.apache.org/r/43285/
",,,,,,,,,,,,,,,,,,,,,,,,,
Subprocess should be more intelligent about setting/inheriting libprocess environment variables ,MESOS-4609,12937109,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,kaysoky,kaysoky,05/Feb/16 18:21,26/Nov/18 13:36,29/Oct/20 16:32,,0.27.0,,,,,,,,,,,,,,libprocess,,,,,0,mesosphere,,,,,,,,"Mostly copied from [this comment|https://issues.apache.org/jira/browse/MESOS-4598?focusedCommentId=15133497&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15133497]

A subprocess inheriting the environment variables {{LIBPROCESS_*}} may run into some accidental fatalities:

| || Subprocess uses libprocess || Subprocess is something else ||
|| Subprocess sets/inherits the same {{PORT}} by accident | Bind failure -> exit | Nothing happens (?) |
|| Subprocess sets a different {{PORT}} on purpose | Bind success (?) | Nothing happens (?) |

(?) = means this is usually the case, but not 100%.

A complete fix would look something like:
* If the {{subprocess}} call gets {{environment = None()}}, we should automatically remove {{LIBPROCESS_PORT}} from the inherited environment.  
* The parts of [{{executorEnvironment}}|https://github.com/apache/mesos/blame/master/src/slave/containerizer/containerizer.cpp#L265] dealing with libprocess & libmesos should be refactored into libprocess as a helper.  We would use this helper for the Containerizer, Fetcher, and ContainerLogger module.
* If the {{subprocess}} call is given {{LIBPROCESS_PORT == os::getenv(""LIBPROCESS_PORT"")}}, we can LOG(WARN) and unset the env var locally.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4598,,,,,,MESOS-4601,MESOS-4353,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-08-25 02:34:36.935,,,false,MESOS-4086,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 23 22:01:28 UTC 2016,,,,,,,"0|hzzzn2:zzzzi",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 28,,,,,,,,,,,2.0,,0.28.0,,,,,,,,,"05/Feb/16 21:06;kaysoky;|| Reviews || Summary || 
| https://reviews.apache.org/r/43260/
https://reviews.apache.org/r/43261/ | Some refactoring of {{process::initialize}} |
| https://reviews.apache.org/r/43271/ | Modifications to {{subprocess}} |
| https://reviews.apache.org/r/43272/ | Refactor of containerizer, fetcher, container logger |","25/Aug/16 02:34;tillt;[~kaysoky] shall we remove you and bernd from this issue to signal that the ball got dropped for now?","23/Nov/16 22:01;kaysoky;Committed the refactoring parts of this ticket: {code}
commit d022300acc907bfe507a7cb8f1c5767a2bd6d7bb
Author: Armand Grillet <armand.grillet@gmail.com>
Date:   Wed Nov 23 13:21:31 2016 -0800

    Added net::IP parsing template to the flags parsers.
    
    This will allow us to specify the `net::IP` type
    as a field inside flags.
    
    Review: https://reviews.apache.org/r/53558/
{code}
{code}
commit 1d33559fad35a4ca84f275b1c1544aa5afa48e28
Author: Armand Grillet <armand.grillet@gmail.com>
Date:   Wed Nov 23 13:21:32 2016 -0800

    Transformed env variable parsing into Flags in libprocess.
    
    This retains existing environment variables read by libprocess
    (LIBPROCESS + IP, ADVERTISE_IP, PORT, ADVERTISE_PORT) but parsing
    is done via a Flags object.  This also documents the behavior and
    expectations of the flags, and prints a more helpful error message
    if the environment variables are set incorrectly.
    
    Review: https://reviews.apache.org/r/53559/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,
ROOT_DOCKER_DockerHealthyTask is flaky.,MESOS-4604,12936993,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Not A Problem,kaysoky,nfnt,nfnt,05/Feb/16 09:24,26/Nov/18 12:48,29/Oct/20 16:32,26/Nov/18 12:48,,,,,,,,,,,,,,,test,,,,,0,flaky-test,health-check,mesosphere,test,,,,,"Log from Teamcity that is running {{sudo ./bin/mesos-tests.sh}} on AWS EC2 instances:
{noformat}
[18:27:14][Step 8/8] [----------] 8 tests from HealthCheckTest
[18:27:14][Step 8/8] [ RUN      ] HealthCheckTest.HealthyTask
[18:27:17][Step 8/8] [       OK ] HealthCheckTest.HealthyTask (2222 ms)
[18:27:17][Step 8/8] [ RUN      ] HealthCheckTest.ROOT_DOCKER_DockerHealthyTask
[18:27:36][Step 8/8] ../../src/tests/health_check_tests.cpp:388: Failure
[18:27:36][Step 8/8] Failed to wait 15secs for termination
[18:27:36][Step 8/8] F0204 18:27:35.981302 23085 logging.cpp:64] RAW: Pure virtual method called
[18:27:36][Step 8/8]     @     0x7f7077055e1c  google::LogMessage::Fail()
[18:27:36][Step 8/8]     @     0x7f707705ba6f  google::RawLog__()
[18:27:36][Step 8/8]     @     0x7f70760f76c9  __cxa_pure_virtual
[18:27:36][Step 8/8]     @           0xa9423c  mesos::internal::tests::Cluster::Slaves::shutdown()
[18:27:36][Step 8/8]     @          0x1074e45  mesos::internal::tests::MesosTest::ShutdownSlaves()
[18:27:36][Step 8/8]     @          0x1074de4  mesos::internal::tests::MesosTest::Shutdown()
[18:27:36][Step 8/8]     @          0x1070ec7  mesos::internal::tests::MesosTest::TearDown()
[18:27:36][Step 8/8]     @          0x16eb7b2  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16e61a9  testing::internal::HandleExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16c56aa  testing::Test::Run()
[18:27:36][Step 8/8]     @          0x16c5e89  testing::TestInfo::Run()
[18:27:36][Step 8/8]     @          0x16c650a  testing::TestCase::Run()
[18:27:36][Step 8/8]     @          0x16cd1f6  testing::internal::UnitTestImpl::RunAllTests()
[18:27:36][Step 8/8]     @          0x16ec513  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16e6df1  testing::internal::HandleExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16cbe26  testing::UnitTest::Run()
[18:27:36][Step 8/8]     @           0xe54c84  RUN_ALL_TESTS()
[18:27:36][Step 8/8]     @           0xe54867  main
[18:27:36][Step 8/8]     @     0x7f7071560a40  (unknown)
[18:27:36][Step 8/8]     @           0x9b52d9  _start
[18:27:36][Step 8/8] Aborted (core dumped)
[18:27:36][Step 8/8] Process exited with code 134
{noformat}
Happens with Ubuntu 15.04, CentOS 6, CentOS 7 _quite_ often. ","CentOS 6/7, Ubuntu 15.04 on AWS.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-08 23:13:11.462,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 10 01:06:24 UTC 2017,,,,,,,"0|i2sgvz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,"08/Feb/16 23:13;kaysoky;I'm convinced this is a [problem in docker|https://github.com/docker/docker/issues/12738].  The tests fail when [{{docker stop}}|https://github.com/apache/mesos/blob/7aafb8e44d347a03cbef83d3f7ee4705b9d23c09/src/slave/containerizer/docker.cpp#L1547] hangs indefinitely.  
Note: docker [doesn't support this Ubuntu15.04 anymore|https://github.com/docker/docker/pull/18809].

To get rid of the {{__cxa_pure_virtual}}, we'll need to do some refactoring (see [MESOS-2017|https://issues.apache.org/jira/browse/MESOS-2017]).  At the moment, most of our containerizer tests must call {{Shutdown()}} before they exit the scope of the test.  Otherwise, {{MesosTest}} will call {{Shutdown()}} and dereference some stack-allocated containerizers.

I propose: 
* Change {{MesosTest::StartSlave}} to take a {{Shared<Containerizer>}}.  Change all tests to dynamically allocate containerizers.
* We remove all manual {{Shutdown()}} calls if they occur at the end of the test.","09/Feb/16 06:38;haosdent@gmail.com;Nice work!","07/Jan/17 07:03;avinash.mesos;After refactoring of the tests is this still an issue?","10/Jan/17 01:06;kaysoky;Closing this as Docker is not supported on Ubuntu 15.04",,,,,,,,,,,,,,,,,,,,,,,,
Logrotate ContainerLogger should not remove IP from environment.,MESOS-4598,12936838,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,kaysoky,kaysoky,kaysoky,04/Feb/16 19:41,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,0.27.0,,,,,,,,0.27.1,0.28.0,,,,,,,,,,0,mesosphere,,,,,,,,"The {{LogrotateContainerLogger}} starts libprocess-using subprocesses.  Libprocess initialization will attempt to resolve the IP from the hostname.  If a DNS service is not available, this step will fail, which terminates the logger subprocess prematurely.

Since the logger subprocesses live on the agent, they should use the same {{LIBPROCESS_IP}} supplied to the agent.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-05 01:27:57.223,,,false,MESOS-4086,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 05 03:15:44 UTC 2016,,,,,,,"0|i2sfxj:",9223372036854775807,,,,,tillt,,,,,,Mesosphere Sprint 28,,,,,,,,,,,1.0,,0.27.1,,,,,,,,,"04/Feb/16 21:30;kaysoky;Fix: https://reviews.apache.org/r/43207/","05/Feb/16 01:27;tillt;Seems we have two problems which are related and solved locally;

1. any executor forked by an agent should share the same {{LIBPROCESS_IP}} to prevent reverse dns failures.
see https://github.com/apache/mesos/blame/master/src/slave/containerizer/containerizer.cpp#L265

2. the loggers (pipe processing subprocesses) forked by a logger module should not have the same {{LIBPROCESS_PORT}} to prevent bind failures.
see your patch

So the problem here is that initially, the logger fixed the problem of bind-errors with its subprocesses by exec'ing them with completely unset {{LIBPROCESS_}} vars. The above fix then keeps the IP as per issue #1. 
Currently, I believe that #1 and #2 are actually true for ANY libprocess parent and child process.

While the above RR might be a fine workaround, I am not sure that we should regard it as a proper fix.

The problem here is a rather central one I believe, not specific to the logger at all. 

Any libprocess os-process forked by a libprocess os-process does run into these issues. We should consider a central solution.

There seem options like;

- removing {{LIBPROCESS_PORT}} from the environment of a libprocess process once it gathered that value;
insert at https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/process.cpp#L872

- inheriting {{LIBPROCESS_IP}} by default when forking via libprocess's subprocess

Are my assumptions wrong or any opinions and suggestions?","05/Feb/16 01:59;kaysoky;I agree that we need a centralized fix.

I see the following scenarios:
| || Subprocess uses libprocess || Subprocess is something else ||
|| Subprocess sets/inherits the same {{PORT}} by accident | Bind failure -> exit
Option #1 above prevents accidental inheritance | Nothing happens (?) |
|| Subprocess sets a different {{PORT}} on purpose | Bind success (?) | Nothing happens (?) |

My thought for a complete fix is the following changes:
* If the {{subprocess}} call gets {{environment = None()}}, we should automatically remove {{LIBPROCESS_PORT}} from the inherited environment.  
** I'd prefer not to unset {{LIBPROCESS_PORT}} on initialization because this makes it harder to catch the upper-left error above.  Also, the V1 HTTP scheduler library tests will eventually need to re-initialize libprocess between tests.
* The parts of [{{executorEnvironment}}|https://github.com/apache/mesos/blame/master/src/slave/containerizer/containerizer.cpp#L265] dealing with libprocess & libmesos should be refactored into libprocess as a helper.  We would use this helper for the Containerizer, Fetcher, and ContainerLogger module.
* If the {{subprocess}} call is given {{LIBPROCESS_PORT == os::getenv(""LIBPROCESS_PORT"")}}, we can probably return an {{Error}} immediately.  Or log a warning and unset the env var locally.","05/Feb/16 02:15;tillt;Yes, I like the idea of using {{subprocess}} default environment for triggering the PORT-removal - much less intrusive and irritating than my option.

Yes, {{executorEnvironment}} should'nt really have to deal in all details with this.

For warning / erroring - fixing the problem for the user with a warning seems rather nice.

Great suggestions Joseph!

","05/Feb/16 03:15;tillt;{noformat}
commit aa76317e4da9282e04b7f1eddf0f8768f1c15c3a
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Fri Feb 5 03:54:24 2016 +0100

    Don't remove IP from the logger's environment.

    Unsetting the `LIBPROCESS_IP` was originally unnecessary, as this
    variable does not result in port conflicts.  Unsetting this environment
    variable may cause the subprocess to exit if DNS cannot resolve the
    agent's hostname.

    Review: https://reviews.apache.org/r/43207/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,
Rename `examples/event_call_framework.cpp` to `examples/test_http_framework.cpp`,MESOS-4583,12936157,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,yongtang,anandmazumdar,anandmazumdar,02/Feb/16 22:41,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.28.0,,,,,,,,,,,0,mesosphere,newbie,,,,,,,We already have {{examples/test_framework.cpp}} for testing {{PID}} based frameworks. We would ideally want to rename {{event_call_framework}} to correctly reflect that it's an example for HTTP based framework.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-02 15:12:17.061,,,false,MESOS-3302,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 03 22:48:42 UTC 2016,,,,,,,"0|hzzzql:j",9223372036854775807,,,,,vinodkone,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,"02/Mar/16 15:12;yongtang;Added a Review Request:
https://reviews.apache.org/r/44266/","03/Mar/16 22:48;vinodkone;commit 7276019d601045e1e78fe74777f6990b88392d35
Author: Yong Tang <yong.tang.github@outlook.com>
Date:   Thu Mar 3 14:47:11 2016 -0800

    Renamed event_call_framework.cpp to test_http_framework.cpp.
    
    This change rename event_call_framework to test_http_framework
    in order to correctly reflect that it's an example for HTTP based
    framework. (MESOS-4583)
    
    Review: https://reviews.apache.org/r/44266/
",,,,,,,,,,,,,,,,,,,,,,,,,,
Design doc for scheduler HTTP Stream IDs,MESOS-4573,12935663,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,greggomann,greggomann,greggomann,01/Feb/16 16:28,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.28.0,,,,,,HTTP API,,,,,0,http,mesosphere,,,,,,,"This ticket is for the design of HTTP stream IDs, for use with HTTP schedulers. These IDs allow Mesos to distinguish between different instances of HTTP framework schedulers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-3583,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-3302,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 24 21:50:17 UTC 2016,,,,,,,"0|hzzzn2:zzzx",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 28,Mesosphere Sprint 29,,,,,,,,,,5.0,,,,,,,,,,,"03/Feb/16 18:42;greggomann;I'm compiling research, including the HTTP session management approaches of other projects, in the above linked Google doc.","24/Feb/16 21:50;greggomann;The design document can be found here: https://docs.google.com/document/d/141wvs8upivIRw7I-tW5pW9ABP2gXKMmCB8hsV36ELc0/edit?usp=sharing",,,,,,,,,,,,,,,,,,,,,,,,,,
NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate fails on CentOS 6,MESOS-4540,12934658,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,avinash.mesos,greggomann,greggomann,28/Jan/16 01:40,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.27.0,,,,,,,,,,,0,cgroups,mesosphere,test-failure,tests,,,,,"This test fails in my CentOS 6 VM due to a cgroups issue:

{code}
[ RUN      ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate
I0127 19:15:06.637328 25347 exec.cpp:134] Version: 0.28.0
I0127 19:15:06.648378 25378 exec.cpp:208] Executor registered on slave 6edafba0-9dbd-4e6e-b10e-c6f935e58d41-S0
Registered executor on localhost
Starting task b745d88e-3fbe-4af9-80b3-e43484e37acf
sh -c 'sleep 1000'
Forked command at 25385
../../src/tests/containerizer/isolator_tests.cpp:926: Failure
pids: Failed to read cgroups control 'cgroup.procs': '/sys/fs/cgroup/net_cls' is not a valid hierarchy
I0127 19:15:06.662083 25376 exec.cpp:381] Executor asked to shutdown
Shutting down
Sending SIGTERM to process tree at pid 25385
[  FAILED  ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate (335 ms)
{code}","CentOS 6, libevent and SSL enabled, gcc",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-28 01:43:49.647,,,false,MESOS-4343,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 29 04:45:22 UTC 2016,,,,,,,"0|i2s2if:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 27,,,,,,,,,,,1.0,,0.27.0,,,,,,,,,"28/Jan/16 01:43;avinash.mesos;https://reviews.apache.org/r/42878/","28/Jan/16 01:45;avinash.mesos;In the test-case we were using flags.cgroups_hierarchy to build the mountpoint of the net_cls subsystem. The correct way to do this would be to use cgroups::hierarchy.  This leads to a failure in CentoOS 6 since the cgroup are mounted on /cgroup instead of /sys/fs/cgroup (which is what flags.cgroups_hierarchy points too). ","29/Jan/16 04:45;mcypark;{noformat}
commit 1a35d59e9933e43328a92ba72160f9e0f28e5ee0
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Wed Jan 27 17:31:14 2016 -0800

    Fixed the NetClsIsolatorTest to correctly learn the net_cls hierarchy.

    The test relied on the flags.cgroups_hierarchy to learn the net_cls
    hierarchy, instead it should be making a call to cgroups::hierarchy to
    learn the hierarchy.

    Review: https://reviews.apache.org/r/42878/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,
Logrotate ContainerLogger may not handle FD ownership correctly,MESOS-4535,12934569,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,kaysoky,kaysoky,kaysoky,27/Jan/16 20:12,29/Apr/19 09:26,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,0.27.0,,,,,,modules,,,,,0,logging,mesosphere,,,,,,,"One of the patches for [MESOS-4136] introduced the {{FDType::OWNED}} enum for {{Subprocess::IO::FD}}.

The way the logrotate module uses this is slightly incorrect:
# The module starts a subprocess with an output {{Subprocess::PIPE()}}.
# That pipe's FD is passed into another subprocess via {{Subprocess::IO::FD(pipe, IO::OWNED)}}.
# When the second subprocess starts, the pipe's FD is closed in the parent.
# When the first subprocess terminates, the existing code will try to close the pipe again.  This effectively closes a random FD.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-28 00:26:36.153,,,false,MESOS-4086,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 28 00:26:36 UTC 2016,,,,,,,"0|i2s1yn:",9223372036854775807,,,,,benjaminhindman,,,,,,Mesosphere Sprint 27,,,,,,,,,,,1.0,,0.27.0,,,,,,,,,"27/Jan/16 20:45;kaysoky;|| Review || Summary ||
| https://reviews.apache.org/r/42865/ | Fix for logrotate logger |
| https://reviews.apache.org/r/42880/ | Regression test |","28/Jan/16 00:26;benjaminhindman;{code}
commit f3536fad9dfb013a2b4718ad5bddb60148b14334
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Wed Jan 27 16:25:40 2016 -0800

    Fixed LogrotateContainerLogger's FD ownership.
    
    Changes the logrotate container logger to manually construct and deal
    with pipes.  Specifically, both read and write ends of the pipe must
    end up in the child processes (read -> logger executables, write ->
    container).
    
    If ownership is not transferred, the pipe's FDs may be closed (again)
    when `Subprocess` is destructed, which may unexpectedly close random
    FDs belonging to other threads.
    
    Review: https://reviews.apache.org/r/42865/

commit 256a053a6a50f9e5ce44e134d96a72bb98a1b885
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Wed Jan 27 16:25:28 2016 -0800

    Add test for LogrotateContainerLogger's FD management.
    
    Adds a test which checks for erroneous calls to `os::close` by the
    LogrotateContainerLogger.  This may happen by accident if the
    container logger module uses `Subprocess::PIPE` when launching child
    processes; as libprocess will track these FDs and close them (possibly
    even if they've already been closed) when the child processes exit.
    
    Review: https://reviews.apache.org/r/42880/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate is flaky,MESOS-4530,12934474,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,avinash.mesos,arojas,arojas,27/Jan/16 14:40,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,0.27.0,,,,,,,,0.27.0,,,,,,containerization,,,,,0,mesosphere,,,,,,,,"While running the command
{noformat}
sudo ./bin/mesos-tests.sh --gtest_filter=""-CgroupsAnyHierarchyWithCpuMemoryTest.ROOT_CGROUPS_Listen:CgroupsAnyHierarchyMemoryPressureTest.ROOT_IncreaseRSS"" --gtest_repeat=10 --gtest_break_on_failure
{noformat}
One eventually gets the following output:
{noformat}
[ RUN      ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate
../../src/tests/containerizer/isolator_tests.cpp:870: Failure
containerizer: Could not create isolator 'cgroups/net_cls': Unexpected subsystems found attached to the hierarchy /sys/fs/cgroup/net_cls,net_prio
[  FAILED  ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate (75 ms)
{noformat}",Debian 8 - 0.27.0-rc1 - root test-run,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-27 17:34:45.284,,,false,MESOS-4343,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 29 04:44:33 UTC 2016,,,,,,,"0|i2s1dj:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 27,,,,,,,,,,,1.0,,0.27.0,,,,,,,,,"27/Jan/16 17:34;avinash.mesos;This test was introduced for the cgroup/net_cls isolator. Looks on Debian 8 multiple subsystems (net_prio and net_cls) and mounted on the same hierarchy. The cgroup/net_cls isolator does not cater for multiple subsystems existing in the same hierarchy as the net_cls subsystem. The isolator needs to cater for multiple subsystems existing at the same hierarchy. ","27/Jan/16 20:03;greggomann;I just saw this on CentOS 6 as well:

{code}
[ RUN      ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate
I0127 19:15:06.637328 25347 exec.cpp:134] Version: 0.28.0
I0127 19:15:06.648378 25378 exec.cpp:208] Executor registered on slave 6edafba0-9dbd-4e6e-b10e-c6f935e58d41-S0
Registered executor on localhost
Starting task b745d88e-3fbe-4af9-80b3-e43484e37acf
sh -c 'sleep 1000'
Forked command at 25385
../../src/tests/containerizer/isolator_tests.cpp:926: Failure
pids: Failed to read cgroups control 'cgroup.procs': '/sys/fs/cgroup/net_cls' is not a valid hierarchy
I0127 19:15:06.662083 25376 exec.cpp:381] Executor asked to shutdown
Shutting down
Sending SIGTERM to process tree at pid 25385
[  FAILED  ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate (335 ms)
{code}","27/Jan/16 23:19;avinash.mesos;https://reviews.apache.org/r/42878/","29/Jan/16 04:44;mcypark;{noformat}
commit 502c2d35bb744db8c542d5f72429f9ebd024a0ce
Author: Avinash sridharan <avinash@mesosphere.io>
Date:   Wed Jan 27 17:22:59 2016 -0800

    Relaxed the subsystem check for cgroups net_cls isolator.

    The check ensures that only the net_cls and net_prio subsystem can be
    mounted in the same hierarchy.

    Review: https://reviews.apache.org/r/42872/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,
Introduce docker runtime isolator.,MESOS-4517,12934242,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Implemented,gilbert,gilbert,gilbert,26/Jan/16 21:35,26/Nov/18 12:49,29/Oct/20 16:32,26/Nov/18 12:49,,,,,,,,,0.28.0,,,,,,containerization,,,,,0,mesosphere,,,,,,,,"Currently docker image default configuration are included in `ProvisionInfo`. We should grab necessary config from `ProvisionInfo` into `ContainerInfo`, and handle all these runtime informations inside of docker runtime isolator. Return a `ContainerLaunchInfo` containing `working_dir`, `env` and merged `commandInfo`, etc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4004,MESOS-4005,MESOS-4383,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 05 21:47:05 UTC 2016,,,,,,,"0|i2rzxz:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 28,,,,,,,,,,,3.0,,0.28.0,,,,,,,,,"05/Feb/16 21:34;gilbert;https://reviews.apache.org/r/43021/
https://reviews.apache.org/r/43022/
https://reviews.apache.org/r/43036/","05/Feb/16 21:47;gilbert;commit 0b0a3dc5467224511b1963dd0ac530bca7506376
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Feb 3 17:14:23 2016 -0800

    Plugged in docker runtime isolator.
    
    Review: https://reviews.apache.org/r/43036/

commit 2d5d14f092405c7971c21097820a91223b22ab27
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Feb 3 17:09:59 2016 -0800

    Added protobuf fields for docker runtime isolator.
    
    Review: https://reviews.apache.org/r/43022/

commit aa1a27f73c21e31d1e426c692a6d405f16bf2660
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Feb 3 17:08:54 2016 -0800

    Implemented docker runtime isolator interface.
    
    Review: https://reviews.apache.org/r/43021",,,,,,,,,,,,,,,,,,,,,,,,,,
ContainerLoggerTest.LOGROTATE_RotateInSandbox breaks when running on Centos6.,MESOS-4515,12934167,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,kaysoky,tillt,tillt,26/Jan/16 17:39,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.27.0,,,,,,,,,,,0,mesosphere,module,test,,,,,,"{noformat}
[17:24:58][Step 7/7] logrotate: bad argument --version: unknown error
[17:24:58][Step 7/7] F0126 17:24:57.913729  4503 container_logger_tests.cpp:380] CHECK_SOME(containerizer): Failed to create container logger: Failed to create container logger module 'org_apache_mesos_LogrotateContainerLogger': Error creating Module instance for 'org_apache_mesos_LogrotateContainerLogger' 
[17:24:58][Step 7/7] *** Check failure stack trace: ***
[17:24:58][Step 7/7]     @     0x7f11ae0d2d40  google::LogMessage::Fail()
[17:24:58][Step 7/7]     @     0x7f11ae0d2c9c  google::LogMessage::SendToLog()
[17:24:58][Step 7/7]     @     0x7f11ae0d2692  google::LogMessage::Flush()
[17:24:58][Step 7/7]     @     0x7f11ae0d544c  google::LogMessageFatal::~LogMessageFatal()
[17:24:58][Step 7/7]     @           0x983927  _CheckFatal::~_CheckFatal()
[17:24:58][Step 7/7]     @           0xa9a18b  mesos::internal::tests::ContainerLoggerTest_LOGROTATE_RotateInSandbox_Test::TestBody()
[17:24:58][Step 7/7]     @          0x1623a4e  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[17:24:58][Step 7/7]     @          0x161eab2  testing::internal::HandleExceptionsInMethodIfSupported<>()
[17:24:58][Step 7/7]     @          0x15ffdfd  testing::Test::Run()
[17:24:58][Step 7/7]     @          0x160058b  testing::TestInfo::Run()
[17:24:58][Step 7/7]     @          0x1600bc6  testing::TestCase::Run()
[17:24:58][Step 7/7]     @          0x1607515  testing::internal::UnitTestImpl::RunAllTests()
[17:24:58][Step 7/7]     @          0x16246dd  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[17:24:58][Step 7/7]     @          0x161f608  testing::internal::HandleExceptionsInMethodIfSupported<>()
[17:24:58][Step 7/7]     @          0x1606245  testing::UnitTest::Run()
[17:24:58][Step 7/7]     @           0xde36b6  RUN_ALL_TESTS()
[17:24:58][Step 7/7]     @           0xde32cc  main
[17:24:58][Step 7/7]     @     0x7f11a8896d5d  __libc_start_main
[17:24:58][Step 7/7]     @           0x981fc9  (unknown)
{noformat}","Centos6, gcc-4.9.3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-26 20:37:22.415,,,false,MESOS-4086,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 27 00:01:47 UTC 2016,,,,,,,"0|i2rzhb:",9223372036854775807,,,,,benjaminhindman,,,,,,Mesosphere Sprint 27,,,,,,,,,,,1.0,,0.27.0,,,,,,,,,"26/Jan/16 20:37;kaysoky;Fix: https://reviews.apache.org/r/42820/","27/Jan/16 00:01;benjaminhindman;{code}
commit 8d2504b3d6e705ebbaccc1e0836f1036c85b0bd5
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Tue Jan 26 16:00:56 2016 -0800

    Fixed logrotate ContainerLogger on Centos6.
    
    The `logrotate --version` flag is actually a very recent addition (3.9.1).
    
    The `--help` flag is more likely to be present on a supported
    system-with-logrotate, given logrotate's usage of popopt.
    
    Review: https://reviews.apache.org/r/42820/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Render quota status consistently with other endpoints.,MESOS-4512,12934148,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Won't Fix,alexr,alexr,alexr,26/Jan/16 16:22,26/Nov/18 12:32,29/Oct/20 16:32,26/Nov/18 12:32,,,,,,,,,,,,,,,master,,,,,0,http,mesosphere,,,,,,,"Currently quota status endpoint returns a collection of {{QuotaInfo}} protos converted to JSON. An example response looks like this:
{code:xml}
{
  ""infos"": [
    {
      ""role"": ""role1"",
      ""guarantee"": [
        {
          ""name"": ""cpus"",
          ""role"": ""*"",
          ""type"": ""SCALAR"",
          ""scalar"": { ""value"": 12 }
        },
        {
          ""name"": ""mem"",
          ""role"": ""*"",
          ""type"": ""SCALAR"",
          ""scalar"": { ""value"": 6144 }
        }
      ]
    }
  ]
}
{code}

Presence of some fields, e.g. ""role"", is misleading. To address this issue and make the output more informative, we should probably introduce a  {{model()}} function for {{QuotaStatus}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-29 21:46:25.766,,,false,MESOS-4791,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 30 20:29:33 UTC 2016,,,,,,,"0|i2rzd3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"29/May/16 21:46;vinodkone;Why is `role` here misleading? In the v1 master API we are modeling the output as QuotaInfo as well. Let me know how you want it to look instead. Note that the output needs to be a protobuf not a model() in v1.","30/May/16 01:32;vinodkone;Will fix this in v1 API.","30/May/16 20:29;alexr;I meant the role in {{Resource}}, not the one on {{QuotaInfo}}. Printing '\*' there is misleading, because the resource is set aside for quota, but since technically it is not a reserved resource, {{Resource.role}} must be '\*'. 

Since we are deprecating {{model()}} functions, we can't really do better here and will have to print {{Resource}} proto. I'm fine with closing the ticket.",,,,,,,,,,,,,,,,,,,,,,,,,
Expose ExecutorInfo and TaskInfo for isolators.,MESOS-4500,12933879,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,gilbert,gilbert,gilbert,25/Jan/16 17:19,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,,,,,,,,,,,,0,mesosphere,unified-containerizer-mvp,,,,,,,"Currently we do not have these info for isolator. Image once we have docker runtime isolator, CommandInfo is necessary to support either custom executor or command executor. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 28 19:47:56 UTC 2016,,,,,,,"0|i2rxpj:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 27,,,,,,,,,,,2.0,,,,,,,,,,,"28/Jan/16 19:47;gilbert;https://reviews.apache.org/r/42690/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Docker provisioner store should reuse existing layers in the cache.,MESOS-4499,12933873,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,jieyu,jieyu,25/Jan/16 17:04,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.28.0,,,,,,,,,,,0,mesosphere,,,,,,,,"Currently, the docker provisioner store will download all the layers associated with an image if the image is not found locally, even though some layers of it might already exist in the cache.

This is problematic because anytime a user deploys a new image, Mesos will fetch all layers of that new image, even though most of the layers are already cached locally.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-25 17:53:38.523,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 23 23:12:31 UTC 2016,,,,,,,"0|hzzzn2:zzv",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 27,Mesosphere Sprint 28,Mesosphere Sprint 29,,,,,,,,,5.0,,,,,,,,,,,"25/Jan/16 17:53;haosdent@gmail.com;Does the cache here mean Docker engine cache or flags.docker_store_dir?","23/Feb/16 01:17;jieyu;https://reviews.apache.org/r/43801/
https://reviews.apache.org/r/43860/","23/Feb/16 23:12;jieyu;commit 66d0f4496f146fb0c68d23e08d25a9af108c6218
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Mon Feb 22 16:52:01 2016 -0800

    Used uri::Fetcher to pull docker images in docker registry puller.
    
    Review: https://reviews.apache.org/r/43860

commit caf6c02989cfc9236d0ae03eaad4844d6763d5c0
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Feb 19 21:41:01 2016 -0800

    Refactored and simplified the docker puller interfaces.
    
    Review: https://reviews.apache.org/r/43801",,,,,,,,,,,,,,,,,,,,,,,,,
Delete `os::chown` on Windows,MESOS-4495,12933764,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hausdorff,hausdorff,hausdorff,25/Jan/16 10:02,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.28.0,,,,,,stout,,,,,0,mesosphere,stout,windows-mvp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-29 06:00:10.018,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 29 06:00:32 UTC 2016,,,,,,,"0|i2rx07:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 27,,,,,,,,,,,1.0,,0.28.0,,,,,,,,,"29/Jan/16 06:00;mcypark;https://reviews.apache.org/r/40613/","29/Jan/16 06:00;mcypark;{noformat}
commit e4547eedfce01c24c3faa3b11cf51b526409dc0b
Author: Alex Naparu <alex.naparu@microsoft.com>
Date:   Mon Nov 23 12:50:19 2015 -0800

    Windows: Made 'os::chown' a POSIX-only function.

    https://reviews.apache.org/r/40613/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Implement process querying/counting in Windows,MESOS-4471,12933500,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hausdorff,hausdorff,hausdorff,22/Jan/16 22:11,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,1.0.0,,,,,,stout,,,,,0,mesosphere,stout,windows-mvp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-19 14:28:24.398,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 22 05:31:23 UTC 2016,,,,,,,"0|i2rvdr:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 33,,,,,,,,,,,2.0,,,,,,,,,,,"19/Apr/16 14:28;jvanremoortere;https://reviews.apache.org/r/46012/","19/Apr/16 14:28;jvanremoortere;{code}
commit ebd23256907a602823348dcd6a32daa0b53b5054
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Tue Apr 19 10:03:56 2016 -0400

    Windows: Stout: Implemented `os::pids`.
    
    Review: https://reviews.apache.org/r/46012/
{code}","22/Apr/16 05:31;mcypark;{noformat}
commit 3fd64f4d7a569199e184733c6c448c91ad70733a
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Thu Apr 21 11:51:33 2016 -0700

    Moved process tests to their own file to libprocess.

    Review: https://reviews.apache.org/r/46015/
{noformat}
{noformat}
commit f79b8bbdc38ab7afafbf37eb63b87497650e12d9
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Thu Apr 21 11:51:28 2016 -0700

    Moved process tests to their own file to stout.

    Review: https://reviews.apache.org/r/46014/
{noformat}
{noformat}
commit d534d64def535179ff7db06570283184278c8a4c
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Thu Apr 21 11:51:25 2016 -0700

    Implemented `os::processes` on Windows in stout.

    Review: https://reviews.apache.org/r/46013/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,
Implement `waitpid` in Windows,MESOS-4466,12933495,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hausdorff,hausdorff,hausdorff,22/Jan/16 22:07,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,1.0.0,,,,,,stout,,,,,0,mesosphere,stout,windows-mvp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-22 21:12:28.759,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 22 21:12:39 UTC 2016,,,,,,,"0|i2rvcn:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 33,,,,,,,,,,,5.0,,,,,,,,,,,"22/Apr/16 21:12;jvanremoortere;    https://reviews.apache.org/r/46341/
https://reviews.apache.org/r/46340/","22/Apr/16 21:12;jvanremoortere;{code}
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Fri Apr 22 12:50:11 2016 -0700

    Stout: [2/2] Transitioned reap.cpp to `os::waitpid`.
    
    Review: https://reviews.apache.org/r/46341/

commit 66ee32a839976ab804040f9bfd8b8e1029d8abc6
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Fri Apr 22 12:49:58 2016 -0700

    Stout: [1/2] Implement `os::waitpid`.
    
    Review: https://reviews.apache.org/r/46340/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Create common sha512 compute utility function.,MESOS-4454,12933407,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jojy,jojy,jojy,22/Jan/16 17:51,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,0.28.0,,,,,,containerization,,,,,0,mesosphere,unified-containerizer-mvp,,,,,,,Add common utility function for computing digests. Start with `sha512` since its immediately needed by appc image fetcher. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-29 02:04:41.836,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 29 02:04:41 UTC 2016,,,,,,,"0|i2rutj:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 27,,,,,,,,,,,2.0,,,,,,,,,,,"22/Jan/16 17:54;jojy;https://reviews.apache.org/r/42662/","29/Jan/16 02:04;jieyu;commit ebc85487c125d05745bffbee0bbbd0943b99210f
Author: Jojy Varghese <jojy@mesosphere.io>
Date:   Thu Jan 28 17:36:34 2016 -0800

    Added utility function to compute SHA512 digest.
    
    Review: https://reviews.apache.org/r/42773/",,,,,,,,,,,,,,,,,,,,,,,,,,
"Install 3rdparty package boost, glog, protobuf and picojson when installing Mesos",MESOS-4434,12932545,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,karya,karya,karya,19/Jan/16 22:01,29/Apr/19 09:26,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,1.0.0,,,,,,build,modules,,,,1,mesosphere,,,,,,,,Mesos modules depend on having these packages installed with the exact version as Mesos was compiled with.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-12 05:43:07.339,,,false,MESOS-4690,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 12 05:43:07 UTC 2016,,,,,,,"0|hzzzhy:zzv",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 33,Mesosphere Sprint 34,,,,,,,,,,3.0,,,,,,,,,,,"21/Apr/16 22:51;karya;Here are the RRs:

https://reviews.apache.org/r/46537 Added --enable-install-module-dependencies.
https://reviews.apache.org/r/46538 Install module dependencies in build/3rdparty as well.
https://reviews.apache.org/r/46540 Updated pkg-config file to include module dependency info.","11/May/16 23:00;karya;{code}
commit 4f26c7d8dbd0702c7dcd8023717636d5dcc2f90e
Author: Kapil Arya <kapil@mesosphere.io>
Date:   Thu Apr 21 11:51:04 2016 -0400

    Updated pkg-config file to include module dependency info.
    
    Review: https://reviews.apache.org/r/46540

commit bb703634da5d4822142849b75bbcc79105973297
Author: Kapil Arya <kapil@mesosphere.io>
Date:   Thu Apr 21 01:45:50 2016 -0400

    Install module dependencies in build/3rdparty as well.
    
    If configured with --enable-install-module-dependencies, install
    3rdparty dependencies in build/3rdparty.
    
    Review: https://reviews.apache.org/r/46538

commit 6bfd859923965a55218003623de65064983b1e13
Author: Kapil Arya <kapil@mesosphere.io>
Date:   Wed Apr 20 15:30:41 2016 -0400

    Added --enable-install-module-dependencies.
    
    Review: https://reviews.apache.org/r/46537
{code}","12/May/16 05:43;guoger;This definitely ease the compilation of modules. It would be good to have flag `--enable-install-module-dependencies` reflected in documentation as well",,,,,,,,,,,,,,,,,,,,,,,,,
Introduce filtering test abstractions for HTTP events to libprocess,MESOS-4425,12932281,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,anandmazumdar,anandmazumdar,19/Jan/16 02:07,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.27.0,,,,,,libprocess,,,,,0,mesosphere,,,,,,,,"We need a test abstraction for {{HttpEvent}} similar to the already existing one's for {{DispatchEvent}}, {{MessageEvent}} in libprocess.

The abstraction can look similar in semantics to the already existing {{FUTURE_DISPATCH}}/{{FUTURE_MESSAGE}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4255,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-27 01:22:29.078,,,false,MESOS-3302,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 27 01:22:29 UTC 2016,,,,,,,"0|hzzzqx:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 27,,,,,,,,,,,3.0,,,,,,,,,,,"19/Jan/16 02:08;anandmazumdar;An initial patch: https://reviews.apache.org/r/42183","27/Jan/16 01:22;vinodkone;commit 85d4baa00c29d8064e0c5ff9fef0f24df1e5e769
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Tue Jan 26 17:21:28 2016 -0800

    Introduced `FUTURE_HTTP_*` for filtering HTTP based events.
    
    This change introduces `FUTURE_HTTP_*` for filtering HTTP based events
    similar to the already existing filters for Messages/Dispatch events.
    Also, added `FUTURE_HTTP_CALL_*` for union based protobufs.
    
    Review: https://reviews.apache.org/r/42184/

commit a95829b68624d4d01bdbe29aaf9981b12f37ee61
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Tue Jan 26 17:21:07 2016 -0800

    Introduced HttpEvent based filters in libprocess.
    
    This change introduces the ability to filter `HTTPEvent` in libprocess
    similar to the already existing `MessageEvent` and `DispatchEvent`.
    
    Review: https://reviews.apache.org/r/42183/
",,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent allocator from crashing on successful recovery.,MESOS-4417,12931738,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,alexr,alexr,alexr,17/Jan/16 08:43,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.27.0,,,,,,allocation,,,,,0,mesosphere,,,,,,,,"There might be a bug that may crash the master as pointed out by [~bmahler] in https://reviews.apache.org/r/42222/:
{noformat}
It looks like if we trip the resume call in addSlave, this delayed resume will crash the master 
due to the CHECK(paused) that currently resides in resume.
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-21 07:48:09.549,,,false,MESOS-1791,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 21 07:48:09 UTC 2016,,,,,,,"0|i2rkjb:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 27,,,,,,,,,,,3.0,,0.27.0,,,,,,,,,"20/Jan/16 02:09;alexr;https://reviews.apache.org/r/42535/","21/Jan/16 07:48;jvanremoortere;{code}
commit 7c5e89ed704c02f3d00dc5742de0cf7ef9989c3b
Author: Alexander Rukletsov <rukletsov@gmail.com>
Date:   Wed Jan 20 23:45:29 2016 -0800

    Quota: Made allocator's pause/resume idempotent.
    
    Review: https://reviews.apache.org/r/42535/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Traverse all roles for quota allocation.,MESOS-4411,12931609,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,gyliu,alexr,alexr,15/Jan/16 23:43,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.27.0,,,,,,allocation,,,,,0,mesosphere,,,,,,,,There might be a bug in how resources are allocated to multiple quota'ed roles if one role's quota is met. We need to investigate this behavior.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-16 12:45:42.651,,,false,MESOS-1791,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 19 23:47:25 UTC 2016,,,,,,,"0|i2rjqn:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 27,,,,,,,,,,,3.0,,0.27.0,,,,,,,,,"15/Jan/16 23:45;alexr;Initial review, discussing the problem and (partially) addressing it: https://reviews.apache.org/r/41769/","16/Jan/16 12:45;gyliu;Its ready for review https://reviews.apache.org/r/41769/","19/Jan/16 17:17;alexr;https://reviews.apache.org/r/42511/","19/Jan/16 23:47;jvanremoortere;{code}
commit e7d71f9043476486f2cf2e125cb257c4f985f9ac
Author: Alexander Rukletsov <rukletsov@gmail.com>
Date:   Tue Jan 19 15:37:14 2016 -0800

    Quota: Updated comments around hierarchical tests.
    
    Review: https://reviews.apache.org/r/42511/

commit 472628ae5f6ea106ede99461db0747e665212fef
Author: Guangya Liu <gyliu513@gmail.com>
Date:   Tue Jan 19 15:37:02 2016 -0800

    Quota: Fixed allocator traversal for roles for quota.
    
    Review: https://reviews.apache.org/r/41769/
{code}",,,,,,,,,,,,,,,,,,,,,,,,
Synchronously handle AuthZ errors for the Scheduler endpoint.,MESOS-4398,12931530,Bug,Open,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,anandmazumdar,anandmazumdar,15/Jan/16 18:14,22/Jan/16 20:11,29/Oct/20 16:32,,0.25.0,,,,,,,,,,,,,,,,,,,0,mesosphere,,,,,,,,"Currently, any AuthZ errors for the {{/scheduler}} endpoint are handled asynchronously as {{FrameworkErrorMessage}}. Here is an example:

{code}
  if (authorizationError.isSome()) {
    LOG(INFO) << ""Refusing subscription of framework""
              << "" '"" << frameworkInfo.name() << ""'""
              << "": "" << authorizationError.get().message;

    FrameworkErrorMessage message;
    message.set_message(authorizationError.get().message);
    http.send(message);
    http.close();
    return;
  }
{code}

We would like to handle such errors synchronously when the request is received similar to what other endpoints like {{/reserve}}/{{/quota}} do. We already have the relevant functions {{authorizeXXX}} etc in {{master.cpp}}. We should just make the requests pass through once the relevant {{Future}} from the {{authorizeXXX}} function is fulfilled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-3302,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-01-15 18:14:10.0,,,,,,,"0|hzzzy6:tzy",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add persistent volume endpoint tests with no principal,MESOS-4395,12931512,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,greggomann,greggomann,greggomann,15/Jan/16 16:28,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.28.0,,,,,,,,,,,0,mesosphere,persistent-volumes,tests,,,,,,"There are currently no persistent volume endpoint tests that do not use a principal; they should be added.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-29 20:56:49.485,,,false,MESOS-1554,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 29 20:56:49 UTC 2016,,,,,,,"0|i2rj53:",9223372036854775807,,,,,karya,,,,,,Mesosphere Sprint 27,,,,,,,,,,,1.0,,,,,,,,,,,"21/Jan/16 21:05;greggomann;Review here: https://reviews.apache.org/r/42362/","29/Jan/16 20:56;karya;commit 222446f4859b04c740bab551835499089b8f9421
Author: Greg Mann <greg@mesosphere.io>
Date:   Fri Jan 29 12:43:25 2016 -0500

    Added persistent volume endpoint test without authentication.

    Added persistent volume endpoint tests with HTTP authentication
    disabled.

    The persistent volume endpoint tests allow volume creation and
    destruction when HTTP authentication is disabled; this patch introduces
    a test for this scenario:
      `PersistentVolumeEndpointsTest.NoAuthentication`.

    Review: https://reviews.apache.org/r/42362/",,,,,,,,,,,,,,,,,,,,,,,,,,
Offers and InverseOffers cannot be accepted in the same ACCEPT call,MESOS-4385,12930269,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Resolved,kaysoky,kaysoky,kaysoky,14/Jan/16 21:32,26/Nov/18 12:43,29/Oct/20 16:32,26/Nov/18 12:43,0.25.0,,,,,,,,,,,,,,framework,master,,,,0,maintenance,mesosphere,,,,,,,"*Problem*
* In {{Master::accept}}, {{validation::offer::validate}} returns an error when an {{InverseOffer}} is included in the list of {{OfferIDs}} in an {{ACCEPT}} call.
* If an {{Offer}} is part of the same {{ACCEPT}}, the master sees {{error.isSome()}} and returns a {{TASK_LOST}} for normal offers.  (https://github.com/apache/mesos/blob/fafbdca610d0a150b9fa9cb62d1c63cb7a6fdaf3/src/master/master.cpp#L3117)

Here's a regression test:
https://reviews.apache.org/r/42092/

*Proprosal*
The question is whether we want to allow the mixing of {{Offers}} and {{InverseOffers}}.

Arguments for mixing:
* The design/structure of the maintenance originally intended to overload {{ACCEPT}} and {{DECLINE}} to take inverse offers.
* Enforcing non-mixing may require breaking changes to {{scheduler.proto}}.

Arguments against mixing:
* Some semantics are difficult to explain.  What does it mean to supply {{InverseOffers}} with {{Offer::Operations}}?  What about {{DECLINE}} with {{Offers}} and {{InverseOffers}}, including a ""reason""?
* What happens if we presumably add a third type of offer?
* Does it make sense to {{TASK_LOST}} valid normal offers if {{InverseOffers}} are invalid?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-1474,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 03 17:26:19 UTC 2016,,,,,,,"0|i2rbgv:",9223372036854775807,,,,,bmahler,,,,,,,,,,,,,,,,,2.0,,0.28.0,,,,,,,,,"03/Jun/16 17:26;kaysoky;Fixed via [MESOS-5296].  Offers and inverse offers are now separate and cannot be accepted/declined in the same call.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Support docker runtime configuration env var from image.,MESOS-4383,12930261,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Implemented,gilbert,gilbert,gilbert,14/Jan/16 21:09,26/Nov/18 12:50,29/Oct/20 16:32,26/Nov/18 12:50,,,,,,,,,0.28.0,,,,,,containerization,,,,,0,mesosphere,unified-containerizer-mvp,,,,,,,We need to support env var configuration returned from docker image in mesos containerizer.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 05 21:46:15 UTC 2016,,,,,,,"0|hzzzqe:zzzzz",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 27,Mesosphere Sprint 28,,,,,,,,,,2.0,,0.28.0,,,,,,,,,"22/Jan/16 22:22;gilbert;https://reviews.apache.org/r/43037/
","05/Feb/16 21:46;gilbert;commit 40445848ac70642959ee2052991f8ed0c56cbff6
Author: Gilbert Song <songzihao1990@gmail.com>
Date:   Wed Feb 3 17:58:08 2016 -0800

    Supported env var in docker runtime isolator.
    
    Review: https://reviews.apache.org/r/43037/",,,,,,,,,,,,,,,,,,,,,,,,,,
Create common tar/untar utility function.,MESOS-4360,12929990,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jojy,jojy,jojy,13/Jan/16 23:18,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,,,,,,,,,,,,0,mesosphere,,,,,,,,"As part of refactoring and creating a common place to add all command utilities, add *tar* and *untar* as the first POC.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-25 17:08:16.596,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 28 02:20:49 UTC 2016,,,,,,,"0|hzzzr2:zzzi",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 26,Mesosphere Sprint 27,,,,,,,,,,3.0,,,,,,,,,,,"13/Jan/16 23:19;jojy;https://reviews.apache.org/r/42662","25/Jan/16 17:08;hartem;[~jojy], is this review link still valid? I am getting a 'permission denied' on RB.","25/Jan/16 18:39;jojy;I have updated the review link. Thanks Artem.","28/Jan/16 02:20;jieyu;commit 011121bd686a7c82e9d659d7e1751f72278fb0be
Author: Jojy Varghese <jojy@mesosphere.io>
Date:   Wed Jan 27 18:18:22 2016 -0800

    Added common command utils file.
    
    This common file is a good place to add common command line utilities
    like tar, digests(sha256, sha512, etc). Currently this functionality is
    spread in the code base and this change would enable all those call
    sites to be replaced with a common code.
    
    Review: https://reviews.apache.org/r/42662/",,,,,,,,,,,,,,,,,,,,,,,,
GMock warning in DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard,MESOS-4359,12929978,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,,,greggomann,greggomann,13/Jan/16 22:33,26/Nov/18 13:36,29/Oct/20 16:32,,0.26.0,,,,,,,,,,,,,,test,,,,,0,docker,flaky-test,mesosphere,,,,,,"The following GMock warning was seen on CentOS 7.1:

{code}
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: executorLost(0x7ffdd74f73e0, @0x7f3e3c00fa20 e1, @0x7f3e3c00f4b0 cf212bb4-c8c5-4a43-b71f-c17b27458627-S0, -1)
Stack trace:
[       OK ] DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard (405 ms)
{code}","CentOS 7.1, libevent & SSL enabled",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-01-13 22:33:26.0,,,,,,,"0|i2r9of:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GMock warning on `offerRescinded` in `ReservationTest` fixture,MESOS-4350,12929610,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,greggomann,greggomann,12/Jan/16 22:26,06/Feb/19 17:34,29/Oct/20 16:32,,,,,,,,,,,,,,,,test,,,,,0,flaky-test,foundations,mesosphere,reservations,,,,,"Several tests involving checkpointing of resources in the {{ReservationTest}} fixture are throwing GMock warnings occasionally. Here is the output of {{GTEST_FILTER=""ReservationTest.*"" bin/mesos-tests.sh --gtest_repeat=10000 --gtest_break_on_failure=1 | grep -B 3 -A 6 WARNING}}:

{code}
-------------------------------------------------------------
We cannot run any Docker tests because:
Docker tests not supported on non-Linux systems
-------------------------------------------------------------
[       OK ] ReservationTest.MasterFailover (89 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResources

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec320fab0 65537c10-285c-419e-b89f-191283402d85-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResources (52 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (45 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (46 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec796f220 bf4e1b52-02db-4763-8be0-3c759c80f1ba-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (63 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (45 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (42 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec7ad92b0 42a9f1ff-122e-4df7-9530-a96126e36f84-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (65 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (46 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (49 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec7af4310 d5e1005f-abb8-4bfd-92e0-3976ee150fbf-O1)
Stack trace:
[       OK ] ReservationTest.IncompatibleCheckpointedResources (94 ms)
[ RUN      ] ReservationTest.GoodACLReserveThenUnreserve
[       OK ] ReservationTest.GoodACLReserveThenUnreserve (57 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (43 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec7cdadc0 36e15f52-3299-46fa-850d-970097fef8e2-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (62 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (46 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (47 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feec8c1b580 c8dd35ab-7363-40e0-8e20-8c7dc76a8497-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (62 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (45 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (47 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feecbd9b5b0 031c2148-8a20-4532-b77f-b6200c3791c8-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (62 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (46 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (47 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feecd52adb0 edc5a322-b220-4b13-a39b-99a523b172ba-O1)
Stack trace:
[       OK ] ReservationTest.IncompatibleCheckpointedResources (76 ms)
[ RUN      ] ReservationTest.GoodACLReserveThenUnreserve
[       OK ] ReservationTest.GoodACLReserveThenUnreserve (63 ms)
--
--
[       OK ] ReservationTest.SendingCheckpointResourcesMessage (45 ms)
[ RUN      ] ReservationTest.ResourcesCheckpointing

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f015df8, @0x7feecfe16f00 09a90e67-a40f-4e42-8802-1a5644733a06-O1)
Stack trace:
[       OK ] ReservationTest.ResourcesCheckpointing (60 ms)
[ RUN      ] ReservationTest.MasterFailover
[       OK ] ReservationTest.MasterFailover (89 ms)
--
--
[       OK ] ReservationTest.CompatibleCheckpointedResources (43 ms)
[ RUN      ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: offerRescinded(0x7fff5f014960, @0x7feecacceba0 84965984-28cd-4bc8-b25b-746583477d09-O1)
Stack trace:
[       OK ] ReservationTest.CompatibleCheckpointedResourcesWithPersistentVolumes (58 ms)
[ RUN      ] ReservationTest.IncompatibleCheckpointedResources
[       OK ] ReservationTest.IncompatibleCheckpointedResources (68 ms)
{code}",OSX 10.10.5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4347,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-01-12 22:26:30.0,,,,,,,"0|i2r7en:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GMock warning in ReservationTest.ACLMultipleOperations,MESOS-4347,12929595,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,Fixed,greggomann,neilc,neilc,12/Jan/16 21:08,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.27.0,,,,,,test,,,,,0,mesosphere,reservations,tests,,,,,,"{noformat}
[ RUN      ] ReservationTest.ACLMultipleOperations

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: shutdown(0x7fa2a311b300)
Stack trace:
[       OK ] ReservationTest.ACLMultipleOperations (174 ms)
[----------] 1 test from ReservationTest (174 ms total)
{noformat}

Seems to occur non-deterministically for me, maybe once per 50 runs or so. OSX 10.10",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4357,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-12 22:35:51.189,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 13 22:17:55 UTC 2016,,,,,,,"0|i2r7bb:",9223372036854775807,,,,,tnachen,,,,,,Mesosphere Sprint 26,,,,,,,,,,,1.0,,,,,,,,,,,"12/Jan/16 21:17;neilc;Similar warning in some other linked tests, possibly related.","12/Jan/16 22:35;greggomann;Thanks for filing and triaging this ticket, [~neilc]! You made my job quite easy. Since it's a simple fix I went ahead and made a patch; if I can find a shepherd I'll add the ticket to this sprint.

Review here: https://reviews.apache.org/r/42215/","13/Jan/16 22:17;tnachen;commit 3e24f5f210f28256f63bad3459131594e71365ca
Author: Greg Mann <greg@mesosphere.io>
Date:   Wed Jan 13 13:24:04 2016 -0800

    Added expectations for call to mocked executor shutdown.

    Review: https://reviews.apache.org/r/42215/",,,,,,,,,,,,,,,,,,,,,,,,,
Create utilities for common shell commands used.,MESOS-4338,12929341,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jojy,jojy,jojy,12/Jan/16 01:37,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,0.28.0,,,,,,containerization,,,,,0,mesosphere,,,,,,,,"We spawn shell for command line utilities like tar, untar, sha256 etc. Would be great for resuse if we can create a common utilities class/file for all these utilities.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4360,MESOS-4454,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-01-12 01:37:50.0,,,,,,,"0|i2r5qv:",9223372036854775807,,,,,jieyu,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SlaveTest.LaunchTaskInfoWithContainerInfo cannot be execute in isolation,MESOS-4329,12929164,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bbannier,bbannier,bbannier,11/Jan/16 12:38,26/Nov/18 09:33,29/Oct/20 16:32,26/Nov/18 09:33,,,,,,,,,0.28.0,,,,,,test,,,,,0,mesosphere,tech-debt,,,,,,,"Executing {{SlaveTest.LaunchTaskInfoWithContainerInfo}} from {{468b8ec}} under OS X 10.10.5 in isolation fails due to missing cleanup,
{code}
% ./bin/mesos-tests.sh --gtest_filter=SlaveTest.LaunchTaskInfoWithContainerInfo
Source directory: /ABC/DEF/src/mesos
Build directory: /ABC/DEF/src/mesos/build
-------------------------------------------------------------
We cannot run any Docker tests because:
Docker tests not supported on non-Linux systems
-------------------------------------------------------------
/usr/bin/nc
/usr/bin/curl
Note: Google Test filter = SlaveTest.LaunchTaskInfoWithContainerInfo-HealthCheckTest.ROOT_DOCKER_DockerHealthyTask:HealthCheckTest.ROOT_DOCKER_DockerHealthStatusChange:HierarchicalAllocator_BENCHMARK_Test.DeclineOffers:HookTest.ROOT_DOCKER_VerifySlavePreLaunchDockerHook:SlaveTest.ROOT_RunTaskWithCommandInfoWithoutUser:SlaveTest.DISABLED_ROOT_RunTaskWithCommandInfoWithUser:DockerContainerizerTest.ROOT_DOCKER_Launch:DockerContainerizerTest.ROOT_DOCKER_Kill:DockerContainerizerTest.ROOT_DOCKER_Usage:DockerContainerizerTest.ROOT_DOCKER_Recover:DockerContainerizerTest.ROOT_DOCKER_SkipRecoverNonDocker:DockerContainerizerTest.ROOT_DOCKER_Logs:DockerContainerizerTest.ROOT_DOCKER_Default_CMD:DockerContainerizerTest.ROOT_DOCKER_Default_CMD_Override:DockerContainerizerTest.ROOT_DOCKER_Default_CMD_Args:DockerContainerizerTest.ROOT_DOCKER_SlaveRecoveryTaskContainer:DockerContainerizerTest.DISABLED_ROOT_DOCKER_SlaveRecoveryExecutorContainer:DockerContainerizerTest.ROOT_DOCKER_NC_PortMapping:DockerContainerizerTest.ROOT_DOCKER_LaunchSandboxWithColon:DockerContainerizerTest.ROOT_DOCKER_DestroyWhileFetching:DockerContainerizerTest.ROOT_DOCKER_DestroyWhilePulling:DockerContainerizerTest.ROOT_DOCKER_ExecutorCleanupWhenLaunchFailed:DockerContainerizerTest.ROOT_DOCKER_FetchFailure:DockerContainerizerTest.ROOT_DOCKER_DockerPullFailure:DockerContainerizerTest.ROOT_DOCKER_DockerInspectDiscard:DockerTest.ROOT_DOCKER_interface:DockerTest.ROOT_DOCKER_parsing_version:DockerTest.ROOT_DOCKER_CheckCommandWithShell:DockerTest.ROOT_DOCKER_CheckPortResource:DockerTest.ROOT_DOCKER_CancelPull:DockerTest.ROOT_DOCKER_MountRelative:DockerTest.ROOT_DOCKER_MountAbsolute:CopyBackendTest.ROOT_CopyBackend:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/0:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/1:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/2:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/3:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/4:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/5:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/6:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/7:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/8:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/9:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/10:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/11:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/12:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/13:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/14:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/15:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/16:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/17:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/18:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/19:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/20:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/21:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/22:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/23:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/24:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/25:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/26:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/27:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/28:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/29:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/30:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/31:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/32:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/33:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/34:SlaveAndFrameworkCount/HierarchicalAllocator_BENCHMARK_Test.AddAndUpdateSlave/35:SlaveCount/Registrar_BENCHMARK_Test.Performance/0:SlaveCount/Registrar_BENCHMARK_Test.Performance/1:SlaveCount/Registrar_BENCHMARK_Test.Performance/2:SlaveCount/Registrar_BENCHMARK_Test.Performance/3
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from SlaveTest
[ RUN      ] SlaveTest.LaunchTaskInfoWithContainerInfo
[       OK ] SlaveTest.LaunchTaskInfoWithContainerInfo (79 ms)
[----------] 1 test from SlaveTest (79 ms total)

[----------] Global test environment tear-down
../../src/tests/environment.cpp:569: Failure
Failed
Tests completed with child processes remaining:
-+- 54487 /ABC/DEF/src/mesos/build/src/.libs/mesos-tests --gtest_filter=SlaveTest.LaunchTaskInfoWithContainerInfo
 \--- 54503 /bin/sh /ABC/DEF/src/mesos/build/src/mesos-containerizer launch --command={""shell"":true,""value"":""\/ABC\/DEF\/src\/mesos\/build\/src\/mesos-executor""} --commands={""commands"":[]} --directory=/tmp --help=false --pipe_read=10 --pipe_write=13 --user=test
[==========] 1 test from 1 test case ran. (87 ms total)
[  PASSED  ] 1 test.
[  FAILED  ] 0 tests, listed below:

 0 FAILED TESTS
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-01 00:43:22.31,,,false,MESOS-4809,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 01 00:43:22 UTC 2016,,,,,,,"0|i2r4nr:",9223372036854775807,,,,,tillt,,,,,,Mesosphere Sprint 27,,,,,,,,,,,1.0,,0.28.0,,,,,,,,,"18/Jan/16 10:39;bbannier;Review: https://reviews.apache.org/r/42247/","01/Feb/16 00:43;tillt;{noformat}
commit e29ea6a9215ca0bc2405b85a197ee81740707bb8
Author: Benjamin Bannier <benjamin.bannier@mesosphere.io>
Date:   Mon Feb 1 00:59:25 2016 +0100

    Made sure the container launcher terminated before we leave the test.

    Waiting for the launch result is not enough as it is unrelated to
    whether the launched container has terminated. If we do not wait for
    that the global teardown might find it still running and fail the tests
    (this can happen if we e.g., execute this test in isolation).

    Also aline the name of the launch result to follow the canonical
    scheme.

    Review: https://reviews.apache.org/r/42247/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
PersistentVolumeTest.BadACLNoPrincipal is flaky,MESOS-4318,12928743,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,greggomann,jieyu,jieyu,08/Jan/16 21:27,13/Jan/16 00:08,29/Oct/20 16:32,13/Jan/16 00:08,,,,,,,,,,,,,,,,,,,,0,flaky-test,,,,,,,,"https://builds.apache.org/job/Mesos/1457/COMPILER=gcc,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=centos:7,label_exp=docker%7C%7CHadoop/consoleFull

{noformat}
[ RUN      ] PersistentVolumeTest.BadACLNoPrincipal
I0108 01:13:16.117883  1325 leveldb.cpp:174] Opened db in 2.614722ms
I0108 01:13:16.118650  1325 leveldb.cpp:181] Compacted db in 706567ns
I0108 01:13:16.118702  1325 leveldb.cpp:196] Created db iterator in 24489ns
I0108 01:13:16.118723  1325 leveldb.cpp:202] Seeked to beginning of db in 2436ns
I0108 01:13:16.118738  1325 leveldb.cpp:271] Iterated through 0 keys in the db in 397ns
I0108 01:13:16.118793  1325 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0108 01:13:16.119627  1348 recover.cpp:447] Starting replica recovery
I0108 01:13:16.120352  1348 recover.cpp:473] Replica is in EMPTY status
I0108 01:13:16.121750  1357 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (7084)@172.17.0.2:32801
I0108 01:13:16.122297  1353 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0108 01:13:16.122747  1350 recover.cpp:564] Updating replica status to STARTING
I0108 01:13:16.123625  1354 master.cpp:365] Master 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2 (d9632dd1c41e) started on 172.17.0.2:32801
I0108 01:13:16.123946  1347 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 728242ns
I0108 01:13:16.123999  1347 replica.cpp:320] Persisted replica status to STARTING
I0108 01:13:16.123708  1354 master.cpp:367] Flags at startup: --acls=""create_volumes {
  principals {
    values: ""test-principal""
  }
  volume_types {
    type: ANY
  }
}
create_volumes {
  principals {
    type: ANY
  }
  volume_types {
    type: NONE
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/f2rA75/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role1"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/_inst/share/mesos/webui"" --work_dir=""/tmp/f2rA75/master"" --zk_session_timeout=""10secs""
I0108 01:13:16.124219  1354 master.cpp:414] Master allowing unauthenticated frameworks to register
I0108 01:13:16.124236  1354 master.cpp:417] Master only allowing authenticated slaves to register
I0108 01:13:16.124248  1354 credentials.hpp:35] Loading credentials for authentication from '/tmp/f2rA75/credentials'
I0108 01:13:16.124294  1358 recover.cpp:473] Replica is in STARTING status
I0108 01:13:16.124644  1354 master.cpp:456] Using default 'crammd5' authenticator
I0108 01:13:16.124820  1354 master.cpp:493] Authorization enabled
W0108 01:13:16.124843  1354 master.cpp:553] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information
I0108 01:13:16.125154  1348 hierarchical.cpp:147] Initialized hierarchical allocator process
I0108 01:13:16.125334  1345 whitelist_watcher.cpp:77] No whitelist given
I0108 01:13:16.126065  1346 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (7085)@172.17.0.2:32801
I0108 01:13:16.126806  1348 recover.cpp:193] Received a recover response from a replica in STARTING status
I0108 01:13:16.128237  1354 recover.cpp:564] Updating replica status to VOTING
I0108 01:13:16.128402  1359 master.cpp:1629] The newly elected leader is master@172.17.0.2:32801 with id 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2
I0108 01:13:16.128489  1359 master.cpp:1642] Elected as the leading master!
I0108 01:13:16.128523  1359 master.cpp:1387] Recovering from registrar
I0108 01:13:16.128756  1355 registrar.cpp:307] Recovering registrar
I0108 01:13:16.129259  1344 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 531437ns
I0108 01:13:16.129292  1344 replica.cpp:320] Persisted replica status to VOTING
I0108 01:13:16.129425  1358 recover.cpp:578] Successfully joined the Paxos group
I0108 01:13:16.129680  1358 recover.cpp:462] Recover process terminated
I0108 01:13:16.130187  1358 log.cpp:659] Attempting to start the writer
I0108 01:13:16.131613  1352 replica.cpp:493] Replica received implicit promise request from (7086)@172.17.0.2:32801 with proposal 1
I0108 01:13:16.131983  1352 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 333646ns
I0108 01:13:16.132004  1352 replica.cpp:342] Persisted promised to 1
I0108 01:13:16.132627  1348 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0108 01:13:16.133896  1349 replica.cpp:388] Replica received explicit promise request from (7087)@172.17.0.2:32801 for position 0 with proposal 2
I0108 01:13:16.134289  1349 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 349652ns
I0108 01:13:16.134317  1349 replica.cpp:712] Persisted action at 0
I0108 01:13:16.135470  1351 replica.cpp:537] Replica received write request for position 0 from (7088)@172.17.0.2:32801
I0108 01:13:16.135537  1351 leveldb.cpp:436] Reading position from leveldb took 36181ns
I0108 01:13:16.135901  1351 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 308752ns
I0108 01:13:16.135924  1351 replica.cpp:712] Persisted action at 0
I0108 01:13:16.136529  1347 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0108 01:13:16.136889  1347 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 327106ns
I0108 01:13:16.136916  1347 replica.cpp:712] Persisted action at 0
I0108 01:13:16.136943  1347 replica.cpp:697] Replica learned NOP action at position 0
I0108 01:13:16.137707  1359 log.cpp:675] Writer started with ending position 0
I0108 01:13:16.138844  1348 leveldb.cpp:436] Reading position from leveldb took 31371ns
I0108 01:13:16.139878  1356 registrar.cpp:340] Successfully fetched the registry (0B) in 0ns
I0108 01:13:16.140012  1356 registrar.cpp:439] Applied 1 operations in 42063ns; attempting to update the 'registry'
I0108 01:13:16.140797  1355 log.cpp:683] Attempting to append 170 bytes to the log
I0108 01:13:16.140974  1345 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0108 01:13:16.141744  1354 replica.cpp:537] Replica received write request for position 1 from (7089)@172.17.0.2:32801
I0108 01:13:16.142226  1354 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 441971ns
I0108 01:13:16.142251  1354 replica.cpp:712] Persisted action at 1
I0108 01:13:16.142860  1351 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0108 01:13:16.143198  1351 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 305928ns
I0108 01:13:16.143223  1351 replica.cpp:712] Persisted action at 1
I0108 01:13:16.143241  1351 replica.cpp:697] Replica learned APPEND action at position 1
I0108 01:13:16.144271  1354 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I0108 01:13:16.144435  1354 registrar.cpp:370] Successfully recovered registrar
I0108 01:13:16.144567  1359 log.cpp:702] Attempting to truncate the log to 1
I0108 01:13:16.144780  1359 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0108 01:13:16.144989  1348 hierarchical.cpp:165] Skipping recovery of hierarchical allocator: nothing to recover
I0108 01:13:16.144928  1354 master.cpp:1439] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0108 01:13:16.145690  1357 replica.cpp:537] Replica received write request for position 2 from (7090)@172.17.0.2:32801
I0108 01:13:16.146072  1357 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 345113ns
I0108 01:13:16.146097  1357 replica.cpp:712] Persisted action at 2
I0108 01:13:16.146667  1358 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0108 01:13:16.147060  1358 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 283648ns
I0108 01:13:16.147116  1358 leveldb.cpp:399] Deleting ~1 keys from leveldb took 32174ns
I0108 01:13:16.147135  1358 replica.cpp:712] Persisted action at 2
I0108 01:13:16.147153  1358 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0108 01:13:16.166832  1325 containerizer.cpp:139] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0108 01:13:16.167556  1325 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I0108 01:13:16.170526  1349 slave.cpp:191] Slave started on 231)@172.17.0.2:32801
I0108 01:13:16.170718  1349 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY""
I0108 01:13:16.171269  1349 credentials.hpp:83] Loading credential for authentication from '/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/credential'
I0108 01:13:16.171505  1349 slave.cpp:322] Slave using credential for: test-principal
I0108 01:13:16.171747  1349 resources.cpp:481] Parsing resources as JSON failed: cpus:2;mem:1024;disk(role1):2048
Trying semicolon-delimited string format instead
I0108 01:13:16.172266  1349 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I0108 01:13:16.172327  1349 slave.cpp:400] Slave attributes: [  ]
I0108 01:13:16.172340  1349 slave.cpp:405] Slave hostname: d9632dd1c41e
I0108 01:13:16.172353  1349 slave.cpp:410] Slave checkpoint: true
I0108 01:13:16.173418  1353 state.cpp:58] Recovering state from '/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/meta'
I0108 01:13:16.173521  1325 sched.cpp:164] Version: 0.27.0
I0108 01:13:16.174054  1345 status_update_manager.cpp:200] Recovering status update manager
I0108 01:13:16.174289  1353 containerizer.cpp:387] Recovering containerizer
I0108 01:13:16.174295  1356 sched.cpp:268] New master detected at master@172.17.0.2:32801
I0108 01:13:16.174387  1356 sched.cpp:278] No credentials provided. Attempting to register without authentication
I0108 01:13:16.174409  1356 sched.cpp:722] Sending SUBSCRIBE call to master@172.17.0.2:32801
I0108 01:13:16.174515  1356 sched.cpp:755] Will retry registration in 1.699889272secs if necessary
I0108 01:13:16.174653  1349 master.cpp:2197] Received SUBSCRIBE call for framework 'no-principal' at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.174823  1349 master.cpp:1668] Authorizing framework principal '' to receive offers for role 'role1'
I0108 01:13:16.175250  1347 master.cpp:2268] Subscribing framework no-principal with checkpointing disabled and capabilities [  ]
I0108 01:13:16.175359  1353 slave.cpp:4429] Finished recovery
I0108 01:13:16.175715  1345 hierarchical.cpp:260] Added framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.175734  1351 sched.cpp:649] Framework registered with 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.175792  1345 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.175833  1345 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.175853  1353 slave.cpp:4601] Querying resource estimator for oversubscribable resources
I0108 01:13:16.175869  1345 hierarchical.cpp:1079] Performed allocation for 0 slaves in 127881ns
I0108 01:13:16.175923  1351 sched.cpp:663] Scheduler::registered took 27956ns
I0108 01:13:16.176110  1353 slave.cpp:729] New master detected at master@172.17.0.2:32801
I0108 01:13:16.176187  1353 slave.cpp:792] Authenticating with master master@172.17.0.2:32801
I0108 01:13:16.176216  1353 slave.cpp:797] Using default CRAM-MD5 authenticatee
I0108 01:13:16.176398  1357 status_update_manager.cpp:174] Pausing sending status updates
I0108 01:13:16.176404  1353 slave.cpp:765] Detecting new master
I0108 01:13:16.176463  1358 authenticatee.cpp:121] Creating new client SASL connection
I0108 01:13:16.176553  1353 slave.cpp:4615] Received oversubscribable resources  from the resource estimator
I0108 01:13:16.176709  1353 master.cpp:5445] Authenticating slave(231)@172.17.0.2:32801
I0108 01:13:16.176823  1359 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(516)@172.17.0.2:32801
I0108 01:13:16.177135  1348 authenticator.cpp:98] Creating new server SASL connection
I0108 01:13:16.177373  1356 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0108 01:13:16.177399  1356 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0108 01:13:16.177502  1344 authenticator.cpp:203] Received SASL authentication start
I0108 01:13:16.177563  1344 authenticator.cpp:325] Authentication requires more steps
I0108 01:13:16.177680  1346 authenticatee.cpp:258] Received SASL authentication step
I0108 01:13:16.177848  1354 authenticator.cpp:231] Received SASL authentication step
I0108 01:13:16.177883  1354 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'd9632dd1c41e' server FQDN: 'd9632dd1c41e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0108 01:13:16.177894  1354 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0108 01:13:16.177944  1354 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0108 01:13:16.177994  1354 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'd9632dd1c41e' server FQDN: 'd9632dd1c41e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0108 01:13:16.178014  1354 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0108 01:13:16.178040  1354 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0108 01:13:16.178066  1354 authenticator.cpp:317] Authentication success
I0108 01:13:16.178256  1355 authenticatee.cpp:298] Authentication success
I0108 01:13:16.178315  1354 master.cpp:5475] Successfully authenticated principal 'test-principal' at slave(231)@172.17.0.2:32801
I0108 01:13:16.178356  1355 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(516)@172.17.0.2:32801
I0108 01:13:16.178710  1354 slave.cpp:860] Successfully authenticated with master master@172.17.0.2:32801
I0108 01:13:16.178865  1354 slave.cpp:1254] Will retry registration in 13.009431ms if necessary
I0108 01:13:16.179138  1350 master.cpp:4154] Registering slave at slave(231)@172.17.0.2:32801 (d9632dd1c41e) with id 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0
I0108 01:13:16.179628  1345 registrar.cpp:439] Applied 1 operations in 71663ns; attempting to update the 'registry'
I0108 01:13:16.180505  1356 log.cpp:683] Attempting to append 343 bytes to the log
I0108 01:13:16.180711  1352 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0108 01:13:16.181499  1350 replica.cpp:537] Replica received write request for position 3 from (7103)@172.17.0.2:32801
I0108 01:13:16.182080  1350 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 537757ns
I0108 01:13:16.182112  1350 replica.cpp:712] Persisted action at 3
I0108 01:13:16.182749  1351 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0108 01:13:16.183120  1351 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 340999ns
I0108 01:13:16.183151  1351 replica.cpp:712] Persisted action at 3
I0108 01:13:16.183177  1351 replica.cpp:697] Replica learned APPEND action at position 3
I0108 01:13:16.184787  1348 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I0108 01:13:16.185287  1348 log.cpp:702] Attempting to truncate the log to 3
I0108 01:13:16.185484  1349 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0108 01:13:16.186043  1353 slave.cpp:3371] Received ping from slave-observer(230)@172.17.0.2:32801
I0108 01:13:16.186074  1345 master.cpp:4222] Registered slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I0108 01:13:16.186224  1353 slave.cpp:904] Registered with master master@172.17.0.2:32801; given slave ID 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0
I0108 01:13:16.186441  1353 fetcher.cpp:81] Clearing fetcher cache
I0108 01:13:16.186486  1349 hierarchical.cpp:465] Added slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 (d9632dd1c41e) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000] (allocated: )
I0108 01:13:16.186658  1346 status_update_manager.cpp:181] Resuming sending status updates
I0108 01:13:16.186885  1353 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/PersistentVolumeTest_BadACLNoPrincipal_yqJjLY/meta/slaves/773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0/slave.info'
I0108 01:13:16.186905  1350 replica.cpp:537] Replica received write request for position 4 from (7104)@172.17.0.2:32801
I0108 01:13:16.187595  1350 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 645704ns
I0108 01:13:16.187628  1350 replica.cpp:712] Persisted action at 4
I0108 01:13:16.188347  1349 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.188475  1349 hierarchical.cpp:1101] Performed allocation for slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 in 1.861833ms
I0108 01:13:16.188560  1348 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0108 01:13:16.188385  1353 slave.cpp:963] Forwarding total oversubscribed resources 
I0108 01:13:16.189275  1344 master.cpp:5274] Sending 1 offers to framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.189792  1344 master.cpp:4564] Received update of slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) with total oversubscribed resources 
I0108 01:13:16.189851  1348 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.204958ms
I0108 01:13:16.190150  1348 leveldb.cpp:399] Deleting ~2 keys from leveldb took 62381ns
I0108 01:13:16.190265  1348 replica.cpp:712] Persisted action at 4
I0108 01:13:16.190402  1348 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0108 01:13:16.191192  1349 sched.cpp:819] Scheduler::resourceOffers took 126783ns
I0108 01:13:16.191253  1359 hierarchical.cpp:521] Slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 (d9632dd1c41e) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048)
I0108 01:13:16.191529  1359 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.191591  1359 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.191627  1359 hierarchical.cpp:1101] Performed allocation for slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 in 310808ns
I0108 01:13:16.195103  1349 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.195171  1349 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.195205  1349 hierarchical.cpp:1079] Performed allocation for 1 slaves in 368834ns
I0108 01:13:16.205402  1351 master.cpp:3055] Processing ACCEPT call for offers: [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-O0 ] on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.205471  1351 master.cpp:2843] Authorizing principal 'ANY' to create volumes
E0108 01:13:16.206641  1351 master.cpp:1737] Dropping CREATE offer operation from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801: Not authorized to create persistent volumes as ''
I0108 01:13:16.207283  1351 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.216485  1348 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.216562  1348 hierarchical.cpp:1079] Performed allocation for 1 slaves in 983574ns
I0108 01:13:16.216915  1345 master.cpp:5274] Sending 1 offers to framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.217514  1345 sched.cpp:819] Scheduler::resourceOffers took 82354ns
I0108 01:13:16.227466  1348 master.cpp:3592] Processing DECLINE call for offers: [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-O1 ] for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.227843  1325 sched.cpp:164] Version: 0.27.0
I0108 01:13:16.228489  1344 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.228989  1346 sched.cpp:268] New master detected at master@172.17.0.2:32801
I0108 01:13:16.229118  1346 sched.cpp:278] No credentials provided. Attempting to register without authentication
I0108 01:13:16.229143  1346 sched.cpp:722] Sending SUBSCRIBE call to master@172.17.0.2:32801
I0108 01:13:16.229277  1346 sched.cpp:755] Will retry registration in 1.383902465secs if necessary
I0108 01:13:16.229912  1348 master.cpp:2650] Processing SUPPRESS call for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.230171  1346 hierarchical.cpp:953] Suppressed offers for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.230262  1348 master.cpp:2197] Received SUBSCRIBE call for framework 'default' at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.230370  1348 master.cpp:1668] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0108 01:13:16.230788  1348 master.cpp:2268] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0108 01:13:16.231477  1346 hierarchical.cpp:260] Added framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:16.232698  1346 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.232795  1346 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.282992ms
I0108 01:13:16.233512  1348 master.cpp:5274] Sending 1 offers to framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.233728  1351 sched.cpp:649] Framework registered with 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:16.233800  1351 sched.cpp:663] Scheduler::registered took 29498ns
I0108 01:13:16.234381  1359 sched.cpp:819] Scheduler::resourceOffers took 113212ns
I0108 01:13:16.239941  1348 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.240223  1348 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.240275  1348 hierarchical.cpp:1079] Performed allocation for 1 slaves in 633949ns
I0108 01:13:16.251688  1357 master.cpp:3055] Processing ACCEPT call for offers: [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-O2 ] on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e) for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.251785  1357 master.cpp:2843] Authorizing principal 'test-principal' to create volumes
I0108 01:13:16.253445  1352 master.cpp:3384] Applying CREATE operation for volumes disk(role1)[id1:path1]:128 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801 to slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e)
I0108 01:13:16.253911  1352 master.cpp:6508] Sending checkpointed resources disk(role1)[id1:path1]:128 to slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 at slave(231)@172.17.0.2:32801 (d9632dd1c41e)
I0108 01:13:16.255210  1352 slave.cpp:2277] Updated checkpointed resources from  to disk(role1)[id1:path1]:128
I0108 01:13:16.257128  1356 hierarchical.cpp:642] Updated allocation of framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 to cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128
I0108 01:13:16.257844  1356 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:16.262976  1344 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.263068  1344 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.435723ms
I0108 01:13:16.263535  1353 master.cpp:5274] Sending 1 offers to framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.264181  1356 sched.cpp:819] Scheduler::resourceOffers took 139353ns
I0108 01:13:16.271931  1355 master.cpp:3671] Processing REVIVE call for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:16.272141  1359 hierarchical.cpp:973] Removed offer filters for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:16.272177  1355 master.cpp:3592] Processing DECLINE call for offers: [ 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-O3 ] for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.272423  1359 hierarchical.cpp:1329] No resources available to allocate!
I0108 01:13:16.272483  1359 hierarchical.cpp:1423] No inverse offers to send out!
I0108 01:13:16.272514  1359 hierarchical.cpp:1079] Performed allocation for 1 slaves in 344563ns
I0108 01:13:16.272924  1355 master.cpp:2650] Processing SUPPRESS call for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:16.272989  1359 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0 from framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:16.273309  1359 hierarchical.cpp:953] Suppressed offers for framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
2016-01-08 01:13:18,959:1325(0x7fb7cd6ae700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50826] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2016-01-08 01:13:22,295:1325(0x7fb7cd6ae700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50826] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2016-01-08 01:13:25,631:1325(0x7fb7cd6ae700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50826] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2016-01-08 01:13:28,968:1325(0x7fb7cd6ae700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50826] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
../../src/tests/persistent_volume_tests.cpp:1211: Failure
Failed to wait 15secs for offers
I0108 01:13:31.277577  1354 master.cpp:1130] Framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801 disconnected
I../../src/tests/persistent_volume_tests.cpp:1204: Failure
Actual function call count doesn't match EXPECT_CALL(sched1, resourceOffers(&driver1, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
0108 01:13:31.277909  1354 master.cpp:2493] Disconnecting framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:31.279088  1354 master.cpp:2517] Deactivating framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801
I0108 01:13:31.279496  1354 master.cpp:1154] Giving framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001 (default) at scheduler-4eccbeb6-560d-41be-b1d6-1e2971db4bb3@172.17.0.2:32801 0ns to failover
I0108 01:13:31.280046  1354 master.cpp:1130] Framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801 disconnected
I0108 01:13:31.280603  1354 master.cpp:2493] Disconnecting framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:31.280644  1354 master.cpp:2517] Deactivating framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801
I0108 01:13:31.280863  1354 master.cpp:1154] Giving framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000 (no-principal) at scheduler-bf0ed267-b4c4-412d-9fb0-84c85cd2fbce@172.17.0.2:32801 0ns to failover
I0108 01:13:31.280563  1348 hierarchical.cpp:366] Deactivated framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:31.281056  1348 hierarchical.cpp:366] Deactivated framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:31.281097  1354 master.cpp:930] Master terminating
I0108 01:13:31.281910  1355 hierarchical.cpp:496] Removed slave 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-S0
I0108 01:13:31.282516  1352 hierarchical.cpp:321] Removed framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0001
I0108 01:13:31.282817  1352 hierarchical.cpp:321] Removed framework 773d31e8-383d-4e4b-aa68-f9a3fb9f1fc2-0000
I0108 01:13:31.282985  1352 slave.cpp:3417] master@172.17.0.2:32801 exited
W0108 01:13:31.283144  1352 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected
I0108 01:13:31.313812  1346 slave.cpp:601] Slave terminating
[  FAILED  ] PersistentVolumeTest.BadACLNoPrincipal (15203 ms)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-09 01:19:36.323,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 09 01:19:36 UTC 2016,,,,,,,"0|i2r227:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 26,,,,,,,,,,,1.0,,,,,,,,,,,"09/Jan/16 01:19;greggomann;Review here: https://reviews.apache.org/r/42096/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Accepting an inverse offer prints misleading logs,MESOS-4301,12928056,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Won't Fix,kaysoky,kaysoky,kaysoky,06/Jan/16 16:21,26/Nov/18 12:32,29/Oct/20 16:32,26/Nov/18 12:32,0.25.0,,,,,,,,,,,,,,master,,,,,0,log,maintenance,mesosphere,,,,,,"Whenever a scheduler accepts an inverse offer, Mesos will print a line like this in the master logs:
{code}
W1125 10:05:53.155109 29362 master.cpp:2897] ACCEPT call used invalid offers '[ 932f7d7b-f2d4-42c7-9391-222c19b9d35b-O2 ]': Offer 932f7d7b-f2d4-42c7-9391-222c19b9d35b-O2 is no longer valid
{code}

Inverse offers should not trigger this warning.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4385,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-1474,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 19 21:44:10 UTC 2016,,,,,,,"0|i2qxtz:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 26,,,,,,,,,,,1.0,,,,,,,,,,,"08/Jan/16 23:05;kaysoky;While fixing this log line, found another bug.

Essentially:
# {{validation::offer::validate}} returns an error when an {{InverseOffer}} is accepted.
# If an {{Offer}} is part of the same {{Call::ACCEPT}}, the master sees {{error.isSome()}} and returns a {{TASK_LOST}} for normal offers.  (https://github.com/apache/mesos/blob/fafbdca610d0a150b9fa9cb62d1c63cb7a6fdaf3/src/master/master.cpp#L3117)

Regression test:
https://reviews.apache.org/r/42092/ (discarded)","08/Jan/16 23:25;kaysoky;Review to:
* Fix the logging.
* Fix the bug found above.
* Refactor {{Master::accept}} to read more sequentially.

https://reviews.apache.org/r/42086/ (discarded)","14/Jan/16 21:34;kaysoky;The plan is to:
* Fix the misleading log message here (for 0.27)
* Defer the decision for mixing {{Offers}} and {{InverseOffers}} for a future release.  This means keeping the above ""bug"" in-tact.  (Note: The current behavior is effectively prohibiting the mixing of offer types.)","14/Jan/16 22:41;kaysoky;Minimal patch just to silence the log line:
https://reviews.apache.org/r/42324/","19/Jan/16 21:44;kaysoky;We (BenM & I) decided it was not worthwhile to silence the log line without also fixing the underlying semantics that go along with mixing offers & inverse offers.  

The more comprehensive fix is tracked here: [MESOS-4385]",,,,,,,,,,,,,,,,,,,,,,,
fs::enter(rootfs) does not work if 'rootfs' is read only.,MESOS-4291,12927150,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,jieyu,jieyu,05/Jan/16 01:06,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,0.28.0,,,,,,,,,,,0,mesosphere,unified-containerizer-mvp,,,,,,,"I noticed this when I was testing the unified containerizer with the bind mount backend and no volumes.

The current implementation of fs::enter will put the old root under /tmp/._old_root_.XXXXXX in the new rootfs. It assumes that /tmp is writable in the new rootfs, but this might not be true, especially if the bind mount backend is used.

To solve the problem, what we can do is to mount tmpfs to /tmp in the new rootfs and umount it after pivot_root.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2386,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 23 23:13:12 UTC 2016,,,,,,,"0|hzzzqe:zi",9223372036854775807,,,,,idownes,,,,,,Mesosphere Sprint 26,Mesosphere Sprint 27,Mesosphere Sprint 29,,,,,,,,,2.0,,,,,,,,,,,"23/Feb/16 23:13;jieyu;commit 14f070fda25c98c0a8ba29da84c607f2dd86da6a
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Tue Feb 23 10:49:11 2016 -0800

    Removed the restriction that /tmp needs to be writable in new rootfs.
    
    Review: https://reviews.apache.org/r/43896",,,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos command task doesn't support volumes with image,MESOS-4285,12927070,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,tnachen,tnachen,tnachen,04/Jan/16 20:13,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.28.0,,,,,,containerization,,,,,0,mesosphere,unified-containerizer-mvp,,,,,,,Currently volumes are stripped when an image is specified running a command task with Mesos containerizer. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2386,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 08 17:28:31 UTC 2016,,,,,,,"0|hzzzqe:z",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 26,Mesosphere Sprint 27,Mesosphere Sprint 28,,,,,,,,,3.0,,,,,,,,,,,"14/Jan/16 00:16;tnachen;https://reviews.apache.org/r/42278/","08/Feb/16 17:28;tnachen;commit 4c9e3f419d9e74dce9a84a8f6f140dd4631bf0c0
Author: Timothy Chen <tnachen@apache.org>
Date:   Sun Feb 7 17:42:37 2016 +0800

    Fixed volume paths for command tasks with image.

    Review: https://reviews.apache.org/r/42278/
",,,,,,,,,,,,,,,,,,,,,,,,,,
Update isolator prepare function to use ContainerLaunchInfo,MESOS-4282,12927019,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Duplicate,gilbert,gilbert,gilbert,04/Jan/16 17:16,26/Nov/18 12:35,29/Oct/20 16:32,26/Nov/18 12:35,,,,,,,,,,,,,,,containerization,,,,,0,mesosphere,unified-containerizer-mvp,,,,,,,"Currently we have the isolator's prepare function returning ContainerPrepareInfo protobuf. We should enable ContainerLaunchInfo (contains environment variables, namespaces, etc.) to be returned which will be used by Mesos containerize to launch containers. 

By doing this (ContainerPrepareInfo -> ContainerLaunchInfo), we can select any necessary information and passing then to launcher.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-15 00:32:44.528,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 15 04:20:23 UTC 2016,,,,,,,"0|i2qrfr:",9223372036854775807,,,,,jieyu,,,,,,,,,,,,,,,,,2.0,,0.27.0,,,,,,,,,"15/Jan/16 00:32;qianzhang;Should this one be part of the epic ""Unified Container""?","15/Jan/16 00:36;gilbert;Forget to label it. Thank you, Qian!","15/Jan/16 00:39;gilbert;We target this issue for version 0.27, because we changed the slave isolator interface in MESOS-4240, and we do not want to break the slave isolator twice. Information will be included in upgrade note.","15/Jan/16 04:20;gilbert;https://reviews.apache.org/r/42331/
https://reviews.apache.org/r/42276/",,,,,,,,,,,,,,,,,,,,,,,,
Correctly handle disk quota usage when volumes are bind mounted into the container.,MESOS-4281,12927016,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hartem,hartem,hartem,04/Jan/16 16:59,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.27.0,,,,,,,,,,,0,mesosphere,,,,,,,,In its current implementation disk quota enforcement on the task sandbox will not work correctly when disk volumes are bind mounted into the task sandbox (this happens when Linux filesystem isolator is used).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-26 22:00:55.485,,,false,MESOS-1554,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 26 22:00:55 UTC 2016,,,,,,,"0|hzzzr2:zzr",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 26,Mesosphere Sprint 27,,,,,,,,,,3.0,,0.27.0,,,,,,,,,"12/Jan/16 23:51;hartem;https://reviews.apache.org/r/41818/","26/Jan/16 22:00;jieyu;commit 7ba659cdccb29365b79e7ed62ce435fd1cb91920
Author: Artem Harutyunyan <artem@mesosphere.io>
Date:   Tue Jan 26 11:04:09 2016 -0800

    Handled quota when volumes are bind mounted into the sandbox.
    
    Review: https://reviews.apache.org/r/41818/",,,,,,,,,,,,,,,,,,,,,,,,,,
Report volume usage through ResourceStatistics.,MESOS-4263,12924966,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,hartem,hartem,30/Dec/15 23:14,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,1.3.0,,,,,,,,,,,0,mesosphere,,,,,,,,POSIX disk isolator does not currently report volume usage through ResourceStatistics. {{PosixDiskIsolatorProcess::usage()}} should be amended to take into account volume usage as well. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-04-22 02:49:29.256,,,false,MESOS-7230,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 26 21:06:57 UTC 2017,,,,,,,"0|i2qf5r:",9223372036854775807,,,,,jieyu,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"22/Apr/17 02:49;jieyu;https://reviews.apache.org/r/58641/
https://reviews.apache.org/r/58642/
https://reviews.apache.org/r/58643/
https://reviews.apache.org/r/58644/
https://reviews.apache.org/r/58645/
https://reviews.apache.org/r/58646/","26/Apr/17 21:06;jieyu;commit 17139c5e57a1a8b153c7f2584eb7f28922859825
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Apr 21 19:46:24 2017 -0700

    Fixed a style issue in v1 mesos.proto.

    Review: https://reviews.apache.org/r/58646

commit df5038f254d34e9bac16372e4d091811ffb52f64
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Apr 21 19:45:52 2017 -0700

    Updated the tests to verify persistent volume usage.

    Review: https://reviews.apache.org/r/58645

commit 62ea5ad976ce243ea6f9989d2fda1b633934744f
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Apr 21 19:44:43 2017 -0700

    Improved POSIX disk isolator to report usage for persistent volumes.

    Review: https://reviews.apache.org/r/58644

commit dff76aa4f639a19052e723d522f7057229e43fad
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Apr 21 17:26:39 2017 -0700

    Added a TODO about an issue in POSIX disk isolator.

    The 'exclusive' list might not be accurate if a new persistent volumes
    is later added to the resources list. That might result in incorrect
    sandbox usage being reported.

    Review: https://reviews.apache.org/r/58643

commit c0cfacfb134f74ff35f48e36af2f914e5530057d
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Apr 21 15:24:10 2017 -0700

    Cancelled the disk usage collection if no longer needed.

    We actively cancel the disk usage collection if its result is no
    longer needed. This is a performance optimization.

    Review: https://reviews.apache.org/r/58642

commit d680ff65e79d1d31de2cdc3ee70401c44fb2ac63
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Apr 21 13:28:47 2017 -0700

    Fixed a style issue in mesos.proto.

    Review: https://reviews.apache.org/r/58641",,,,,,,,,,,,,,,,,,,,,,,,,,
Add mechanism for testing recovery of HTTP based executors,MESOS-4255,12924605,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,anandmazumdar,anandmazumdar,29/Dec/15 03:51,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,0.28.0,,,,,,,,,,,0,mesosphere,,,,,,,,"Currently, the slave process generates a process ID every time it is initialized via {{process::ID::generate}} function call. This is a problem for testing HTTP executors as it can't retry if there is a disconnection after an agent restart since the prefix is incremented. 

{code}
Agent PID before:
slave(1)@127.0.0.1:43915

Agent PID after restart:
slave(2)@127.0.0.1:43915
{code}

There are a couple of ways to fix this:
- Add a constructor to {{Slave}} exclusively for testing that passes on a fixed {{ID}} instead of relying on {{ID::generate}}.
- Currently we delegate to slave(1)@ i.e. (1) when nothing is specified as the URL in libprocess i.e. {{127.0.0.1:43915/api/v1/executor}} would delegate to {{slave(1)@127.0.0.1:43915/api/v1/executor}}. Instead of defaulting to (1), we can default to the last known active ID.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-12 07:01:20.933,,,false,MESOS-4793,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 04 22:44:15 UTC 2016,,,,,,,"0|hzzzy6:t9i",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 28,,,,,,,,,,,5.0,,0.28.0,,,,,,,,,"12/Jan/16 07:01;tnachen;Anand is this going to make 0.27.0? Let me know before end of wednesday or I'll remove it myself.","12/Jan/16 09:25;anandmazumdar;Review Chain: https://reviews.apache.org/r/42181/","04/Feb/16 22:44;vinodkone;commit 2dd8dc85a1443b2192c548cabeab41aad6da3e17
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Thu Feb 4 14:41:42 2016 -0800

    Added tests for recovery for HTTP based executors.
    
    Review: https://reviews.apache.org/r/42186/

commit 8bb8667602b78da74374e669a6ebfdb8e5d56972
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Thu Feb 4 14:41:33 2016 -0800

    Added an example executor based on the new V1 API.
    
    This change adds a custom executor based on the new executor library.
    
    Review: https://reviews.apache.org/r/42185/

commit e3c3cf88688d08db5699d5c70616702d92c361e5
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Thu Feb 4 14:41:15 2016 -0800

    Drop `404 NotFound` responses in the executor library.
    
    Previously, we did not use to drop `404 NotFound` responses in the library and
    send `Event::Error` to executor. However, this can be trigerred upon an agent
    restart when it has not yet set up HTTP routes.
    
    Review: https://reviews.apache.org/r/42844/

commit 4a211e5f9f95dbffb819d5802ffcb8617be202b8
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Thu Feb 4 14:40:56 2016 -0800

    Modified existing usage of Slave constructor.
    
    This change adds the argument process ID wherever the `Slave` object is
    constructed.
    
    Review: https://reviews.apache.org/r/43131/

commit d1068c17981d04aa621ec557c73f2d060c26e2fe
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Thu Feb 4 14:39:47 2016 -0800

    Added a process ID argument to Slave constructor.
    
    This change modifies the existing `Slave` constructor to take in the process ID
    string as an argument. This is necessary for testing recovery of HTTP based
    executors. (Previously, every invocation of `StartSlave` used to generate a new
    ID via `ID::generate`). There was no process delegate set via
    `process::initialize` that led to problems for the existing HTTP based executor
    to connect back to the slave.
    
    Also, modified the tests to introduce a new `StartSlave` function that takes this process ID as argument.
    
    Review: https://reviews.apache.org/r/42181/

",,,,,,,,,,,,,,,,,,,,,,,,,
Add `dist` target to CMake solution,MESOS-4245,12924092,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,srbrahma,hausdorff,hausdorff,23/Dec/15 21:33,29/Apr/19 09:27,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,1.3.0,,,,,,cmake,,,,,0,cmake,mesosphere,microsoft,windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-18 01:12:46.113,,,false,MESOS-898,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 28 22:37:02 UTC 2017,,,,,,,"0|i2q9uf:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 52,,,,,,,,,,,3.0,,,,,,,,,,,"18/Jan/17 01:12;srbrahma;https://reviews.apache.org/r/55657/","28/Feb/17 22:37;kaysoky;{code}
commit 3eccf7e66ea4a76d869b6e726cfac7d91115976a
Author: Srinivas Brahmaroutu <srbrahma@us.ibm.com>
Date:   Mon Feb 27 17:33:10 2017 -0800

    CMake: Added CPack support for generating packages.
    
    The inclusion of CPack adds two new build targets:
      * `package` will generate a binary installer for the current platform.
        This includes everything installed via CMake's install command.
      * `package_source` will generate a source installer for the current
        platform.  This includes all the source files.
    
    We also expose a variety of CPack configuration options to the user,
    which are mostly disabled by default.  By default, the `package_source`
    target will only generate a `.tar.gz` of the sources.  The `package`
    target will not generate anything.
    
    Review: https://reviews.apache.org/r/55657/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Test for Quota Status Endpoint,MESOS-4218,12923483,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,js84,js84,js84,21/Dec/15 17:10,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.27.0,,,,,,,,,,,0,mesosphere,quota,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4013,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-06 01:52:46.171,,,false,MESOS-3982,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 06 01:52:46 UTC 2016,,,,,,,"0|i2q63b:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 26,,,,,,,,,,,3.0,,0.27.0,,,,,,,,,"21/Dec/15 17:11;js84;https://reviews.apache.org/r/39614/","06/Jan/16 01:52;jvanremoortere;{code}
commit bcf33f896b7f02d615e8edb9441cfbb96e3a5152
Author: Joerg Schad <joerg@mesosphere.io>
Date:   Tue Jan 5 17:30:01 2016 -0800

    Quota: Added status endpoint validation tests.
    
    Review: https://reviews.apache.org/r/39614/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
PersistentVolumeTest.BadACLDropCreateAndDestroy is flaky,MESOS-4208,12923241,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,greggomann,jieyu,jieyu,19/Dec/15 18:18,11/Jan/16 00:42,29/Oct/20 16:32,09/Jan/16 01:25,,,,,,,,,,,,,,,,,,,,0,flaky-test,mesosphere,persistent-volumes,,,,,,"{noformat}
[ RUN      ] PersistentVolumeTest.BadACLDropCreateAndDestroy
I1219 09:51:32.623245 31878 leveldb.cpp:174] Opened db in 4.393596ms
I1219 09:51:32.624084 31878 leveldb.cpp:181] Compacted db in 709447ns
I1219 09:51:32.624186 31878 leveldb.cpp:196] Created db iterator in 21252ns
I1219 09:51:32.624290 31878 leveldb.cpp:202] Seeked to beginning of db in 11391ns
I1219 09:51:32.624378 31878 leveldb.cpp:271] Iterated through 0 keys in the db in 611ns
I1219 09:51:32.624505 31878 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1219 09:51:32.625195 31904 recover.cpp:447] Starting replica recovery
I1219 09:51:32.625641 31904 recover.cpp:473] Replica is in EMPTY status
I1219 09:51:32.627305 31904 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (6740)@172.17.0.3:36408
I1219 09:51:32.627749 31904 recover.cpp:193] Received a recover response from a replica in EMPTY status
I1219 09:51:32.628330 31904 recover.cpp:564] Updating replica status to STARTING
I1219 09:51:32.629068 31906 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 410494ns
I1219 09:51:32.629169 31906 replica.cpp:320] Persisted replica status to STARTING
I1219 09:51:32.629598 31906 recover.cpp:473] Replica is in STARTING status
I1219 09:51:32.630782 31912 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (6741)@172.17.0.3:36408
I1219 09:51:32.631166 31901 recover.cpp:193] Received a recover response from a replica in STARTING status
I1219 09:51:32.632467 31902 recover.cpp:564] Updating replica status to VOTING
I1219 09:51:32.633600 31907 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 311370ns
I1219 09:51:32.633627 31907 replica.cpp:320] Persisted replica status to VOTING
I1219 09:51:32.633719 31907 recover.cpp:578] Successfully joined the Paxos group
I1219 09:51:32.633874 31907 recover.cpp:462] Recover process terminated
I1219 09:51:32.636409 31909 master.cpp:365] Master bded856d-1c7f-4fad-a8bc-3629ba8c59d3 (60ab6e727501) started on 172.17.0.3:36408
I1219 09:51:32.636593 31909 master.cpp:367] Flags at startup: --acls=""create_volumes {
  principals {
    values: ""creator-principal""
  }
  volume_types {
    type: ANY
  }
}
create_volumes {
  principals {
    type: ANY
  }
  volume_types {
    type: NONE
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/SpPF7B/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role1"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/_inst/share/mesos/webui"" --work_dir=""/tmp/SpPF7B/master"" --zk_session_timeout=""10secs""
I1219 09:51:32.637055 31909 master.cpp:414] Master allowing unauthenticated frameworks to register
I1219 09:51:32.637068 31909 master.cpp:417] Master only allowing authenticated slaves to register
I1219 09:51:32.637094 31909 credentials.hpp:35] Loading credentials for authentication from '/tmp/SpPF7B/credentials'
I1219 09:51:32.637403 31909 master.cpp:456] Using default 'crammd5' authenticator
I1219 09:51:32.637555 31909 master.cpp:493] Authorization enabled
W1219 09:51:32.637575 31909 master.cpp:553] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information
I1219 09:51:32.637806 31897 whitelist_watcher.cpp:77] No whitelist given
I1219 09:51:32.637820 31910 hierarchical.cpp:147] Initialized hierarchical allocator process
I1219 09:51:32.639677 31909 master.cpp:1629] The newly elected leader is master@172.17.0.3:36408 with id bded856d-1c7f-4fad-a8bc-3629ba8c59d3
I1219 09:51:32.639768 31909 master.cpp:1642] Elected as the leading master!
I1219 09:51:32.639892 31909 master.cpp:1387] Recovering from registrar
I1219 09:51:32.640136 31907 registrar.cpp:307] Recovering registrar
I1219 09:51:32.640929 31901 log.cpp:659] Attempting to start the writer
I1219 09:51:32.642199 31912 replica.cpp:493] Replica received implicit promise request from (6742)@172.17.0.3:36408 with proposal 1
I1219 09:51:32.642719 31912 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 445876ns
I1219 09:51:32.642755 31912 replica.cpp:342] Persisted promised to 1
I1219 09:51:32.643478 31904 coordinator.cpp:238] Coordinator attempting to fill missing positions
I1219 09:51:32.645009 31909 replica.cpp:388] Replica received explicit promise request from (6743)@172.17.0.3:36408 for position 0 with proposal 2
I1219 09:51:32.645356 31909 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 310064ns
I1219 09:51:32.645382 31909 replica.cpp:712] Persisted action at 0
I1219 09:51:32.646662 31909 replica.cpp:537] Replica received write request for position 0 from (6744)@172.17.0.3:36408
I1219 09:51:32.646721 31909 leveldb.cpp:436] Reading position from leveldb took 29298ns
I1219 09:51:32.647047 31909 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 283424ns
I1219 09:51:32.647073 31909 replica.cpp:712] Persisted action at 0
I1219 09:51:32.647722 31909 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I1219 09:51:32.648052 31909 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 300825ns
I1219 09:51:32.648077 31909 replica.cpp:712] Persisted action at 0
I1219 09:51:32.648095 31909 replica.cpp:697] Replica learned NOP action at position 0
I1219 09:51:32.655295 31899 log.cpp:675] Writer started with ending position 0
I1219 09:51:32.656543 31905 leveldb.cpp:436] Reading position from leveldb took 32788ns
I1219 09:51:32.658164 31905 registrar.cpp:340] Successfully fetched the registry (0B) in 0ns
I1219 09:51:32.658604 31905 registrar.cpp:439] Applied 1 operations in 38183ns; attempting to update the 'registry'
I1219 09:51:32.660102 31905 log.cpp:683] Attempting to append 170 bytes to the log
I1219 09:51:32.660538 31906 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I1219 09:51:32.661872 31906 replica.cpp:537] Replica received write request for position 1 from (6745)@172.17.0.3:36408
I1219 09:51:32.662719 31906 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 483018ns
I1219 09:51:32.663054 31906 replica.cpp:712] Persisted action at 1
I1219 09:51:32.664008 31902 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I1219 09:51:32.664330 31902 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 287310ns
I1219 09:51:32.664355 31902 replica.cpp:712] Persisted action at 1
I1219 09:51:32.664376 31902 replica.cpp:697] Replica learned APPEND action at position 1
I1219 09:51:32.665365 31902 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I1219 09:51:32.665493 31902 registrar.cpp:370] Successfully recovered registrar
I1219 09:51:32.665894 31902 master.cpp:1439] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I1219 09:51:32.665990 31902 hierarchical.cpp:165] Skipping recovery of hierarchical allocator: nothing to recover
I1219 09:51:32.666266 31902 log.cpp:702] Attempting to truncate the log to 1
I1219 09:51:32.666424 31902 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I1219 09:51:32.667181 31907 replica.cpp:537] Replica received write request for position 2 from (6746)@172.17.0.3:36408
I1219 09:51:32.667768 31907 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 335947ns
I1219 09:51:32.668067 31907 replica.cpp:712] Persisted action at 2
I1219 09:51:32.668942 31906 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I1219 09:51:32.669240 31906 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 266566ns
I1219 09:51:32.669292 31906 leveldb.cpp:399] Deleting ~1 keys from leveldb took 27852ns
I1219 09:51:32.669314 31906 replica.cpp:712] Persisted action at 2
I1219 09:51:32.669334 31906 replica.cpp:697] Replica learned TRUNCATE action at position 2
I1219 09:51:32.691251 31878 containerizer.cpp:141] Using isolation: posix/cpu,posix/mem,filesystem/posix
W1219 09:51:32.691759 31878 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1219 09:51:32.697428 31901 slave.cpp:191] Slave started on 228)@172.17.0.3:36408
I1219 09:51:32.697459 31901 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc""
I1219 09:51:32.697963 31901 credentials.hpp:83] Loading credential for authentication from '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/credential'
I1219 09:51:32.698210 31901 slave.cpp:322] Slave using credential for: test-principal
I1219 09:51:32.698449 31901 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:1024;disk(role1):2048
Trying semicolon-delimited string format instead
I1219 09:51:32.699065 31901 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I1219 09:51:32.699137 31901 slave.cpp:400] Slave attributes: [  ]
I1219 09:51:32.699151 31901 slave.cpp:405] Slave hostname: 60ab6e727501
I1219 09:51:32.699161 31901 slave.cpp:410] Slave checkpoint: true
I1219 09:51:32.699364 31878 sched.cpp:164] Version: 0.27.0
I1219 09:51:32.700614 31911 sched.cpp:262] New master detected at master@172.17.0.3:36408
I1219 09:51:32.700703 31911 sched.cpp:272] No credentials provided. Attempting to register without authentication
I1219 09:51:32.700724 31911 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.0.3:36408
I1219 09:51:32.700839 31911 sched.cpp:747] Will retry registration in 620.399428ms if necessary
I1219 09:51:32.701244 31903 master.cpp:2197] Received SUBSCRIBE call for framework 'default' at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.701313 31903 master.cpp:1668] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I1219 09:51:32.701625 31903 master.cpp:2268] Subscribing framework default with checkpointing disabled and capabilities [  ]
I1219 09:51:32.702308 31903 hierarchical.cpp:260] Added framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.702386 31903 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.702422 31903 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.702448 31903 hierarchical.cpp:1079] Performed allocation for 0 slaves in 114358ns
I1219 09:51:32.702638 31903 sched.cpp:641] Framework registered with bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.702688 31903 sched.cpp:655] Scheduler::registered took 25558ns
I1219 09:51:32.703553 31901 state.cpp:58] Recovering state from '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/meta'
I1219 09:51:32.704118 31897 status_update_manager.cpp:200] Recovering status update manager
I1219 09:51:32.704407 31907 containerizer.cpp:383] Recovering containerizer
I1219 09:51:32.705373 31907 slave.cpp:4427] Finished recovery
I1219 09:51:32.705991 31907 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1219 09:51:32.706277 31907 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1219 09:51:32.706666 31907 slave.cpp:729] New master detected at master@172.17.0.3:36408
I1219 09:51:32.706738 31907 slave.cpp:792] Authenticating with master master@172.17.0.3:36408
I1219 09:51:32.706760 31907 slave.cpp:797] Using default CRAM-MD5 authenticatee
I1219 09:51:32.706886 31899 status_update_manager.cpp:174] Pausing sending status updates
I1219 09:51:32.706941 31907 slave.cpp:765] Detecting new master
I1219 09:51:32.707036 31899 authenticatee.cpp:121] Creating new client SASL connection
I1219 09:51:32.707291 31910 master.cpp:5423] Authenticating slave(228)@172.17.0.3:36408
I1219 09:51:32.707479 31910 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(510)@172.17.0.3:36408
I1219 09:51:32.707849 31910 authenticator.cpp:98] Creating new server SASL connection
I1219 09:51:32.708082 31910 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I1219 09:51:32.708112 31910 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I1219 09:51:32.708196 31910 authenticator.cpp:203] Received SASL authentication start
I1219 09:51:32.708395 31910 authenticator.cpp:325] Authentication requires more steps
I1219 09:51:32.708611 31902 authenticatee.cpp:258] Received SASL authentication step
I1219 09:51:32.708773 31910 authenticator.cpp:231] Received SASL authentication step
I1219 09:51:32.708889 31910 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '60ab6e727501' server FQDN: '60ab6e727501' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1219 09:51:32.708976 31910 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I1219 09:51:32.709096 31910 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1219 09:51:32.709200 31910 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '60ab6e727501' server FQDN: '60ab6e727501' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1219 09:51:32.709285 31910 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1219 09:51:32.709363 31910 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1219 09:51:32.709452 31910 authenticator.cpp:317] Authentication success
I1219 09:51:32.709707 31910 authenticatee.cpp:298] Authentication success
I1219 09:51:32.710252 31910 slave.cpp:860] Successfully authenticated with master master@172.17.0.3:36408
I1219 09:51:32.710525 31910 slave.cpp:1254] Will retry registration in 17.44437ms if necessary
I1219 09:51:32.709839 31908 master.cpp:5453] Successfully authenticated principal 'test-principal' at slave(228)@172.17.0.3:36408
I1219 09:51:32.710985 31908 master.cpp:4132] Registering slave at slave(228)@172.17.0.3:36408 (60ab6e727501) with id bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0
I1219 09:51:32.711645 31908 registrar.cpp:439] Applied 1 operations in 83191ns; attempting to update the 'registry'
I1219 09:51:32.709908 31912 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(510)@172.17.0.3:36408
I1219 09:51:32.713407 31908 log.cpp:683] Attempting to append 343 bytes to the log
I1219 09:51:32.713646 31912 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I1219 09:51:32.714884 31911 replica.cpp:537] Replica received write request for position 3 from (6758)@172.17.0.3:36408
I1219 09:51:32.715221 31911 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 288909ns
I1219 09:51:32.715250 31911 replica.cpp:712] Persisted action at 3
I1219 09:51:32.716145 31912 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I1219 09:51:32.716689 31912 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 512217ns
I1219 09:51:32.716716 31912 replica.cpp:712] Persisted action at 3
I1219 09:51:32.716737 31912 replica.cpp:697] Replica learned APPEND action at position 3
I1219 09:51:32.718426 31911 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I1219 09:51:32.719441 31902 slave.cpp:3371] Received ping from slave-observer(228)@172.17.0.3:36408
I1219 09:51:32.719843 31909 log.cpp:702] Attempting to truncate the log to 3
I1219 09:51:32.719908 31911 master.cpp:4200] Registered slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I1219 09:51:32.720064 31911 slave.cpp:904] Registered with master master@172.17.0.3:36408; given slave ID bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0
I1219 09:51:32.720088 31911 fetcher.cpp:81] Clearing fetcher cache
I1219 09:51:32.720491 31911 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/meta/slaves/bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0/slave.info'
I1219 09:51:32.720844 31909 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I1219 09:51:32.720929 31911 slave.cpp:963] Forwarding total oversubscribed resources 
I1219 09:51:32.721017 31903 status_update_manager.cpp:181] Resuming sending status updates
I1219 09:51:32.721099 31911 master.cpp:4542] Received update of slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) with total oversubscribed resources 
I1219 09:51:32.721141 31905 hierarchical.cpp:465] Added slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 (60ab6e727501) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000] (allocated: )
I1219 09:51:32.721879 31911 replica.cpp:537] Replica received write request for position 4 from (6759)@172.17.0.3:36408
I1219 09:51:32.722293 31905 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.722337 31905 hierarchical.cpp:1101] Performed allocation for slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 in 1.155563ms
I1219 09:51:32.722681 31905 hierarchical.cpp:521] Slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 (60ab6e727501) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048)
I1219 09:51:32.722713 31909 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.723031 31905 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.723073 31905 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.723095 31905 hierarchical.cpp:1101] Performed allocation for slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 in 368889ns
I1219 09:51:32.723191 31909 sched.cpp:811] Scheduler::resourceOffers took 113921ns
I1219 09:51:32.723410 31911 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.418243ms
I1219 09:51:32.723497 31911 replica.cpp:712] Persisted action at 4
I1219 09:51:32.724326 31907 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I1219 09:51:32.724758 31907 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 329678ns
I1219 09:51:32.724917 31907 leveldb.cpp:399] Deleting ~2 keys from leveldb took 58317ns
I1219 09:51:32.725025 31907 replica.cpp:712] Persisted action at 4
I1219 09:51:32.725127 31907 replica.cpp:697] Replica learned TRUNCATE action at position 4
I1219 09:51:32.731515 31910 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.731564 31910 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.731591 31910 hierarchical.cpp:1079] Performed allocation for 1 slaves in 239271ns
I1219 09:51:32.741710 31910 master.cpp:3055] Processing ACCEPT call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O0 ] on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.741770 31910 master.cpp:2843] Authorizing principal 'test-principal' to create volumes
E1219 09:51:32.742707 31910 master.cpp:1737] Dropping CREATE offer operation from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408: Not authorized to create persistent volumes as 'test-principal'
I1219 09:51:32.743219 31910 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.752542 31908 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.752590 31908 hierarchical.cpp:1079] Performed allocation for 1 slaves in 888401ns
I1219 09:51:32.753018 31908 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.753435 31908 sched.cpp:811] Scheduler::resourceOffers took 92252ns
I1219 09:51:32.761533 31878 sched.cpp:164] Version: 0.27.0
I1219 09:51:32.761931 31897 master.cpp:3570] Processing DECLINE call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O1 ] for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.762373 31897 sched.cpp:262] New master detected at master@172.17.0.3:36408
I1219 09:51:32.762451 31897 sched.cpp:272] No credentials provided. Attempting to register without authentication
I1219 09:51:32.762470 31897 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.0.3:36408
I1219 09:51:32.762543 31897 sched.cpp:747] Will retry registration in 465.481193ms if necessary
I1219 09:51:32.762572 31898 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.762722 31898 master.cpp:2197] Received SUBSCRIBE call for framework 'creator-framework' at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.762785 31898 master.cpp:1668] Authorizing framework principal 'creator-principal' to receive offers for role 'role1'
I1219 09:51:32.763036 31897 master.cpp:2268] Subscribing framework creator-framework with checkpointing disabled and capabilities [  ]
I1219 09:51:32.763464 31898 hierarchical.cpp:260] Added framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.763562 31897 sched.cpp:641] Framework registered with bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.763605 31897 sched.cpp:655] Scheduler::registered took 20669ns
I1219 09:51:32.763804 31908 master.cpp:2650] Processing SUPPRESS call for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.764343 31898 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.764382 31898 hierarchical.cpp:1079] Performed allocation for 1 slaves in 893765ns
I1219 09:51:32.764428 31898 hierarchical.cpp:953] Suppressed offers for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.764746 31898 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.765127 31898 sched.cpp:811] Scheduler::resourceOffers took 83608ns
I1219 09:51:32.773298 31900 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.773339 31900 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.773365 31900 hierarchical.cpp:1079] Performed allocation for 1 slaves in 201759ns
I1219 09:51:32.782901 31898 master.cpp:3055] Processing ACCEPT call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O2 ] on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.782961 31898 master.cpp:2843] Authorizing principal 'creator-principal' to create volumes
I1219 09:51:32.784190 31904 master.cpp:3362] Applying CREATE operation for volumes disk(role1)[id1:path1]:128 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408 to slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501)
I1219 09:51:32.784548 31904 master.cpp:6486] Sending checkpointed resources disk(role1)[id1:path1]:128 to slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501)
I1219 09:51:32.786471 31904 hierarchical.cpp:642] Updated allocation of framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 to cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128
I1219 09:51:32.786929 31904 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.788035 31904 slave.cpp:2277] Updated checkpointed resources from  to disk(role1)[id1:path1]:128
I1219 09:51:32.795177 31902 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.795250 31902 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.357898ms
I1219 09:51:32.795897 31902 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.796540 31897 sched.cpp:811] Scheduler::resourceOffers took 138880ns
I1219 09:51:32.803026 31902 master.cpp:3570] Processing DECLINE call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O3 ] for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.804143 31902 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.804622 31907 master.cpp:2650] Processing SUPPRESS call for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.804729 31907 hierarchical.cpp:953] Suppressed offers for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.805140 31897 master.cpp:3649] Processing REVIVE call for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.805250 31897 hierarchical.cpp:973] Removed offer filters for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.806507 31897 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.806562 31897 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.284779ms
I1219 09:51:32.807067 31897 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
../../src/tests/persistent_volume_tests.cpp:1336: Failure
Mock function called more times than expected - returning directly.
    Function call: resourceOffers(0x7ffff9edb3a0, @0x7f71079798f0 { 144-byte object <F0-1B 42-14 71-7F 00-00 00-00 00-00 00-00 00-00 D0-96 02-F0 70-7F 00-00 50-97 02-F0 70-7F 00-00 20-A1 02-F0 70-7F 00-00 50-E0 01-F0 70-7F 00-00 B0-9F 02-F0 70-7F 00-00 00-32 01-F0 70-7F 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 70-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 1F-00 00-00> })
         Expected: to be called once
           Actual: called twice - over-saturated and active
I1219 09:51:32.807899 31897 sched.cpp:811] Scheduler::resourceOffers took 406435ns
I1219 09:51:32.820523 31909 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.820611 31909 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.820642 31909 hierarchical.cpp:1079] Performed allocation for 1 slaves in 448034ns
2015-12-19 09:51:33,146:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:36,482:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:39,818:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:43,155:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:46,490:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
../../src/tests/persistent_volume_tests.cpp:1411: Failure
Failed to wait 15secs for offers
I1219 09:51:47.829073 31909 master.cpp:1130] Framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408 disconnected
I1219 09:51:47.829169 31909 master.cpp:2493] Disconnecting framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.829200 31909 master.cpp:2517] Deactivating framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.829366 31909 master.cpp:1154] Giving framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408 0ns to failover
I1219 09:51:47.829720 31909 hierarchical.cpp:366] Deactivated framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:47.831614 31907 master.cpp:5100] Framework failover timeout, removing framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.831748 31907 master.cpp:5835] Removing framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.833314 31897 slave.cpp:2012] Asked to shut down framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 by master@172.17.0.3:36408
W1219 09:51:47.833421 31897 slave.cpp:2027] Cannot shut down unknown framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:47.834002 31897 hierarchical.cpp:321] Removed framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:47.843332 31908 master.cpp:1130] Framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408 disconnected
I1219 09:51:47.843521 31908 master.cpp:2493] Disconnecting framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:47.843663 31908 master.cpp:2517] Deactivating framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
W1219 09:51:47.844665 31908 master.hpp:1758] Master attempted to send message to disconnected framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:47.845077 31908 master.cpp:1154] Giving framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408 0ns to failover
I1219 09:51:47.844887 31903 hierarchical.cpp:366] Deactivated framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:47.845728 31903 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
../../src/tests/persistent_volume_tests.cpp:1404: Failure
Actual function call count doesn't match EXPECT_CALL(sched1, resourceOffers(&driver1, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
I1219 09:51:47.847968 31902 master.cpp:5100] Framework failover timeout, removing framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:47.848068 31902 master.cpp:5835] Removing framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:47.848553 31902 slave.cpp:2012] Asked to shut down framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 by master@172.17.0.3:36408
W1219 09:51:47.848644 31902 slave.cpp:2027] Cannot shut down unknown framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:47.848999 31902 hierarchical.cpp:321] Removed framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:47.849782 31912 master.cpp:930] Master terminating
I1219 09:51:47.851934 31899 hierarchical.cpp:496] Removed slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0
I1219 09:51:47.855919 31907 slave.cpp:3417] master@172.17.0.3:36408 exited
W1219 09:51:47.856021 31907 slave.cpp:3420] Master disconnected! Waiting for a new master to be elected
I1219 09:51:47.908278 31878 slave.cpp:601] Slave terminating
[  FAILED  ] PersistentVolumeTest.BadACLDropCreateAndDestroy (15298 ms)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-19 18:44:35.32,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 06 17:54:40 UTC 2016,,,,,,,"0|i2q4lj:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 26,,,,,,,,,,,1.0,,,,,,,,,,,"19/Dec/15 18:44;neilc;cc [~greggomann]","20/Dec/15 19:55;greggomann;Thus far, I've been unable to reproduce this issue. I ran the test several thousand times in a CentOS 7.1 VM with no failures. I suspect it's related to specifics of the build environment in some way, I'll keep investigating.","06/Jan/16 17:54;greggomann;Review here: https://reviews.apache.org/r/41986/",,,,,,,,,,,,,,,,,,,,,,,,,
Race in SSL socket shutdown ,MESOS-4202,12923162,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jojy,jojy,jojy,18/Dec/15 23:13,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.27.0,,,,,,,,,,,0,libprocess,mesosphere,,,,,,,"libprocess Socket shares the ownership of the file descriptor with libevent. In
the destructor of the libprocess libevent_ssl socket, we call ssl shutdown which
is executed asynchronously. This causes the libprocess socket file descriptor tobe closed (and possibly reused) when the same file descriptor could be used bylibevent/ssl. Since we set the shutdown options as SSL_RECEIVED_SHUTDOWN, we leave the any write operations to continue with possibly closed file descriptor.

This issue manifests as junk characters written to the file that has been handled the closed socket file descriptor (by OS) that has the above issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-19 00:00:41.495,,,false,MESOS-7930,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 19 00:00:41 UTC 2015,,,,,,,"0|i2q43z:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 24,,,,,,,,,,,5.0,,0.27.0,,,,,,,,,"18/Dec/15 23:14;jojy;https://reviews.apache.org/r/41253/","19/Dec/15 00:00;jvanremoortere;{code}
commit ca3667f4e97e11ad30811753fdb52bc02854113f
Author: Jojy Varghese <jojy@mesosphere.io>
Date:   Fri Dec 18 14:54:03 2015 -0800

    Libevent: SSL: Changed ownership semantics of ssl connect socket.
    
    libprocess Socket shares the ownership of the file descriptor with
    libevent. In the destructor of the libprocess libevent_ssl socket, we
    call ssl shutdown which is executed asynchronously. This causes the
    libprocess socket file descriptor to be closed (and possibly reused)
    when the same file descriptor could be used by libevent/ssl. Since we
    set the shutdown options as SSL_RECEIVED_SHUTDOWN, we allow write
    operations to continue with possibly closed file descriptor.
    
    This change solves the above issue by copying(dup) the original file
    descriptor and hands over the copy to libevent ssl. The copied
    descriptor is then managed by libprocess Socket.
    
    Review: https://reviews.apache.org/r/41253/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Disk Resource Reservation is NOT Enforced for Persistent Volumes,MESOS-4198,12923101,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hartem,gabriel.hartmann@gmail.com,gabriel.hartmann@gmail.com,18/Dec/15 19:12,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,,,,,,,,,,,,0,isolation,mesosphere,persistent-volumes,reservations,,,,,"If I create a persistent volume on a reserved disk resource, I am able to write data in excess of my reserved size.

Disk resource reservation should be enforced just as ""cpus"" and ""mem"" reservations are enforced.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4203,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-18 19:30:45.017,,,false,MESOS-1554,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 04 19:31:36 UTC 2016,,,,,,,"0|i2q3qf:",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 25,,,,,,,,,,,3.0,,,,,,,,,,,"18/Dec/15 19:30;anandmazumdar;[~gabriel.hartmann@gmail.com] Mesos does not enforce container disk quota limits by default. You would need to use the {{posix/disk}} isolator via the {{--isolation}} flag and set {{enforce_container_disk_quota}}. 

You can also configure the {{--container_disk_watch_interval}} to a suitable value. 

Are you seeing this behavior even after setting the above options ?","18/Dec/15 19:33;gabriel.hartmann@gmail.com;I was unaware of those options.  I'll look into it.  Thanks Anand.","18/Dec/15 23:15;neilc;At minimum, we want to document that disk quota limits are not enforced by default -- I'll open a ticket for that. Otherwise, is there anything we can do here?","18/Dec/15 23:49;gabriel.hartmann@gmail.com;Disk quotas are enforced when you launch tasks, but not for persistent volumes:

https://github.com/apache/mesos/blob/3539b7a0e15b594148308319bf052d28b1429b98/src/slave/containerizer/mesos/isolators/posix/disk.cpp#L174

I've changed the title to reflect this.","28/Dec/15 19:32;hartem;https://reviews.apache.org/r/41704/
https://reviews.apache.org/r/41705/","04/Jan/16 19:31;hartem;{noformat}
author	Artem Harutyunyan <artem@mesosphere.io>	
Mon, 4 Jan 2016 09:20:30 -0800 (09:20 -0800)
committer	Jie Yu <yujie.jay@gmail.com>	
Mon, 4 Jan 2016 09:20:30 -0800 (09:20 -0800)
commit	5682052be45d67dc2bcdf969ee38bb55cb4e2019


author	Artem Harutyunyan <artem@mesosphere.io>	
Mon, 4 Jan 2016 09:20:36 -0800 (09:20 -0800)
committer	Jie Yu <yujie.jay@gmail.com>	
Mon, 4 Jan 2016 09:20:36 -0800 (09:20 -0800)
commit	4706504c51446f31253a88a733d7aa30e9fea842
{noformat}",,,,,,,,,,,,,,,,,,,,,,
Port `process/file.hpp`,MESOS-4193,12922866,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hausdorff,hausdorff,hausdorff,17/Dec/15 22:10,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,0.27.0,,,,,,libprocess,,,,,0,libprocess,mesosphere,windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-17 22:34:47.581,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 17 22:35:57 UTC 2015,,,,,,,"0|i2q2af:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 24,,,,,,,,,,,3.0,,0.27.0,,,,,,,,,"17/Dec/15 22:34;jvanremoortere;https://reviews.apache.org/r/39852/
https://reviews.apache.org/r/39888/
https://reviews.apache.org/r/39889/","17/Dec/15 22:35;jvanremoortere;{code}
commit a798048b24bfba9de552a1551c14dd167bcbd3a8
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Thu Dec 17 13:35:04 2015 -0800

    Windows: Added support for `files/files.hpp`.
    
    Review: https://reviews.apache.org/r/39889/

commit 33d332ef4bbb339df96f442a90e8775d48a0f817
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Wed Dec 16 17:17:49 2015 -0800

    Windows: Stout: Added compatibility code for `grp.h` and `pwd.h`.
    
    This code will be particularly useful when we expand Windows support for
    `files/files.hpp`.
    
    Review: https://reviews.apache.org/r/39888/

commit ce09c584134e0dbcbebd8b724c88ff6c81fc9247
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Wed Dec 16 17:09:48 2015 -0800

    Windows: Replaced global `GetMessage` macro with inline function.
    
    Review: https://reviews.apache.org/r/39852/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Add documentation for API Versioning,MESOS-4192,12922831,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,anandmazumdar,anandmazumdar,17/Dec/15 19:59,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.27.0,,,,,,documentation,,,,,0,documentation,mesosphere,,,,,,,"Currently, we don't have any documentation for:

- How Mesos implements API versioning ?
- How are protobufs versioned and how does mesos handle them internally ?
- What do contributors need to do when they make a change to a external user facing protobuf ?

The relevant design doc:
https://docs.google.com/document/d/1-iQjo6778H_fU_1Zi_Yk6szg8qj-wqYgVgnx7u3h6OU/edit#heading=h.2gkbjz6amn7b
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-3302,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 31 20:27:49 UTC 2015,,,,,,,"0|i2q22n:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 25,,,,,,,,,,,3.0,,,,,,,,,,,"22/Dec/15 21:44;anandmazumdar;https://reviews.apache.org/r/41661","31/Dec/15 20:27;anandmazumdar;{code}
commit 4568e584d15e2da68f777aff2570d6b0ceaa14fa
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Wed Dec 30 17:11:21 2015 -0800

    Added documentation for API versioning.

    Review: https://reviews.apache.org/r/41661/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Extend `Master` to authorize persistent volumes,MESOS-4179,12922137,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,greggomann,greggomann,greggomann,15/Dec/15 21:08,19/Dec/15 01:31,29/Oct/20 16:32,19/Dec/15 01:31,,,,,,,,,,,,,,,,,,,,0,persistent-volumes,,,,,,,,"This ticket is the second in a series that adds authorization support for persistent volumes.

Methods {{Master::authorizeCreateVolume()}} and {{Master::authorizeDestroyVolume}} must be added to allow the Master to authorize these operations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4178,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-1554,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 15 21:11:28 UTC 2015,,,,,,,"0|i2pxsf:",9223372036854775807,,,,,mcypark,,,,,,Mesosphere Sprint 24,,,,,,,,,,,1.0,,,,,,,,,,,"15/Dec/15 21:11;greggomann;Review here: https://reviews.apache.org/r/40169/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add persistent volume support to the Authorizer,MESOS-4178,12922136,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,greggomann,greggomann,greggomann,15/Dec/15 21:04,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,,,,,,,,,,,,0,mesosphere,persistent-volumes,,,,,,,"This ticket is the first in a series that adds authorization support for persistent volume creation and destruction.

Persistent volumes should be authorized with the {{principal}} of the reserving entity (framework or master). The idea is to introduce {{Create}} and {{Destroy}} into the ACL.

{code}
  message Create {
    // Subjects.
    required Entity principals = 1;

    // Objects? Perhaps the kind of volume? allowed permissions?
  }

  message Destroy {
    // Subjects.
    required Entity principals = 1;

    // Objects.
    required Entity creator_principals = 2;
  }
{code}

ACLs for volume creation and destruction must be added to {{authorizer.proto}}, and the appropriate function overloads must be added to the Authorizer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-1554,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 15 21:12:39 UTC 2015,,,,,,,"0|i2pxs7:",9223372036854775807,,,,,mcypark,,,,,,Mesosphere Sprint 24,,,,,,,,,,,1.0,,,,,,,,,,,"15/Dec/15 21:12;greggomann;Reviews here:
https://reviews.apache.org/r/40167/
https://reviews.apache.org/r/40168/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a user doc for Executor HTTP API,MESOS-4177,12922103,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,anandmazumdar,anandmazumdar,15/Dec/15 19:18,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,0.27.0,,,,,,,,,,,0,mesosphere,,,,,,,,We need a user doc similar to the corresponding one for the Scheduler HTTP API.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-06 01:10:06.075,,,false,MESOS-4793,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 06 01:10:06 UTC 2016,,,,,,,"0|hzzzt2:y",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 24,Mesosphere Sprint 25,Mesosphere Sprint 26,,,,,,,,,3.0,,0.27.0,,,,,,,,,"16/Dec/15 18:36;anandmazumdar;https://reviews.apache.org/r/41454/","06/Jan/16 01:10;vinodkone;commit e4a68e1827837cb459afc3e87a3d9edc335ffe4e
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Tue Jan 5 16:57:25 2016 -0800

    Added initial draft of executor HTTP API user doc.
    
    Review: https://reviews.apache.org/r/41454/
",,,,,,,,,,,,,,,,,,,,,,,,,,
Reserve/UnReserve Dynamic Reservation Endpoints allow reservations on non-existing roles,MESOS-4143,12921507,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,Fixed,gradywang,mandoskippy,mandoskippy,13/Dec/15 18:27,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,0.25.0,0.26.0,,,,,,,0.27.0,,,,,,master,,,,,1,mesosphere,reservations,,,,,,,"When working with Dynamic reservations via the /reserve and /unreserve endpoints, it is possible to reserve resources for roles that have not been specified via the --roles flag on the master.  However, these roles are not usable because the roles have not been defined, nor are they added to the list of roles available. 

Per the mailing list, changing roles after the fact is not possible at this time. (That may be another JIRA), more importantly, the /reserve and /unreserve end points should not allow reservation of roles not specified by --roles.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-14 07:22:46.648,,,false,MESOS-2018,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 17 22:39:24 UTC 2017,,,,,,,"0|hzzzty:zx",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 24,Mesosphere Sprint 25,,,,,,,,,,2.0,,0.27.0,,,,,,,,,"14/Dec/15 07:22;JamesYongQiaoWang;In explicit roles(--roles is specified when Mesos master startup), this is a bug, but in implicit roles(--roles is NOT specified), this is not. Suggest to fix this issue after Implicit Roles(MESOS-4085) commit, then we can call  Master::validRole to do check.","16/Dec/15 22:49;neilc;https://reviews.apache.org/r/41472/","18/Dec/15 09:12;alexr;Shall we disallow registration of agents that have statically reserved resources for non-existing roles?","07/Jan/16 00:59;neilc;On reflection, I'm probably not going to try to fix this in the near-term.

* Per discussion with [~mcypark], properly fixing it would require some refactoring to how we represent the master's role whitelist. We want to validate that the role is in the whitelist in {{validation::operation::validate}}; to do this without passing a {{Master*}} to {{validate}}, the idea would be to refactor the whitelist so it is represented as a {{RoleWhitelist}} object, and then pass that to {{validate}}.
* However, I don't think this is a high-priority issue. Also, all of this code is going away in ~six months when we remove the role whitelist.

If someone else wants to take a shot at fixing this, go for it!","17/Jan/17 22:39;mcypark;This capability I believe is now made possible with implicit roles. Please reopen the ticket and ping me if this is not the case.",,,,,,,,,,,,,,,,,,,,,,,
Implement `WindowsError` to correspond with `ErrnoError`.,MESOS-4110,12920643,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hausdorff,hausdorff,hausdorff,10/Dec/15 02:14,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.27.0,,,,,,stout,,,,,0,cmake,mesosphere,stout,windows,,,,,"In the C standard library, `errno` records the last error on a thread. You can pretty-print it with `strerror`.

In Stout, we report these errors with `ErrnoError`.

The Windows API has something similar, called `GetLastError()`. The way to pretty-print this is hilariously unintuitive and terrible, so in this case it is actually very beneficial to wrap it with something similar to `ErrnoError`, maybe called `WindowsError`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-14 23:49:57.92,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 14 23:50:22 UTC 2015,,,,,,,"0|i2pp3z:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 24,,,,,,,,,,,5.0,,0.27.0,,,,,,,,,"14/Dec/15 23:49;jvanremoortere;https://reviews.apache.org/r/39583/","14/Dec/15 23:50;jvanremoortere;{code}
commit 5fbd65fbd62424b003f75d916b63442e6e5050f1
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Mon Dec 14 14:26:26 2015 -0800

    Windows: Added `WindowsError` to parallel `ErrnoError`.
    
    Review: https://reviews.apache.org/r/39583/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Implement `os::mkdtemp` for Windows,MESOS-4108,12920623,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hausdorff,hausdorff,hausdorff,10/Dec/15 01:14,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.27.0,,,,,,stout,,,,,0,mesosphere,stout,windows,,,,,,"Used basically exclusively for testing, this insecure and otherwise-not-quite-suitable-for-prod function needs to work to run what will eventually become the FS tests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-10 01:28:02.964,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 11 21:38:13 UTC 2015,,,,,,,"0|i2pozj:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 24,,,,,,,,,,,5.0,,0.27.0,,,,,,,,,"10/Dec/15 01:28;jvanremoortere;https://reviews.apache.org/r/39559","11/Dec/15 21:38;jvanremoortere;{code}
commit 69d7e8e377bc1ccdd497e951ba64b2c447b3c6e8
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Fri Dec 11 13:00:59 2015 -0800

    Windows: Implemented `os::mkdtemp`.
    
    Review: https://reviews.apache.org/r/39559
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
`os::strerror_r` breaks the Windows build,MESOS-4107,12920617,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hausdorff,hausdorff,hausdorff,10/Dec/15 00:58,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.27.0,,,,,,stout,,,,,0,mesosphere,stout,,,,,,,`os::strerror_r` does not exist on Windows.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-10 01:08:41.489,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 10 01:09:17 UTC 2015,,,,,,,"0|i2poy7:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 24,,,,,,,,,,,1.0,,0.27.0,,,,,,,,,"10/Dec/15 01:08;jvanremoortere;https://reviews.apache.org/r/40382/","10/Dec/15 01:09;jvanremoortere;{code}
commit 95e48c6cd87f76f613b3b8f95bdc80e2dbff0e1c
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Wed Dec 9 16:03:36 2015 -0800

    Windows: Added threadsafe `strerror_r` implementation.
    
    Review: https://reviews.apache.org/r/40382
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Quota doesn't allocate resources on slave joining.,MESOS-4102,12920267,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,klaus1982,neilc,neilc,09/Dec/15 01:30,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.27.0,,,,,,allocation,,,,,0,mesosphere,quota,,,,,,,"See attached patch. {{framework1}} is not allocated any resources, despite the fact that the resources on {{agent2}} can safely be allocated to it without risk of violating {{quota1}}. If I understand the intended quota behavior correctly, this doesn't seem intended.

Note that if the framework is added _after_ the slaves are added, the resources on {{agent2}} are allocated to {{framework1}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4116,MESOS-3078,MESOS-3157,,,,,,"09/Dec/15 01:34;neilc;quota_absent_framework_test-1.patch;https://issues.apache.org/jira/secure/attachment/12776460/quota_absent_framework_test-1.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,2015-12-09 11:05:20.304,,,false,MESOS-1791,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 19 23:10:44 UTC 2016,,,,,,,"0|i2pmsf:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 26,,,,,,,,,,,5.0,,0.27.0,,,,,,,,,"09/Dec/15 11:05;alexr;The reason you see this behaviour is
  a) because event-triggered allocations do not include all available agents *and*
  b) because we do not persist set aside resources across allocations.

However, during the next batch allocation cycle, we will observe all active agents and will be able to properly allocate resources not set aside for quota. Now the question is: *do you find this behaviour surprising*, i.e. *shall we fix it*?

In your case, you can make your test succeed if you add
{code}
  Clock::advance(flags.allocation_interval);
  Clock::settle();
{code}
before {{Future<Allocation> allocation = allocations.get();}}

The reason why we do b) is because we do not want to ""attach"" unallocated part of quota to particular agents. Technically, we do not even set aside resources, rather we stop allocating to non quota'ed frameworks if remaining resources are less than the unsatisfied quota part. 

For posterity, let me elaborate on the sequence of events happening in your test.
# Quota {{cpus:2;mem:1024}} is set for {{QUOTA_ROLE}}
#* {{allocate()}} for all agents is triggered
#* total resources are {{0}}
#* no resources to allocate, hence allocation callback is not called, hence nothing is pushed into the {{allocations}} queue
# {{framework1}} is added to {{NO_QUOTA_ROLE}}
#* {{allocate()}} for all agents is triggered
#* total resources are {{0}}
#* no resources to allocate, hence allocation callback is not called, hence nothing is pushed into the {{allocations}} queue
# {{slave1}} with {{cpus(* ):2; mem(* ):1024}} is added
#* {{allocate()}} for {{slave1}} *only* is triggered
#* total resources are {{cpus(* ):2; mem(* ):1024}} from {{slave1}}
#* total resources are less or equal than unallocated part of quota
#* no resources to allocate, hence allocation callback is not called, hence nothing is pushed into the {{allocations}} queue
# {{slave2}} with {{cpus(* ):1; mem(* ):512}} is added
#* {{allocate()}} for {{slave2}} *only* is triggered
#* total resources are {{cpus(* ):1; mem(* ):512}} from {{slave2}}
#* total resources are less or equal than unallocated part of quota
#* no resources to allocate, hence allocation callback is not called, hence nothing is pushed into the {{allocations}} queue
# {{AWAIT_READY(allocation);}} fails since not a single allocation happened in the test.","09/Dec/15 17:56;neilc;Thanks for the explanation. I understand what is going on, so the question is whether this is the best behavior. Basically, the current implementation of event-triggered allocations will always make legal allocations (per quota), but might not make all the allocations that legally could be made. Is that considered a problem and/or something we want to change?

It would be helpful for me to understand why we have event-triggered allocations in the first place. If we need regular batch allocations to ensure that all resources are allocated appropriately, then I guess event-triggered allocations are just intended to be a ""best-effort"" mechanism, to do _something_ about a change in cluster state until the next batch allocation occurs?","20/Dec/15 22:34;alexr;I see two approaches how we can mitigate current behaviour. One is to persist laid aside resources for quota between allocations, so that when a new agent joins, we can check whether we can allocate its resources to non-quota'ed frameworks. This approach however prevents quota'ed frameworks to ""see"" the new agent and therefore feels a bit wrong, because quota should go first and fair share second. The other approach is whenever allocation is triggered, add all agents with laid aside resources (for example, if we have laid aside resource on {{agent3}} and {{agent6}}, when a new {{agent20}} joins the cluster, we call {{allocate()}} not only for {{agent20}}, but for all three: {{agent3}}, {{agent6}}, and {{agent20}}). This may increase the time we spend in {{allocate()}} but seems to be more fair than the first approach.","05/Jan/16 01:03;bmahler;Linked in tickets with discussions about batch vs. event driven allocation.","14/Jan/16 06:51;klaus1982;[~alexr]/[~neilc]/[~bmahler], I think the major gap is that allocator consider {{slaveIds}} includes all slaves when calculating {{remainingClusterResources}}; but it's not true when new slave added. So   one option is to calculate it based on all active slaves. The performance impact should be acceptable, because this loop is simple.

Draft RR to show the option: https://reviews.apache.org/r/42289/
","16/Jan/16 05:48;klaus1982;Ping [~alexr]/[~neilc] :).","19/Jan/16 15:11;alexr;https://reviews.apache.org/r/42289
https://reviews.apache.org/r/42510/","19/Jan/16 23:10;jvanremoortere;{code}
commit 46ac43a8fa7a603fd75bb998de4201c91584b578
Author: Alexander Rukletsov <rukletsov@gmail.com>
Date:   Tue Jan 19 14:52:03 2016 -0800

    Quota: Updated comments around hierarchical tests.
    
    Review: https://reviews.apache.org/r/42510/

commit ca5cf5c5b290e4dc8edcf890743e356c31d5f468
Author: Klaus Ma <klaus1982.cn@gmail.com>
Date:   Tue Jan 19 14:51:43 2016 -0800

    Quota: Calculated 'remainingClusterResources' globally in 'allocate'.
    
    Event-triggered allocations do not include all available agents. If we
    calculate remaining resources in the cluster using the partial view,
    we may overlook already laid away resources for quota'ed roles and lay
    away more. Hence we may unnecessarily deprive non-quota'ed frameworks
    of resources.
    
    Review: https://reviews.apache.org/r/42289/
{code}",,,,,,,,,,,,,,,,,,,,
parallel make tests does not build all test targets,MESOS-4099,12920184,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,karya,jvanremoortere,jvanremoortere,08/Dec/15 20:40,29/Apr/19 09:26,29/Oct/20 16:32,26/Nov/18 12:22,0.26.0,,,,,,,,1.0.0,,,,,,build,libprocess,,,,0,mesosphere,,,,,,,,"When inside 3rdparty/libprocess:
Running {{make -j8 tests}} from a clean build does not yield the {{libprocess-tests}} binary.
Running it a subsequent time triggers more compilation and ends up yielding the {{libprocess-tests}} binary.
This suggests the {{test}} target is not being built correctly.","Ubuntu 15.04
clang-3.6 as well as gcc-4.9",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-24 02:52:39.925,,,false,MESOS-4690,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 05 19:06:12 UTC 2016,,,,,,,"0|hzzzd8:r",9223372036854775807,,,,,tillt,,,,,,Mesosphere Sprint 35,Mesosphere Sprint 36,Mesosphere Sprint 37,Mesosphere Sprint 38,,,,,,,,1.0,,,,,,,,,,,"24/May/16 02:52;karya;RRs:

https://reviews.apache.org/r/47755/ : Libprocess: `make tests` now automatically builds dependencies.
https://reviews.apache.org/r/47756/ : Stout: `make tests` now automatically builds dependencies.","05/Jul/16 19:06;karya;{code}
commit f79660b6f757f85441bc58303ed1b505a2b4b740
Author: Kapil Arya <kapil@mesosphere.io>
Date:   Tue Jul 5 16:23:11 2016 +0200

    Stout: `make tests` now automatically builds dependencies.
    
    Earlier, doing `make tests` within stout failed if bundled
    dependencies were not already built.
    
    Review: https://reviews.apache.org/r/47756/

commit e7226450e40cf588791fe1b02e36249ce9cf1a46
Author: Kapil Arya <kapil@mesosphere.io>
Date:   Tue Jul 5 16:22:24 2016 +0200

    Libprocess: `make tests` now automatically builds dependencies.
    
    Earlier, doing `make tests` within libprocess failed if bundled
    dependencies were not already built.
    
    Review: https://reviews.apache.org/r/47755/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
libevent_ssl_socket assertion fails ,MESOS-4069,12919055,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jojy,jojy,jojy,04/Dec/15 19:32,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.24.2,0.25.1,0.26.1,0.27.0,,,libprocess,,,,,0,mesosphere,,,,,,,,"Have been seeing the following socket  receive error frequently:

{code}
F1204 11:12:47.301839 54104 libevent_ssl_socket.cpp:245] Check failed: length > 0 
*** Check failure stack trace: ***
    @     0x7f73227fe5a6  google::LogMessage::Fail()
    @     0x7f73227fe4f2  google::LogMessage::SendToLog()
    @     0x7f73227fdef4  google::LogMessage::Flush()
    @     0x7f7322800e08  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f73227b93e2  process::network::LibeventSSLSocketImpl::recv_callback()
    @     0x7f73227b9182  process::network::LibeventSSLSocketImpl::recv_callback()
    @     0x7f731cbc75cc  bufferevent_run_deferred_callbacks_locked
    @     0x7f731cbbdc5d  event_base_loop
    @     0x7f73227d9ded  process::EventLoop::run()
    @     0x7f73227a3101  _ZNSt12_Bind_simpleIFPFvvEvEE9_M_invokeIJEEEvSt12_Index_tupleIJXspT_EEE
    @     0x7f73227a305b  std::_Bind_simple<>::operator()()
    @     0x7f73227a2ff4  std::thread::_Impl<>::_M_run()
    @     0x7f731e0d1a40  (unknown)
    @     0x7f731de0a182  start_thread
    @     0x7f731db3730d  (unknown)
    @              (nil)  (unknown)

{code}

In this case this was a HTTP get over SSL. The url being:

https://dseasb33srnrn.cloudfront.net:443/registry-v2/docker/registry/v2/blobs/sha256/44/44be94a95984bb47dc3a193f59bf8c04d5e877160b745b119278f38753a6f58f/data?Expires=1449259252&Signature=Q4CQdr1LbxsiYyVebmetrx~lqDgQfHVkGxpbMM3PoISn6r07DXIzBX6~tl1iZx9uXdfr~5awH8Kxwh-y8b0dTV3mLTZAVlneZlHbhBAX9qbYMd180-QvUvrFezwOlSmX4B3idvo-zK0CarUu3Ev1hbJz5y3olwe2ZC~RXHEwzkQ_&Key-Pair-Id=APKAJECH5M7VWIS5YZ6Q


*Steps to reproduce:*

1. Run master
2. Run slave from your build directory as  as:

{code}
 GLOG_v=1;SSL_ENABLED=1;SSL_KEY_FILE=<path_to_key>;SSL_CERT_FILE=<path_to_cert>;sudo -E ./bin/mesos-slave.sh \
      --master=127.0.0.1:5050 \                                                  
      --executor_registration_timeout=5mins \                                    
      --containerizers=mesos  \                                                  
      --isolation=filesystem/linux \                                             
      --image_providers=DOCKER  \                                                
      --docker_puller_timeout=600 \                                              
      --launcher_dir=$MESOS_BUILD_DIR/src/.libs \                                
      --switch_user=""false"" \                                                    
      --docker_puller=""registry""          
{code} 

3. Run mesos-execute from your build directory as :

{code}                                                        
    ./src/mesos-execute \                                                        
    --master=127.0.0.1:5050 \                                                    
    --command=""uname -a""  \                                                      
    --name=test \                                                                
    --docker_image=ubuntu 
{code}",ubuntu 14.04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-22 20:55:28.599,,,false,MESOS-910,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 22 20:55:28 UTC 2015,,,,,,,"0|i2pfbj:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 25,,,,,,,,,,,8.0,,0.27.0,,,,,,,,,"04/Dec/15 21:07;jojy;Adding tcpdump analysis:

Tcp dump file: https://drive.google.com/file/d/0B-aoVvwDtYZNWGdWbVFZdUN3R0k/view?usp=sharing

- Trace shows that the slave socket at 192.168.87.237:39287 sent a RST back for a long running streaming download (see captured frame #16959) most likely due to the assertion described in the issue.
- The frames received does not show a 0 length received. 

Do we understand all the circumstance in which *bufferevent_read*  will return a 0?","07/Dec/15 15:24;jojy;https://reviews.apache.org/r/41026/","22/Dec/15 20:55;jvanremoortere;{code}
commit 0461aaba695a33ef8e92f3818e217dd23926cb50
Author: Jojy Varghese <jojy@mesosphere.io>
Date:   Tue Dec 22 15:09:46 2015 -0500

    Libevent SSL: Added check for buffer length before swapping request.
    
    recv_callback could be called from libevent's receive callback and
    Socket::recv for the same buffer event and different requests. There
    is a check for buffer length at Socket::recv but not at libevent's
    receive callback. This could lead to the incoming request for
    Socket::recv being swapped out even though the buffer length is zero.
    This change adds a check for buffer length before swapping out the
    receive request object.
    
    Review: https://reviews.apache.org/r/41026/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,
Investigate remaining flakiness in MasterMaintenanceTest.InverseOffersFilters,MESOS-4059,12918067,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,Fixed,kaysoky,neilc,neilc,03/Dec/15 16:53,26/Nov/18 12:51,29/Oct/20 16:32,26/Nov/18 12:51,,,,,,,,,,,,,,,,,,,,0,flaky-test,mesosphere,,,,,,,"Per comments in MESOS-3916, the fix for that issue decreased the degree of flakiness, but it seems that some intermittent test failures do occur -- should be investigated.

*Flakiness in task acknowledgment*
{code}
I1203 18:25:04.609817 28732 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 6afd012e-8e88-41b2-8239-a9b852d07ca1) for task 26305fdd-edb0-4764-8b8a-2558f2b2d81b of framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000
W1203 18:25:04.610076 28732 status_update_manager.cpp:762] Unexpected status update acknowledgement (received 6afd012e-8e88-41b2-8239-a9b852d07ca1, expecting 82fc7a7b-e64a-4f4d-ab74-76abac42b4e6) for update TASK_RUNNING (UUID: 82fc7a7b-e64a-4f4d-ab74-76abac42b4e6) for task 26305fdd-edb0-4764-8b8a-2558f2b2d81b of framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000
E1203 18:25:04.610339 28736 slave.cpp:2339] Failed to handle status update acknowledgement (UUID: 6afd012e-8e88-41b2-8239-a9b852d07ca1) for task 26305fdd-edb0-4764-8b8a-2558f2b2d81b of framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000: Duplicate acknowledgemen
{code}

This is a race between [launching and acknowledging two tasks|https://github.com/apache/mesos/blob/75aaaacb89fa961b249c9ab7fa0f45dfa9d415a5/src/tests/master_maintenance_tests.cpp#L1486-L1517].  The status updates for each task are not necessarily received in the same order as launching the tasks.

*Flakiness in first inverse offer filter*
See [this comment in MESOS-3916|https://issues.apache.org/jira/browse/MESOS-3916?focusedCommentId=15027478&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15027478] for the explanation.  The related logs are above the comment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-04 17:30:47.556,,,false,MESOS-1474,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 05 23:13:26 UTC 2016,,,,,,,"0|i2p987:",9223372036854775807,,,,,,,,,,,Mesosphere Sprint 26,,,,,,,,,,,1.0,,,,,,,,,,,"03/Dec/15 23:49;neilc;https://reviews.apache.org/r/40935/","04/Dec/15 17:30;jvanremoortere;{code}
commit fe4be25fa6011787751547b06f70676fd79bb87b
Author: Neil Conway <neil.conway@gmail.com>
Date:   Fri Dec 4 11:54:18 2015 -0500

    Fixed flakiness in MasterMaintenanceTest.InverseOffersFilters.
    
    There were two problems:
    
    (1) After launching two tasks, we assumed that we would see TASK_RUNNING
        updates for the tasks in the same order they were launched. This is
        not guaranteed, so adjust the test to handle TASK_RUNNING updates in
        the order they are received.
    
    (2) The test used this pattern:
    
            Mesos m;
            Call c;
    
            m.send(c);
            Clock::settle();
            // Trigger a new batch allocation that reflects the call
            Clock::advance();
    
        However, this is actually unsafe (see MESOS-3760): the send() call
        might not have reached the master by the time `Clock::settle()`
        happens. This was fixed by blocking using `FUTURE_DISPATCH` on the
        downstream logic in the allocator that is invoked to handle the
        delivered event.
    
    Review: https://reviews.apache.org/r/40935
{code}","04/Dec/15 18:05;kaysoky;Note: This issue is not resolved yet because the above commit does not completely resolve the ""flakiness in the first inverse offer filter"".  See [this comment in the review|https://reviews.apache.org/r/40935/#review108911].","05/Jan/16 21:24;kaysoky;Fix for the other flakiness (and hopefully the last one):
https://reviews.apache.org/r/41945/","05/Jan/16 23:13;bmahler;{noformat}
commit c9cc01cb153ca01e6109848ee9832ff9c84c8421
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Tue Jan 5 15:12:12 2016 -0800

    Fixed remaining flakiness in MasterMaintenanceTest.InverseOffersFilters.

    There were three inverse offer updates in the test body, but only two
    `FUTURE_DISPATCH` expectations on `updateInverseOffer`. This patch adds
    the missing `FUTURE_DISPATCH` after the first inverse offer update.

    Prior to this change, the first `FUTURE_DISPATCH` was expected to
    correspond to the second inverse offer.  In some cases, the first
    inverse offer would trigger the expectation instead (and/or the
    test would reach the `Clock::settle()` prior to one of the
    `Call`'s reaching the master).

    Review: https://reviews.apache.org/r/41945/
{noformat}",,,,,,,,,,,,,,,,,,,,,,,
MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery is flaky,MESOS-4047,12917682,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,arojas,kaysoky,kaysoky,02/Dec/15 19:19,14/Mar/16 20:01,29/Oct/20 16:32,04/Mar/16 00:12,0.26.0,,,,,,,,0.28.0,,,,,,test,,,,,0,flaky,flaky-test,,,,,,,"{code:title=Output from passed test}
[----------] 1 test from MemoryPressureMesosTest
1+0 records in
1+0 records out
1048576 bytes (1.0 MB) copied, 0.000430889 s, 2.4 GB/s
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery
I1202 11:09:14.319327  5062 exec.cpp:134] Version: 0.27.0
I1202 11:09:14.333317  5079 exec.cpp:208] Executor registered on slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0
Registered executor on ubuntu
Starting task 4e62294c-cfcf-4a13-b699-c6a4b7ac5162
sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
Forked command at 5085
I1202 11:09:14.391739  5077 exec.cpp:254] Received reconnect request from slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0
I1202 11:09:14.398598  5082 exec.cpp:231] Executor re-registered on slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0
Re-registered executor on ubuntu
Shutting down
Sending SIGTERM to process tree at pid 5085
Killing the following process trees:
[ 
-+- 5085 sh -c while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done 
 \--- 5086 dd count=512 bs=1M if=/dev/zero of=./temp 
]
[       OK ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery (1096 ms)
{code}

{code:title=Output from failed test}
[----------] 1 test from MemoryPressureMesosTest
1+0 records in
1+0 records out
1048576 bytes (1.0 MB) copied, 0.000404489 s, 2.6 GB/s
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery
I1202 11:09:15.509950  5109 exec.cpp:134] Version: 0.27.0
I1202 11:09:15.568183  5123 exec.cpp:208] Executor registered on slave 88734acc-718e-45b0-95b9-d8f07cea8a9e-S0
Registered executor on ubuntu
Starting task 14b6bab9-9f60-4130-bdc4-44efba262bc6
Forked command at 5132
sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
I1202 11:09:15.665498  5129 exec.cpp:254] Received reconnect request from slave 88734acc-718e-45b0-95b9-d8f07cea8a9e-S0
I1202 11:09:15.670995  5123 exec.cpp:381] Executor asked to shutdown
Shutting down
Sending SIGTERM to process tree at pid 5132
../../src/tests/containerizer/memory_pressure_tests.cpp:283: Failure
(usage).failure(): Unknown container: ebe90e15-72fa-4519-837b-62f43052c913
*** Aborted at 1449083355 (unix time) try ""date -d @1449083355"" if you are using GNU date ***
{code}

Notice that in the failed test, the executor is asked to shutdown when it tries to reconnect to the agent.","Ubuntu 14, gcc 4.8.4",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-3586,MESOS-4940,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-18 11:39:58.824,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 04 00:12:47 UTC 2016,,,,,,,"0|hzzzty:zz",9223372036854775807,,,,,bernd-mesos,,,,,,Mesosphere Sprint 23,Mesosphere Sprint 24,Mesosphere Sprint 29,,,,,,,,,1.0,,,,,,,,,,,"02/Dec/15 19:47;kaysoky;Note: {{MesosContainerizerSlaveRecoveryTest.ResourceStatistics}} has similar logic for restarting the agent, re-registering an executor, and [calling {{MesosContainerizer::usage}}|https://github.com/apache/mesos/blob/master/src/tests/slave_recovery_tests.cpp#L3267].  But this test is stable.  

The flaky test waits on:
{code}
  Future<Nothing> _recover = FUTURE_DISPATCH(_, &Slave::_recover);

  Future<SlaveReregisteredMessage> slaveReregisteredMessage =
    FUTURE_PROTOBUF(SlaveReregisteredMessage(), _, _);
{code}

Whereas the stable test waits on:
{code}
  // Set up so we can wait until the new slave updates the container's
  // resources (this occurs after the executor has re-registered).
  Future<Nothing> update =
    FUTURE_DISPATCH(_, &MesosContainerizerProcess::update);
{code}","02/Dec/15 20:44;kaysoky;Review: https://reviews.apache.org/r/40880/","18/Dec/15 11:39;bernd-mesos;commit aa497e81c945677c570484a8aa1a8c8b2e979dfd
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Fri Dec 18 12:38:24 2015 +0100

    Fixed flaky MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery test.
    
    `MesosContainerizerSlaveRecoveryTest.ResourceStatistics` has very
    similar logic for restarting an agent, re-registering the executor,
    and even getting `ResourceStatistics`.
    But `MesosContainerizerSlaveRecoveryTest.ResourceStatistics`
    is stable. This patch updates the flaky test's wait-for-agent-recovery
    logic to match the stable test.
    
    Review: https://reviews.apache.org/r/40880/
","22/Feb/16 21:17;arojas;Reproduced again with following message (CentOS 6.7):

{noformat}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from MemoryPressureMesosTest
1+0 records in
1+0 records out
1048576 bytes (1.0 MB) copied, 0.000394345 s, 2.7 GB/s
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery
I0222 09:32:20.622694 20868 leveldb.cpp:174] Opened db in 5.153509ms
I0222 09:32:20.624688 20868 leveldb.cpp:181] Compacted db in 1.914323ms
I0222 09:32:20.624778 20868 leveldb.cpp:196] Created db iterator in 24549ns
I0222 09:32:20.624795 20868 leveldb.cpp:202] Seeked to beginning of db in 2610ns
I0222 09:32:20.624804 20868 leveldb.cpp:271] Iterated through 0 keys in the db in 323ns
I0222 09:32:20.624874 20868 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0222 09:32:20.625977 20888 recover.cpp:447] Starting replica recovery
I0222 09:32:20.626901 20888 recover.cpp:473] Replica is in EMPTY status
I0222 09:32:20.634701 20889 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (11193)@127.0.0.1:54769
I0222 09:32:20.634953 20888 master.cpp:376] Master 17b7da64-0c4d-4e46-ae1f-2b356dc5f266 (localhost) started on 127.0.0.1:54769
I0222 09:32:20.634986 20888 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/0rXncF/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/0rXncF/master"" --zk_session_timeout=""10secs""
W0222 09:32:20.635417 20888 master.cpp:381]
**************************************************
Master bound to loopback interface! Cannot communicate with remote schedulers or slaves. You might want to set '--ip' flag to a routable IP address.
**************************************************
I0222 09:32:20.635587 20888 master.cpp:423] Master only allowing authenticated frameworks to register
I0222 09:32:20.635601 20888 master.cpp:428] Master only allowing authenticated slaves to register
I0222 09:32:20.635622 20888 credentials.hpp:35] Loading credentials for authentication from '/tmp/0rXncF/credentials'
I0222 09:32:20.636018 20888 master.cpp:468] Using default 'crammd5' authenticator
I0222 09:32:20.636190 20888 master.cpp:537] Using default 'basic' HTTP authenticator
I0222 09:32:20.636174 20887 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0222 09:32:20.636425 20888 master.cpp:571] Authorization enabled
I0222 09:32:20.637810 20885 recover.cpp:564] Updating replica status to STARTING
I0222 09:32:20.640805 20887 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.741248ms
I0222 09:32:20.640964 20887 replica.cpp:320] Persisted replica status to STARTING
I0222 09:32:20.641525 20885 recover.cpp:473] Replica is in STARTING status
I0222 09:32:20.642133 20888 master.cpp:1712] The newly elected leader is master@127.0.0.1:54769 with id 17b7da64-0c4d-4e46-ae1f-2b356dc5f266
I0222 09:32:20.642236 20888 master.cpp:1725] Elected as the leading master!
I0222 09:32:20.642253 20888 master.cpp:1470] Recovering from registrar
I0222 09:32:20.642496 20885 registrar.cpp:307] Recovering registrar
I0222 09:32:20.643162 20889 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (11195)@127.0.0.1:54769
I0222 09:32:20.643590 20885 recover.cpp:193] Received a recover response from a replica in STARTING status
I0222 09:32:20.644120 20887 recover.cpp:564] Updating replica status to VOTING
I0222 09:32:20.646817 20889 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.190281ms
I0222 09:32:20.646870 20889 replica.cpp:320] Persisted replica status to VOTING
I0222 09:32:20.647094 20885 recover.cpp:578] Successfully joined the Paxos group
I0222 09:32:20.647337 20885 recover.cpp:462] Recover process terminated
I0222 09:32:20.647781 20887 log.cpp:659] Attempting to start the writer
I0222 09:32:20.648854 20890 replica.cpp:493] Replica received implicit promise request from (11196)@127.0.0.1:54769 with proposal 1
I0222 09:32:20.650074 20890 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.17538ms
I0222 09:32:20.650133 20890 replica.cpp:342] Persisted promised to 1
I0222 09:32:20.650879 20883 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0222 09:32:20.652029 20889 replica.cpp:388] Replica received explicit promise request from (11197)@127.0.0.1:54769 for position 0 with proposal 2
I0222 09:32:20.653084 20889 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.006874ms
I0222 09:32:20.653152 20889 replica.cpp:712] Persisted action at 0
I0222 09:32:20.654109 20888 replica.cpp:537] Replica received write request for position 0 from (11198)@127.0.0.1:54769
I0222 09:32:20.654157 20888 leveldb.cpp:436] Reading position from leveldb took 19941ns
I0222 09:32:20.655339 20888 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 914288ns
I0222 09:32:20.655375 20888 replica.cpp:712] Persisted action at 0
I0222 09:32:20.655944 20890 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0222 09:32:20.657099 20890 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.124148ms
I0222 09:32:20.657166 20890 replica.cpp:712] Persisted action at 0
I0222 09:32:20.657232 20890 replica.cpp:697] Replica learned NOP action at position 0
I0222 09:32:20.657861 20890 log.cpp:675] Writer started with ending position 0
I0222 09:32:20.658861 20886 leveldb.cpp:436] Reading position from leveldb took 30090ns
I0222 09:32:20.659822 20888 registrar.cpp:340] Successfully fetched the registry (0B) in 17.27488ms
I0222 09:32:20.660028 20888 registrar.cpp:439] Applied 1 operations in 123880ns; attempting to update the 'registry'
I0222 09:32:20.660781 20886 log.cpp:683] Attempting to append 160 bytes to the log
I0222 09:32:20.661144 20885 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0222 09:32:20.663028 20890 replica.cpp:537] Replica received write request for position 1 from (11199)@127.0.0.1:54769
I0222 09:32:20.666872 20890 leveldb.cpp:341] Persisting action (179 bytes) to leveldb took 3.746467ms
I0222 09:32:20.666934 20890 replica.cpp:712] Persisted action at 1
I0222 09:32:20.667937 20890 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0222 09:32:20.670030 20890 leveldb.cpp:341] Persisting action (181 bytes) to leveldb took 2.029339ms
I0222 09:32:20.670084 20890 replica.cpp:712] Persisted action at 1
I0222 09:32:20.670105 20890 replica.cpp:697] Replica learned APPEND action at position 1
I0222 09:32:20.671183 20889 registrar.cpp:484] Successfully updated the 'registry' in 11.046144ms
I0222 09:32:20.671344 20889 registrar.cpp:370] Successfully recovered registrar
I0222 09:32:20.671454 20890 log.cpp:702] Attempting to truncate the log to 1
I0222 09:32:20.672252 20889 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0222 09:32:20.672366 20885 master.cpp:1522] Recovered 0 slaves from the Registry (122B) ; allowing 10mins for slaves to re-register
I0222 09:32:20.673305 20884 replica.cpp:537] Replica received write request for position 2 from (11200)@127.0.0.1:54769
I0222 09:32:20.675009 20884 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.632734ms
I0222 09:32:20.675067 20884 replica.cpp:712] Persisted action at 2
I0222 09:32:20.675851 20883 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0222 09:32:20.677466 20883 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.548407ms
I0222 09:32:20.677618 20883 leveldb.cpp:399] Deleting ~1 keys from leveldb took 69924ns
I0222 09:32:20.677649 20883 replica.cpp:712] Persisted action at 2
I0222 09:32:20.677686 20883 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0222 09:32:20.685191 20868 containerizer.cpp:149] Using isolation: cgroups/mem,filesystem/posix
I0222 09:32:20.692539 20868 linux_launcher.cpp:101] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher
I0222 09:32:20.717495 20889 slave.cpp:193] Slave started on 247)@127.0.0.1:54769
I0222 09:32:20.717723 20889 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos_test_7adedc17-08f0-4c8b-9974-66c009b1da18"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_YcfvOI/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_YcfvOI/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""cgroups/mem"" --launcher_dir=""/home/alexander/workspace/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_YcfvOI""
W0222 09:32:20.718128 20889 slave.cpp:197]
**************************************************
Slave bound to loopback interface! Cannot communicate with remote master(s). You might want to set '--ip' flag to a routable IP address.
**************************************************
I0222 09:32:20.718827 20889 credentials.hpp:83] Loading credential for authentication from '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_YcfvOI/credential'
I0222 09:32:20.719223 20889 slave.cpp:324] Slave using credential for: test-principal
W0222 09:32:20.719915 20868 sched.cpp:1642]
**************************************************
Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address.
**************************************************
I0222 09:32:20.720688 20889 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0222 09:32:20.720878 20889 slave.cpp:472] Slave attributes: [  ]
I0222 09:32:20.720979 20889 slave.cpp:477] Slave hostname: localhost
I0222 09:32:20.721812 20868 sched.cpp:222] Version: 0.28.0
I0222 09:32:20.722302 20889 state.cpp:58] Recovering state from '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_YcfvOI/meta'
I0222 09:32:20.722332 20886 sched.cpp:326] New master detected at master@127.0.0.1:54769
I0222 09:32:20.725953 20885 status_update_manager.cpp:200] Recovering status update manager
I0222 09:32:20.726177 20886 sched.cpp:382] Authenticating with master master@127.0.0.1:54769
I0222 09:32:20.726552 20886 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0222 09:32:20.726557 20889 containerizer.cpp:407] Recovering containerizer
I0222 09:32:20.727334 20884 authenticatee.cpp:121] Creating new client SASL connection
I0222 09:32:20.727854 20886 master.cpp:5526] Authenticating scheduler-9fd9adc7-27ff-4054-a6ed-d613cf04ec62@127.0.0.1:54769
I0222 09:32:20.728514 20889 authenticator.cpp:98] Creating new server SASL connection
I0222 09:32:20.728842 20889 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0222 09:32:20.728987 20889 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0222 09:32:20.729140 20889 authenticator.cpp:203] Received SASL authentication start
I0222 09:32:20.729305 20889 authenticator.cpp:325] Authentication requires more steps
I0222 09:32:20.729557 20889 authenticatee.cpp:258] Received SASL authentication step
I0222 09:32:20.729763 20889 authenticator.cpp:231] Received SASL authentication step
I0222 09:32:20.730895 20889 authenticator.cpp:317] Authentication success
I0222 09:32:20.731767 20889 master.cpp:5556] Successfully authenticated principal 'test-principal' at scheduler-9fd9adc7-27ff-4054-a6ed-d613cf04ec62@127.0.0.1:54769
I0222 09:32:20.731767 20885 authenticatee.cpp:298] Authentication success
I0222 09:32:20.735591 20885 sched.cpp:471] Successfully authenticated with master master@127.0.0.1:54769
I0222 09:32:20.736048 20890 provisioner.cpp:245] Provisioner recovery complete
I0222 09:32:20.739279 20889 slave.cpp:4565] Finished recovery
I0222 09:32:20.739347 20887 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-9fd9adc7-27ff-4054-a6ed-d613cf04ec62@127.0.0.1:54769
I0222 09:32:20.739572 20887 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0222 09:32:20.740470 20889 master.cpp:2351] Subscribing framework default with checkpointing enabled and capabilities [  ]
I0222 09:32:20.740790 20887 slave.cpp:796] New master detected at master@127.0.0.1:54769
I0222 09:32:20.741118 20885 hierarchical.cpp:265] Added framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000
I0222 09:32:20.741659 20883 sched.cpp:703] Framework registered with 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000
I0222 09:32:20.740885 20886 status_update_manager.cpp:174] Pausing sending status updates
I0222 09:32:20.741971 20887 slave.cpp:859] Authenticating with master master@127.0.0.1:54769
I0222 09:32:20.742007 20887 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0222 09:32:20.742254 20887 slave.cpp:832] Detecting new master
I0222 09:32:20.742301 20889 authenticatee.cpp:121] Creating new client SASL connection
I0222 09:32:20.742700 20885 master.cpp:5526] Authenticating slave(247)@127.0.0.1:54769
I0222 09:32:20.743103 20884 authenticator.cpp:98] Creating new server SASL connection
I0222 09:32:20.743357 20884 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0222 09:32:20.743404 20884 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0222 09:32:20.743563 20884 authenticator.cpp:203] Received SASL authentication start
I0222 09:32:20.743628 20884 authenticator.cpp:325] Authentication requires more steps
I0222 09:32:20.743881 20886 authenticatee.cpp:258] Received SASL authentication step
I0222 09:32:20.743976 20886 authenticator.cpp:231] Received SASL authentication step
I0222 09:32:20.744055 20886 authenticator.cpp:317] Authentication success
I0222 09:32:20.744320 20886 authenticatee.cpp:298] Authentication success
I0222 09:32:20.744364 20883 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(247)@127.0.0.1:54769
I0222 09:32:20.744813 20883 slave.cpp:927] Successfully authenticated with master master@127.0.0.1:54769
I0222 09:32:20.745275 20889 master.cpp:4240] Registering slave at slave(247)@127.0.0.1:54769 (localhost) with id 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0
I0222 09:32:20.745687 20883 registrar.cpp:439] Applied 1 operations in 46410ns; attempting to update the 'registry'
I0222 09:32:20.746387 20883 log.cpp:683] Attempting to append 327 bytes to the log
I0222 09:32:20.746670 20889 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0222 09:32:20.747566 20886 replica.cpp:537] Replica received write request for position 3 from (11220)@127.0.0.1:54769
I0222 09:32:20.749735 20886 leveldb.cpp:341] Persisting action (346 bytes) to leveldb took 2.033429ms
I0222 09:32:20.749877 20886 replica.cpp:712] Persisted action at 3
I0222 09:32:20.751174 20890 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0222 09:32:20.752976 20883 master.cpp:4228] Ignoring register slave message from slave(247)@127.0.0.1:54769 (localhost) as admission is already in progress
I0222 09:32:20.753378 20890 leveldb.cpp:341] Persisting action (348 bytes) to leveldb took 2.085441ms
I0222 09:32:20.753435 20890 replica.cpp:712] Persisted action at 3
I0222 09:32:20.753473 20890 replica.cpp:697] Replica learned APPEND action at position 3
I0222 09:32:20.755097 20888 registrar.cpp:484] Successfully updated the 'registry' in 9.328128ms
I0222 09:32:20.755506 20884 log.cpp:702] Attempting to truncate the log to 3
I0222 09:32:20.755728 20888 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0222 09:32:20.756162 20886 master.cpp:4308] Registered slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0 at slave(247)@127.0.0.1:54769 (localhost) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0222 09:32:20.756397 20883 hierarchical.cpp:473] Added slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0 (localhost) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0222 09:32:20.756494 20889 slave.cpp:971] Registered with master master@127.0.0.1:54769; given slave ID 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0
I0222 09:32:20.756803 20888 status_update_manager.cpp:181] Resuming sending status updates
I0222 09:32:20.757164 20889 slave.cpp:1030] Forwarding total oversubscribed resources
I0222 09:32:20.757809 20883 replica.cpp:537] Replica received write request for position 4 from (11221)@127.0.0.1:54769
I0222 09:32:20.757522 20888 master.cpp:5355] Sending 1 offers to framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000 (default) at scheduler-9fd9adc7-27ff-4054-a6ed-d613cf04ec62@127.0.0.1:54769
I0222 09:32:20.758471 20888 master.cpp:4649] Received update of slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0 at slave(247)@127.0.0.1:54769 (localhost) with total oversubscribed resources
I0222 09:32:20.759210 20883 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.362326ms
I0222 09:32:20.759312 20883 replica.cpp:712] Persisted action at 4
I0222 09:32:20.760362 20888 hierarchical.cpp:531] Slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0 (localhost) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0222 09:32:20.760388 20889 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0222 09:32:20.762329 20888 master.cpp:3138] Processing ACCEPT call for offers: [ 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-O0 ] on slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0 at slave(247)@127.0.0.1:54769 (localhost) for framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000 (default) at scheduler-9fd9adc7-27ff-4054-a6ed-d613cf04ec62@127.0.0.1:54769
I0222 09:32:20.762415 20888 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d as user 'root'
I0222 09:32:20.762817 20889 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.201003ms
I0222 09:32:20.764372 20889 leveldb.cpp:399] Deleting ~2 keys from leveldb took 1.136088ms
I0222 09:32:20.764535 20889 replica.cpp:712] Persisted action at 4
I0222 09:32:20.764650 20889 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0222 09:32:20.765740 20884 master.hpp:176] Adding task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d with resources cpus(*):1; mem(*):256; disk(*):1024 on slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0 (localhost)
I0222 09:32:20.766749 20884 master.cpp:3623] Launching task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000 (default) at scheduler-9fd9adc7-27ff-4054-a6ed-d613cf04ec62@127.0.0.1:54769 with resources cpus(*):1; mem(*):256; disk(*):1024 on slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0 at slave(247)@127.0.0.1:54769 (localhost)
I0222 09:32:20.768129 20888 slave.cpp:1361] Got assigned task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d for framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000
I0222 09:32:20.769053 20888 slave.cpp:1480] Launching task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d for framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000
I0222 09:32:20.769868 20888 paths.cpp:474] Trying to chown '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_YcfvOI/slaves/17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0/frameworks/17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000/executors/24458ae6-dde3-40e0-a4f7-84a901bd9f7d/runs/0e8d2278-bb2b-4480-bfb1-39b47abcb707' to user 'root'
I0222 09:32:20.774318 20888 slave.cpp:5367] Launching executor 24458ae6-dde3-40e0-a4f7-84a901bd9f7d of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_YcfvOI/slaves/17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0/frameworks/17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000/executors/24458ae6-dde3-40e0-a4f7-84a901bd9f7d/runs/0e8d2278-bb2b-4480-bfb1-39b47abcb707'
I0222 09:32:20.775251 20890 containerizer.cpp:666] Starting container '0e8d2278-bb2b-4480-bfb1-39b47abcb707' for executor '24458ae6-dde3-40e0-a4f7-84a901bd9f7d' of framework '17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000'
I0222 09:32:20.775647 20888 slave.cpp:1698] Queuing task '24458ae6-dde3-40e0-a4f7-84a901bd9f7d' for executor '24458ae6-dde3-40e0-a4f7-84a901bd9f7d' of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000
I0222 09:32:20.779909 20883 mem.cpp:602] Started listening for OOM events for container 0e8d2278-bb2b-4480-bfb1-39b47abcb707
I0222 09:32:20.780675 20883 mem.cpp:722] Started listening on low memory pressure events for container 0e8d2278-bb2b-4480-bfb1-39b47abcb707
I0222 09:32:20.781409 20883 mem.cpp:722] Started listening on medium memory pressure events for container 0e8d2278-bb2b-4480-bfb1-39b47abcb707
I0222 09:32:20.781992 20883 mem.cpp:722] Started listening on critical memory pressure events for container 0e8d2278-bb2b-4480-bfb1-39b47abcb707
I0222 09:32:20.782661 20883 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 288MB for container 0e8d2278-bb2b-4480-bfb1-39b47abcb707
I0222 09:32:20.783485 20883 mem.cpp:388] Updated 'memory.limit_in_bytes' to 288MB for container 0e8d2278-bb2b-4480-bfb1-39b47abcb707
I0222 09:32:20.785579 20890 linux_launcher.cpp:304] Cloning child process with flags =
I0222 09:32:20.788130 20890 containerizer.cpp:1104] Checkpointing executor's forked pid 26589 to '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_YcfvOI/meta/slaves/17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0/frameworks/17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000/executors/24458ae6-dde3-40e0-a4f7-84a901bd9f7d/runs/0e8d2278-bb2b-4480-bfb1-39b47abcb707/pids/forked.pid'
I0222 09:32:20.911543 26589 exec.cpp:143] Version: 0.28.0
I0222 09:32:20.920542 20884 slave.cpp:2643] Got registration for executor '24458ae6-dde3-40e0-a4f7-84a901bd9f7d' of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000 from executor(1)@127.0.0.1:49224
I0222 09:32:20.923596 20884 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 288MB for container 0e8d2278-bb2b-4480-bfb1-39b47abcb707
I0222 09:32:20.923749 26619 exec.cpp:217] Executor registered on slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0
I0222 09:32:20.924268 20886 slave.cpp:1863] Sending queued task '24458ae6-dde3-40e0-a4f7-84a901bd9f7d' to executor '24458ae6-dde3-40e0-a4f7-84a901bd9f7d' of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000 at executor(1)@127.0.0.1:49224
Registered executor on localhost
Starting task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d
sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
Forked command at 26627
I0222 09:32:20.929365 20884 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: e57e1d63-60c7-4132-812e-4d980eeb3039) for task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000 from executor(1)@127.0.0.1:49224
I0222 09:32:20.930857 20886 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: e57e1d63-60c7-4132-812e-4d980eeb3039) for task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000
I0222 09:32:20.931435 20886 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_RUNNING (UUID: e57e1d63-60c7-4132-812e-4d980eeb3039) for task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000
I0222 09:32:20.935792 20887 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: e57e1d63-60c7-4132-812e-4d980eeb3039) for task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000 to master@127.0.0.1:54769
I0222 09:32:20.936246 20887 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: e57e1d63-60c7-4132-812e-4d980eeb3039) for task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000 to executor(1)@127.0.0.1:49224
I0222 09:32:20.936532 20886 master.cpp:4794] Status update TASK_RUNNING (UUID: e57e1d63-60c7-4132-812e-4d980eeb3039) for task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000 from slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0 at slave(247)@127.0.0.1:54769 (localhost)
I0222 09:32:20.936666 20886 master.cpp:4842] Forwarding status update TASK_RUNNING (UUID: e57e1d63-60c7-4132-812e-4d980eeb3039) for task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000
I0222 09:32:20.936980 20886 master.cpp:6450] Updating the state of task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0222 09:32:20.938391 20885 master.cpp:3952] Processing ACKNOWLEDGE call e57e1d63-60c7-4132-812e-4d980eeb3039 for task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000 (default) at scheduler-9fd9adc7-27ff-4054-a6ed-d613cf04ec62@127.0.0.1:54769 on slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0
I0222 09:32:20.939096 20883 status_update_manager.cpp:392] Received status update acknowledgement (UUID: e57e1d63-60c7-4132-812e-4d980eeb3039) for task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000
I0222 09:32:20.939386 20868 slave.cpp:668] Slave terminating
I0222 09:32:20.939414 20883 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: e57e1d63-60c7-4132-812e-4d980eeb3039) for task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000
I0222 09:32:20.939702 20889 master.cpp:1174] Slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0 at slave(247)@127.0.0.1:54769 (localhost) disconnected
I0222 09:32:20.939720 20889 master.cpp:2635] Disconnecting slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0 at slave(247)@127.0.0.1:54769 (localhost)
I0222 09:32:20.939901 20889 master.cpp:2654] Deactivating slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0 at slave(247)@127.0.0.1:54769 (localhost)
I0222 09:32:20.940335 20887 hierarchical.cpp:560] Slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0 deactivated
I0222 09:32:20.944319 20868 containerizer.cpp:149] Using isolation: cgroups/mem,filesystem/posix
I0222 09:32:20.949115 20868 linux_launcher.cpp:101] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher
I0222 09:32:21.021484 20890 slave.cpp:193] Slave started on 248)@127.0.0.1:54769
I0222 09:32:21.021664 20890 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos_test_7adedc17-08f0-4c8b-9974-66c009b1da18"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_YcfvOI/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_YcfvOI/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""cgroups/mem"" --launcher_dir=""/home/alexander/workspace/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_YcfvOI""
W0222 09:32:21.021950 20890 slave.cpp:197]
**************************************************
Slave bound to loopback interface! Cannot communicate with remote master(s). You might want to set '--ip' flag to a routable IP address.
**************************************************
I0222 09:32:21.021999 20890 credentials.hpp:83] Loading credential for authentication from '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_YcfvOI/credential'
I0222 09:32:21.022282 20890 slave.cpp:324] Slave using credential for: test-principal
I0222 09:32:21.022814 20890 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0222 09:32:21.022861 20890 slave.cpp:472] Slave attributes: [  ]
I0222 09:32:21.022871 20890 slave.cpp:477] Slave hostname: localhost
I0222 09:32:21.024055 20890 state.cpp:58] Recovering state from '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_YcfvOI/meta'
I0222 09:32:21.024163 20890 state.cpp:698] No checkpointed resources found at '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_YcfvOI/meta/resources/resources.info'
I0222 09:32:21.026868 20889 slave.cpp:4653] Recovering framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000
I0222 09:32:21.026999 20889 slave.cpp:5476] Recovering executor '24458ae6-dde3-40e0-a4f7-84a901bd9f7d' of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000
I0222 09:32:21.028094 20884 status_update_manager.cpp:200] Recovering status update manager
I0222 09:32:21.028137 20884 status_update_manager.cpp:208] Recovering executor '24458ae6-dde3-40e0-a4f7-84a901bd9f7d' of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000
I0222 09:32:21.031335 20887 containerizer.cpp:407] Recovering containerizer
I0222 09:32:21.031548 20887 containerizer.cpp:462] Recovering container '0e8d2278-bb2b-4480-bfb1-39b47abcb707' for executor '24458ae6-dde3-40e0-a4f7-84a901bd9f7d' of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000
I0222 09:32:21.035538 20885 mem.cpp:602] Started listening for OOM events for container 0e8d2278-bb2b-4480-bfb1-39b47abcb707
I0222 09:32:21.036461 20885 mem.cpp:722] Started listening on low memory pressure events for container 0e8d2278-bb2b-4480-bfb1-39b47abcb707
I0222 09:32:21.037284 20885 mem.cpp:722] Started listening on medium memory pressure events for container 0e8d2278-bb2b-4480-bfb1-39b47abcb707
I0222 09:32:21.038064 20885 mem.cpp:722] Started listening on critical memory pressure events for container 0e8d2278-bb2b-4480-bfb1-39b47abcb707
I0222 09:32:21.040393 20886 provisioner.cpp:245] Provisioner recovery complete
I0222 09:32:21.041451 20883 slave.cpp:4505] Sending reconnect request to executor '24458ae6-dde3-40e0-a4f7-84a901bd9f7d' of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000 at executor(1)@127.0.0.1:49224
I0222 09:32:21.044780 26624 exec.cpp:263] Received reconnect request from slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0
I0222 09:32:21.048050 20886 slave.cpp:2792] Re-registering executor '24458ae6-dde3-40e0-a4f7-84a901bd9f7d' of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000
I0222 09:32:21.051213 26620 exec.cpp:240] Executor re-registered on slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0
I0222 09:32:21.052510 20888 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 288MB for container 0e8d2278-bb2b-4480-bfb1-39b47abcb707
Re-registered executor on localhost
I0222 09:32:21.053912 20888 mem.cpp:388] Updated 'memory.limit_in_bytes' to 288MB for container 0e8d2278-bb2b-4480-bfb1-39b47abcb707
W0222 09:32:21.210090 20886 master.cpp:3869] Cannot kill task 24458ae6-dde3-40e0-a4f7-84a901bd9f7d of framework 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-0000 (default) at scheduler-9fd9adc7-27ff-4054-a6ed-d613cf04ec62@127.0.0.1:54769 because the slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0 at slave(247)@127.0.0.1:54769 (localhost) is disconnected. Kill will be retried if the slave re-registers
../../src/tests/containerizer/memory_pressure_tests.cpp:300: Failure
Expected: (usage.get().mem_low_pressure_counter()) >= (usage.get().mem_medium_pressure_counter()), actual: 6 vs 8
*** Aborted at 1456162341 (unix time) try ""date -d @1456162341"" if you are using GNU date ***
PC: @          0x16b24b0 testing::UnitTest::AddTestPartResult()
*** SIGSEGV (@0x0) received by PID 20868 (TID 0x7f0ee95ae840) from PID 0; stack trace: ***
    @     0x7f0ee14ee790 (unknown)
    @          0x16b24b0 testing::UnitTest::AddTestPartResult()
    @          0x16a6ee9 testing::internal::AssertHelper::operator=()
    @          0x1688fc0 mesos::internal::tests::MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_Test::TestBody()
    @          0x16cff10 testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x16caf3e testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x16ac289 testing::Test::Run()
    @          0x16aca17 testing::TestInfo::Run()
    @          0x16ad052 testing::TestCase::Run()
    @          0x16b39a1 testing::internal::UnitTestImpl::RunAllTests()
    @          0x16d0b9f testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x16cbaca testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x16b26d1 testing::UnitTest::Run()
    @           0xe3bcd6 RUN_ALL_TESTS()
    @           0xe3b8ec main
    @     0x7f0ee0356d5d __libc_start_main
    @           0x9c4d89 (unknown)
I0222 09:32:21.558915 26624 exec.cpp:463] Slave exited, but framework has checkpointing enabled. Waiting 15mins to reconnect with slave 17b7da64-0c4d-4e46-ae1f-2b356dc5f266-S0
I0222 09:32:21.559326 26620 exec.cpp:472] Slave exited ... shutting down
Shutting down
Sending SIGTERM to process tree at pid 26627
/var/tmp/sclcApPT3: line 8: 20868 Segmentation fault      './bin/mesos-tests.sh' '--gtest_filter=MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery' '--gtest_repeat=1000' '--gtest_break_on_failure'
Sent SIGTERM to the following process trees:
[
-+- 26627 sh -c while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done
 \--- 26628 dd count=512 bs=1M if=/dev/zero of=./temp
]
Command terminated with signal Terminated (pid: 26627)
E0222 09:32:21.704880 26626 process.cpp:1963] Failed to shutdown socket with fd 9: Transport endpoint is not connected
{noformat}

after running:

{code}
MESOS_VERBOSE=1 sudo .libs/mesos-tests --gtest_filter=""MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery"" --gtest_repeat=1000 --gtest_break_on_failure
{code}","22/Feb/16 21:18;arojas;[r/43850/|https://reviews.apache.org/r/43850/]: Added a wait for killed tast to prevent misscount of events.","25/Feb/16 12:51;bernd-mesos;commit 61ff6848ad9871f98ec00f517f16b47aff3e58d7
Author: Alexander Rojas <alexander@mesosphere.io>
Date:   Thu Feb 25 13:40:01 2016 +0100

    Added waits in MemoryPressureTests to ensure deterministic behavior.
    
    Sometimes _MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery_ will
    fail because the tracker of the cgroups pressure counter is not
    finished by the time the expectations are set.
    
    This patch ensures that different test milestones are reached;
    e.g. slave is registered, `KILLED_TASK` reached the scheduler,
    etc; before continuing with the test.
    
    Review: https://reviews.apache.org/r/43850/
","25/Feb/16 16:50;arojas;So after fixing the issues raised in previous comments, I managed to reproduce the issue mentioned in the logs posted here. Apparently there is yet another race, where the executor exits before the line {{Future<ResourceStatistics> usage = containerizer2.get()->usage(containerId);}}. I managed to collect two verbose logs for a good and a bad run. I add only the important sections. Pay attention to lines which look like {{I0224 13:53:53.169703 25060 slave.cpp:3528] executor(1)@127.0.0.1:38732 exited}}

The good run:

{noformat}
...
I0224 13:53:52.219846 25063 slave.cpp:1891] Asked to kill task 21236fe6-f5b3-4647-b4b0-fd83827436a3 of framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000
Received killTask
Shutting down
Sending SIGTERM to process tree at pid 31659
Sent SIGTERM to the following process trees:
[
-+- 31659 sh -c while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done
 \--- 31661 dd count=512 bs=1M if=/dev/zero of=./temp
]
Command terminated with signal Terminated (pid: 31659)
I0224 13:53:52.369876 25062 slave.cpp:3002] Handling status update TASK_KILLED (UUID: 4f1a8c80-c4de-4e27-8fd1-79ecb89dcbd8) for task 21236fe6-f5b3-4647-b4b0-fd83827436a3 of framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000 from executor(1)@127.0.0.1:38732
I0224 13:53:52.386056 25059 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 32MB for container d78a1f77-a3a1-44e4-9898-a62523a1c1e0
I0224 13:53:53.113471 25059 mem.cpp:388] Updated 'memory.limit_in_bytes' to 32MB for container d78a1f77-a3a1-44e4-9898-a62523a1c1e0
I0224 13:53:53.117938 25059 status_update_manager.cpp:320] Received status update TASK_KILLED (UUID: 4f1a8c80-c4de-4e27-8fd1-79ecb89dcbd8) for task 21236fe6-f5b3-4647-b4b0-fd83827436a3 of framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000
I0224 13:53:53.118013 25059 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_KILLED (UUID: 4f1a8c80-c4de-4e27-8fd1-79ecb89dcbd8) for task 21236fe6-f5b3-4647-b4b0-fd83827436a3 of framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000
I0224 13:53:53.146458 25058 slave.cpp:3400] Forwarding the update TASK_KILLED (UUID: 4f1a8c80-c4de-4e27-8fd1-79ecb89dcbd8) for task 21236fe6-f5b3-4647-b4b0-fd83827436a3 of framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000 to master@127.0.0.1:57058
I0224 13:53:53.146702 25058 slave.cpp:3310] Sending acknowledgement for status update TASK_KILLED (UUID: 4f1a8c80-c4de-4e27-8fd1-79ecb89dcbd8) for task 21236fe6-f5b3-4647-b4b0-fd83827436a3 of framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000 to executor(1)@127.0.0.1:38732
I0224 13:53:53.147956 25062 master.cpp:4794] Status update TASK_KILLED (UUID: 4f1a8c80-c4de-4e27-8fd1-79ecb89dcbd8) for task 21236fe6-f5b3-4647-b4b0-fd83827436a3 of framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000 from slave 92632338-e777-41c7-a9a3-39dc62fdea4c-S0 at slave(278)@127.0.0.1:57058 (localhost)
I0224 13:53:53.147989 25062 master.cpp:4842] Forwarding status update TASK_KILLED (UUID: 4f1a8c80-c4de-4e27-8fd1-79ecb89dcbd8) for task 21236fe6-f5b3-4647-b4b0-fd83827436a3 of framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000
I0224 13:53:53.148143 25062 master.cpp:6450] Updating the state of task 21236fe6-f5b3-4647-b4b0-fd83827436a3 of framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I0224 13:53:53.149320 25061 master.cpp:3952] Processing ACKNOWLEDGE call 4f1a8c80-c4de-4e27-8fd1-79ecb89dcbd8 for task 21236fe6-f5b3-4647-b4b0-fd83827436a3 of framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000 (default) at scheduler-79245611-a7d2-4220-bae1-4702a34ecf14@127.0.0.1:57058 on slave 92632338-e777-41c7-a9a3-39dc62fdea4c-S0
I0224 13:53:53.149684 25061 master.cpp:6516] Removing task 21236fe6-f5b3-4647-b4b0-fd83827436a3 with resources cpus(*):1; mem(*):256; disk(*):1024 of framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000 on slave 92632338-e777-41c7-a9a3-39dc62fdea4c-S0 at slave(278)@127.0.0.1:57058 (localhost)
I0224 13:53:53.150146 25061 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 4f1a8c80-c4de-4e27-8fd1-79ecb89dcbd8) for task 21236fe6-f5b3-4647-b4b0-fd83827436a3 of framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000
I0224 13:53:53.150410 25061 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_KILLED (UUID: 4f1a8c80-c4de-4e27-8fd1-79ecb89dcbd8) for task 21236fe6-f5b3-4647-b4b0-fd83827436a3 of framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000
I0224 13:53:53.153118 25056 sched.cpp:1903] Asked to stop the driver
I0224 13:53:53.153228 25064 sched.cpp:1143] Stopping framework '92632338-e777-41c7-a9a3-39dc62fdea4c-0000'
I0224 13:53:53.154057 25061 master.cpp:5926] Processing TEARDOWN call for framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000 (default) at scheduler-79245611-a7d2-4220-bae1-4702a34ecf14@127.0.0.1:57058
I0224 13:53:53.154201 25061 master.cpp:5938] Removing framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000 (default) at scheduler-79245611-a7d2-4220-bae1-4702a34ecf14@127.0.0.1:57058
I0224 13:53:53.154716 25062 slave.cpp:2079] Asked to shut down framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000 by master@127.0.0.1:57058
I0224 13:53:53.154887 25062 slave.cpp:2104] Shutting down framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000
I0224 13:53:53.154953 25062 slave.cpp:4198] Shutting down executor '21236fe6-f5b3-4647-b4b0-fd83827436a3' of framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000 at executor(1)@127.0.0.1:38732
I0224 13:53:53.154963 25061 master.cpp:1027] Master terminating
I0224 13:53:53.155953 31653 exec.cpp:390] Executor asked to shutdown
I0224 13:53:53.156373 25061 slave.cpp:3528] master@127.0.0.1:57058 exited
W0224 13:53:53.156425 25061 slave.cpp:3531] Master disconnected! Waiting for a new master to be elected
I0224 13:53:53.157037 25057 hierarchical.cpp:375] Deactivated framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000
I0224 13:53:53.157402 25057 hierarchical.cpp:326] Removed framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000
I0224 13:53:53.160271 25062 containerizer.cpp:1378] Destroying container 'd78a1f77-a3a1-44e4-9898-a62523a1c1e0'
I0224 13:53:53.162210 25062 cgroups.cpp:2427] Freezing cgroup /cgroup/freezer/mesos_test_82cf0b7f-b476-49b0-bfbb-42f4dd0110e9/d78a1f77-a3a1-44e4-9898-a62523a1c1e0
I0224 13:53:53.163861 25059 cgroups.cpp:1409] Successfully froze cgroup /cgroup/freezer/mesos_test_82cf0b7f-b476-49b0-bfbb-42f4dd0110e9/d78a1f77-a3a1-44e4-9898-a62523a1c1e0 after 1.487104ms
I0224 13:53:53.165483 25060 cgroups.cpp:2445] Thawing cgroup /cgroup/freezer/mesos_test_82cf0b7f-b476-49b0-bfbb-42f4dd0110e9/d78a1f77-a3a1-44e4-9898-a62523a1c1e0
I0224 13:53:53.167999 25059 cgroups.cpp:1438] Successfullly thawed cgroup /cgroup/freezer/mesos_test_82cf0b7f-b476-49b0-bfbb-42f4dd0110e9/d78a1f77-a3a1-44e4-9898-a62523a1c1e0 after 2.372864ms
I0224 13:53:53.169703 25060 slave.cpp:3528] executor(1)@127.0.0.1:38732 exited
I0224 13:53:53.226868 25058 containerizer.cpp:1594] Executor for container 'd78a1f77-a3a1-44e4-9898-a62523a1c1e0' has exited
I0224 13:53:53.339517 25057 provisioner.cpp:306] Ignoring destroy request for unknown container d78a1f77-a3a1-44e4-9898-a62523a1c1e0
I0224 13:53:53.340080 25063 slave.cpp:3886] Executor '21236fe6-f5b3-4647-b4b0-fd83827436a3' of framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000 terminated with signal Killed
I0224 13:53:53.340206 25063 slave.cpp:3990] Cleaning up executor '21236fe6-f5b3-4647-b4b0-fd83827436a3' of framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000 at executor(1)@127.0.0.1:38732
I0224 13:53:53.340931 25059 gc.cpp:54] Scheduling '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_JVQWxS/slaves/92632338-e777-41c7-a9a3-39dc62fdea4c-S0/frameworks/92632338-e777-41c7-a9a3-39dc62fdea4c-0000/executors/21236fe6-f5b3-4647-b4b0-fd83827436a3/runs/d78a1f77-a3a1-44e4-9898-a62523a1c1e0' for gc 6.99999605990815days in the future
I0224 13:53:53.341127 25063 slave.cpp:4078] Cleaning up framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000
I0224 13:53:53.341518 25059 gc.cpp:54] Scheduling '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_JVQWxS/slaves/92632338-e777-41c7-a9a3-39dc62fdea4c-S0/frameworks/92632338-e777-41c7-a9a3-39dc62fdea4c-0000/executors/21236fe6-f5b3-4647-b4b0-fd83827436a3' for gc 6.9999960536days in the future
I0224 13:53:53.341814 25059 gc.cpp:54] Scheduling '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_JVQWxS/meta/slaves/92632338-e777-41c7-a9a3-39dc62fdea4c-S0/frameworks/92632338-e777-41c7-a9a3-39dc62fdea4c-0000/executors/21236fe6-f5b3-4647-b4b0-fd83827436a3/runs/d78a1f77-a3a1-44e4-9898-a62523a1c1e0' for gc 6.99999605247704days in the future
I0224 13:53:53.342157 25059 gc.cpp:54] Scheduling '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_JVQWxS/meta/slaves/92632338-e777-41c7-a9a3-39dc62fdea4c-S0/frameworks/92632338-e777-41c7-a9a3-39dc62fdea4c-0000/executors/21236fe6-f5b3-4647-b4b0-fd83827436a3' for gc 6.99999605214222days in the future
I0224 13:53:53.342463 25060 status_update_manager.cpp:282] Closing status update streams for framework 92632338-e777-41c7-a9a3-39dc62fdea4c-0000
I0224 13:53:53.342669 25059 gc.cpp:54] Scheduling '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_JVQWxS/slaves/92632338-e777-41c7-a9a3-39dc62fdea4c-S0/frameworks/92632338-e777-41c7-a9a3-39dc62fdea4c-0000' for gc 6.99999603612148days in the future
I0224 13:53:53.343171 25063 gc.cpp:54] Scheduling '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_JVQWxS/meta/slaves/92632338-e777-41c7-a9a3-39dc62fdea4c-S0/frameworks/92632338-e777-41c7-a9a3-39dc62fdea4c-0000' for gc 6.99999603449185days in the future
I0224 13:53:53.343410 25056 slave.cpp:668] Slave terminating
I0224 13:53:53.349254 25059 cgroups.cpp:2427] Freezing cgroup /cgroup/freezer/mesos_test_82cf0b7f-b476-49b0-bfbb-42f4dd0110e9
I0224 13:53:53.350904 25059 cgroups.cpp:1409] Successfully froze cgroup /cgroup/freezer/mesos_test_82cf0b7f-b476-49b0-bfbb-42f4dd0110e9 after 1.454848ms
I0224 13:53:53.352203 25059 cgroups.cpp:2445] Thawing cgroup /cgroup/freezer/mesos_test_82cf0b7f-b476-49b0-bfbb-42f4dd0110e9
I0224 13:53:53.353524 25060 cgroups.cpp:1438] Successfullly thawed cgroup /cgroup/freezer/mesos_test_82cf0b7f-b476-49b0-bfbb-42f4dd0110e9 after 1.264128ms
[       OK ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery (4261 ms)
{noformat} 

The bad run (the segfault is related to the break on failure flag):

{noformat}
...
I0224 13:53:56.421175 25057 slave.cpp:1891] Asked to kill task 3dd870d0-aa26-47fc-b647-dcf95ef87e06 of framework 21daeed7-a99d-4c93-bc94-956e8e381ab9-0000
Received killTask
Shutting down
Sending SIGTERM to process tree at pid 31706
Sent SIGTERM to the following process trees:
[
-+- 31706 sh -c while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done
 \--- 31708 dd count=512 bs=1M if=/dev/zero of=./temp
]
Command terminated with signal Terminated (pid: 31706)
I0224 13:53:56.576330 25064 slave.cpp:3002] Handling status update TASK_KILLED (UUID: 70055746-96ac-427d-9a40-df962a06ad51) for task 3dd870d0-aa26-47fc-b647-dcf95ef87e06 of framework 21daeed7-a99d-4c93-bc94-956e8e381ab9-0000 from executor(1)@127.0.0.1:52634
I0224 13:53:56.649286 25061 mem.cpp:353] Updated 'memory.soft_limit_in_bytes' to 32MB for container 664b1778-5e51-432d-8e66-a6f275dc6d80
I0224 13:53:57.599311 25063 slave.cpp:3528] executor(1)@127.0.0.1:52634 exited
I0224 13:53:57.684360 25059 containerizer.cpp:1594] Executor for container '664b1778-5e51-432d-8e66-a6f275dc6d80' has exited
I0224 13:53:57.688079 25059 containerizer.cpp:1378] Destroying container '664b1778-5e51-432d-8e66-a6f275dc6d80'
I0224 13:53:57.704903 25057 cgroups.cpp:2427] Freezing cgroup /cgroup/freezer/mesos_test_8f53be60-0c43-42da-9210-2d9ec670cd8b/664b1778-5e51-432d-8e66-a6f275dc6d80
I0224 13:53:57.715003 25057 cgroups.cpp:1409] Successfully froze cgroup /cgroup/freezer/mesos_test_8f53be60-0c43-42da-9210-2d9ec670cd8b/664b1778-5e51-432d-8e66-a6f275dc6d80 after 9.914112ms
I0224 13:53:57.745836 25057 cgroups.cpp:2445] Thawing cgroup /cgroup/freezer/mesos_test_8f53be60-0c43-42da-9210-2d9ec670cd8b/664b1778-5e51-432d-8e66-a6f275dc6d80
I0224 13:53:57.778103 25058 cgroups.cpp:1438] Successfullly thawed cgroup /cgroup/freezer/mesos_test_8f53be60-0c43-42da-9210-2d9ec670cd8b/664b1778-5e51-432d-8e66-a6f275dc6d80 after 29.500928ms
I0224 13:53:57.841120 25061 mem.cpp:388] Updated 'memory.limit_in_bytes' to 32MB for container 664b1778-5e51-432d-8e66-a6f275dc6d80
I0224 13:53:57.852152 25062 status_update_manager.cpp:320] Received status update TASK_KILLED (UUID: 70055746-96ac-427d-9a40-df962a06ad51) for task 3dd870d0-aa26-47fc-b647-dcf95ef87e06 of framework 21daeed7-a99d-4c93-bc94-956e8e381ab9-0000
I0224 13:53:57.856850 25062 status_update_manager.cpp:824] Checkpointing UPDATE for status update TASK_KILLED (UUID: 70055746-96ac-427d-9a40-df962a06ad51) for task 3dd870d0-aa26-47fc-b647-dcf95ef87e06 of framework 21daeed7-a99d-4c93-bc94-956e8e381ab9-0000
I0224 13:53:57.960736 25058 provisioner.cpp:306] Ignoring destroy request for unknown container 664b1778-5e51-432d-8e66-a6f275dc6d80
I0224 13:53:57.967319 25058 slave.cpp:3886] Executor '3dd870d0-aa26-47fc-b647-dcf95ef87e06' of framework 21daeed7-a99d-4c93-bc94-956e8e381ab9-0000 exited with status 0
I0224 13:53:57.996193 25058 slave.cpp:3400] Forwarding the update TASK_KILLED (UUID: 70055746-96ac-427d-9a40-df962a06ad51) for task 3dd870d0-aa26-47fc-b647-dcf95ef87e06 of framework 21daeed7-a99d-4c93-bc94-956e8e381ab9-0000 to master@127.0.0.1:57058
I0224 13:53:57.997020 25058 slave.cpp:3310] Sending acknowledgement for status update TASK_KILLED (UUID: 70055746-96ac-427d-9a40-df962a06ad51) for task 3dd870d0-aa26-47fc-b647-dcf95ef87e06 of framework 21daeed7-a99d-4c93-bc94-956e8e381ab9-0000 to executor(1)@127.0.0.1:52634
I0224 13:53:57.997710 25059 master.cpp:4794] Status update TASK_KILLED (UUID: 70055746-96ac-427d-9a40-df962a06ad51) for task 3dd870d0-aa26-47fc-b647-dcf95ef87e06 of framework 21daeed7-a99d-4c93-bc94-956e8e381ab9-0000 from slave 21daeed7-a99d-4c93-bc94-956e8e381ab9-S0 at slave(280)@127.0.0.1:57058 (localhost)
I0224 13:53:57.997799 25059 master.cpp:4842] Forwarding status update TASK_KILLED (UUID: 70055746-96ac-427d-9a40-df962a06ad51) for task 3dd870d0-aa26-47fc-b647-dcf95ef87e06 of framework 21daeed7-a99d-4c93-bc94-956e8e381ab9-0000
I0224 13:53:57.998181 25059 master.cpp:6450] Updating the state of task 3dd870d0-aa26-47fc-b647-dcf95ef87e06 of framework 21daeed7-a99d-4c93-bc94-956e8e381ab9-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
E0224 13:53:57.999061 25058 process.cpp:1963] Failed to shutdown socket with fd 392: Transport endpoint is not connected
I0224 13:53:57.999202 25064 master.cpp:3952] Processing ACKNOWLEDGE call 70055746-96ac-427d-9a40-df962a06ad51 for task 3dd870d0-aa26-47fc-b647-dcf95ef87e06 of framework 21daeed7-a99d-4c93-bc94-956e8e381ab9-0000 (default) at scheduler-594ed0b4-ba1d-4439-82df-2bfb52d87c29@127.0.0.1:57058 on slave 21daeed7-a99d-4c93-bc94-956e8e381ab9-S0
I0224 13:53:57.999512 25064 master.cpp:6516] Removing task 3dd870d0-aa26-47fc-b647-dcf95ef87e06 with resources cpus(*):1; mem(*):256; disk(*):1024 of framework 21daeed7-a99d-4c93-bc94-956e8e381ab9-0000 on slave 21daeed7-a99d-4c93-bc94-956e8e381ab9-S0 at slave(280)@127.0.0.1:57058 (localhost)
../../src/tests/containerizer/memory_pressure_tests.cpp:322: Failure
(usage).failure(): Unknown container: 664b1778-5e51-432d-8e66-a6f275dc6d80
*** Aborted at 1456350838 (unix time) try ""date -d @1456350838"" if you are using GNU date ***
I0224 13:53:58.006049 25060 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 70055746-96ac-427d-9a40-df962a06ad51) for task 3dd870d0-aa26-47fc-b647-dcf95ef87e06 of framework 21daeed7-a99d-4c93-bc94-956e8e381ab9-0000
PC: @          0x1675fa0 testing::UnitTest::AddTestPartResult()
I0224 13:53:58.010462 25060 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_KILLED (UUID: 70055746-96ac-427d-9a40-df962a06ad51) for task 3dd870d0-aa26-47fc-b647-dcf95ef87e06 of framework 21daeed7-a99d-4c93-bc94-956e8e381ab9-0000
*** SIGSEGV (@0x0) received by PID 25056 (TID 0x7faace5ed840) from PID 0; stack trace: ***
    @     0x7faac6780790 (unknown)
    @          0x1675fa0 testing::UnitTest::AddTestPartResult()
    @          0x166a9d9 testing::internal::AssertHelper::operator=()
I0224 13:53:58.042770 25057 slave.cpp:3990] Cleaning up executor '3dd870d0-aa26-47fc-b647-dcf95ef87e06' of framework 21daeed7-a99d-4c93-bc94-956e8e381ab9-0000 at executor(1)@127.0.0.1:52634
I0224 13:53:58.051208 25057 slave.cpp:4078] Cleaning up framework 21daeed7-a99d-4c93-bc94-956e8e381ab9-0000
I0224 13:53:58.051672 25057 gc.cpp:54] Scheduling '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_uw6mf9/slaves/21daeed7-a99d-4c93-bc94-956e8e381ab9-S0/frameworks/21daeed7-a99d-4c93-bc94-956e8e381ab9-0000/executors/3dd870d0-aa26-47fc-b647-dcf95ef87e06/runs/664b1778-5e51-432d-8e66-a6f275dc6d80' for gc 6.99999942607704days in the future
I0224 13:53:58.051944 25060 status_update_manager.cpp:282] Closing status update streams for framework 21daeed7-a99d-4c93-bc94-956e8e381ab9-0000
I0224 13:53:58.052243 25057 gc.cpp:54] Scheduling '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_uw6mf9/slaves/21daeed7-a99d-4c93-bc94-956e8e381ab9-S0/frameworks/21daeed7-a99d-4c93-bc94-956e8e381ab9-0000/executors/3dd870d0-aa26-47fc-b647-dcf95ef87e06' for gc 6.99999942398222days in the future
I0224 13:53:58.052500 25057 gc.cpp:54] Scheduling '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_uw6mf9/meta/slaves/21daeed7-a99d-4c93-bc94-956e8e381ab9-S0/frameworks/21daeed7-a99d-4c93-bc94-956e8e381ab9-0000/executors/3dd870d0-aa26-47fc-b647-dcf95ef87e06/runs/664b1778-5e51-432d-8e66-a6f275dc6d80' for gc 6.99999942333333days in the future
I0224 13:53:58.056956 25057 gc.cpp:54] Scheduling '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_uw6mf9/meta/slaves/21daeed7-a99d-4c93-bc94-956e8e381ab9-S0/frameworks/21daeed7-a99d-4c93-bc94-956e8e381ab9-0000/executors/3dd870d0-aa26-47fc-b647-dcf95ef87e06' for gc 6.99999940888889days in the future
I0224 13:53:58.057041 25057 gc.cpp:54] Scheduling '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_uw6mf9/slaves/21daeed7-a99d-4c93-bc94-956e8e381ab9-S0/frameworks/21daeed7-a99d-4c93-bc94-956e8e381ab9-0000' for gc 6.99999940421333days in the future
I0224 13:53:58.057101 25057 gc.cpp:54] Scheduling '/tmp/MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_uw6mf9/meta/slaves/21daeed7-a99d-4c93-bc94-956e8e381ab9-S0/frameworks/21daeed7-a99d-4c93-bc94-956e8e381ab9-0000' for gc 6.99999940354074days in the future54074days in the future
    @          0x164b847 mesos::internal::tests::MemoryPressureMesosTest_CGROUPS_ROOT_SlaveRecovery_Test::TestBody()
    @          0x1693a00 testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x168ea2e testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x166fd79 testing::Test::Run()
    @          0x1670507 testing::TestInfo::Run()
    @          0x1670b42 testing::TestCase::Run()
    @          0x1677491 testing::internal::UnitTestImpl::RunAllTests()
    @          0x169468f testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x168f5ba testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x16761c1 testing::UnitTest::Run()
    @           0xe1da5e RUN_ALL_TESTS()
    @           0xe1d674 main
    @     0x7faac55e8d5d __libc_start_main
    @           0x9a56b9 (unknown)
/var/tmp/sclnbI5N2: line 8: 25056 Segmentation fault      './.libs/mesos-tests' '--gtest_filter=MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery' '--gtest_repeat=1000' '--gtest_break_on_failure'
{noformat}","29/Feb/16 05:57;arojas;Commit 16aa038949741f4dc6bf43423dc0340f869605ce solves the issue.","29/Feb/16 09:55;bernd-mesos;https://reviews.apache.org/r/43799/","02/Mar/16 18:19;arojas;My previous [comment|https://issues.apache.org/jira/browse/MESOS-4047?focusedCommentId=15167418&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15167418] still describes the situation. I feel the problem resides in the assumptions the tests makes which may be incorrect. 

With what I know, I would recommend rethink the whole tests, but more investigation is advised before proceeding.","03/Mar/16 23:40;arojas;[r/44362/|https://reviews.apache.org/r/44362/]: Prevents early container destruction in MemoryPressureTests.","04/Mar/16 00:12;vinodkone;commit 5387a4d8ac44a8ba7a7f7d62bee0f7276b82545a
Author: Alexander Rojas <alexander@mesosphere.io>
Date:   Thu Mar 3 16:00:53 2016 -0800

    Prevents early container destruction in MemoryPressureTests.
    
    Prevents the container to be reaped by pausing the clock before
    killing the task, so that measurements from the containerizer can be
    taken even if the executor has already exited.
    
    Review: https://reviews.apache.org/r/44362/
",,,,,,,,,,,,,,,,
ContentType/SchedulerTest is flaky.,MESOS-4029,12917103,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,tillt,tillt,01/Dec/15 00:06,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,0.26.0,,,,,,,,0.28.0,,,,,,,,,,,0,flaky,flaky-test,mesosphere,,,,,,"SSL build, [Ubuntu 14.04|https://github.com/tillt/mesos-vagrant-ci/blob/master/ubuntu14/setup.sh], non-root test run.

{noformat}
[----------] 22 tests from ContentType/SchedulerTest
[ RUN      ] ContentType/SchedulerTest.Subscribe/0
[       OK ] ContentType/SchedulerTest.Subscribe/0 (48 ms)
*** Aborted at 1448928007 (unix time) try ""date -d @1448928007"" if you are using GNU date ***
[ RUN      ] ContentType/SchedulerTest.Subscribe/1
PC: @          0x1451b8e testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
*** SIGSEGV (@0x100000030) received by PID 21320 (TID 0x2b549e5d4700) from PID 48; stack trace: ***
    @     0x2b54c95940b7 os::Linux::chained_handler()
    @     0x2b54c9598219 JVM_handle_linux_signal
    @     0x2b5496300340 (unknown)
    @          0x1451b8e testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
    @           0xe2ea6d _ZN7testing8internal18FunctionMockerBaseIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeIS6_SaIS6_EEEEE10InvokeWithERKSt5tupleIJSC_EE
    @           0xe2b1bc testing::internal::FunctionMocker<>::Invoke()
    @          0x1118aed mesos::internal::tests::SchedulerTest::Callbacks::received()
    @          0x111c453 _ZNKSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS0_2v19scheduler5EventESt5dequeIS8_SaIS8_EEEEEclIJSE_EvEEvRS4_DpOT_
    @          0x111c001 _ZNSt5_BindIFSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS1_2v19scheduler5EventESt5dequeIS9_SaIS9_EEEEESt17reference_wrapperIS5_ESt12_PlaceholderILi1EEEE6__callIvJSF_EJLm0ELm1EEEET_OSt5tupleIJDpT0_EESt12_Index_tupleIJXspT1_EEE
    @          0x111b90d _ZNSt5_BindIFSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS1_2v19scheduler5EventESt5dequeIS9_SaIS9_EEEEESt17reference_wrapperIS5_ESt12_PlaceholderILi1EEEEclIJSF_EvEET0_DpOT_
    @          0x111ae09 std::_Function_handler<>::_M_invoke()
    @     0x2b5493c6da09 std::function<>::operator()()
    @     0x2b5493c688ee process::AsyncExecutorProcess::execute<>()
    @     0x2b5493c6db2a _ZZN7process8dispatchI7NothingNS_20AsyncExecutorProcessERKSt8functionIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeIS8_SaIS8_EEEEESC_PvSG_SC_SJ_EENS_6FutureIT_EERKNS_3PIDIT0_EEMSO_FSL_T1_T2_T3_ET4_T5_T6_ENKUlPNS_11ProcessBaseEE_clES11_
    @     0x2b5493c765a4 _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchI7NothingNS0_20AsyncExecutorProcessERKSt8functionIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeISC_SaISC_EEEEESG_PvSK_SG_SN_EENS0_6FutureIT_EERKNS0_3PIDIT0_EEMSS_FSP_T1_T2_T3_ET4_T5_T6_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x2b54946b1201 std::function<>::operator()()
    @     0x2b549469960f process::ProcessBase::visit()
    @     0x2b549469d480 process::DispatchEvent::visit()
    @           0x9dc0ba process::ProcessBase::serve()
    @     0x2b54946958cc process::ProcessManager::resume()
    @     0x2b5494692a9c _ZZN7process14ProcessManager12init_threadsEvENKUlRKSt11atomic_boolE_clES3_
    @     0x2b549469ccac _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEE6__callIvIEILm0EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE
    @     0x2b549469cc5c _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEEclIIEvEET0_DpOT_
    @     0x2b549469cbee _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEE9_M_invokeIIEEEvSt12_Index_tupleIIXspT_EEE
    @     0x2b549469cb45 _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEEclEv
    @     0x2b549469cade _ZNSt6thread5_ImplISt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS6_EEEvEEE6_M_runEv
    @     0x2b5495b81a40 (unknown)
    @     0x2b54962f8182 start_thread
    @     0x2b549660847d (unknown)
make[3]: *** [check-local] Segmentation fault
make[3]: Leaving directory `/home/vagrant/mesos/build/src'
make[2]: *** [check-am] Error 2
make[2]: Leaving directory `/home/vagrant/mesos/build/src'
make[1]: *** [check] Error 2
make[1]: Leaving directory `/home/vagrant/mesos/build/src'
make: *** [check-recursive] Error 1
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-7111,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-01 01:25:58.96,,,false,MESOS-2174,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 04 00:11:49 UTC 2016,,,,,,,"0|hzzzmn:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 23,Mesosphere Sprint 30,,,,,,,,,,3.0,,,,,,,,,,,"01/Dec/15 00:40;tillt;{noformat}
[ RUN      ] ContentType/SchedulerTest.TaskRunning/0
*** Aborted at 1448930379 (unix time) try ""date -d @1448930379"" if you are using GNU date ***
I1201 00:39:39.100646 17042 leveldb.cpp:176] Opened db in 2.364817ms
PC: @          0x1451b8e testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
*** SIGSEGV (@0x100000030) received by PID 17042 (TID 0x7f0b0cc27700) from PID 48; stack trace: ***
I1201 00:39:39.101135 17042 leveldb.cpp:183] Compacted db in 408797ns
I1201 00:39:39.101176 17042 leveldb.cpp:198] Created db iterator in 16480ns
I1201 00:39:39.101187 17042 leveldb.cpp:204] Seeked to beginning of db in 1246ns
I1201 00:39:39.101194 17042 leveldb.cpp:273] Iterated through 0 keys in the db in 143ns
I1201 00:39:39.101230 17042 replica.cpp:780] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
    @     0x7f0b158fc340 (unknown)
I1201 00:39:39.101763 17057 recover.cpp:449] Starting replica recovery
I1201 00:39:39.102267 17063 recover.cpp:475] Replica is in EMPTY status
I1201 00:39:39.102772 17063 master.cpp:367] Master 9f2eedb2-8e4b-465d-95c9-9989325a0267 (ubuntu14) started on 127.0.1.1:44839
I1201 00:39:39.102807 17063 master.cpp:369] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/C9wpvk/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/C9wpvk/master"" --zk_session_timeout=""10secs""
I1201 00:39:39.103037 17063 master.cpp:416] Master allowing unauthenticated frameworks to register
I1201 00:39:39.103060 17063 master.cpp:419] Master only allowing authenticated slaves to register
I1201 00:39:39.103071 17063 credentials.hpp:37] Loading credentials for authentication from '/tmp/C9wpvk/credentials'
I1201 00:39:39.103323 17063 master.cpp:458] Using default 'crammd5' authenticator
I1201 00:39:39.103426 17063 master.cpp:495] Authorization enabled
I    @          0x1451b8e testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
1201 00:39:39.103624 17061 replica.cpp:676] Replica in EMPTY status received a broadcasted recover request from (13624)@127.0.1.1:44839
I1201 00:39:39.105423 17061 master.cpp:1606] The newly elected leader is master@127.0.1.1:44839 with id 9f2eedb2-8e4b-465d-95c9-9989325a0267
I1201 00:39:39.105872 17061 master.cpp:1619] Elected as the leading master!
I1201 00:39:39.105892 17061 master.cpp:1379] Recovering from registrar
I1201 00:39:39.105700 17062 recover.cpp:195] Received a recover response from a replica in EMPTY status
I1201 00:39:39.105999 17063 registrar.cpp:309] Recovering registrar
    @           0xe2ea6d _ZN7testing8internal18FunctionMockerBaseIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeIS6_SaIS6_EEEEE10InvokeWithERKSt5tupleIJSC_EE
I1201 00:39:39.106724 17062 recover.cpp:566] Updating replica status to STARTING
I1201 00:39:39.107606 17057 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 712403ns
I1201 00:39:39.107657 17057 replica.cpp:323] Persisted replica status to STARTING
I1201 00:39:39.107832 17057 recover.cpp:475] Replica is in STARTING status
I1201 00:39:39.108538 17062 replica.cpp:676] Replica in STARTING status received a broadcasted recover request from (13625)@127.0.1.1:44839
I1201 00:39:39.108942 17062 recover.cpp:195] Received a recover response from a replica in STARTING status
    @           0xe2b1bc testing::internal::FunctionMocker<>::Invoke()
I1201 00:39:39.109571 17062 recover.cpp:566] Updating replica status to VOTING
I1201 00:39:39.110317 17057 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 286144ns
I1201 00:39:39.110373 17057 replica.cpp:323] Persisted replica status to VOTING
I1201 00:39:39.110457 17057 recover.cpp:580] Successfully joined the Paxos group
I1201 00:39:39.110757 17057 recover.cpp:464] Recover process terminated
    @          0x1118aed mesos::internal::tests::SchedulerTest::Callbacks::received()
I1201 00:39:39.111263 17062 log.cpp:661] Attempting to start the writer
    @          0x111c453 _ZNKSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS0_2v19scheduler5EventESt5dequeIS8_SaIS8_EEEEEclIJSE_EvEEvRS4_DpOT_
I1201 00:39:39.112156 17058 replica.cpp:496] Replica received implicit promise request from (13626)@127.0.1.1:44839 with proposal 1
I1201 00:39:39.112442 17058 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 232139ns
I1201 00:39:39.112490 17058 replica.cpp:345] Persisted promised to 1
I    @          0x111c001 _ZNSt5_BindIFSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS1_2v19scheduler5EventESt5dequeIS9_SaIS9_EEEEESt17reference_wrapperIS5_ESt12_PlaceholderILi1EEEE6__callIvJSF_EJLm0ELm1EEEET_OSt5tupleIJDpT0_EESt12_Index_tupleIJXspT1_EEE
1201 00:39:39.113093 17057 coordinator.cpp:240] Coordinator attempting to fill missing positions
I1201 00:39:39.114286 17060 replica.cpp:391] Replica received explicit promise request from (13627)@127.0.1.1:44839 for position 0 with proposal 2
    @          0x111b90d _ZNSt5_BindIFSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS1_2v19scheduler5EventESt5dequeIS9_SaIS9_EEEEESt17reference_wrapperIS5_ESt12_PlaceholderILi1EEEEclIJSF_EvEET0_DpOT_
I1201 00:39:39.114645 17060 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 301419ns
I1201 00:39:39.114687 17060 replica.cpp:715] Persisted action at 0
    @          0x111ae09 std::_Function_handler<>::_M_invoke()
I1201 00:39:39.115533 17058 replica.cpp:540] Replica received write request for position 0 from (13628)@127.0.1.1:44839
I1201 00:39:39.115600 17058 leveldb.cpp:438] Reading position from leveldb took 20141ns
I1201 00:39:39.115886 17058 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 257171ns
I1201 00:39:39.115928 17058 replica.cpp:715] Persisted action at 0
    @     0x7f0b19e18979 std::function<>::operator()()
I1201 00:39:39.116281 17058 replica.cpp:694] Replica received learned notice for position 0 from @0.0.0.0:0
I1201 00:39:39.116608 17058 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 303631ns
I1201 00:39:39.116652 17058 replica.cpp:715] Persisted action at 0
I1201 00:39:39.116668 17058 replica.cpp:700] Replica learned NOP action at position 0
I1201 00:39:39.117130 17058 log.cpp:677] Writer started with ending position 0
I1201 00:39:39.118099 17058 leveldb.cpp:438] Reading position from leveldb took 22654ns
    @     0x7f0b19e1385e process::AsyncExecutorProcess::execute<>()
I1201 00:39:39.118892 17056 registrar.cpp:342] Successfully fetched the registry (0B) in 12.718848ms
I1201 00:39:39.119061 17056 registrar.cpp:441] Applied 1 operations in 34716ns; attempting to update the 'registry'
I1201 00:39:39.119572 17062 log.cpp:685] Attempting to append 158 bytes to the log
I1201 00:39:39.119781 17061 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 1
I1201 00:39:39.120560 17061 replica.cpp:540] Replica received write request for position 1 from (13629)@127.0.1.1:44839
I1201 00:39:39.120977 17061 leveldb.cpp:343] Persisting action (177 bytes) to leveldb took 314676ns
I1201 00:39:39.121022 17061 replica.cpp:715] Persisted action at 1
    @     0x7f0b19e18a9a _ZZN7process8dispatchI7NothingNS_20AsyncExecutorProcessERKSt8functionIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeIS8_SaIS8_EEEEESC_PvSG_SC_SJ_EENS_6FutureIT_EERKNS_3PIDIT0_EEMSO_FSL_T1_T2_T3_ET4_T5_T6_ENKUlPNS_11ProcessBaseEE_clES11_
I1201 00:39:39.121557 17063 replica.cpp:694] Replica received learned notice for position 1 from @0.0.0.0:0
I1201 00:39:39.121863 17063 leveldb.cpp:343] Persisting action (179 bytes) to leveldb took 276125ns
I1201 00:39:39.121886 17063 replica.cpp:715] Persisted action at 1
I1201 00:39:39.121906 17063 replica.cpp:700] Replica learned APPEND action at position 1
I1201 00:39:39.122795 17057 registrar.cpp:486] Successfully updated the 'registry' in 3.64288ms
I1201 00:39:39.122915 17057 registrar.cpp:372] Successfully recovered registrar
I1201 00:39:39.122982 17060 log.cpp:704] Attempting to truncate the log to 1
I1201 00:39:39.123113 17060 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 2
I1201 00:39:39.123134 17057 master.cpp:1416] Recovered 0 slaves from the Registry (120B) ; allowing 10mins for slaves to re-register
I    @     0x7f0b19e21514 _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchI7NothingNS0_20AsyncExecutorProcessERKSt8functionIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeISC_SaISC_EEEEESG_PvSK_SG_SN_EENS0_6FutureIT_EERKNS0_3PIDIT0_EEMSS_FSP_T1_T2_T3_ET4_T5_T6_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
1201 00:39:39.124070 17062 replica.cpp:540] Replica received write request for position 2 from (13630)@127.0.1.1:44839
I1201 00:39:39.124531 17062 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 348386ns
I1201 00:39:39.124588 17062 replica.cpp:715] Persisted action at 2
I1201 00:39:39.125355 17062 replica.cpp:694] Replica received learned notice for position 2 from @0.0.0.0:0
I1201 00:39:39.125849 17062 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 314890ns
I1201 00:39:39.125913 17062 leveldb.cpp:401] Deleting ~1 keys from leveldb took 22454ns
I1201 00:39:39.125928 17062 replica.cpp:715] Persisted action at 2
I1201 00:39:39.125946 17062 replica.cpp:700] Replica learned TRUNCATE action at position 2
    @     0x7f0b1a85c16f std::function<>::operator()()
    @     0x7f0b1a84457d process::ProcessBase::visit()
    @     0x7f0b1a8483ee process::DispatchEvent::visit()
    @           0x9dc0ba process::ProcessBase::serve()
    @     0x7f0b1a84083a process::ProcessManager::resume()
    @     0x7f0b1a83da0a _ZZN7process14ProcessManager12init_threadsEvENKUlRKSt11atomic_boolE_clES3_
    @     0x7f0b1a847c1a _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEE6__callIvIEILm0EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE
    @     0x7f0b1a847bca _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEEclIIEvEET0_DpOT_
    @     0x7f0b1a847b5c _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEE9_M_invokeIIEEEvSt12_Index_tupleIIXspT_EEE
    @     0x7f0b1a847ab3 _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEEclEv
    @     0x7f0b1a847a4c _ZNSt6thread5_ImplISt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS6_EEEvEEE6_M_runEv
    @     0x7f0b160d7a40 (unknown)
    @     0x7f0b158f4182 start_thread
    @     0x7f0b1562147d (unknown)
{noformat}

The above happened in tight loop run of the suite on Ubuntu 14 in iteration #34:
{noformat}
sudo ./bin/mesos-tests.sh --gtest_repeat=-1 --gtest_break_on_failure --gtest_filter=""ContentType/SchedulerTest*"" —verbose
{noformat}","01/Dec/15 01:03;tillt;This one just popped up. Different test but similarly aborted before things even really started...

{noformat}
[ RUN      ] MasterMaintenanceTest.PreV1SchedulerSupport
*** Aborted at 1448931496 (unix time) try ""date -d @1448931496"" if you are using GNU date ***
PC: @     0x7fff40a9d3f0 (unknown)
*** SIGSEGV (@0x7fff40a9d3f0) received by PID 20263 (TID 0x7f5fec377700) from PID 1084871664; stack trace: ***
    @     0x7f5fda6180b7 os::Linux::chained_handler()
I1201 00:58:16.616706 20263 leveldb.cpp:176] Opened db in 2.582909ms
I1201 00:58:16.617106 20263 leveldb.cpp:183] Compacted db in 347144ns
I1201 00:58:16.617167 20263 leveldb.cpp:198] Created db iterator in 14080ns
I1201 00:58:16.617182 20263 leveldb.cpp:204] Seeked to beginning of db in 1223ns
I1201 00:58:16.617188 20263 leveldb.cpp:273] Iterated through 0 keys in the db in 158ns
I1201 00:58:16.617216 20263 replica.cpp:780] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
    @     0x7f5fda61c219 JVM_handle_linux_signal
I1201 00:58:16.617596 20284 recover.cpp:449] Starting replica recovery
I1201 00:58:16.617769 20284 recover.cpp:475] Replica is in EMPTY status
    @     0x7f5ff504c340 (unknown)
I    @     0x7fff40a9d3f0 (unknown)
{noformat}
","01/Dec/15 01:25;bmahler;Took a look. For the traces with the mock expectation crashing (traces containing UntypedInvokeWith), the issue appears to be that our stack object dependencies in the tests are not safely ordered.

Specifically, we pass a pointer of the {{Queue<Events>}} ([here|https://github.com/apache/mesos/blob/0.26.0-rc2/src/tests/scheduler_tests.cpp#L147]) into the expectations on {{Callbacks}} above. During destruction of the test, the {{Mesos}} class will destruct **after** the {{Queue<Events>}} is already destructed. If a non-HEARTBEAT event arrives in this window, the expectation will try to dereference the destructed {{Queue<Events>}} object.","01/Dec/15 03:39;anandmazumdar;https://reviews.apache.org/r/40811/","01/Dec/15 14:42;arojas;After applying the patch I still got the following crashes:

{noformat}
[       OK ] ContentType/SchedulerTest.Subscribe/0 (66 ms)
[ RUN      ] ContentType/SchedulerTest.Subscribe/1
    @     0x7fb100193686  google::LogMessage::Fail()
    @     0x7fb100198dac  google::RawLog__()
    @     0x7fb0ff3d9c14  __cxa_pure_virtual
    @          0x14e3c38  testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
    @           0xe20259  _ZN7testing8internal18FunctionMockerBaseIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeIS6_SaIS6_EEE
EE10InvokeWithERKSt5tupleIJSC_EE
    @           0xe1c9a8  testing::internal::FunctionMocker<>::Invoke()
    @          0x118d6b9  mesos::internal::tests::SchedulerTest::Callbacks::received()
    @          0x119101f  _ZNKSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS0_2v19scheduler5EventES
t5dequeIS8_SaIS8_EEEEEclIJSE_EvEEvRS4_DpOT_
    @          0x1190bcd  _ZNSt5_BindIFSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS1_2v19schedule
r5EventESt5dequeIS9_SaIS9_EEEEESt17reference_wrapperIS5_ESt12_PlaceholderILi1EEEE6__callIvJSF_EJLm0ELm1EEEET_OSt5tupleIJDpT0_EES
t12_Index_tupleIJXspT1_EEE
    @          0x11904d9  _ZNSt5_BindIFSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS1_2v19schedule
r5EventESt5dequeIS9_SaIS9_EEEEESt17reference_wrapperIS5_ESt12_PlaceholderILi1EEEEclIJSF_EvEET0_DpOT_
    @          0x118f9d5  std::_Function_handler<>::_M_invoke()
    @     0x7fb0ff69b103  std::function<>::operator()()
    @     0x7fb0ff695fe8  process::AsyncExecutorProcess::execute<>()
    @     0x7fb0ff69b224  _ZZN7process8dispatchI7NothingNS_20AsyncExecutorProcessERKSt8functionIFvRKSt5queueIN5mesos2v19schedule
r5EventESt5dequeIS8_SaIS8_EEEEESC_PvSG_SC_SJ_EENS_6FutureIT_EERKNS_3PIDIT0_EEMSO_FSL_T1_T2_T3_ET4_T5_T6_ENKUlPNS_11ProcessBaseEE
_clES11_
    @     0x7fb0ff6a3c9e  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchI7NothingNS0_20AsyncExecutorProcessE
RKSt8functionIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeISC_SaISC_EEEEESG_PvSK_SG_SN_EENS0_6FutureIT_EERKNS0_3PIDIT0_EEMSS
_FSP_T1_T2_T3_ET4_T5_T6_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x7fb1001015e1  std::function<>::operator()()
    @     0x7fb1000e9927  process::ProcessBase::visit()
    @     0x7fb1000ed516  process::DispatchEvent::visit()
    @           0x9e844a  process::ProcessBase::serve()
    @     0x7fb1000e5bf0  process::ProcessManager::resume()
    @     0x7fb1000e2ca6  _ZZN7process14ProcessManager12init_threadsEvENKUlRKSt11atomic_boolE_clES3_
    @     0x7fb1000eccd8  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_
EEE6__callIvIEILm0EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE
    @     0x7fb1000ecc88  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_
EEEclIIEvEET0_DpOT_
    @     0x7fb1000ecc1a  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17ref
erence_wrapperIS4_EEEvEE9_M_invokeIIEEEvSt12_Index_tupleIIXspT_EEE
    @     0x7fb1000ecb71  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17ref
erence_wrapperIS4_EEEvEEclEv
    @     0x7fb1000ecb0a  _ZNSt6thread5_ImplISt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atom
ic_boolE_St17reference_wrapperIS6_EEEvEEE6_M_runEv
    @     0x7fb0fb4a9a40  (unknown)
    @     0x7fb0facc6182  start_thread
    @     0x7fb0fa9f347d  (unknown)
Aborted (core dumped)
{noformat}

And

{noformat}
[ RUN      ] ContentType/SchedulerTest.Subscribe/1
I1201 15:20:59.848814 32637 leveldb.cpp:174] Opened db in 5.713001ms
I1201 15:20:59.850643 32637 leveldb.cpp:181] Compacted db in 1.722714ms
I1201 15:20:59.851052 32637 leveldb.cpp:196] Created db iterator in 120371ns
I1201 15:20:59.851768 32637 leveldb.cpp:202] Seeked to beginning of db in 3411ns
I1201 15:20:59.851850 32637 leveldb.cpp:271] Iterated through 0 keys in the db in 15133ns
I1201 15:20:59.852177 32637 replica.cpp:778] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1201 15:20:59.853752 32657 recover.cpp:447] Starting replica recovery
I1201 15:20:59.854022 32657 recover.cpp:473] Replica is in EMPTY status
I1201 15:20:59.855265 32652 replica.cpp:674] Replica in EMPTY status received a broadcasted recover request from (6918)@127.0.1.1:46010
I1201 15:20:59.855675 32652 recover.cpp:193] Received a recover response from a replica in EMPTY status
I1201 15:20:59.855649 32656 master.cpp:365] Master b893dcee-362e-4fcf-81ac-d190058b8682 (ubuntu-vm) started on 127.0.1.1:46010
I1201 15:20:59.856055 32656 master.cpp:367] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/UWVLwW/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/UWVLwW/master"" --zk_session_timeout=""10secs""
I1201 15:20:59.856403 32656 master.cpp:414] Master allowing unauthenticated frameworks to register
I1201 15:20:59.856416 32656 master.cpp:417] Master only allowing authenticated slaves to register
I1201 15:20:59.856422 32656 credentials.hpp:35] Loading credentials for authentication from '/tmp/UWVLwW/credentials'
I1201 15:20:59.856691 32656 master.cpp:456] Using default 'crammd5' authenticator
I1201 15:20:59.856819 32656 master.cpp:493] Authorization enabled
I1201 15:20:59.858414 32658 recover.cpp:564] Updating replica status to STARTING
I1201 15:20:59.858800 32652 master.cpp:1637] The newly elected leader is master@127.0.1.1:46010 with id b893dcee-362e-4fcf-81ac-d190058b8682
I1201 15:20:59.859093 32652 master.cpp:1650] Elected as the leading master!
I1201 15:20:59.859112 32652 master.cpp:1395] Recovering from registrar
I1201 15:20:59.859347 32652 registrar.cpp:307] Recovering registrar
I1201 15:20:59.861415 32655 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.863725ms
I1201 15:20:59.861465 32655 replica.cpp:321] Persisted replica status to STARTING
I1201 15:20:59.861812 32655 recover.cpp:473] Replica is in STARTING status
I1201 15:20:59.862898 32655 replica.cpp:674] Replica in STARTING status received a broadcasted recover request from (6919)@127.0.1.1:46010
I1201 15:20:59.863400 32655 recover.cpp:193] Received a recover response from a replica in STARTING status
I1201 15:20:59.863903 32655 recover.cpp:564] Updating replica status to VOTING
I1201 15:20:59.865389 32655 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.275702ms
I1201 15:20:59.865453 32655 replica.cpp:321] Persisted replica status to VOTING
I1201 15:20:59.865746 32655 recover.cpp:578] Successfully joined the Paxos group
I1201 15:20:59.866067 32655 recover.cpp:462] Recover process terminated
I1201 15:20:59.866922 32654 log.cpp:659] Attempting to start the writer
I1201 15:20:59.868453 32657 replica.cpp:494] Replica received implicit promise request from (6920)@127.0.1.1:46010 with proposal 1
I1201 15:20:59.871423 32657 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.822531ms
I1201 15:20:59.871608 32657 replica.cpp:343] Persisted promised to 1
I1201 15:20:59.874410 32652 coordinator.cpp:238] Coordinator attempting to fill missing positions
I1201 15:20:59.875857 32654 replica.cpp:389] Replica received explicit promise request from (6921)@127.0.1.1:46010 for position 0 with proposal 2
I1201 15:20:59.876679 32654 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 768796ns
I1201 15:20:59.876721 32654 replica.cpp:713] Persisted action at 0
I1201 15:20:59.877816 32658 replica.cpp:538] Replica received write request for position 0 from (6922)@127.0.1.1:46010
I1201 15:20:59.877890 32658 leveldb.cpp:436] Reading position from leveldb took 26504ns
I1201 15:20:59.878808 32658 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 883858ns
I1201 15:20:59.878846 32658 replica.cpp:713] Persisted action at 0
I1201 15:20:59.879611 32653 replica.cpp:692] Replica received learned notice for position 0 from @0.0.0.0:0
I1201 15:20:59.880426 32653 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 774539ns
I1201 15:20:59.880475 32653 replica.cpp:713] Persisted action at 0
I1201 15:20:59.880491 32653 replica.cpp:698] Replica learned NOP action at position 0
I1201 15:20:59.881324 32659 log.cpp:675] Writer started with ending position 0
I1201 15:20:59.882750 32654 leveldb.cpp:436] Reading position from leveldb took 62469ns
I1201 15:20:59.883872 32659 registrar.cpp:340] Successfully fetched the registry (0B) in 24.481792ms
I1201 15:20:59.884150 32659 registrar.cpp:439] Applied 1 operations in 70136ns; attempting to update the 'registry'
I1201 15:20:59.885272 32656 log.cpp:683] Attempting to append 160 bytes to the log
I1201 15:20:59.885668 32656 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I1201 15:20:59.886644 32654 replica.cpp:538] Replica received write request for position 1 from (6923)@127.0.1.1:46010
I1201 15:20:59.887362 32654 leveldb.cpp:341] Persisting action (179 bytes) to leveldb took 648381ns
I1201 15:20:59.887398 32654 replica.cpp:713] Persisted action at 1
I1201 15:20:59.887964 32654 replica.cpp:692] Replica received learned notice for position 1 from @0.0.0.0:0
I1201 15:20:59.888622 32654 leveldb.cpp:341] Persisting action (181 bytes) to leveldb took 620296ns
I1201 15:20:59.888674 32654 replica.cpp:713] Persisted action at 1
I1201 15:20:59.888689 32654 replica.cpp:698] Replica learned APPEND action at position 1
I1201 15:20:59.889732 32655 registrar.cpp:484] Successfully updated the 'registry' in 5.531136ms
I1201 15:20:59.889833 32655 registrar.cpp:370] Successfully recovered registrar
I1201 15:20:59.890943 32654 log.cpp:702] Attempting to truncate the log to 1
I1201 15:20:59.891580 32655 master.cpp:1447] Recovered 0 slaves from the Registry (122B) ; allowing 10mins for slaves to re-register
I1201 15:20:59.892067 32655 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I1201 15:20:59.892249 32654 hierarchical.cpp:174] Allocator recovery is not supported yet
I1201 15:20:59.893961 32656 replica.cpp:538] Replica received write request for position 2 from (6924)@127.0.1.1:46010
I1201 15:20:59.894907 32656 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 869068ns
I1201 15:20:59.894945 32656 replica.cpp:713] Persisted action at 2
I1201 15:20:59.895684 32656 replica.cpp:692] Replica received learned notice for position 2 from @0.0.0.0:0
I1201 15:20:59.896409 32656 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 687474ns
I1201 15:20:59.896469 32656 leveldb.cpp:399] Deleting ~1 keys from leveldb took 25911ns
I1201 15:20:59.896479 32656 replica.cpp:713] Persisted action at 2
I1201 15:20:59.896493 32656 replica.cpp:698] Replica learned TRUNCATE action at position 2
I1201 15:20:59.904182 32637 scheduler.cpp:154] Version: 0.27.0
I1201 15:20:59.909343 32656 http.cpp:336] HTTP POST for /master/api/v1/scheduler from 127.0.0.1:50993
I1201 15:20:59.911022 32656 master.cpp:1899] Received subscription request for HTTP framework 'default'
I1201 15:20:59.911417 32656 master.cpp:1676] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1201 15:20:59.912972 32656 master.cpp:1991] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I1201 15:20:59.913431 32656 hierarchical.cpp:220] Added framework b893dcee-362e-4fcf-81ac-d190058b8682-0000
I1201 15:20:59.918591 32653 http.cpp:336] HTTP POST for /master/api/v1/scheduler from 127.0.0.1:50994
I1201 15:20:59.918957 32653 master.cpp:1899] Received subscription request for HTTP framework 'default'
I1201 15:20:59.919139 32653 master.cpp:1676] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1201 15:20:59.919734 32653 master.cpp:1991] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I1201 15:20:59.919807 32653 master.cpp:2050] Updating info for framework b893dcee-362e-4fcf-81ac-d190058b8682-0000
I1201 15:20:59.919915 32653 master.cpp:2058] Framework b893dcee-362e-4fcf-81ac-d190058b8682-0000 (default) failed over
I1201 15:20:59.923687 32637 master.cpp:938] Master terminating
[       OK ] ContentType/SchedulerTest.Subscribe/1 (89 ms)
[ RUN      ] ContentType/SchedulerTest.TaskRunning/0
*** Aborted at 1448979659 (unix time) try ""date -d @1448979659"" if you are using GNU date ***
PC: @                0x0 (unknown)
*** SIGSEGV (@0x0) received by PID 32637 (TID 0x7f43f04ad700) from PID 0; stack trace: ***
    @     0x7f43f9e6c340 (unknown)
{noformat}

{noformat}
[       OK ] ContentType/SchedulerTest.Subscribe/1 (91 ms)
[ RUN      ] ContentType/SchedulerTest.TaskRunning/0
*** Aborted at 1448980825 (unix time) try ""date -d @1448980825"" if you are using GNU date ***
PC: @     0x7fa83ae55414 __GI___pthread_mutex_lock
*** SIGSEGV (@0x0) received by PID 3369 (TID 0x7fa8334a0700) from PID 0; stack trace: ***
    @     0x7fa83ae5b340 (unknown)
    @     0x7fa83ae55414 __GI___pthread_mutex_lock
    @           0x96e739 testing::internal::MutexBase::Lock()
I1201 15:40:25.114123  3369 leveldb.cpp:174] Opened db in 9.862725ms
    @           0x96e978 testing::internal::GTestMutexLock::GTestMutexLock()
I1201 15:40:25.115399  3369 leveldb.cpp:181] Compacted db in 1.183746ms
I1201 15:40:25.115507  3369 leveldb.cpp:196] Created db iterator in 29481ns
I1201 15:40:25.115527  3369 leveldb.cpp:202] Seeked to beginning of db in 3391ns
I1201 15:40:25.115536  3369 leveldb.cpp:271] Iterated through 0 keys in the db in 360ns
I1201 15:40:25.115612  3369 replica.cpp:778] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1201 15:40:25.117419  3387 recover.cpp:447] Starting replica recovery
I1201 15:40:25.117822  3387 recover.cpp:473] Replica is in EMPTY status
I1201 15:40:25.118917  3388 replica.cpp:674] Replica in EMPTY status received a broadcasted recover request from (16268)@127.0.1.1:41339
I    @          0x14e3236 testing::internal::ExpectationBase::CheckActionCountIfNotDone()
1201 15:40:25.120273  3385 recover.cpp:193] Received a recover response from a replica in EMPTY status
I1201 15:40:25.121109  3390 recover.cpp:564] Updating replica status to STARTING
I1201 15:40:25.123486  3390 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.178964ms
I1201 15:40:25.123651  3390 replica.cpp:321] Persisted replica status to STARTING
I1201 15:40:25.124387  3386 recover.cpp:473] Replica is in STARTING status
I1201 15:40:25.125872  3383 replica.cpp:674] Replica in STARTING status received a broadcasted recover request from (16269)@127.0.1.1:41339
    @           0xe2e582 _ZNK7testing8internal16TypedExpectationIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeIS6_SaIS6_EEEEE21ShouldHandleArgumentsERKSt5tupleIJSC_EE
I1201 15:40:25.126283  3388 recover.cpp:193] Received a recover response from a replica in STARTING status
I1201 15:40:25.126700  3390 recover.cpp:564] Updating replica status to VOTING
I1201 15:40:25.127899  3388 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.098503ms
I1201 15:40:25.127975  3388 replica.cpp:321] Persisted replica status to VOTING
I1201 15:40:25.128196  3390 recover.cpp:578] Successfully joined the Paxos group
I1201 15:40:25.128583  3390 recover.cpp:462] Recover process terminated
    @           0xe2d9c0 testing::internal::FunctionMockerBase<>::FindMatchingExpectationLocked()
    @           0xe2cdcb testing::internal::FunctionMockerBase<>::UntypedFindMatchingExpectation()
    @          0x14e3de1 testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
    @           0xe20259 _ZN7testing8internal18FunctionMockerBaseIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeIS6_SaIS6_EEEEE10InvokeWithERKSt5tupleIJSC_EE
    @           0xe1c9a8 testing::internal::FunctionMocker<>::Invoke()
    @          0x118d6b9 mesos::internal::tests::SchedulerTest::Callbacks::received()
    @          0x119101f _ZNKSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS0_2v19scheduler5EventESt5dequeIS8_SaIS8_EEEEEclIJSE_EvEEvRS4_DpOT_
    @          0x1190bcd _ZNSt5_BindIFSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS1_2v19scheduler5EventESt5dequeIS9_SaIS9_EEEEESt17reference_wrapperIS5_ESt12_PlaceholderILi1EEEE6__callIvJSF_EJLm0ELm1EEEET_OSt5tupleIJDpT0_EESt12_Index_tupleIJXspT1_EEE
    @          0x11904d9 _ZNSt5_BindIFSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS1_2v19scheduler5EventESt5dequeIS9_SaIS9_EEEEESt17reference_wrapperIS5_ESt12_PlaceholderILi1EEEEclIJSF_EvEET0_DpOT_
    @          0x118f9d5 std::_Function_handler<>::_M_invoke()
    @     0x7fa83f828103 std::function<>::operator()()
    @     0x7fa83f822fe8 process::AsyncExecutorProcess::execute<>()
    @     0x7fa83f828224 _ZZN7process8dispatchI7NothingNS_20AsyncExecutorProcessERKSt8functionIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeIS8_SaIS8_EEEEESC_PvSG_SC_SJ_EENS_6FutureIT_EERKNS_3PIDIT0_EEMSO_FSL_T1_T2_T3_ET4_T5_T6_ENKUlPNS_11ProcessBaseEE_clES11_
    @     0x7fa83f830c9e _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchI7NothingNS0_20AsyncExecutorProcessERKSt8functionIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeISC_SaISC_EEEEESG_PvSK_SG_SN_EENS0_6FutureIT_EERKNS0_3PIDIT0_EEMSS_FSP_T1_T2_T3_ET4_T5_T6_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x7fa84028e5e1 std::function<>::operator()()
    @     0x7fa840276927 process::ProcessBase::visit()
    @     0x7fa84027a516 process::DispatchEvent::visit()
    @           0x9e844a process::ProcessBase::serve()
    @     0x7fa840272bf0 process::ProcessManager::resume()
    @     0x7fa84026fca6 _ZZN7process14ProcessManager12init_threadsEvENKUlRKSt11atomic_boolE_clES3_
    @     0x7fa840279cd8 _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEE6__callIvIEILm0EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE
    @     0x7fa840279c88 _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEEclIIEvEET0_DpOT_
    @     0x7fa840279c1a _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEE9_M_invokeIIEEEvSt12_Index_tupleIIXspT_EEE
    @     0x7fa840279b71 _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEEclEv
    @     0x7fa840279b0a _ZNSt6thread5_ImplISt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS6_EEEvEEE6_M_runEv
    @     0x7fa83b636a40 (unknown)
Segmentation fault (core dumped)
{noformat}

Though, perhaps it does solve one of the crashes.","01/Dec/15 15:32;bernd-mesos;Talking to Anand and Alexander I am getting the impression this is likely a test bug. ","02/Dec/15 00:33;anandmazumdar;The culprit is this: https://github.com/apache/mesos/blob/master/src/scheduler/scheduler.cpp#L260 
We pass the {{Callbacks}} mock object by reference and not by value. Since we do an {{async}} , the call is queued on another thread but it does not ensure that it is invoked before the object is destroyed. Hence, we might invoke the {{received}} callback even after the original {{Callbacks}} object is destroyed.","12/Jan/16 19:39;tnachen;Anand are you able to fix this before end of this week? We need to update the target version if not.","16/Jan/16 00:16;tnachen;Looks like this is not a blocker for 0.27.0 as it's only local to tests.","09/Feb/16 18:53;anandmazumdar;Marking this as a blocker on MESOS-3339. We should be doing similar changes to pass the {{Callbacks}} object as a {{shared_ptr}} as is being done for the Executor API in MESOS-4433. This would ensure that the object is not destroyed first and in turn fix this issue.","02/Mar/16 18:22;anandmazumdar;Review chain: https://reviews.apache.org/r/44274/","04/Mar/16 00:11;vinodkone;commit 118b6ea88615cc95003a7ba4d0518a61e9948e14
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Thu Mar 3 16:09:58 2016 -0800

    Fixed flakiness in tests using the scheduler library.
    
    This change fixes the tests using the scheduler library by
    explicitly invoking `stop()` from the testing interface. This
    ensures that no further callbacks are delivered to the scheduler.
    
    For one-off async callbacks that are already on the libprocess queue
    we need to do a `Clock::settle` to ensure they are executed before
    the mock object goes out of scope.
    
    Review: https://reviews.apache.org/r/44275/

commit d27b8661654c5b02032731ec8298394381552b87
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Thu Mar 3 16:09:54 2016 -0800

    Added the ability to stop running the scheduler library process.
    
    This change adds the ability to stop running the scheduler library
    process so that no future callbacks are delivered to the scheduler.
    
    This helps us during testing to ensure no further callbacks happen
    to stack allocated mock objects.
    
    Review: https://reviews.apache.org/r/44274/
",,,,,,,,,,,,,,,,
Pass agent work_dir to isolator modules,MESOS-4003,12915933,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Won't Fix,greggomann,greggomann,greggomann,24/Nov/15 21:12,26/Nov/18 12:32,29/Oct/20 16:32,26/Nov/18 12:32,,,,,,,,,,,,,,,,,,,,0,external-volumes,mesosphere,,,,,,,"Some isolator modules can benefit from access to the agent's {{work_dir}}. For example, the DVD isolator (https://github.com/emccode/mesos-module-dvdi) is currently forced to mount external volumes in a hard-coded directory. Making the {{work_dir}} accessible to the isolator via {{Isolator::recover()}} would allow the isolator to mount volumes within the agent's {{work_dir}}. This can be accomplished by simply adding an overloaded signature for {{Isolator::recover()}} which includes the {{work_dir}} as a parameter.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-3590,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 11 18:18:40 UTC 2015,,,,,,,"0|hzzzty:zr",9223372036854775807,,,,,adam-mesos,,,,,,Mesosphere Sprint 23,Mesosphere Sprint 24,,,,,,,,,,1.0,,0.27.0,,,,,,,,,"10/Dec/15 00:08;greggomann;In order to prevent breaking the isolator interface in the future when more parameters may be added, a new protobuf message was added and made the sole parameter of {{Isolator::recover()}}.

Review is posted here: https://reviews.apache.org/r/41113/","11/Dec/15 18:18;greggomann;Thanks to [~jieyu] for presenting a much simpler solution for this: since {{work_dir}} is set at the command line anyway, we can just pass it to the modules via the {{parameters}} that they receive in their JSON command-line input.

Closing this ticket as a ""won't fix"".",,,,,,,,,,,,,,,,,,,,,,,,,,
C++ HTTP Scheduler Library does not work with SSL enabled,MESOS-3976,12914971,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,kaysoky,kaysoky,20/Nov/15 20:01,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,1.0.0,,,,,,framework,HTTP API,,,,0,mesosphere,security,,,,,,,"The C++ HTTP scheduler library does not work against Mesos when SSL is enabled (without downgrade).

The fix should be simple:
* The library should detect if SSL is enabled.
* If SSL is enabled, connections should be made with HTTPS instead of HTTP.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-29 17:36:19.995,,,false,MESOS-3302,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 30 18:14:53 UTC 2016,,,,,,,"0|hzzzy6:tzi",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 32,,,,,,,,,,,3.0,,,,,,,,,,,"29/Mar/16 17:36;anandmazumdar;Review: https://reviews.apache.org/r/45435/","30/Mar/16 18:14;vinodkone;commit 0a4e1e467afd978b9bae61a2050d86c6e945b39b
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Wed Mar 30 11:14:09 2016 -0700

    Modified scheduler library to properly handle SSL connections.
    
    This fixes the scheduler library to properly set the scheme
    as \`https\` if SSL is enabled. If not, default to \`http\` as
    is the case now. Would follow up with a similar change for the
    executor library.
    
    Review: https://reviews.apache.org/r/45435/
",,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure resources in `QuotaInfo` protobuf do not contain `role`,MESOS-3965,12914815,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,alexr,alexr,alexr,20/Nov/15 11:32,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,0.27.0,,,,,,master,,,,,0,mesosphere,,,,,,,,"{{QuotaInfo}} protobuf currently stores per-role quotas, including {{Resource}} objects. These resources are neither statically nor dynamically reserved, hence they may not contain {{role}} field. We should ensure this field is unset, as well as update validation routine for {{QuotaInfo}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-11-22 21:14:17.226,,,false,MESOS-1791,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 04 20:28:20 UTC 2015,,,,,,,"0|i2op67:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 23,,,,,,,,,,,3.0,,0.27.0,,,,,,,,,"22/Nov/15 21:14;jvanremoortere;This is a great invariant for the C++ wrapper {{Quota}} to maintain about the {{QuotaInfo}} that it wraps :-)","04/Dec/15 11:54;alexr;https://reviews.apache.org/r/40964/","04/Dec/15 11:55;alexr;Why not use validation for {{QuotaInfo}} instead? Or do you mean we should leverage existing {{internal::master::quota::validation::quotaInfo()}} to ensure {{Quota.QuotaInfo}} is always valid?","04/Dec/15 20:28;jvanremoortere;{code}
commit e5a7f9b3d0d4c227cb5dd72e87225ee4971f8979
Author: Alexander Rukletsov <rukletsov@gmail.com>
Date:   Fri Dec 4 13:29:32 2015 -0500

    Quota: Ensured resources in `QuotaInfo` do not have non-default role.
    
    Review: https://reviews.apache.org/r/40964
{code}",,,,,,,,,,,,,,,,,,,,,,,,
/reserve and /unreserve should be permissive under a master without authentication.,MESOS-3940,12913936,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,greggomann,mcypark,mcypark,18/Nov/15 00:46,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,0.28.0,,,,,,,,,,,0,authentication,mesosphere,reservations,,,,,,"Currently, the {{/reserve}} and {{/unreserve}} endpoints do not work without authentication enabled on the master. When authentication is disabled on the master, these endpoints should just be permissive.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-3987,,,,,,MESOS-4382,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-11-18 11:35:02.951,,,false,MESOS-2018,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 26 23:24:39 UTC 2016,,,,,,,"0|hzzztf:0i9",9223372036854775807,,,,,mcypark,,,,,,Mesosphere Sprint 29,,,,,,,,,,,1.0,,,,,,,,,,,"18/Nov/15 11:35;neilc;[~mcypark] -- I'd be happy to take a look at implementing this, if you can shepherd?","19/Nov/15 06:51;mcypark;Yep, that sounds great! Thank you [~neilc]","23/Nov/15 12:43;anandmazumdar;[~neilc] It won't be a bad idea to wait for MESOS-3233 ? Once that is implemented, all you need to do is remove the boiler plate code inside the handler function that tries to extract the {{Authorization}} header itself.","23/Nov/15 13:39;gyliu;Its a good idea, [~anandmazumdar] [~arojas] can you please show more detail for how MESOS-3233 works, maybe an example? Both this and MESOS-3987 can benefit from this.","23/Nov/15 13:50;arojas;Under MESOS-3233, endpoints which require authentication will give a [realm|http://tools.ietf.org/html/rfc1945#section-11] when they call {{route}}. The handler function also differs in that it now requires an additional parameter of {{const Option<std::string>& principal}}. If authentication is turned off or no authenticators were set, the principal is {{None}}, otherwise it is, well, the principal.

The logic of what to do when no principal is provided is left to the handler itself. So I guess the ACLs should enable themselves the permissive behavior.","23/Nov/15 17:12;neilc;Sounds good, I'm happy to wait on this until 3233 is landed.","11/Jan/16 22:40;greggomann;Reviews here:

https://reviews.apache.org/r/42164/
https://reviews.apache.org/r/42165/","14/Jan/16 20:48;greggomann;After some discussion, we're going to wait on implementing this change. First, we will make the {{ReservationInfo}}'s {{principal}} field optional, but have the master enforce that it must be set for backwards-compatibility. We can eliminate that enforcement after a deprecation cycle, and then implement this ticket.

Ticket for the optional principal change here: https://issues.apache.org/jira/browse/MESOS-4382","17/Feb/16 20:47;greggomann;Reviews here:

https://reviews.apache.org/r/43639/
https://reviews.apache.org/r/43641/
https://reviews.apache.org/r/43642/","23/Feb/16 03:07;mcypark;{noformat}
commit 866c52c51366da5998db879bbf55ecc278d2c6f2
Author: Greg Mann <greg@mesosphere.io>
Date:   Mon Feb 22 17:21:38 2016 -0800

    Updated comments/docs for using reservation endpoints without principal.

    Review: https://reviews.apache.org/r/43642/
{noformat}
{noformat}
commit a6d27b138c140239edb4650a744c73dab79d0c06
Author: Greg Mann <greg@mesosphere.io>
Date:   Mon Feb 22 17:20:16 2016 -0800

    Removed unnecessary parameter from validation function.

    Since unreserve operations are now possible without a principal,
    the `bool hasPrincipal` parameter to the Unreserve operation validation
    function is no longer necessary.

    Review: https://reviews.apache.org/r/43641/
{noformat}
{noformat}
commit efbdef8dfd96ff08c1342b171ef89dcb266bdce7
Author: Greg Mann <greg@mesosphere.io>
Date:   Mon Feb 22 16:43:09 2016 -0800

    Allowed dynamic reservation without a principal.

    The `ReservationInfo.principal` field has been migrated to `optional`,
    which means we can now allow dynamic reservation and unreservation
    without a principal. This allows the use of the `/reserve` and
    `/unreserve` HTTP endpoints when HTTP authentication is disabled.

    Note that we still require that frameworks/operators set the
    `ReservationInfo.principal` field to match their own principal,
    if present. It may be desirable to remove this requirement;
    this improvement is tracked in MESOS-4696.

    Review: https://reviews.apache.org/r/43639/
{noformat}","26/Feb/16 23:24;mcypark;{noformat}
commit 4211789fbf446f77346241a1e9031a1cd38c635e
Author: Greg Mann <greg@mesosphere.io>
Date:   Fri Feb 26 15:18:56 2016 -0800

    Added checks for presence of `ReservationInfo.principal`.

    The `ReservationInfo.principal` field was recently made `optional`.
    There remain a few places in the code that assume this field is always
    set. This patch uses `has_principal()` before this field is accessed to
    ensure correctness.

    Review: https://reviews.apache.org/r/42733/
{noformat}",,,,,,,,,,,,,,,,,
Implement AuthN handling in Master for the Scheduler endpoint,MESOS-3923,12912951,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,BenWhitehead,BenWhitehead,13/Nov/15 20:05,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,0.25.0,,,,,,,,1.0.0,,,,,,framework,HTTP API,master,,,0,mesosphere,,,,,,,,"If authentication(AuthN) is enabled on a master, frameworks attempting to use the HTTP Scheduler API can't register.

{code}
$ cat /tmp/subscribe-943257503176798091.bin | http --print=HhBb --stream --pretty=colors --auth verification:password1 POST :5050/api/v1/scheduler Accept:application/x-protobuf Content-Type:application/x-protobuf
POST /api/v1/scheduler HTTP/1.1
Connection: keep-alive
Content-Type: application/x-protobuf
Accept-Encoding: gzip, deflate
Accept: application/x-protobuf
Content-Length: 126
User-Agent: HTTPie/0.9.0
Host: localhost:5050
Authorization: Basic dmVyaWZpY2F0aW9uOnBhc3N3b3JkMQ==



+-----------------------------------------+
| NOTE: binary data not shown in terminal |
+-----------------------------------------+

HTTP/1.1 401 Unauthorized
Date: Fri, 13 Nov 2015 20:00:45 GMT
WWW-authenticate: Basic realm=""Mesos master""
Content-Length: 65

HTTP schedulers are not supported when authentication is required
{code}

Authorization(AuthZ) is already supported for HTTP based frameworks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-3402,MESOS-4459,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-12 22:26:52.732,,,false,MESOS-3302,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 15 21:02:19 UTC 2016,,,,,,,"0|hzzzy6:ty",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 33,,,,,,,,,,,5.0,,,,,,,,,,,"12/Apr/16 22:26;anandmazumdar;Review chain: https://reviews.apache.org/r/46113/","12/Apr/16 22:36;adam-mesos;Is there a (brief) design doc for this? Does this use the HTTP authenticator or the framework authenticator (module)?","12/Apr/16 23:03;anandmazumdar;[~adam-mesos] This uses the HTTP authenticator like the other HTTP operator endpoints do. The change looked pretty straight-forward and hence we decided to not have an explicit design doc. Do you see any potential issues that we need to be aware of with this approach?","12/Apr/16 23:32;adam-mesos;Ok, just curious how framework schedulers are expected to know how to fill out the request headers, etc. to accommodate a custom authenticator module, e.g. for SPNEGO or token-based authn. We can discuss offline.","15/Apr/16 21:02;vinodkone;commit a080650aa0833b380ec86239360145688da9a4c1
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Fri Apr 15 15:59:26 2016 -0500

    Added AuthN for HTTP based frameworks.
    
    This change adds AuthN support for HTTP based frameworks.
    
    We allow AuthZ without AuthN for `/scheduler` endpoint. Also,
    we ensure that `FrameworkInfo.principal` (if present) equals
    the authenticated principal. We also allow a framework to
    omit specifying the `FrameworkInfo.principal` in case
    it is not interested in authorization or if it is disabled.
    We do log a warning for this cases similar to what driver
    based frameworks already do.
    
    Review: https://reviews.apache.org/r/46115/

commit 792b08d5a6871610dcaa2df83a1d7905bc7adc09
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Fri Apr 15 15:59:22 2016 -0500

    Added documentation around using AuthN for HTTP frameworks.
    
    This change adds config docs around authenticating HTTP
    frameworks using the newly introduced flags. Also added a
    small note to the `authenticate` flag that this does not
    work for HTTP based frameworks.
    
    Review: https://reviews.apache.org/r/46212/

commit 8996b6e5849d97f27f39ef7cef49d6d46404d683
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Fri Apr 15 15:59:17 2016 -0500

    Added flags for authenticating HTTP frameworks to master.
    
    This change introduces two new flags `authenticate_http_frameworks`
    and `http_framework_authenticators` to the master. This allows us
    to selectively turn on/off framework authentication and decouple
    them from authentication for operator endpoints.
    
    Review: https://reviews.apache.org/r/46211/
",,,,,,,,,,,,,,,,,,,,,,,
Identify and implement test cases for handling a race between optimistic lender and tenant offers.,MESOS-3898,12912179,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,hartem,hartem,11/Nov/15 13:59,26/Nov/18 13:36,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,mesosphere,,,,,,,,An example is the when lender launches the task on an agent followed by a  borrower launching a task on the same agent before the optimistic offer is rescinded. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-4967,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-11-11 13:59:36.0,,,,,,,"0|i2o8zr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Identify and implement test cases for verifying eviction logic in the agent,MESOS-3897,12912178,Bug,Open,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,hartem,hartem,11/Nov/15 13:56,10/Jul/17 19:30,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,mesosphere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-03 09:09:46.664,,,false,MESOS-4967,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 11 07:19:04 UTC 2016,,,,,,,"0|i2o8zj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,13.0,,,,,,,,,,,"03/Jan/16 09:09;gyliu;The eviction logic will be only in agent, [~hartem], do we still need this ticket? Thanks.","11/Jan/16 07:19;klaus1982;[~hartem], we're going to update this JIRA to focus on eviction logic in agent.",,,,,,,,,,,,,,,,,,,,,,,,,,
Add accounting for reservation slack in the allocator.,MESOS-3896,12912176,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,hartem,hartem,11/Nov/15 13:54,26/Nov/18 13:36,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,mesosphere,,,,,,,,"MESOS-XXX: Optimsistic accounter

{code}
    class HierarchicalAllocatorProcess 
    {
      struct Slave
      {
        ...
        struct Optimistic 
        {
          Resources total; // The total allocation slack resources
          Resources allocated; // The allocated allocation slack resources
        };
    
        Optimistic optimistic;
      };
    }
{code}

MESOS-4146: flatten & allocationSlack for Optimistic Offer

{code}
    class Resources
    {
        // Returns a Resources object with the same amount of each resource
        // type as these Resources, but with all Resource objects marked as
        // the specified `RevocableInfo::Type`; the other attribute is not
        // affected.
        Resources flatten(Resource::RevocableInfo::Type type);

        // Return a Resources object that:
        //   - if role is given, the resources did not include role's reserved
        //     resources.
        //   - the resources's revocable type is `ALLOCATION_SLACK`
        //   - the role of resources is set to ""*""
        Resources allocationSlack(Option<string> role = None());
    }
{code}

MESOS-XXX: Allocate the allocation_slack resources to framework

{code}
    void HierarchicalAllocatorProcess::allocate(
        const hashset<SlaveID>& slaveIds_)
    {
      foreach slave; foreach role; foreach framework
      {
        Resource optimistic;

        if (framework.revocable) {
          Resources total = slaves[slaveId].optimistic.total.allocationSlack(role);
          optimistic = total - slaves[slaveId].optimistic.allocated;
        }

        ...
        offerable[frameworkId][slaveId] += resources + optimistic;

        ...
        slaves[slaveId].optimistic.allocated += optimistic;
      }
    }
{code}
  
Here's some consideration about `ALLOCATION_SLACK`:

1. 'Old' resources (available/total) did not include ALLOCATION_SLACK
2. After `Quota`, `remainingClusterResources.contains` should not check ALLOCATION_SLACK; if there no enough resources,  master can still offer ALLOCATION_SALCK resources.
3. In sorter, it'll not include ALLOCATION_SLACK; as those resources are borrowed from other role/framework
4. If either normal resources or ALLOCATION_SLACK resources are allocable/!filtered, it can be offered to framework
5. Currently, allocator will assign all ALLOCATION_SALCK resources in slave to one framework

MESOS-XXX: Update ALLOCATION_SLACK for dynamic reservation (updateAllocation)

{code}
    void HierarchicalAllocatorProcess::updateAllocation(
        const FrameworkID& frameworkId,
        const SlaveID& slaveId,
        const vector<Offer::Operation>& operations)
    {
        ...
        Try<Resources> updatedOptimistic =
            slaves[slaveId].optimistic.total.apply(operations);
        CHECK_SOME(updatedTotal);

        slaves[slaveId].optimistic.total =
            updatedOptimistic.get().stateless().reserved().flatten(ALLOCATION_SLACK);
        ...
    }
{code}
    
MESOS-XXX: Add ALLOCATION_SLACK when slaver register/re-register (addSlave)

{code}
    void HierarchicalAllocatorProcess::addSlave(
        const SlaveID& slaveId,
        const SlaveInfo& slaveInfo,
        const Option<Unavailability>& unavailability,
        const Resources& total,
        const hashmap<FrameworkID, Resources>& used)
    {
      ...
      slaves[slaveId].optimistic.total =
          total.stateless().reserved().flatten(ALLOCATION_SLACK);
      ...
    }
{code}
  
No need to handle `removeSlave`, it'll all related info from `slaves` including `optimistic`.

MESOS-XXX: return resources to allocator (recoverResources)

{code}
    void HierarchicalAllocatorProcess::recoverResources(
        const FrameworkID& frameworkId,
        const SlaveID& slaveId,
        const Resources& resources,
        const Option<Filters>& filters)
    {
      if (slaves.contains(slaveId))
      {
        ...
        slaves[slaveId].optimistic.allocated -= resources.allocationSlack();
        ...
      }
    }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-04 03:29:00.638,,,false,MESOS-4967,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 04 03:29:00 UTC 2016,,,,,,,"0|i2o8z3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,13.0,,,,,,,,,,,"04/Jan/16 03:29;gyliu;I think that this one was already covered in https://issues.apache.org/jira/browse/MESOS-4145",,,,,,,,,,,,,,,,,,,,,,,,,,,
Update reservation slack allocator state during agent failover.,MESOS-3895,12912173,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,hartem,hartem,hartem,11/Nov/15 13:51,26/Nov/18 13:36,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,mesosphere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-4967,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-11-11 13:51:54.0,,,,,,,"0|i2o8yf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,13.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rebuild reservation slack allocator state during master failover.,MESOS-3894,12912171,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,gyliu,hartem,hartem,11/Nov/15 13:50,26/Nov/18 13:36,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,mesosphere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-05 03:44:42.726,,,false,MESOS-4967,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 05 03:44:42 UTC 2016,,,,,,,"0|i2o8xz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,13.0,,,,,,,,,,,"05/Jan/16 03:44;gyliu;The allocation slack will be rebuild every time before sending offers, so the master failover will not impact the allocator, will do some tests for this.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement tests for verifying allocator resource math.,MESOS-3893,12912169,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,gyliu,hartem,hartem,11/Nov/15 13:38,26/Nov/18 13:36,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,mesosphere,,,,,,,,Write a test to ensure that the allocator performs the reservation slack calculations correctly.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-4967,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-11-11 13:38:18.0,,,,,,,"0|i2o8xj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add a helper function to the Agent to retrieve the list of executors that are using optimistically offered, revocable resources.",MESOS-3892,12912163,Bug,Open,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,hartem,hartem,11/Nov/15 13:27,10/Jul/17 19:29,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,mesosphere,,,,,,,,"In the agent, add a helper function to get the list of the exeuctor using ALLOCATION_SLACK.

It's short term solution which is different the design document, because master did not have executor for command line executor. Send evicatble executors from master to slave will addess in post-MVP after MESOS-1718.

{noformat}
class Slave {
...
  // If the executor used revocable resources, add it into `evictableExecutors`
  // list.
  void addEvictableExecutor(Executor* executor);

  // If the executor used revocable resources, remove it from
  // `evictableExecutors` list.
  void removeEvictableExecutor(Executor* executor);

  // Get evictable executor ID list by `request resources`. The return value is `Result<list<Executor*>>`:
  //  - if `isError()`, there's not enough resources to launch tasks
  //  - if `isNone()`, no evictable exectuors need to be terminated
  //  - if !`isNone()`, the list of executors that need to be evicted for resources
  Result<std::list<Executor*>> getEvictableExecutors(const Resources& request);

...

  // The map of evictable executor list. If there's not enough resources,
  // the evictable executor will be terminated by slave to release resources.
  hashmap<FrameworkID, std::set<ExecutorID>> evictableExecutors;
...
}
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-11-22 14:05:51.413,,,false,MESOS-4967,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 16 03:15:40 UTC 2015,,,,,,,"0|i2o8w7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,"22/Nov/15 14:05;klaus1982;For command line tasks, there's no executor_info in master. In MESOS-1718, we're going to build {{ExecutorInfo}} in master instead of slave/agent; so master will aware of Executor of command line.","03/Dec/15 07:10;gyliu;I know that we already have lot of discussions to talk whether the master can kill task or not. For some frameworks, it may not implement the {{termiateTask()}} so that once the executor was killed, the task will become TASK_FAILED and all resources will be recovered but task continues running, this will cause the host over committed, kubernetes on mesos is such a case, I filed a ticket here to trace the k8s on mesos issue: https://github.com/kubernetes/kubernetes/issues/18066

Killing task directly by master may not able to make sure QoS but it can make sure the resources usage is correct in case some executors do not implement {{terminateTask()}} API. Is it possible to add a new field in Framework to control kill task or executor when the tasks on this framework is being  preempted.","11/Dec/15 09:34;gyliu;Is it possible that we do not generate the executor list in master but let slave handle this in MVP? The slave already knows all of the executors/tasks who is using allocation slack resources, so when launch a new task which request resource preemption, the slave can just check and kill some executors/tasks to recover those resources, make sense? [~jvanremoortere] [~kaysoky] [~hartem] [~klaus1982] ","11/Dec/15 16:44;klaus1982;I have update the code diff of MESOS-1718 at https://reviews.apache.org/r/40759/; would you also help to review them? I'll start to work on this one when MESOS-1718 under review :).","11/Dec/15 18:28;kaysoky;Yes that's reasonable (and what we discussed in the [work group meeting|https://docs.google.com/document/d/1CKMelV6xD_HOsqwbqH3PM24P7ypS_G4oz_MDNxE85D8/edit#bookmark=id.xlfbqnql7ngq]).

Can you update the relevant JIRA's accordingly (rename, update descriptions, etc)?","16/Dec/15 03:15;klaus1982;https://reviews.apache.org/r/41855/
https://reviews.apache.org/r/41856/
https://reviews.apache.org/r/41857/
https://reviews.apache.org/r/41850/",,,,,,,,,,,,,,,,,,,,,,
Add a helper function to the Agent to check available resources before launching a task. ,MESOS-3891,12912160,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,gyliu,hartem,hartem,11/Nov/15 13:23,26/Nov/18 13:36,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,mesosphere,,,,,,,,"Launching a task using revocable resources should be funnelled through an accounting system:

* If a task is launched using revocable resources, the resources must not be in use when launching the task.  If they are in use, then the task should fail to start.
* If a task is launched using reserved resources, the resources must be made available.  This means potentially evicting tasks which are using revocable resources.

Both cases could be implemented by adding a check in Slave::runTask, like a new helper method:

{noformat}
class Slave {
  ...
  // Checks if the given resources are available (i.e. not utilized)
  // for starting a task.  If not, the task should either fail to
  // start or result in the eviction of revocable resources.
  virtual process::Future<bool> checkAvailableResources(
      const Resources& resources);
  ...
}
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-10 08:41:14.752,,,false,MESOS-4967,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 16 15:11:25 UTC 2015,,,,,,,"0|i2o8vj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,"10/Dec/15 08:41;gyliu;There is a ticket MESOS-2647 handling same issue for usage slack.","16/Dec/15 15:11;gyliu;
Adding a queue to slave to queue all of the tasks related to allocation slack. The reason that we need this queue is because the task launch related with the allocation slack may trigger executor eviction and the eviction may take some time. Adding a queue here can make sure other tasks without allocation slack will not be delayed when the previous tasks related with allocation slack are evicting tasks.

Adding a queue can also help resolve the race condition issue: All of the tasks in the queue will be handled with FCFS policy.

{code}
_runTask() {
  ...
  if (!Resources(task.resources()).allocSlackRevocable().empty()) {
     // Put task to pending queue.
  }
 
 if (!Resources(task.resources()).stateless().reserved.empty()) {
     // Put task to pending queue.
  }
  ...
  // Handle other tasks.
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Add notion of evictable task to RunTaskMessage,MESOS-3890,12912157,Bug,Reviewable,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,gyliu,hartem,hartem,11/Nov/15 13:17,26/Nov/18 13:31,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,mesosphere,,,,,,,,"{code}
// Evict Resources to launch tasks.
  message Revocation {
    optional FrameworkID framework_id = 1;
    required string role = 2;
    repeated Resource revocable_resources = 3;
  }
  repeated Revocation revocations = 5;
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-11-24 05:44:50.114,,,false,MESOS-4967,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 03 09:20:22 UTC 2016,,,,,,,"0|i2o8uv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,"24/Nov/15 05:44;gyliu;https://reviews.apache.org/r/40532/","03/Jan/16 09:06;gyliu;[~hartem] [~jvanremoortere] [~kaysoky] Since the master cannot get the evictable executors when launch task, the current implementation is we leave this to agent to kill executors, shall we close this one directly? Thanks.","03/Jan/16 09:20;klaus1982;Let's keep this open; we need this JIRA after post MVP (move CLI executor from slave to master).",,,,,,,,,,,,,,,,,,,,,,,,,
Modify Oversubscription documentation to explicitly forbid the QoS Controller from killing executors running on optimistically offered resources.,MESOS-3889,12912156,Bug,Accepted,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,hartem,hartem,11/Nov/15 13:16,26/Nov/18 13:36,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,mesosphere,,,,,,,,"The oversubcription documentation currently assumes that oversubscribed resources ({{USAGE_SLACK}}) are the only type of revocable resources.  Optimistic offers will add a second type of revocable resource ({{ALLOCATION_SLACK}}) that should not be acted upon by oversubscription components.

For example, the [oversubscription doc|http://mesos.apache.org/documentation/latest/oversubscription/] says the following:
{quote}
NOTE: If any resource used by a task or executor is revocable, the whole container is treated as a revocable container and can therefore be killed or throttled by the QoS Controller.
{quote}
which we may amend to something like:
{quote}
NOTE: If any resource used by a task or executor is revocable usage slack, the whole container is treated as an oversubscribed container and can therefore be killed or throttled by the QoS Controller.
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-19 13:59:49.263,,,false,MESOS-4967,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 19 13:59:49 UTC 2016,,,,,,,"0|i2o8un:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,"19/Jan/16 13:59;nnielsen;[~hartem] [~klaus1982] Can you add a bit of context on this ticket? :)",,,,,,,,,,,,,,,,,,,,,,,,,,,
Support distinguishing revocable resources in the Resource protobuf.,MESOS-3888,12912155,Bug,Open,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,,hartem,hartem,11/Nov/15 13:15,10/Jul/17 19:30,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,mesosphere,,,,,,,,"Add enum type into RevocableInfo: 

* Framework need to assign RevocableInfo when launching task; if it’s not assign, use reserved resources. Framework need to identify which resources it’s using
* Oversubscription resources need to assign the type by Agent (MESOS-3930)
* Update Oversubscription document that OO has over-subscribe the Allocation Slack and recommend QoS to handle the usage slack only. (MESOS-3889)

{code}
message Resource {
  ...
  message RevocableInfo {
   enum Type {
     // Under-utilized, allocated resources.  Controlled by
     // oversubscription (QoSController & ResourceEstimator).
     USAGE_SLACK = 1;

     // Unallocated, reserved resources.
     // Controlled by optimistic offers (Allocator).
     ALLOCATION_SLACK = 2; 
   }

   optional Type type = 1;
  }
 ...
  optional RevocableInfo revocable = 9;
 }
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-11-12 06:39:15.479,,,false,MESOS-4967,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 17 05:07:59 UTC 2015,,,,,,,"0|i2o8uf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,"12/Nov/15 06:39;gyliu;{code}
message Resource {
  ...
  message RevocableInfo {
    enum Type {
      // Under-utilized, allocated resources.  Controlled by 
      // oversubscription (QoSController & ResourceEstimator).
      USAGE_SLACK = 1;

      // Unallocated, reserved resources.
      // Controlled by optimistic offers (Allocator).
      ALLOCATION_SLACK = 2;
    }

    required Type type = 1;
  }
  ...
  optional RevocableInfo revocable = 9; 
}
{code}","17/Nov/15 05:07;klaus1982;RR: https://reviews.apache.org/r/40375/",,,,,,,,,,,,,,,,,,,,,,,,,,
Add a flag to master to enable optimistic offers. ,MESOS-3887,12912154,Bug,Reviewable,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,,gyliu,hartem,hartem,11/Nov/15 13:12,26/Nov/18 13:31,29/Oct/20 16:32,,,,,,,,,,,,,,,,,,,,,0,mesosphere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-11-16 07:37:11.364,,,false,MESOS-4967,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 05:42:12 UTC 2015,,,,,,,"0|i2o8u7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"16/Nov/15 07:37;gyliu;RR: https://reviews.apache.org/r/40339/","24/Nov/15 05:42;gyliu;[~jvanremoortere] can you help review? Since there are two people working on optimistic offer, so it is better that we can first merge some common patches. Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,
Implement `stout/os/pstree.hpp` on Windows,MESOS-3881,12911945,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hausdorff,hausdorff,hausdorff,10/Nov/15 19:10,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,1.0.0,,,,,,stout,,,,,0,mesosphere,windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-08 21:29:39.272,,,false,MESOS-3094,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 08 21:29:49 UTC 2016,,,,,,,"0|i2o7jz:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 34,,,,,,,,,,,2.0,,,,,,,,,,,"08/May/16 21:29;jvanremoortere;https://reviews.apache.org/r/46794/","08/May/16 21:29;jvanremoortere;{code}
commit 0f25c7b9420a7368be0657b3acca2c9fe45f38f2
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Sun May 8 17:21:31 2016 -0400

    Unify POSIX and Windows `pstree` implementations.
    
    Review: https://reviews.apache.org/r/46794/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Add mtime-related fetcher tests,MESOS-3856,12911517,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Won't Fix,bbannier,bbannier,bbannier,09/Nov/15 12:14,26/Nov/18 09:36,29/Oct/20 16:32,26/Nov/18 09:36,,,,,,,,,,,,,,,test,,,,,0,mesosphere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2073,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 04 12:22:15 UTC 2015,,,,,,,"0|hzzzv2:zx",9223372036854775807,,,,,bernd-mesos,,,,,,Mesosphere Sprint 22,Mesosphere Sprint 23,,,,,,,,,,2.0,,,,,,,,,,,"16/Nov/15 15:44;bbannier;Review: https://reviews.apache.org/r/40349/","04/Dec/15 12:22;bbannier;After some discussion we decided to abandon this approach (using the content modification time for cache invalidation); it e.g., encourages invalidating content of active URIs where instead a proper versioning scheme should be employed (currently by the user).",,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos does not set Content-Type for 400 Bad Request,MESOS-3739,12905045,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,vinodkone,BenWhitehead,BenWhitehead,14/Oct/15 23:41,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,0.24.0,0.24.1,0.25.0,,,,,,1.0.0,,,,,,HTTP API,,,,,0,mesosphere,,,,,,,,"While integrating with the HTTP Scheduler API I encountered the following scenario.

The message below was serialized to protobuf and sent as the POST body
{code:title=message}
call {
  type: ACKNOWLEDGE,
  acknowledge: {
    uuid: <bytes>,
    agentID: { value: ""20151012-182734-16777343-5050-8978-S2"" },
    taskID: { value: ""task-1"" }
  }
}
{code}
{code:title=Request Headers}
POST /api/v1/scheduler HTTP/1.1
Content-Type: application/x-protobuf
Accept: application/x-protobuf
Content-Length: 73
Host: localhost:5050
User-Agent: RxNetty Client
{code}

I received the following response
{code:title=Response Headers}
HTTP/1.1 400 Bad Request
Date: Wed, 14 Oct 2015 23:21:36 GMT
Content-Length: 74

Failed to validate Scheduler::Call: Expecting 'framework_id' to be present
{code}

Even though my accept header made no mention of {{text/plain}} the message body returned to me is {{text/plain}}. Additionally, there is no {{Content-Type}} header set on the response so I can't even do anything intelligently in my response handler.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-5853,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-10-18 14:21:55.526,,,false,MESOS-3302,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 11 00:41:06 UTC 2016,,,,,,,"0|hzzzhy:zy",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 33,Mesosphere Sprint 34,,,,,,,,,,2.0,,,,,,,,,,,"18/Oct/15 14:21;klaus1982;It seems only OK have the ContextType for JSON; propose to following the same rules of json for others.
    1. By default, it's {{text/plain}}
    2. Build a class named {{Context}} and sub-class such as {{Html}}, so {{BadRequest}} can get ContextType from by the type

{code}
class Context
{
public:
  virtual std::string type() 
  {
     return ""text/plain"";
  }
}

class Html : public Context
{
public:
  virtual std::string type()
  {
    return ""text/html"";
  }
}

public OK : Response
{
public:
  OK(Html body)
  {
    headers[""Context-type""] = body.type();
  }
}

{code}","27/Apr/16 05:26;vinodkone;https://reviews.apache.org/r/46725/","11/May/16 00:41;vinodkone;commit b2c5d91addbae609af3791f128c53fb3a26c7d53
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Tue Apr 26 19:09:04 2016 -0700

    Set default Content-Type for HTTP responses.
    
    The default Content-Type for HTTP responses that contain body is set
    as ""text/plain; charset=utf-8.""
    
    Review: https://reviews.apache.org/r/46725
",,,,,,,,,,,,,,,,,,,,,,,,,
Implement Quota support in allocator,MESOS-3718,12904520,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,alexr,alexr,alex-mesos,13/Oct/15 12:13,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.27.0,,,,,,allocation,,,,,0,mesosphere,,,,,,,,"The built-in Hierarchical DRF allocator should support Quota. This includes (but not limited to): adding, updating, removing and satisfying quota; avoiding both overcomitting resources and handing them to non-quota'ed roles in presence of master failover.

A [design doc for Quota support in Allocator|https://issues.apache.org/jira/browse/MESOS-2937] provides an overview of a feature set required to be implemented.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-11-16 16:48:57.009,,,false,MESOS-1791,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 03 19:34:36 UTC 2015,,,,,,,"0|hzzzuv:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 21,Mesosphere Sprint 22,Mesosphere Sprint 23,,,,,,,,,5.0,,0.27.0,,,,,,,,,"23/Oct/15 16:41;alexr;https://reviews.apache.org/r/39399/
https://reviews.apache.org/r/39400/
https://reviews.apache.org/r/40551/
https://reviews.apache.org/r/40795/
https://reviews.apache.org/r/40821/","16/Nov/15 16:48;jvanremoortere;{code}
commit dfdf4125762055e047cd9eb539c53eca644ff916
Author: Alexander Rukletsov <rukletsov@gmail.com>
Date:   Mon Nov 16 17:35:11 2015 +0100

    Quota: Refactored hierarchical allocator in preparation for quota.
    
    Review: https://reviews.apache.org/r/39399
{code}","22/Nov/15 20:17;jvanremoortere;{code}
commit 9062bbbb77c77b2ebb0e106f78ff9805a8e6d649
Author: Alexander Rukletsov <rukletsov@gmail.com>
Date:   Sun Nov 22 14:22:30 2015 -0500

    Quota: Implemented quota API in hierarchical allocator.
    
    Review: https://reviews.apache.org/r/39400
{code}","03/Dec/15 19:34;jvanremoortere;{code}
commit ffa8392d71b13f5c65e64acbf433db3bb9257f31
Author: Alexander Rukletsov <rukletsov@gmail.com>
Date:   Fri Dec 4 12:33:30 2015 -0500

    Quota: Filtered revocable resources out of quotaRoleSorter in allocator.
    
    Review: https://reviews.apache.org/r/40821
commit b226be0bcce0ab45334ad58ac56ea72d6af6995d
Author: Alexander Rukletsov <rukletsov@gmail.com>
Date:   Thu Dec 3 11:45:33 2015 -0500

    Quota: Updated allocate() in the hierarchical allocator.
    
    Quota is satisfied in a separate loop over agents. A running total is
    maintained as an exit criterion for the WDRF allocation stage.
    
    Precursory version: https://reviews.apache.org/r/39401/
    
    Review: https://reviews.apache.org/r/40551
{code}",,,,,,,,,,,,,,,,,,,,,,,,
Update Allocator interface to support quota,MESOS-3716,12904512,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,alexr,alexr,alexr,13/Oct/15 11:46,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.26.0,,,,,,allocation,,,,,0,mesosphere,,,,,,,,"An allocator should be notified when a quota is being set/updated or removed. Also to support master failover in presence of quota, allocator should be notified about the reregistering agents and allocations towards quota.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-11-09 17:35:34.341,,,false,MESOS-1791,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 09 17:35:34 UTC 2015,,,,,,,"0|hzzzy3:i",9223372036854775807,,,,,benjaminhindman,,,,,,Mesosphere Sprint 21,,,,,,,,,,,3.0,,0.26.0,,,,,,,,,"23/Oct/15 16:39;alexr;https://reviews.apache.org/r/38218/","09/Nov/15 17:35;jvanremoortere;{code}
commit 6b66fb712224243100065b295efb18a1cb1f7181
Author: Alexander Rukletsov <rukletsov@gmail.com>
Date:   Mon Nov 9 16:33:35 2015 +0100

    Quota: Extended the Allocator interface with quota-related methods.
    
    Review: https://reviews.apache.org/r/38218
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Make Scheduler Library use HTTP Pipelining Abstraction in Libprocess,MESOS-3570,12901940,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,anandmazumdar,anandmazumdar,01/Oct/15 16:54,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,0.28.0,,,,,,,,,,,0,mesosphere,newbie,,,,,,,"Currently, the scheduler library sends calls in order by chaining them and sending them only when it has received a response for the earlier call. This was done because there was no HTTP Pipelining abstraction in Libprocess {{process::post}}.

However once {{MESOS-3332}} is resolved, we should be now able to use the new abstraction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4686,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-10-15 06:04:47.458,,,false,MESOS-3302,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 28 18:01:11 UTC 2016,,,,,,,"0|hzzzn2:x",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 27,Mesosphere Sprint 28,Mesosphere Sprint 29,,,,,,,,,8.0,,,,,,,,,,,"15/Oct/15 06:04;qiujian;This is due to that internal::request is a one-time connection, so we actually may need pipelining abstraction in internal::request?","17/Feb/16 19:23;anandmazumdar;Review Chain: https://reviews.apache.org/r/43657/","28/Feb/16 18:01;vinodkone;commit 9b0a8ce9a7cc1cfbf062157a7baecfffc9b025dc
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Sun Feb 28 09:52:34 2016 -0800

    Modified existing scheduler tests as an aftermath of pipelining change.
    
    This change modifies the existing tests to handle the new
    `connected/disconnected` semantics that were introduced as a result of the
    pipelining change.
    
    Now `connected` is possible to invoked more then once i.e. during test shutdown,
    the master closed the event stream making us call the `disconnected` callback.
    However, the master detector can still detect the same master and establish a
    connection to it while it is shutting down leading to `connected` callback be
    invoked again in some cases.
    
    Review: https://reviews.apache.org/r/43667/

commit 3c3e92efff41526fe6c63f5cd89c904f486e2d00
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Sun Feb 28 09:52:22 2016 -0800

    Added support for pipelining calls to the scheduler library.
    
    Previously, the scheduler library used to chain calls on previous call
    responses. This was inherently slow. This change adds support for pipelining all
    calls to the master on a single connection via the `http::Connection`
    abstraction in libprocess.
    
    This change also adds support for handling various error scenarios when we
    notice a disconnection instead of just relying on the master detector for
    invoking the `disconnected` callback.
    
    Review: https://reviews.apache.org/r/43662/
commit 3a9d887167599f056b69b247c13b2ebaff4c4ddd
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Sun Feb 28 09:52:12 2016 -0800

    Added check to not Subscribe if we are already subscribed.
    
    This change adds a check to ensure that we don't `Subscribe` again in case we
    already have a `SubscribedResponse` stream set. With pipelining, we won't be
    able to send more then 1 subscribe calls on the `Subscribe` connection.
    
    In case, the scheduler sends two subscribe calls simultaneously before the
    subscribed response stream could be established. We would invoke the
    `disconnected`/`connected` callback, making the scheduler subscribe again.
    
    Review: https://reviews.apache.org/r/43661/

commit d891d963531673bb2649ec60ac940905a0576cf8
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Sun Feb 28 09:51:53 2016 -0800

    Renamed Connection to SubscribedResponse.
    
    This change renames the `Connection` object that existed previously to
    `SubscribedResponse`. This would be needed later in the review chain when we
    implement pipelining.
    
    Review: https://reviews.apache.org/r/43660/

commit 5651e44159aea6e737da4122b455e54a7995b3c6
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Sun Feb 28 09:51:36 2016 -0800

    Cleaned up the previous queueing Calls logic.
    
    This change cleans up the previous queueing calls logic in favor of HTTP
    connection pipelining in the scheduler library. This review clears up the noise
    of the pipelining diff in subsequent reviews.
    
    Review: https://reviews.apache.org/r/43659/

commit bf5439a10c38aa67b4ad3371de3d7acdb496a0e8
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Sun Feb 28 09:51:29 2016 -0800

    Modified the received callback argument to be ref.
    
    Review: https://reviews.apache.org/r/43658/

commit 72dce88e2430190d0f4e76280b7b465db2bbed70
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Sun Feb 28 09:51:22 2016 -0800

    Fixed MasterDetector pointer leak in scheduler library.
    
    Moved the detector pointer to `process::Owned` so that it is cleaned up later.
    
    Review: https://reviews.apache.org/r/43657/
",,,,,,,,,,,,,,,,,,,,,,,,,
Revocable task CPU shows as zero in /state.json,MESOS-3563,12901716,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,vinodkone,maximk,maximk,30/Sep/15 22:09,16/Oct/15 19:34,29/Oct/20 16:32,16/Oct/15 19:34,,,,,,,,,0.26.0,,,,,,,,,,,0,,,,,,,,,"The slave's state.json reports revocable task resources as zero:

{noformat}
resources: {
cpus: 0,
disk: 3071,
mem: 1248,
ports: ""[31715-31715]""
},
{noformat}

Also, there is no indication that a task uses revocable CPU. It would be great to have this type of info.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-09-30 23:01:08.647,,,false,MESOS-354,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 16 19:34:59 UTC 2015,,,,,,,"0|i2mh6v:",9223372036854775807,,,,,,,,,,,Twitter Mesos Q3 Sprint 6,,,,,,,,,,,2.0,,,,,,,,,,,"30/Sep/15 23:01;nnielsen;Can you share the full state.json?","30/Sep/15 23:10;maximk;It's just too big to sanitize :) Is there anything in particular you are interested in?","30/Sep/15 23:23;nnielsen;I didn't understand the full context of your problem; assume you are using the fixed resource estimator? At any regard, looks like Vinod is on it.","30/Sep/15 23:25;maximk;Correct, fixed resource estimator.","30/Sep/15 23:41;vinodkone;Looks like this is done intentionally, to preserve backwards compatibility.

https://github.com/apache/mesos/blob/master/src/common/http.cpp#L73

{quote}
  // To maintain backwards compatibility we exclude revocable
  // resources in the reporting.
{quote}

This was done as part of fixing the resource representation in MESOS-2838. cc [~xujyan]","01/Oct/15 00:16;vinodkone;So I think we could/should add the revocable resources information in the endpoints that display ""Resources"".

Couple of options

#1: have a different ""revocable_resources"" object in addition to the ""resources"" object in the JSON.
{code}
revocable_resources: {
  cpus: 2,
  ...
}
{code}

#2: keep one ""resources"" object but add new keys ""cpus_revocable""
{code}
resources : {
  cpus: 2, 
  cpus_revocable: 2, 
  ...
}
{code}

Any preferences?  #1 seems easier to implement and maintain. #2 seems closer to what /metrics/snapshot has (slave/cpus_total, slave/cpus_revocable_total) ","05/Oct/15 08:43;qianzhang;I think #2 might be better since that way user will have a full view of all the resources in the ""resources"" object.","16/Oct/15 00:49;vinodkone;https://reviews.apache.org/r/39368/","16/Oct/15 19:34;vinodkone;commit 253abe7984eec4cc127df7bb9616fe827c2ac1a7
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Thu Oct 15 17:23:17 2015 -0700

    Updated /state.json to show revocable resources.
    
    Review: https://reviews.apache.org/r/39368
",,,,,,,,,,,,,,,,,,,
Validate that slave's work_dir is a shared mount in its own peer group when LinuxFilesystemIsolator is used.,MESOS-3539,12901166,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,jieyu,jieyu,28/Sep/15 22:31,29/Sep/15 23:22,29/Oct/20 16:32,29/Sep/15 23:22,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,"To address this TODO in the code:

{noformat}
src/slave/containerizer/isolators/filesystem/linux.cpp +122


// TODO(jieyu): Currently, we don't check if the slave's work_dir
// mount is a shared mount or not. We just assume it is. We cannot
// simply mark the slave as shared again because that will create a
// new peer group for the mounts. This is a temporary workaround for
// now while we are thinking about fixes.
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2386,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 29 23:22:12 UTC 2015,,,,,,,"0|i2mdqv:",9223372036854775807,,,,,,,,,,,Twitter Mesos Q3 Sprint 6,,,,,,,,,,,3.0,,,,,,,,,,,"29/Sep/15 17:38;jieyu;https://reviews.apache.org/r/38855
https://reviews.apache.org/r/38858/","29/Sep/15 23:22;jieyu;commit 2844a9617e4cec34b41c52632971d2c472c0f63f
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Tue Sep 29 12:08:45 2015 -0700

    Ensured that slave's work_dir is a shared mount in its own peer group
    when LinuxFilesystemIsolator is used.
    
    Review: https://reviews.apache.org/r/38858

commit 68a9d95090c8a84fff0ca0a350e678f9617e1a3e
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Tue Sep 29 10:36:19 2015 -0700

    Added support for getting shared and slave mount peer group ID.
    
    Review: https://reviews.apache.org/r/38855",,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for exposing Accept/Decline responses for inverse offers,MESOS-3489,12895501,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,kaysoky,hartem,hartem,22/Sep/15 16:16,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.25.0,,,,,,,,,,,0,mesosphere,,,,,,,,"Current implementation of maintenance primitives does not support exposing Accept/Decline responses of frameworks to the cluster operators. 

This functionality is necessary to provide visibility to operators into whether a given framework is ready to comply with the posted maintenance schedule.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-09-22 22:47:22.892,,,false,MESOS-1474,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 23 21:19:58 UTC 2015,,,,,,,"0|i2lesf:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 19,,,,,,,,,,,2.0,,0.25.0,,,,,,,,,"22/Sep/15 22:47;nnielsen;Will this land in time for 0.25.0? If not, let's bump to 0.26.0","22/Sep/15 22:57;kaysoky;We should be able to get this done tonight.","22/Sep/15 23:25;kaysoky;Review: https://reviews.apache.org/r/38653/","23/Sep/15 20:31;nnielsen;Awaiting review/shipit from [~jvanremoortere]","23/Sep/15 21:19;jvanremoortere;{code}
commit a0fd3491e2408119f79f7a98e613c2f5ea99c115
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Wed Sep 23 16:47:19 2015 -0400

    Maintenance Primitives: Accept/Decline responses in maintenance/status.
    
    Adds a `getInverseOfferStatuses` method to the allocator, which returns
    some `InverseOfferStatus` objects, grouped by Agent and Framework.
    
    Changes the `/maintenance/status` endpoint to return this additional
    information about draining machines.
    
    Review: https://reviews.apache.org/r/38653
{code}",,,,,,,,,,,,,,,,,,,,,,,
LinuxFilesystemIsolator should make the slave's work_dir a shared mount.,MESOS-3483,12895195,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,jieyu,jieyu,21/Sep/15 17:55,22/Sep/15 00:19,29/Oct/20 16:32,22/Sep/15 00:19,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,"So that when a user task is forked, it does not hold extra references to the sandbox mount and provisioner bind backend mounts. If we don't do that, we could get the following error message when cleaning up bind backend mount points and sandbox mount points.

{noformat}
E0921 17:35:57.268159 47010 bind.cpp:182] Failed to remove rootfs mount point '/var/lib/mesos/provisioner/containers/07eb6660-25ff-4e83-8b2f-06955567e04a/backends/bind/rootfses/30f7e5e2-55d0-4d4d-a662-f8aad0d56b33': Device or resource busy
E0921 17:35:57.268349 47010 provisioner.cpp:403] Failed to remove the provisioned container directory at '/var/lib/mesos/provisioner/containers/07eb6660-25ff-4e83-8b2f-06955567e04a': Device or resource busy
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-09-21 18:11:15.21,,,false,MESOS-2386,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 22 00:19:12 UTC 2015,,,,,,,"0|i2lcwf:",9223372036854775807,,,,,,,,,,,Twitter Mesos Q3 Sprint 5,,,,,,,,,,,3.0,,,,,,,,,,,"21/Sep/15 18:11;xujyan;It would probably make sense to draw a propagation (vfsmount peer group) graph in linux.cpp for posterity as it's becoming harder to grasp for people who didn't work on this.","21/Sep/15 18:15;haosdent@gmail.com;Seems the description is not complete?","21/Sep/15 18:17;jieyu;Fixed.
","21/Sep/15 18:17;haosdent@gmail.com;Could use http://asciiflow.com/ :-)","22/Sep/15 00:10;jieyu;https://reviews.apache.org/r/38569/
https://reviews.apache.org/r/38582/","22/Sep/15 00:19;jieyu;commit 81f61414c7e26e7317eb6f14d8aa2b45e72f4396
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Mon Sep 21 16:47:10 2015 -0700

    Removed the unneeded container work directory mounts.
    
    Review: https://reviews.apache.org/r/38582

commit 2871cbb55b5b8ff08d21d527b26bde7275a8d46c
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Mon Sep 21 14:21:21 2015 -0700

    Made slave's work_dir a shared mount in LinuxFilesystemIsolator.
    
    Review: https://reviews.apache.org/r/38569",,,,,,,,,,,,,,,,,,,,,,
Segfault when accepting or declining inverse offers,MESOS-3458,12875727,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,kaysoky,kaysoky,kaysoky,17/Sep/15 23:31,25/Sep/15 22:46,29/Oct/20 16:32,18/Sep/15 20:35,,,,,,,,,0.25.0,,,,,,master,,,,,0,mesosphere,,,,,,,,"Discovered while writing a test for filters (in regards to inverse offers).

Fix here: https://reviews.apache.org/r/38470/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-09-18 20:35:44.295,,,false,MESOS-1474,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 18 20:35:44 UTC 2015,,,,,,,"0|i2kd6n:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 19,,,,,,,,,,,1.0,,0.25.0,,,,,,,,,"18/Sep/15 20:35;jvanremoortere;{code}
commit e999beb18270601bebbb9131f87a0d1ed6fab37a
Author: Joseph Wu <joseph@mesosphere.io>
Date:   Fri Sep 18 15:39:04 2015 -0400

    Maintenance Primitives: Fixed error in Accept/Decline inverse offers.
    
    Added regression test. Note that the test may be slow until inverse
    offers filters are actually implemented.
    
    Review: https://reviews.apache.org/r/38470
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,
Support running filesystem isolation with Command Executor in MesosContainerizer,MESOS-3428,12864025,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,tnachen,tnachen,tnachen,14/Sep/15 20:53,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.26.0,,,,,,containerization,,,,,0,mesosphere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 06 02:28:13 UTC 2015,,,,,,,"0|hzzzy6:y",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 19,Mesosphere Sprint 20,Mesosphere Sprint 21,,,,,,,,,4.0,,,,,,,,,,,"06/Nov/15 02:28;tnachen;commit 782292dfa29fd00e14069b747c20c7d010459abf
Author: Timothy Chen <tnachen@gmail.com>
Date:   Mon Nov 2 18:56:04 2015 +0000

    Updated command executor to support rootfs.

    Review: https://reviews.apache.org/r/38900/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Docker containerizer does not symlink persistent volumes into sandbox,MESOS-3413,12863393,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,tnachen,neunhoef,neunhoef,11/Sep/15 14:06,26/Nov/18 12:53,29/Oct/20 16:32,26/Nov/18 12:53,0.23.0,,,,,,,,0.28.0,,,,,,agent,containerization,docker,,,2,docker,mesosphere,persistent-volumes,,,,,,"For the ArangoDB framework I am trying to use the persistent primitives. nearly all is working, but I am missing a crucial piece at the end: I have successfully created a persistent disk resource and have set the persistence and volume information in the DiskInfo message. However, I do not see any way to find out what directory on the host the mesos slave has reserved for us. I know it is ${MESOS_SLAVE_WORKDIR}/volumes/roles/<myRole>/<NAME>_<UUID> but we have no way to query this information anywhere. The docker containerizer does not automatically mount this directory into our docker container, or symlinks it into our sandbox. Therefore, I have essentially no access to it. Note that the mesos containerizer (which I cannot use for other reasons) seems to create a symlink in the sandbox to the actual path for the persistent volume. With that, I could mount the volume into our docker container and all would be well.",,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,MESOS-2840,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-09-11 17:09:02.81,,,false,MESOS-1554,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Fri Feb 19 00:42:21 UTC 2016,,,,,,,"0|hzzzn2:zzx",9223372036854775807,,,,,jieyu,,,,,,Mesosphere Sprint 28,Mesosphere Sprint 29,,,,,,,,,,5.0,,,,,,,,,,,"11/Sep/15 17:09;vaibhavkhanduja;I see the code for docker.cpp it is mapping sandbox directory as volumes inside the container. The value of the sandbox directory is coming in as environment variables:""MESOS_SANDBOX""
......
 argv.push_back(""-e"");
 argv.push_back(""MESOS_SANDBOX="" + mappedDirectory);
 argv.push_back(""-e"");
 argv.push_back(""MESOS_CONTAINER_NAME="" + name);
.......

May be good idea to try this for now, though not a good solution ... 

Docker bind mounts these volumes inside container ... a symlink to a directory would be mounted but since it does chroot, the link may not be valid inside the container .... ","11/Sep/15 17:56;haosdent@gmail.com;In meso docker containerizer, we use docker -v to mount. I think symlink is not related to this. [~neunhoef] Do you set the container_path in Volume Resource correctly when driver.acceptOffers? For example, if you set the container_path to ""path1"", it should be accessed in docker container through ""/path1"".","13/Sep/15 19:42;neunhoef;My comments about symlinks were probably wrong or at least misleading. I get the sandbox allright in the docker container. However,  I do not see any way so far to get access from within the docker container to the persistent volume I have previously allocated. I could add a volume (essentially leading to a -v in the docker command line), if I only knew the path that the mesos slave intended for me to use. I do set the container_path value but contrary to what you indicate the persistent volume does not seem to be mounted in my docker container. That is my real problem. Is there any documentation on this other than the source code?","20/Sep/15 18:40;haosdent@gmail.com;[~neunhoef] No sure I understand your ideas correct or not. Let me show how I use persistent volumes in docker below.

I need set the volume info correctly in ContainerInfo, so that docker executor would mount the persistent volumes. Suppose we have already set up a ""path1"" persistent volume correctly.
{code}
  ContainerInfo::DockerInfo dockerInfo;
  dockerInfo.set_image(""busybox"");
  Volume* dockerVolume = containerInfo.add_volumes();
  dockerVolume->set_host_path(""path1"");
  dockerVolume->set_container_path(""/path2"");
  dockerVolume->set_mode(Volume::RW);
  containerInfo.mutable_docker()->CopyFrom(dockerInfo);
{code}

We could found the docker executor would mount the ""path1"" to ""/path2"" in docker docker. I got below log from executor stderr file. 
{code}
docker -v /tmp/xxxx/slaves/db206124-6d5f-493b-8b72-fdfbf65ed744-S0/frameworks/db206124-6d5f-493b-8b72-fdfbf65ed744-0000/executors/1/runs/88cc5c49-50bd-4bab-9e74-f23c43504906/path1:/path2:rw -v /tmp/xxxx/slaves/db206124-6d5f-493b-8b72-fdfbf65ed744-S0/frameworks/db206124-6d5f-493b-8b72-fdfbf65ed744-0000/executors/1/runs/88cc5c49-50bd-4bab-9e74-f23c43504906:/mnt/mesos/sandbox --net host --entrypoint /bin/sh --name mesos-db206124-6d5f-493b-8b72-fdfbf65ed744-S0.88cc5c49-50bd-4bab-9e74-f23c43504906 busybox -c ls /
{code}

From executor stdout file, because I run ""ls /"" command we also could see the /path2 exists.
{code}
total 52
drwxrwxr-x    2 root     root          4096 May 22  2014 bin
drwxr-xr-x    5 root     root           360 Sep 21 00:48 dev
drwxr-xr-x    6 root     root          4096 Sep 21 00:48 etc
drwxrwxr-x    4 root     root          4096 May 22  2014 home
drwxrwxr-x    2 root     root          4096 May 22  2014 lib
lrwxrwxrwx    1 root     root             3 May 22  2014 lib64 -> lib
lrwxrwxrwx    1 root     root            11 May 22  2014 linuxrc -> bin/busybox
drwxrwxr-x    2 root     root          4096 Feb 27  2014 media
drwxrwxr-x    3 root     root          4096 Sep 21 00:48 mnt
drwxrwxr-x    2 root     root          4096 Feb 27  2014 opt
drwxr-xr-x    2 root     root          4096 Sep 21 00:48 path2
dr-xr-xr-x  171 root     root             0 Sep 21 00:48 proc
drwx------    2 root     root          4096 Feb 27  2014 root
lrwxrwxrwx    1 root     root             3 Feb 27  2014 run -> tmp
drwxr-xr-x    2 root     root          4096 May 22  2014 sbin
dr-xr-xr-x   13 root     root             0 Aug 12 09:16 sys
drwxrwxrwt    3 root     root          4096 May 22  2014 tmp
drwxrwxr-x    6 root     root          4096 May 22  2014 usr
drwxrwxr-x    4 root     root          4096 May 22  2014 var
{code}","24/Sep/15 13:37;neunhoef;In understand all this and it works fine exactly like this for me.
My problem is the following: I do not know what path to take for ""path1"" (for the set_host_path() method). When I make the persistent volume (call makePersistent in the scheduler-API) I have to hand in the dynamically reserved disk resource with a diskInfo that contains
a Volume. In there I *must not set* the host_path, otherwise I get an error message. Then the makePersistent actually works, but I do not know where to get the information which path the mesos agent picked for my persistent volume. If I knew that path then I could do what you suggest, but I do not see any way to learn that path!
This is my problem.
My comments about symlinks have probably been misleading, sorry for that.
","24/Sep/15 13:56;haosdent@gmail.com;actually the path1 here is container_path in Volume in Resource::DiskInfo.","24/Sep/15 15:02;neunhoef;In your example ""path1"" was used for host_path, wasn't it?
Why should that path be in container_path? And: in which Resource message? The one I use to make a volume persistent? Or the one I use to launch the executor?","24/Sep/15 17:25;haosdent@gmail.com;Maybe a bit confuse. ""path1"" is host_path in DockerInfo::Volume and it is the container_path name in Resource::DiskInfo::Volumn","24/Sep/15 17:41;jieyu;Yeah, that's a known issue. We simply haven't implemented that yet. cc [~tnachen]

In the meantime, we are working on a unified containerizer (i.e., a single Mesos containerizer works for both docker, appc, and other image format) in MESOS-2840. Once that's completed (soon in the next release), you won't have any issue accessing persistent volumes while using docker images.","25/Sep/15 11:24;neunhoef;It seems to work for me now after I have done what haosdent suggests. Can somebody point be to the code in mesos which does this magic? I am aware of the code round line 382 in mesos-0.23-0/src/docker/docker.cpp taking care of relative names in  ""host_path"" entries of volumes. However, I do not understand how the word I put into ""container_path"" in the volume I make persistent is matched to the same word I put into ""host_path"" in the volume I mount when launching the task, replacing the latter with the relative path to the actual persistent volume...","25/Dec/15 05:53;zhitao;It seem that this task is marked as ""Won't fix"". I wonder whether it's possible to come up with some short fix for the existing docker containerizer so users don't need to blocked until new Unified Containerizer is ready.

There seem to be two possible ways to fix:
- add some feedback response from slave to master/allocator to the resource offer containing the hostPath so offer resources containing this persisted volume has non-empty hostPath;
- at launch time, detect that a persisted volume is included in the ContainerInfo, and automatically mount the resolved host path.

The getPersistentVolumePath() in src/slave/paths.cpp is already available for this purpose.

Can someone comment on this possibility?","29/Dec/15 19:36;jieyu;The key reason we don't support persistent volumes in DockerContainerizer is because when a container has been launched, we don't have a way to update volumes (imaging a new task is sent to the executor with some persistent volumes). Just updating symlinks is not going to work unless you bind mount '/var/lib/mesos/volumes' into the container (which is not a very secure way to do).","29/Dec/15 19:51;zhitao;[~jieyu], thanks for the reply. My comment about symlink was indeed incorrect and I updated my previous comment. I think returning the host path in the resource offer could still work though, as long as Mesos slave would not change the real location of the volume created.

For the fact of unable to update volumes for a running container is a limitation, I acknowledge that's a limitation , but I don't really understand why it's a show stopper. Making it clear to users that for DockerContainerizer, all persistent volumes must be created and mounted before the executor/container is created still sounds reasonable to me, and it allows us to use current DockerContainerizer until the new Unified Containerizer is available and covers other features we may need from docker engine.

One of the reasons I really want this is that our in-house database system (which we are looking towards to run on Mesos) requires running multiple mysqld instances on the same machine, and that team already spent quite some time to dockerize these instances for easy configuration and isolation purpose.

Thanks for you time again. I really looking forward to using persistent volume primitives!","29/Dec/15 20:02;jieyu;Yeah, if persistent volumes are only presented at launch time, we can just modify the DockerContainerizer to achieve that by adding extra '-v <volumes>'  for persistent volumes.","29/Dec/15 20:57;zhitao;Thanks! [~haosdent@gmail.com] Do you have objection for reopening this task? I can try to get this done too if you won't have cycle for this.","04/Jan/16 21:17;zhitao;[~jieyu] and [~haosdent@gmail.com], I've put up https://reviews.apache.org/r/41892 for a first pass at unblocking DockerContainerizer users to use persistent volumes. Please let me know what you think.","16/Feb/16 17:56;jieyu;https://reviews.apache.org/r/43015","18/Feb/16 22:03;tnachen;commit 541b3d963cccf07e979ce5362cbb6ace0144f31a
Author: Timothy Chen <tnachen@gmail.com>
Date:   Fri Jan 29 18:09:52 2016 -0500

    Fixed persistent volumes with docker tasks.

    Review: https://reviews.apache.org/r/43015","19/Feb/16 00:42;jieyu;commit 83bf5671ee33cc03291714e5fd801e49552088fc
Author: Timothy Chen <tnachen@gmail.com>
Date:   Thu Feb 18 23:36:43 2016 +0000

    Fixed compilation issue with clang.",,,,,,,,,
Removing mount point fails with EBUSY in LinuxFilesystemIsolator.,MESOS-3349,12860875,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,bmahler,bmahler,01/Sep/15 02:26,16/Jan/16 08:46,29/Oct/20 16:32,14/Sep/15 23:39,,,,,,,,,0.27.0,,,,,,test,,,,,0,flaky-test,,,,,,,,"When running the tests as root, we found PersistentVolumeTest.AccessPersistentVolume fails consistently on some platforms.

{noformat}
[ RUN      ] PersistentVolumeTest.AccessPersistentVolume
I0901 02:17:26.435140 39432 exec.cpp:133] Version: 0.25.0
I0901 02:17:26.442129 39461 exec.cpp:207] Executor registered on slave 20150901-021726-1828659978-52102-32604-S0
Registered executor on hostname
Starting task d8ff1f00-e720-4a61-b440-e111009dfdc3
sh -c 'echo abc > path1/file'
Forked command at 39484
Command exited with status 0 (pid: 39484)
../../src/tests/persistent_volume_tests.cpp:579: Failure
Value of: os::exists(path::join(directory, ""path1""))
  Actual: true
Expected: false
[  FAILED  ] PersistentVolumeTest.AccessPersistentVolume (777 ms)
{noformat}

Turns out that the 'rmdir' after the 'umount' fails with EBUSY because there's still some references to the mount.

FYI [~jieyu] [~mcypark]","Ubuntu 14.04, CentOS 5",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-09-02 20:37:07.135,,,false,MESOS-2386,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 14 23:39:06 UTC 2015,,,,,,,"0|i2jmnb:",9223372036854775807,,,,,xujyan,,,,,,Twitter Mesos Q3 Sprint 5,,,,,,,,,,,5.0,,,,,,,,,,,"02/Sep/15 20:37;greggomann;What platform is it failing on? This currently passes for me on OSX 10.10.4.","02/Sep/15 20:43;greggomann;I see the failure on Ubuntu 14.04.","02/Sep/15 22:06;greggomann;I just tested this again in Ubuntu 14.04 on a fresh build and it appears to be fixed now? Ran it 100 times with no failures.","02/Sep/15 22:09;bmahler;Are you running as root?","02/Sep/15 22:14;greggomann;:facepalm:

sorry, I wasn't :-) yep, it's still failing","02/Sep/15 22:38;greggomann;This output from Ubuntu 14:
{code}
I0902 22:09:26.596101 31762 linux.cpp:493] Removing mount '/tmp/PersistentVolumeTest_AccessPersistentVolume_9Q2Zk1/slaves/20150902-220926-251789322-39756-31742-S0/frameworks/20150902-220926-251789322-39756-31742-0000/executors/d89c6891-f9c4-4de9-b8ea-f75d0f4fe754/runs/ff43a3f9-fd1c-4923-8640-e3eed2744d5d/path1' for persistent volume disk(role1)[id1:path1]:64 of container ff43a3f9-fd1c-4923-8640-e3eed2744d5d
I0902 22:09:26.596161 31759 process.cpp:2410] Resuming (10)@10.0.2.15:39756 at 2015-09-02 22:09:26.596153856+00:00
I0902 22:09:26.596191 31759 process.cpp:2410] Resuming (11)@10.0.2.15:39756 at 2015-09-02 22:09:26.596188160+00:00
I0902 22:09:26.596211 31759 process.cpp:2410] Resuming __gc__@10.0.2.15:39756 at 2015-09-02 22:09:26.596208128+00:00
I0902 22:09:26.596267 31759 process.cpp:2410] Resuming (24)@10.0.2.15:39756 at 2015-09-02 22:09:26.596262912+00:00
I0902 22:09:26.605408 31762 process.cpp:2410] Resuming (24)@10.0.2.15:39756 at 2015-09-02 22:09:26.605393920+00:00
I0902 22:09:26.605957 31762 process.cpp:2515] Cleaning up (24)@10.0.2.15:39756
I0902 22:09:26.606415 31762 process.cpp:2410] Resuming __gc__@10.0.2.15:39756 at 2015-09-02 22:09:26.606408960+00:00
I0902 22:09:26.606077 31757 process.cpp:2410] Resuming slave(1)@10.0.2.15:39756 at 2015-09-02 22:09:26.606065920+00:00
E0902 22:09:26.607035 31757 slave.cpp:2870] Failed to update resources for container ff43a3f9-fd1c-4923-8640-e3eed2744d5d of executor d89c6891-f9c4-4de9-b8ea-f75d0f4fe754 running task d89c6891-f9c4-4de9-b8ea-f75d0f4fe754 on status update for terminal task, destroying container: Collect failed: Failed to remove persistent volume mount point at '/tmp/PersistentVolumeTest_AccessPersistentVolume_9Q2Zk1/slaves/20150902-220926-251789322-39756-31742-S0/frameworks/20150902-220926-251789322-39756-31742-0000/executors/d89c6891-f9c4-4de9-b8ea-f75d0f4fe754/runs/ff43a3f9-fd1c-4923-8640-e3eed2744d5d/path1': Device or resource busy
{code}

vs. this output from OSX:
{code}
I0902 15:18:19.899541 197115904 posix.cpp:156] Removing symlink '/tmp/PersistentVolumeTest_AccessPersistentVolume_ZPXsHo/slaves/20150902-151819-1229914122-63371-94565-S0/frameworks/20150902-151819-1229914122-63371-94565-0000/executors/e5421321-94ed-4495-9b1d-3f3aed9bb522/runs/d903841d-f1da-46e6-ab05-eb437ac9e4e8/path1' for persistent volume disk(role1)[id1:path1]:64 of container d903841d-f1da-46e6-ab05-eb437ac9e4e8
{code}

So it seems the persistent volume is still in use by the executor at the time it's unmounted, but since a symlink is used on OSX it succeeds anyway.","04/Sep/15 18:35;greggomann;When running the test as non-root user on Ubuntu, {{PosixFilesystemIsolatorProcess}} is used instead of {{LinuxFilesystemIsolatorProcess}} to manage the persistent volume, and so a symlink gets used instead of a mount, just as on OSX. This explains the different behavior for root/non-root on linux.","06/Sep/15 14:13;haosdent@gmail.com;The strange thing here is ""Device or resource busy"" happends when ""os::rmdir"" after umount successfully. I use ""cat /proc/mounts"" both before ""os::umount"" and after ""os::umount"" could see the mount point is removed from mount table. Also use ""stat"" to check the mount point, could see there are different before ""os::umount"" and ""os::umount"". And also try use ""lsof +D "", ""fuser -vm"" to inspect which process still hold the folder before ""os::rmdir"", but could not found anything. 

And add ""os::rmdir"" in PersistentVolumeTest.AccessPersistentVolume also would return same error(""Device or resource busy"").
{code}
--- a/src/tests/persistent_volume_tests.cpp
+++ b/src/tests/persistent_volume_tests.cpp
@@ -576,6 +576,7 @@ TEST_F(PersistentVolumeTest, AccessPersistentVolume)
       frameworkId.get(),
       executorId);

+  Try<Nothing> rmdir = os::rmdir(path::join(directory, ""path1""), true);
   EXPECT_FALSE(os::exists(path::join(directory, ""path1"")));
{code}

Only add add os::sleep() and then call os::rmdir could works.
{code}
-- a/src/slave/containerizer/isolators/filesystem/linux.cpp
+++ b/src/slave/containerizer/isolators/filesystem/linux.cpp
@@ -504,6 +504,7 @@ Future<Nothing> LinuxFilesystemIsolatorProcess::update(
     }

     // NOTE: This is a non-recursive rmdir.
+    os::sleep(Seconds(3));
     Try<Nothing> rmdir = os::rmdir(target, false);
     if (rmdir.isError()) {
{code}

I also try to minimize this problem, but my attempts failed so far. Below code snippet could works well.
{code}
fs::mount(""/tmp/origin"", ""/tmp/mount"", None(), MS_BIND, NULL);
fs::unmount(""/tmp/mount"");
os::rmdir(""/tmp/mount"");
{code}

If I change the target and source variable in ""LinuxFilesystemIsolatorProcess::update"", it could still got ""Device or resource busy"" when ""os::rmdir"".
{code}
diff --git a/src/slave/containerizer/isolators/filesystem/linux.cpp b/src/slave/containerizer/isolators/filesystem/linux.cpp
index a780b45..cdf6e80 100644
--- a/src/slave/containerizer/isolators/filesystem/linux.cpp
+++ b/src/slave/containerizer/isolators/filesystem/linux.cpp
@@ -490,6 +490,7 @@ Future<Nothing> LinuxFilesystemIsolatorProcess::update(
       target = path::join(info->directory, containerPath);
     }

+    target = ""/tmp/mount"";
     LOG(INFO) << ""Removing mount '"" << target << ""' for persistent volume ""
               << resource << "" of container "" << containerId;

@@ -578,6 +579,8 @@ Future<Nothing> LinuxFilesystemIsolatorProcess::update(
       target = path::join(info->directory, containerPath);
     }

+    source = ""/tmp/origin"";
+    target = ""/tmp/mount"";
     if (os::exists(target)) {
       // NOTE: This is possible because 'info->resources' will be
       // reset when slave restarts and recovers. When the slave calls
{code}","06/Sep/15 18:24;haosdent@gmail.com;And further investigation for this, I do similar progress in PosixFilesystemIsolatorProcess::update and it could rmdir successfully. So I guess something in LinuxFilesystemIsolatorProcess led to avoid result. Seem some time to figure out the cause for this problem.","09/Sep/15 18:58;jieyu;That's really strange. We are not using MNT_DETACH flag while doing umount in 'update()'.

That sounds to me a different kernel behavior regarding umount. One workaround is to check the error code of rmdir and retry if ebusy is returned for a while.","10/Sep/15 09:55;haosdent@gmail.com;Sorry for not update the status here. I find the CLONE_NEWNS affect ""Device or resource busy"". So I think the best way is to solve the problem in cause. I have a draft to fix this issue. https://reviews.apache.org/r/38252/diff/2#index_header But I still don't know why set CLONE_NEWNS affect this. Still under investigation.","10/Sep/15 10:10;haosdent@gmail.com;I also change namespaces value in linux_launcher.cpp directly(without change isolators). Also could pass the test.
{code}
diff --git a/src/slave/containerizer/linux_launcher.cpp b/src/slave/containerizer/linux_launcher.cpp
index 12acf4d..0a72a66 100644
--- a/src/slave/containerizer/linux_launcher.cpp
+++ b/src/slave/containerizer/linux_launcher.cpp
@@ -186,6 +186,7 @@ static pid_t clone(const lambda::function<int()>& func, int namespaces)
   // - 8 MiB appears to be the default for ""ulimit -s"" on OSX and Linux.
   static unsigned long long stack[(8*1024*1024)/sizeof(unsigned long long)];

+  namespaces = 0x00020000;
   LOG(INFO) << ""Cloning child process with flags = ""
             << ns::stringify(namespaces);
{code}","10/Sep/15 10:36;haosdent@gmail.com;According [CLONE_NEWNS|http://stackoverflow.com/questions/22889241/linux-understanding-the-mount-namespace-clone-clone-newns-flag], [bind_mount|https://lwn.net/Articles/159092/]. I think could explain the behaviours so far.

In LinuxFilesystemIsolatorProcess, we mount persistent volume (default behaviour make-private) before launch the executor. After LinuxLauncher fork with CLONE_NEWNS, we could umount persistent volume in LinuxFilesystemIsolatorProcess. But this don't affect the executor continue to hold that mount point. When slave receive TASK_FINISH and call LinuxFilesystemIsolatorProcess try to rmdir that mount point, it would failed because executor is still running and holding the mount point (after add some trace code to show when executor exited, I observed this.). So a possible way to fix this is to use make-shared or make-slave when mount persistent volume. By my attempts failed on this. 

{code}
45 22 8:3 /tmp/PersistentVolumeTest_AccessPersistentVolume_L6fY1a/volumes/roles/role1/id1 /tmp/PersistentVolumeTest_AccessPersistentVolume_L6fY1a/slaves/20150911-170559-162297291-49192-21628-S0/frameworks/20150911-170559-162297291-49192-21628-0000/executors/c6bcf76f-7cf5-42e6-8eb8-2d21e393ba3d/runs/454bbfa3-0305-4900-b05d-389f6b215c32/path1 rw,relatime shared:1 - ext4 /dev/disk/by-uuid/98708f21-a59d-4b80-a85c-27b78c22e316 rw,errors=remount-ro,data=ordered
{code}

{code}
78 48 8:3 /tmp/PersistentVolumeTest_AccessPersistentVolume_L6fY1a/volumes/roles/role1/id1 /tmp/PersistentVolumeTest_AccessPersistentVolume_L6fY1a/slaves/20150911-170559-162297291-49192-21628-S0/frameworks/20150911-170559-162297291-49192-21628-0000/executors/c6bcf76f-7cf5-42e6-8eb8-2d21e393ba3d/runs/454bbfa3-0305-4900-b05d-389f6b215c32/path1 rw,relatime shared:1 - ext4 /dev/disk/by-uuid/98708f21-a59d-4b80-a85c-27b78c22e316 rw,errors=remount-ro,data=ordered
{code}

Could see the persistent volumes have already mount as shared, but this test still failed.","11/Sep/15 14:19;haosdent@gmail.com;A test case to simplify this problem: https://reviews.apache.org/r/38300/","11/Sep/15 19:42;jieyu;[~haosdent@gmail.com] We definitely have this test passed on CentOS6 (linux-4.1) running using ROOT. If what you said is true, we should definitely get the same error, but we don't. Also, we cannot get rid of CLONE_NEWNS because the container expects a new mount namespace and many of our code relies on that.

Also, I observed a case where I still got EBUSY when doing rmdir even if all processes in the container are killed (centos5, linux-3.14). That's in the containerizer->destroy path. Still investigating.","11/Sep/15 23:23;jieyu;Digged a thread in chrome project:
https://code.google.com/p/chromium/issues/detail?id=358933

Looks like a kernel issue in old kernel. Not sure which commit fixed this issue.","12/Sep/15 00:17;jieyu;[~wangcong] pointed to me a patch that might explain why kernel 4.1 no longer has this problem. This patch got merged in 3.18.

In the patch, if rmdir target is a mount point, the kernel will try to umount it lazily. But that still does not explain why umount returns OK even if the mount point cannot be completely removed.

commit 8ed936b5671bfb33d89bc60bdcc7cf0470ba52fe
Author: Eric W. Biederman <ebiederman@twitter.com>
Date:   Tue Oct 1 18:33:48 2013 -0700

    vfs: Lazily remove mounts on unlinked files and directories.
    
    With the introduction of mount namespaces and bind mounts it became
    possible to access files and directories that on some paths are mount
    points but are not mount points on other paths.  It is very confusing
    when rm -rf somedir returns -EBUSY simply because somedir is mounted
    somewhere else.  With the addition of user namespaces allowing
    unprivileged mounts this condition has gone from annoying to allowing
    a DOS attack on other users in the system.
    
    The possibility for mischief is removed by updating the vfs to support
    rename, unlink and rmdir on a dentry that is a mountpoint and by
    lazily unmounting mountpoints on deleted dentries.
    
    In particular this change allows rename, unlink and rmdir system calls
    on a dentry without a mountpoint in the current mount namespace to
    succeed, and it allows rename, unlink, and rmdir performed on a
    distributed filesystem to update the vfs cache even if when there is a
    mount in some namespace on the original dentry.
    
    There are two common patterns of maintaining mounts: Mounts on trusted
    paths with the parent directory of the mount point and all ancestory
    directories up to / owned by root and modifiable only by root
    (i.e. /media/xxx, /dev, /dev/pts, /proc, /sys, /sys/fs/cgroup/{cpu,
    cpuacct, ...}, /usr, /usr/local).  Mounts on unprivileged directories
    maintained by fusermount.
    
    In the case of mounts in trusted directories owned by root and
    modifiable only by root the current parent directory permissions are
    sufficient to ensure a mount point on a trusted path is not removed
    or renamed by anyone other than root, even if there is a context
    where the there are no mount points to prevent this.
    
    In the case of mounts in directories owned by less privileged users
    races with users modifying the path of a mount point are already a
    danger.  fusermount already uses a combination of chdir,
    /proc/<pid>/fd/NNN, and UMOUNT_NOFOLLOW to prevent these races.  The
    removable of global rename, unlink, and rmdir protection really adds
    nothing new to consider only a widening of the attack window, and
    fusermount is already safe against unprivileged users modifying the
    directory simultaneously.
    
    In principle for perfect userspace programs returning -EBUSY for
    unlink, rmdir, and rename of dentires that have mounts in the local
    namespace is actually unnecessary.  Unfortunately not all userspace
    programs are perfect so retaining -EBUSY for unlink, rmdir and rename
    of dentries that have mounts in the current mount namespace plays an
    important role of maintaining consistency with historical behavior and
    making imperfect userspace applications hard to exploit.","12/Sep/15 00:41;jieyu;Posted a workaround while still investigating alternatives:
https://reviews.apache.org/r/38327","12/Sep/15 03:30;haosdent@gmail.com;[~jieyu] I try that on Ubuntu 14.04 (3.13.0-32) and CentOS 6 (2.6.32-504) And sure why umount success and rmdir failed is because executor is still running and holding the mount point after I add trace code in executor.cpp. This is also the reason why we could rmdir success after few seconds. Because executor would terminated in few seconds after send TASK_FINISH.

We could reproduce this problem through these simple shell snippet
{code}
mkdir /tmp/source /tmp/target
mount --bind /tmp/source /tmp/target
unshare -m /bin/bash -- -c ""sleep 2"" &
umount /tmp/target && rmdir /tmp/target
{code}

And this test case also simulate this problem: https://reviews.apache.org/r/38300/diff/1#index_header","12/Sep/15 03:37;jieyu;Can you try to make the parent of the persistent volume a shared mount? See if that solves the problem.

Sent from my iPhone

","12/Sep/15 03:39;haosdent@gmail.com;Let me try.","12/Sep/15 04:45;haosdent@gmail.com;Could work if mark the parent as a shared mount. Could not work if mark it as a shared mount directly. So I misunderstand the document. LoL","12/Sep/15 05:19;jieyu;Great. I'll try to come up with a patch for true fix.","12/Sep/15 06:37;jieyu;Alright, this is the true fix. Tested on CentOS5 and CentOS6. [~haosdent@gmail.com], can you test the patches on Ubuntu? Thanks!

https://reviews.apache.org/r/38328/
https://reviews.apache.org/r/38329/","12/Sep/15 06:51;haosdent@gmail.com;I am testing, thank you!","12/Sep/15 07:06;haosdent@gmail.com;Pass on Ubuntu 14.04","14/Sep/15 22:32;jieyu;OK, realized that making '/' a shared mount is too invasive (any bind mount will cause two mounts in the mount table). Instead, decided to make sandbox a shared mount (self bind mount if needed).

https://reviews.apache.org/r/38333/
https://reviews.apache.org/r/38374/","14/Sep/15 23:39;jieyu;commit 1c10fdbfaad1841f9ca93e3a3c255f81b0defe5a
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Mon Sep 14 15:28:44 2015 -0700

    Added logics in LinuxFilesystemIsolator to recover and cleanup orphans.
    
    Review: https://reviews.apache.org/r/38374

commit 304032b4893ec282196bbb7fefd889fe5d49b3f5
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Sat Sep 12 23:28:29 2015 -0700

    Made container sandbox a shared mount to address MESOS-3349.
    
    Review: https://reviews.apache.org/r/38333"
Dynamic reservations are not counted as used resources in the master,MESOS-3338,12860717,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,Invalid,,alexr,alex-mesos,31/Aug/15 15:53,03/Dec/18 22:15,29/Oct/20 16:32,03/Dec/18 22:15,,,,,,,,,,,,,,,allocation,master,,,,0,mesosphere,multitenancy,persistent-volumes,,,,,,"Dynamically reserved resources should be considered used or allocated and hence reflected in Mesos bookkeeping structures and {{state.json}}.

I expanded the {{ReservationTest.ReserveThenUnreserve}} test with the following section:
{code}
  // Check that the Master counts the reservation as a used resource.
  {
    Future<process::http::Response> response =
      process::http::get(master.get(), ""state.json"");
    AWAIT_READY(response);

    Try<JSON::Object> parse = JSON::parse<JSON::Object>(response.get().body);
    ASSERT_SOME(parse);

    Result<JSON::Number> cpus =
      parse.get().find<JSON::Number>(""slaves[0].used_resources.cpus"");

    ASSERT_SOME_EQ(JSON::Number(1), cpus);
  }
{code}
and got
{noformat}
../../../src/tests/reservation_tests.cpp:168: Failure
Value of: (cpus).get()
  Actual: 0
Expected: JSON::Number(1)
Which is: 1
{noformat}

Idea for new resources states: https://docs.google.com/drawings/d/1aquVIqPY8D_MR-cQjZu-wz5nNn3cYP3jXqegUHl-Kzc/edit",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-09-01 09:16:53.897,,,false,MESOS-7229,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 03 22:15:55 UTC 2018,,,,,,,"0|i2jlqn:",9223372036854775807,,,,,mcypark,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,"01/Sep/15 09:16;gyliu;[~mcypark] I see that allocator has an API HierarchicalAllocatorProcess<RoleSorter, FrameworkSorter>::updateAvailable, but this API was not called by master now, can you please show some comments on this? Thanks.","08/Sep/15 02:50;gyliu;After some discussion with [~alex-mesos] , his concern is that if we reserved resources for a role, it's not available any more.

But from my understanding, even if the resource is reserved, if this resource is not allocated, we should still consider it as available resource.

[~mcypark] Can u please show some comments here? Thanks.
","08/Sep/15 07:46;qianzhang;Once a resource is dynamically reserved for a role, it will be considered as available resources for that role, but not for any other roles.

And for this bug, I am not sure we should consider dynamic reserved resource as slave's used resource, my understanding is, slave's used resource is increased when launching task, and decreased when task is terminated, it should not have direct relationship with dynamic reservation.","12/Oct/15 01:37;gyliu;MESOS-1607 is planning to use reserved but not allocated resources as optimistic offer.","13/Oct/15 11:42;alexr;What's the status here? [~gyliu], do you have any progress on this issue?","13/Oct/15 12:53;gyliu;[~alex-mesos] It is still under design but I think that that the design is going to be finalized in coming weeks, please refer to https://docs.google.com/document/d/1RGrkDNnfyjpOQVxk_kUFJCalNMqnFlzaMRww7j7HSKU/edit for more detail. Thanks.","13/Oct/15 13:37;JamesYongQiaoWang;In optimistic offer design, the dynamic reserved resources will be treated as Reserved Resources rather than Used Resources, the Used Resources in that design should be the allocated resources.","14/Oct/15 12:08;alex-mesos;I agree that reserved resource is not yet allocated, but it's not available either. If a resource is reserved, an allocator cannot freely offer this resource. We may want to introduce a new state a resource can be in, ""reserved"".","14/Oct/15 12:10;alexr;But from the allocator point of view, this resource is ""allocated"", because it is assigned to a particular role.","14/Oct/15 12:20;alexr;[~JamesYongQiaoWang], [~gyliu], [~qianzhang] how come a future feature (optimistic offers) is related to this issue? The problem here is that when somebody (say, a quota manager) tries to figure out what resources are available for allocation and what are ""reserved"" or allocated gets a wrong answer.","14/Oct/15 12:43;gyliu;[~alex-mesos] The optimistic offers is not treating reserved resources as used, Do you have any requirement or use case why want to treat those resources as used? Thanks.","14/Oct/15 13:06;qianzhang;The reserved resources should not be simply counted as used resource, see the comments of Slave::usedResources:
{code}
hashmap<FrameworkID, Resources> usedResources;  // Active task / executors.
{code}
As the comments show, only the resources used by active tasks / executors are counted as used resources. But for reserved resources, they can still be available resources (not used by any tasks / executors) to the framework who reserves them.

So I agree we may need a new state for the reserved resources.","20/Oct/15 10:26;alexr;I do, it's quota : )","20/Oct/15 10:30;alexr;[~qianzhang], could we start the conversation around new states? Maybe it is something a working group for optimistic offers can take over?","20/Oct/15 13:54;gyliu;[~alexr] Does this help https://github.com/apache/mesos/blob/master/src/master/http.cpp#L290-L294 

I think that what you want to do is get the unused resources on every agent based on RR (https://reviews.apache.org/r/38110/diff/6?file=1098311#file1098311line129), does the following help?
{code}
Resources unusedOnAgent =
      slave->totalResources - Resources::sum(slave->usedResources) - (slave->totalResources.reserved() - Resources::sum(slave->usedResources.reserved()) );
{code}","23/Oct/15 10:48;alexr;I'll have a look, thanks [~gyliu]! I doubt {{slave->totalResources.reserved()}} includes dynamic reservations, but I have to check it.","23/Oct/15 14:41;alexr;{{.reserved()}} indeed includes dynamically reserved resources. Would you agree that having such non-trivial math for unused resources on an agent is not optimal? I would suggest we revisit the types of resources we store to simplify the math. How about [Total [Reserved] [Offered [Allocated [Used]]]] (Reserved may overlap with offered, allocated and used)?","25/Oct/15 02:11;gyliu;[~alexr] can you please show more detail for what does  `[Total [Reserved] [Offered [Allocated [Used]]]]` means? What is the problem of usign `Resources unusedOnAgent = slave->totalResources - Resources::sum(slave->usedResources) - (slave->totalResources.reserved() - Resources::sum(slave->usedResources.reserved()) );` to calculate the unsed resource on a slave? Thanks.","25/Oct/15 04:59;alexr;The problem is that {{Resources unusedOnAgent = slave->totalResources - Resources::sum(slave->usedResources) - (slave->totalResources.reserved() - Resources::sum(slave->usedResources.reserved()) );}} is too long and cryptic, it takes too much time for a reader to understand—and verify!—what's going on. For example, I think this line misses some {{.flatten()}} calls. I'm thinking about how we can simplify the resources math to make it easier for us to understand and use.","25/Oct/15 05:11;alexr;I've attached a pic to the ticket depicting what I mean under new states. ","25/Oct/15 07:52;gyliu;Thanks [~alexr] Can you please show more detail for how we can calculate the unused resource on one slave with your pic? Thanks.","20/Jan/16 09:21;gyliu;[~alexr] do you still think this is a problem? With MESOS-1607, we have {{allocation slack}} which can lend some un unused reservation resources to other frameworks. So I think that we should not treat {{dynamic reservation}} resources as used.","03/Dec/18 22:15;bmahler;Closing as stale and [~mzhu] and I couldn't see the relevance of this anymore. FYI in MESOS-4527 we made sure that reservations (dynamic and static) are counted towards quota.",,,,,
Make use of C++11 atomics,MESOS-3326,12859765,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,neilc,neilc,neilc,28/Aug/15 00:11,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.26.0,,,,,,,,,,,0,mesosphere,,,,,,,,"Now that we require C++11, we can make use of std::atomic. For example:

* libprocess/process.cpp uses a bare int + __sync_synchronize() for ""running""
* __sync_synchronize() is used in logging.hpp in libprocess and fork.hpp in stout
* sched/sched.cpp uses a volatile int for ""running"" -- this is wrong, ""volatile"" is not sufficient to ensure safe concurrent access
* ""volatile"" is used in a few other places -- most are probably dubious but I haven't looked closely",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-08-28 15:11:32.4,,,false,MESOS-2664,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 20 20:42:32 UTC 2015,,,,,,,"0|i2jh47:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 21,,,,,,,,,,,2.0,,0.26.0,,,,,,,,,"28/Aug/15 07:58;neilc;Initial work:

https://reviews.apache.org/r/37876/
https://reviews.apache.org/r/37877/
https://reviews.apache.org/r/37878/

Style-wise, I opted to explicitly call load() and store(), rather than rely on the overloads for T() and operator=. I think there's some value in being explicit, but let me know if you guys think otherwise.

This updates most of the places where volatile and/or GCC intrinsics were used, but not all. Remaining places I noticed:

* libprocess/src/tests/process.cpp
* libprocess/include/process/owned.hpp
* libprocess/include/process/shared.hpp
* libprocess/include/process/logging.hpp","28/Aug/15 15:11;bernd-mesos;See above.","11/Sep/15 04:24;jvanremoortere;{code}
commit 87de003c6e8a4dfc2d29ae8b0aab1f83ff0c66a3
Author: Neil Conway <neil.conway@gmail.com>
Date:   Thu Sep 10 17:50:33 2015 -0700

    mesos: Replace volatile with std::atomic.
    
    MESOS-3326.
    
    Review: https://reviews.apache.org/r/37878

commit 4b938052b6af124eb1fdaec9b597c620627677ea
Author: Neil Conway <neil.conway@gmail.com>
Date:   Thu Sep 10 17:50:22 2015 -0700

    libprocess: Replace GCC instrinsics and volatile with std::atomic.
    
    MESOS-3326.
    
    Review: https://reviews.apache.org/r/37877

commit 4a01850c554048cfa53ca03d7d6e3cf901ce166d
Author: Neil Conway <neil.conway@gmail.com>
Date:   Thu Sep 10 17:50:13 2015 -0700

    stout: Replace GCC intrinsics with std::atomic.
    
    MESOS-3326.
    
    Review: https://reviews.apache.org/r/37876

commit 941174914d309477b05865078ecf8a0f69db04f8
Author: Neil Conway <neil.conway@gmail.com>
Date:   Thu Sep 10 17:50:04 2015 -0700

    mesos: Update style guide for usage of std::atomic.
    
    mesos: Update style guide for usage of std::atomic.
    
    Review: https://reviews.apache.org/r/38265
{code}","05/Oct/15 17:52;neilc;Re-opening this because there's a few remaining places in which we need to use std::atomic--

    libprocess/src/tests/process.cpp
    libprocess/include/process/owned.hpp
    libprocess/include/process/shared.hpp
    libprocess/include/process/logging.hpp
","14/Oct/15 22:30;neilc;Fix for the remaining issues posted here: https://reviews.apache.org/r/39330/","19/Oct/15 04:21;neilc;Actually here:

https://reviews.apache.org/r/39428/
https://reviews.apache.org/r/39429/
","19/Oct/15 04:36;mcypark;{noformat}
commit 5882d4a2189f954f093e824e5bf04f32a7954697
Author: Neil Conway <neil.conway@gmail.com>
Date:   Sun Oct 18 21:30:32 2015 -0700

    Fixed typo in comment, minor style fixes.

    Review: https://reviews.apache.org/r/39428
{noformat}","20/Oct/15 20:42;jvanremoortere;{code}
commit 474a43e612b693e65e8f20d87a4261accd4679e5
Author: Neil Conway <neil.conway@gmail.com>
Date:   Tue Oct 20 16:33:29 2015 -0400

    Replaced volatile, GCC intrinsics with std::atomic.
    
    See MESOS-3326. We adopted std::atomic in most of the code base earlier
    (commits 87de003c6e8a, 4b938052b6af, and 4a01850c5540), but a few places
    were omitted; those locations are fixed by this commit.
    
    There's one last place to fix: we use the GCC intrinsic
    __sync_synchronize() in 3rdparty/libprocess/include/process/logging.h.
    Because that is used to protect modifications to the FLAGS_v variable
    defined by glog, we can't easily adapt it to use std::atomic.
    
    Review: https://reviews.apache.org/r/39429
{code}",,,,,,,,,,,,,,,,,,,,
Master should drop HTTP calls when it's recovering,MESOS-3290,12856977,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,vinodkone,vinodkone,vinodkone,18/Aug/15 17:47,18/Aug/15 19:27,29/Oct/20 16:32,18/Aug/15 19:12,,,,,,,,,0.24.0,,,,,,,,,,,0,,,,,,,,,"Much like what we do with PID based frameworks, master should drop HTTP calls if it's not the leader and/or still recovering.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-3285,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2288,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 18 19:12:09 UTC 2015,,,,,,,"0|i2j2bj:",9223372036854775807,,,,,bmahler,,,,,,Twitter Mesos Q3 Sprint 3,,,,,,,,,,,3.0,,0.24.0,,,,,,,,,"18/Aug/15 18:29;vinodkone;https://reviews.apache.org/r/37588","18/Aug/15 19:12;vinodkone;commit a8414b507dedfea50ec80a1591c73005a971a60e
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Tue Aug 18 10:28:53 2015 -0700

    Fixed master to drop scheduler HTTP calls during recovery.
    
    Review: https://reviews.apache.org/r/37588
",,,,,,,,,,,,,,,,,,,,,,,,,,
EventCall Test Framework is flaky,MESOS-3273,12856469,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,vinodkone,vinodkone,16/Aug/15 19:23,10/Feb/16 09:03,29/Oct/20 16:32,08/Feb/16 23:34,0.24.0,,,,,,,,0.28.0,,,,,,HTTP API,,,,,0,flaky-test,mesosphere,tech-debt,,,,,,"Observed this on ASF CI. h/t [~haosdent@gmail.com]

Looks like the HTTP scheduler never sent a SUBSCRIBE request to the master.

{code}
[ RUN      ] ExamplesTest.EventCallFramework
Using temporary directory '/tmp/ExamplesTest_EventCallFramework_k4vXkx'
I0813 19:55:15.643579 26085 exec.cpp:443] Ignoring exited event because the driver is aborted!
Shutting down
Sending SIGTERM to process tree at pid 26061
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 26062
Shutting down
Killing the following process trees:
[ 

]
Sending SIGTERM to process tree at pid 26063
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 26098
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 26099
Killing the following process trees:
[ 

]
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0813 19:55:17.161726 26100 process.cpp:1012] libprocess is initialized on 172.17.2.10:60249 for 16 cpus
I0813 19:55:17.161888 26100 logging.cpp:177] Logging to STDERR
I0813 19:55:17.163625 26100 scheduler.cpp:157] Version: 0.24.0
I0813 19:55:17.175302 26100 leveldb.cpp:176] Opened db in 3.167446ms
I0813 19:55:17.176393 26100 leveldb.cpp:183] Compacted db in 1.047996ms
I0813 19:55:17.176496 26100 leveldb.cpp:198] Created db iterator in 77155ns
I0813 19:55:17.176518 26100 leveldb.cpp:204] Seeked to beginning of db in 8429ns
I0813 19:55:17.176527 26100 leveldb.cpp:273] Iterated through 0 keys in the db in 4219ns
I0813 19:55:17.176708 26100 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0813 19:55:17.178951 26136 recover.cpp:449] Starting replica recovery
I0813 19:55:17.179934 26136 recover.cpp:475] Replica is in EMPTY status
I0813 19:55:17.181970 26126 master.cpp:378] Master 20150813-195517-167907756-60249-26100 (297daca2d01a) started on 172.17.2.10:60249
I0813 19:55:17.182317 26126 master.cpp:380] Flags at startup: --acls=""permissive: false
register_frameworks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  roles {
    type: SOME
    values: ""*""
  }
}
run_tasks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  users {
    type: SOME
    values: ""mesos""
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --credentials=""/tmp/ExamplesTest_EventCallFramework_k4vXkx/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.24.0/src/webui"" --work_dir=""/tmp/mesos-II8Gua"" --zk_session_timeout=""10secs""
I0813 19:55:17.183475 26126 master.cpp:427] Master allowing unauthenticated frameworks to register
I0813 19:55:17.183536 26126 master.cpp:432] Master allowing unauthenticated slaves to register
I0813 19:55:17.183615 26126 credentials.hpp:37] Loading credentials for authentication from '/tmp/ExamplesTest_EventCallFramework_k4vXkx/credentials'
W0813 19:55:17.183859 26126 credentials.hpp:52] Permissions on credentials file '/tmp/ExamplesTest_EventCallFramework_k4vXkx/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.
I0813 19:55:17.183969 26123 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0813 19:55:17.184306 26126 master.cpp:469] Using default 'crammd5' authenticator
I0813 19:55:17.184661 26126 authenticator.cpp:512] Initializing server SASL
I0813 19:55:17.185104 26138 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0813 19:55:17.185972 26100 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0813 19:55:17.186058 26135 recover.cpp:566] Updating replica status to STARTING
I0813 19:55:17.187001 26138 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 654586ns
I0813 19:55:17.187037 26138 replica.cpp:323] Persisted replica status to STARTING
I0813 19:55:17.187499 26134 recover.cpp:475] Replica is in STARTING status
I0813 19:55:17.187605 26126 auxprop.cpp:66] Initialized in-memory auxiliary property plugin
I0813 19:55:17.187710 26126 master.cpp:506] Authorization enabled
I0813 19:55:17.188657 26138 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0813 19:55:17.188853 26131 hierarchical.hpp:346] Initialized hierarchical allocator process
I0813 19:55:17.189252 26132 whitelist_watcher.cpp:79] No whitelist given
I0813 19:55:17.189321 26134 recover.cpp:195] Received a recover response from a replica in STARTING status
I0813 19:55:17.190001 26125 recover.cpp:566] Updating replica status to VOTING
I0813 19:55:17.190696 26124 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 357331ns
I0813 19:55:17.190775 26124 replica.cpp:323] Persisted replica status to VOTING
I0813 19:55:17.190970 26133 recover.cpp:580] Successfully joined the Paxos group
I0813 19:55:17.192183 26129 recover.cpp:464] Recover process terminated
I0813 19:55:17.192699 26123 slave.cpp:190] Slave started on 1)@172.17.2.10:60249
I0813 19:55:17.192741 26123 slave.cpp:191] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.24.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-II8Gua/0""
I0813 19:55:17.194514 26100 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0813 19:55:17.194658 26123 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.194854 26123 slave.cpp:384] Slave hostname: 297daca2d01a
I0813 19:55:17.194877 26123 slave.cpp:389] Slave checkpoint: true
I0813 19:55:17.196751 26132 master.cpp:1524] The newly elected leader is master@172.17.2.10:60249 with id 20150813-195517-167907756-60249-26100
I0813 19:55:17.196797 26132 master.cpp:1537] Elected as the leading master!
I0813 19:55:17.196815 26132 master.cpp:1307] Recovering from registrar
I0813 19:55:17.197032 26138 registrar.cpp:311] Recovering registrar
I0813 19:55:17.197845 26132 slave.cpp:190] Slave started on 2)@172.17.2.10:60249
I0813 19:55:17.198420 26125 log.cpp:661] Attempting to start the writer
I0813 19:55:17.197948 26132 slave.cpp:191] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.24.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-II8Gua/1""
I0813 19:55:17.199121 26132 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.199235 26138 state.cpp:54] Recovering state from '/tmp/mesos-II8Gua/0/meta'
I0813 19:55:17.199322 26132 slave.cpp:384] Slave hostname: 297daca2d01a
I0813 19:55:17.199345 26132 slave.cpp:389] Slave checkpoint: true
I0813 19:55:17.199676 26100 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0813 19:55:17.200085 26135 state.cpp:54] Recovering state from '/tmp/mesos-II8Gua/1/meta'
I0813 19:55:17.200317 26132 status_update_manager.cpp:202] Recovering status update manager
I0813 19:55:17.200371 26129 status_update_manager.cpp:202] Recovering status update manager
I0813 19:55:17.202003 26129 replica.cpp:477] Replica received implicit promise request with proposal 1
I0813 19:55:17.202585 26131 slave.cpp:190] Slave started on 3)@172.17.2.10:60249
I0813 19:55:17.202596 26129 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 523191ns
I0813 19:55:17.202756 26129 replica.cpp:345] Persisted promised to 1
I0813 19:55:17.202770 26132 containerizer.cpp:379] Recovering containerizer
I0813 19:55:17.203061 26135 containerizer.cpp:379] Recovering containerizer
I0813 19:55:17.202663 26131 slave.cpp:191] Flags at startup: --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.24.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-II8Gua/2""
I0813 19:55:17.203819 26131 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.203930 26131 slave.cpp:384] Slave hostname: 297daca2d01a
I0813 19:55:17.203948 26131 slave.cpp:389] Slave checkpoint: true
I0813 19:55:17.204674 26137 state.cpp:54] Recovering state from '/tmp/mesos-II8Gua/2/meta'
I0813 19:55:17.205178 26135 status_update_manager.cpp:202] Recovering status update manager
I0813 19:55:17.205323 26135 containerizer.cpp:379] Recovering containerizer
I0813 19:55:17.205521 26136 slave.cpp:4069] Finished recovery
I0813 19:55:17.206074 26136 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:17.206424 26128 slave.cpp:4069] Finished recovery
I0813 19:55:17.206722 26137 status_update_manager.cpp:176] Pausing sending status updates
I0813 19:55:17.206858 26136 slave.cpp:684] New master detected at master@172.17.2.10:60249
I0813 19:55:17.206902 26138 slave.cpp:4069] Finished recovery
I0813 19:55:17.206962 26128 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:17.208312 26134 scheduler.cpp:272] New master detected at master@172.17.2.10:60249
I0813 19:55:17.208364 26136 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0813 19:55:17.208608 26136 slave.cpp:720] Detecting new master
I0813 19:55:17.208839 26138 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:17.209216 26123 coordinator.cpp:231] Coordinator attemping to fill missing position
I0813 19:55:17.209247 26127 status_update_manager.cpp:176] Pausing sending status updates
I0813 19:55:17.209259 26128 slave.cpp:684] New master detected at master@172.17.2.10:60249
I0813 19:55:17.209322 26127 status_update_manager.cpp:176] Pausing sending status updates
I0813 19:55:17.209364 26128 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0813 19:55:17.209344 26138 slave.cpp:684] New master detected at master@172.17.2.10:60249
I0813 19:55:17.209455 26128 slave.cpp:720] Detecting new master
I0813 19:55:17.209492 26138 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0813 19:55:17.209573 26128 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:17.209601 26138 slave.cpp:720] Detecting new master
I0813 19:55:17.209730 26138 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:17.209883 26136 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:17.211266 26136 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0813 19:55:17.211771 26136 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 462128ns
I0813 19:55:17.211797 26136 replica.cpp:679] Persisted action at 0
I0813 19:55:17.212980 26130 replica.cpp:511] Replica received write request for position 0
I0813 19:55:17.213124 26130 leveldb.cpp:438] Reading position from leveldb took 67075ns
I0813 19:55:17.213580 26130 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 301649ns
I0813 19:55:17.213603 26130 replica.cpp:679] Persisted action at 0
I0813 19:55:17.214284 26123 replica.cpp:658] Replica received learned notice for position 0
I0813 19:55:17.214622 26123 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 284547ns
I0813 19:55:17.214648 26123 replica.cpp:679] Persisted action at 0
I0813 19:55:17.214675 26123 replica.cpp:664] Replica learned NOP action at position 0
I0813 19:55:17.215420 26136 log.cpp:677] Writer started with ending position 0
I0813 19:55:17.217463 26133 leveldb.cpp:438] Reading position from leveldb took 47943ns
I0813 19:55:17.220762 26125 registrar.cpp:344] Successfully fetched the registry (0B) in 23.649024ms
I0813 19:55:17.221081 26125 registrar.cpp:443] Applied 1 operations in 136902ns; attempting to update the 'registry'
I0813 19:55:17.223667 26133 log.cpp:685] Attempting to append 174 bytes to the log
I0813 19:55:17.223778 26125 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 1
I0813 19:55:17.224516 26127 replica.cpp:511] Replica received write request for position 1
I0813 19:55:17.225009 26127 leveldb.cpp:343] Persisting action (193 bytes) to leveldb took 466230ns
I0813 19:55:17.225042 26127 replica.cpp:679] Persisted action at 1
I0813 19:55:17.225653 26126 replica.cpp:658] Replica received learned notice for position 1
I0813 19:55:17.225953 26126 leveldb.cpp:343] Persisting action (195 bytes) to leveldb took 286966ns
I0813 19:55:17.225975 26126 replica.cpp:679] Persisted action at 1
I0813 19:55:17.226013 26126 replica.cpp:664] Replica learned APPEND action at position 1
I0813 19:55:17.227545 26137 registrar.cpp:488] Successfully updated the 'registry' in 6.328064ms
I0813 19:55:17.227722 26137 registrar.cpp:374] Successfully recovered registrar
I0813 19:55:17.227918 26124 log.cpp:704] Attempting to truncate the log to 1
I0813 19:55:17.228024 26133 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 2
I0813 19:55:17.228193 26131 master.cpp:1334] Recovered 0 slaves from the Registry (135B) ; allowing 10mins for slaves to re-register
I0813 19:55:17.228659 26127 replica.cpp:511] Replica received write request for position 2
I0813 19:55:17.228972 26127 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 297903ns
I0813 19:55:17.229004 26127 replica.cpp:679] Persisted action at 2
I0813 19:55:17.229565 26127 replica.cpp:658] Replica received learned notice for position 2
I0813 19:55:17.229837 26127 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 260326ns
I0813 19:55:17.229899 26127 leveldb.cpp:401] Deleting ~1 keys from leveldb took 48697ns
I0813 19:55:17.229923 26127 replica.cpp:679] Persisted action at 2
I0813 19:55:17.229956 26127 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0813 19:55:17.325634 26138 slave.cpp:1209] Will retry registration in 445.955946ms if necessary
I0813 19:55:17.326088 26124 master.cpp:3635] Registering slave at slave(2)@172.17.2.10:60249 (297daca2d01a) with id 20150813-195517-167907756-60249-26100-S0
I0813 19:55:17.327446 26124 registrar.cpp:443] Applied 1 operations in 231072ns; attempting to update the 'registry'
I0813 19:55:17.330252 26136 log.cpp:685] Attempting to append 344 bytes to the log
I0813 19:55:17.330407 26132 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 3
I0813 19:55:17.331418 26128 replica.cpp:511] Replica received write request for position 3
I0813 19:55:17.331753 26128 leveldb.cpp:343] Persisting action (363 bytes) to leveldb took 264140ns
I0813 19:55:17.331778 26128 replica.cpp:679] Persisted action at 3
I0813 19:55:17.332324 26133 replica.cpp:658] Replica received learned notice for position 3
I0813 19:55:17.332809 26133 leveldb.cpp:343] Persisting action (365 bytes) to leveldb took 313064ns
I0813 19:55:17.332834 26133 replica.cpp:679] Persisted action at 3
I0813 19:55:17.332865 26133 replica.cpp:664] Replica learned APPEND action at position 3
I0813 19:55:17.334211 26132 registrar.cpp:488] Successfully updated the 'registry' in 6.668032ms
I0813 19:55:17.334430 26127 log.cpp:704] Attempting to truncate the log to 3
I0813 19:55:17.334566 26132 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 4
I0813 19:55:17.335283 26129 replica.cpp:511] Replica received write request for position 4
I0813 19:55:17.335615 26127 slave.cpp:3058] Received ping from slave-observer(1)@172.17.2.10:60249
I0813 19:55:17.335816 26129 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 458268ns
I0813 19:55:17.335908 26137 master.cpp:3698] Registered slave 20150813-195517-167907756-60249-26100-S0 at slave(2)@172.17.2.10:60249 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.335983 26129 replica.cpp:679] Persisted action at 4
I0813 19:55:17.336019 26136 slave.cpp:859] Registered with master master@172.17.2.10:60249; given slave ID 20150813-195517-167907756-60249-26100-S0
I0813 19:55:17.336073 26136 fetcher.cpp:77] Clearing fetcher cache
I0813 19:55:17.336220 26127 status_update_manager.cpp:183] Resuming sending status updates
I0813 19:55:17.336328 26128 hierarchical.hpp:540] Added slave 20150813-195517-167907756-60249-26100-S0 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0813 19:55:17.336599 26138 replica.cpp:658] Replica received learned notice for position 4
I0813 19:55:17.336910 26128 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.336957 26128 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S0 in 580663ns
I0813 19:55:17.337016 26136 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/mesos-II8Gua/1/meta/slaves/20150813-195517-167907756-60249-26100-S0/slave.info'
I0813 19:55:17.337035 26138 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 403607ns
I0813 19:55:17.337138 26138 leveldb.cpp:401] Deleting ~2 keys from leveldb took 77040ns
I0813 19:55:17.337167 26138 replica.cpp:679] Persisted action at 4
I0813 19:55:17.337208 26138 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0813 19:55:17.337514 26136 slave.cpp:918] Forwarding total oversubscribed resources 
I0813 19:55:17.337745 26131 master.cpp:3997] Received update of slave 20150813-195517-167907756-60249-26100-S0 at slave(2)@172.17.2.10:60249 (297daca2d01a) with total oversubscribed resources 
I0813 19:55:17.338240 26131 hierarchical.hpp:600] Slave 20150813-195517-167907756-60249-26100-S0 (297daca2d01a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0813 19:55:17.338479 26131 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.338505 26131 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S0 in 216259ns
I0813 19:55:17.504086 26124 slave.cpp:1209] Will retry registration in 1.92618421secs if necessary
I0813 19:55:17.504408 26124 master.cpp:3635] Registering slave at slave(3)@172.17.2.10:60249 (297daca2d01a) with id 20150813-195517-167907756-60249-26100-S1
I0813 19:55:17.505203 26124 registrar.cpp:443] Applied 1 operations in 144314ns; attempting to update the 'registry'
I0813 19:55:17.507616 26124 log.cpp:685] Attempting to append 511 bytes to the log
I0813 19:55:17.507796 26132 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 5
I0813 19:55:17.508735 26128 replica.cpp:511] Replica received write request for position 5
I0813 19:55:17.509291 26128 leveldb.cpp:343] Persisting action (530 bytes) to leveldb took 527776ns
I0813 19:55:17.509328 26128 replica.cpp:679] Persisted action at 5
I0813 19:55:17.509945 26124 replica.cpp:658] Replica received learned notice for position 5
I0813 19:55:17.510393 26124 leveldb.cpp:343] Persisting action (532 bytes) to leveldb took 438543ns
I0813 19:55:17.510416 26124 replica.cpp:679] Persisted action at 5
I0813 19:55:17.510437 26124 replica.cpp:664] Replica learned APPEND action at position 5
I0813 19:55:17.511907 26125 registrar.cpp:488] Successfully updated the 'registry' in 6624us
I0813 19:55:17.512225 26138 log.cpp:704] Attempting to truncate the log to 5
I0813 19:55:17.512305 26136 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 6
I0813 19:55:17.513066 26133 slave.cpp:3058] Received ping from slave-observer(2)@172.17.2.10:60249
I0813 19:55:17.513242 26133 slave.cpp:859] Registered with master master@172.17.2.10:60249; given slave ID 20150813-195517-167907756-60249-26100-S1
I0813 19:55:17.513221 26126 master.cpp:3698] Registered slave 20150813-195517-167907756-60249-26100-S1 at slave(3)@172.17.2.10:60249 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.513089 26129 replica.cpp:511] Replica received write request for position 6
I0813 19:55:17.513393 26133 fetcher.cpp:77] Clearing fetcher cache
I0813 19:55:17.513380 26138 hierarchical.hpp:540] Added slave 20150813-195517-167907756-60249-26100-S1 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0813 19:55:17.513805 26132 status_update_manager.cpp:183] Resuming sending status updates
I0813 19:55:17.513949 26129 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 340511ns
I0813 19:55:17.514046 26138 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.514050 26129 replica.cpp:679] Persisted action at 6
I0813 19:55:17.514195 26133 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/mesos-II8Gua/2/meta/slaves/20150813-195517-167907756-60249-26100-S1/slave.info'
I0813 19:55:17.514140 26138 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S1 in 417609ns
I0813 19:55:17.514704 26133 slave.cpp:918] Forwarding total oversubscribed resources 
I0813 19:55:17.514708 26138 replica.cpp:658] Replica received learned notice for position 6
I0813 19:55:17.514880 26133 master.cpp:3997] Received update of slave 20150813-195517-167907756-60249-26100-S1 at slave(3)@172.17.2.10:60249 (297daca2d01a) with total oversubscribed resources 
I0813 19:55:17.515244 26127 hierarchical.hpp:600] Slave 20150813-195517-167907756-60249-26100-S1 (297daca2d01a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0813 19:55:17.515454 26138 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 640882ns
I0813 19:55:17.515522 26138 leveldb.cpp:401] Deleting ~2 keys from leveldb took 56550ns
I0813 19:55:17.515547 26138 replica.cpp:679] Persisted action at 6
I0813 19:55:17.515581 26138 replica.cpp:664] Replica learned TRUNCATE action at position 6
I0813 19:55:17.515802 26127 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.515866 26127 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S1 in 591007ns
I0813 19:55:17.984196 26135 slave.cpp:1209] Will retry registration in 1.542495291secs if necessary
I0813 19:55:17.984391 26138 master.cpp:3635] Registering slave at slave(1)@172.17.2.10:60249 (297daca2d01a) with id 20150813-195517-167907756-60249-26100-S2
I0813 19:55:17.985170 26133 registrar.cpp:443] Applied 1 operations in 202126ns; attempting to update the 'registry'
I0813 19:55:17.987498 26133 log.cpp:685] Attempting to append 678 bytes to the log
I0813 19:55:17.987656 26123 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 7
I0813 19:55:17.988704 26138 replica.cpp:511] Replica received write request for position 7
I0813 19:55:17.989223 26138 leveldb.cpp:343] Persisting action (697 bytes) to leveldb took 490422ns
I0813 19:55:17.989248 26138 replica.cpp:679] Persisted action at 7
I0813 19:55:17.989972 26126 replica.cpp:658] Replica received learned notice for position 7
I0813 19:55:17.990401 26126 leveldb.cpp:343] Persisting action (699 bytes) to leveldb took 404333ns
I0813 19:55:17.990420 26126 replica.cpp:679] Persisted action at 7
I0813 19:55:17.990440 26126 replica.cpp:664] Replica learned APPEND action at position 7
I0813 19:55:17.994066 26123 registrar.cpp:488] Successfully updated the 'registry' in 8.788224ms
I0813 19:55:17.994436 26134 log.cpp:704] Attempting to truncate the log to 7
I0813 19:55:17.994575 26123 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 8
I0813 19:55:17.995070 26134 slave.cpp:3058] Received ping from slave-observer(3)@172.17.2.10:60249
I0813 19:55:17.995291 26134 slave.cpp:859] Registered with master master@172.17.2.10:60249; given slave ID 20150813-195517-167907756-60249-26100-S2
I0813 19:55:17.995319 26134 fetcher.cpp:77] Clearing fetcher cache
I0813 19:55:17.995246 26129 master.cpp:3698] Registered slave 20150813-195517-167907756-60249-26100-S2 at slave(1)@172.17.2.10:60249 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0813 19:55:17.995565 26123 status_update_manager.cpp:183] Resuming sending status updates
I0813 19:55:17.995579 26129 replica.cpp:511] Replica received write request for position 8
I0813 19:55:17.996016 26134 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/mesos-II8Gua/0/meta/slaves/20150813-195517-167907756-60249-26100-S2/slave.info'
I0813 19:55:17.996039 26129 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 440511ns
I0813 19:55:17.996067 26129 replica.cpp:679] Persisted action at 8
I0813 19:55:17.996294 26128 hierarchical.hpp:540] Added slave 20150813-195517-167907756-60249-26100-S2 (297daca2d01a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0813 19:55:17.996556 26134 slave.cpp:918] Forwarding total oversubscribed resources 
I0813 19:55:17.996623 26133 replica.cpp:658] Replica received learned notice for position 8
I0813 19:55:17.997095 26134 master.cpp:3997] Received update of slave 20150813-195517-167907756-60249-26100-S2 at slave(1)@172.17.2.10:60249 (297daca2d01a) with total oversubscribed resources 
I0813 19:55:17.997263 26133 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 442619ns
I0813 19:55:17.997385 26133 leveldb.cpp:401] Deleting ~2 keys from leveldb took 95741ns
I0813 19:55:17.997413 26133 replica.cpp:679] Persisted action at 8
I0813 19:55:17.997465 26133 replica.cpp:664] Replica learned TRUNCATE action at position 8
I0813 19:55:17.997756 26128 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.997925 26128 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S2 in 1.14489ms
I0813 19:55:17.998159 26128 hierarchical.hpp:600] Slave 20150813-195517-167907756-60249-26100-S2 (297daca2d01a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0813 19:55:17.998445 26128 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:17.998471 26128 hierarchical.hpp:926] Performed allocation for slave 20150813-195517-167907756-60249-26100-S2 in 218856ns
I0813 19:55:18.190146 26133 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:18.190217 26133 hierarchical.hpp:908] Performed allocation for 3 slaves in 637042ns
I0813 19:55:19.191346 26131 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:19.191915 26131 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.215355ms
I0813 19:55:20.193631 26135 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:20.193709 26135 hierarchical.hpp:908] Performed allocation for 3 slaves in 834491ns
I0813 19:55:21.194805 26134 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:21.194870 26134 hierarchical.hpp:908] Performed allocation for 3 slaves in 536547ns
I0813 19:55:22.196143 26137 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:22.196216 26137 hierarchical.hpp:908] Performed allocation for 3 slaves in 755140ns
I0813 19:55:23.197412 26132 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:23.197979 26132 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.223984ms
I0813 19:55:24.199429 26132 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:24.199735 26132 hierarchical.hpp:908] Performed allocation for 3 slaves in 904654ns
I0813 19:55:25.200978 26127 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:25.201206 26127 hierarchical.hpp:908] Performed allocation for 3 slaves in 939979ns
I0813 19:55:26.203023 26132 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:26.203101 26132 hierarchical.hpp:908] Performed allocation for 3 slaves in 721178ns
I0813 19:55:27.204815 26126 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:27.204888 26126 hierarchical.hpp:908] Performed allocation for 3 slaves in 767983ns
I0813 19:55:28.206374 26126 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:28.206444 26126 hierarchical.hpp:908] Performed allocation for 3 slaves in 745214ns
I0813 19:55:29.207515 26124 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:29.207579 26124 hierarchical.hpp:908] Performed allocation for 3 slaves in 551217ns
I0813 19:55:30.208966 26136 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:30.209053 26136 hierarchical.hpp:908] Performed allocation for 3 slaves in 649887ns
I0813 19:55:31.210078 26123 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:31.210144 26123 hierarchical.hpp:908] Performed allocation for 3 slaves in 558919ns
I0813 19:55:32.211027 26130 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:32.211045 26129 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:32.211084 26132 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0813 19:55:32.211386 26129 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:32.211688 26132 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:32.211853 26133 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:32.212035 26133 hierarchical.hpp:908] Performed allocation for 3 slaves in 898985ns
I0813 19:55:32.212169 26133 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0813 19:55:32.336745 26135 slave.cpp:3058] Received ping from slave-observer(1)@172.17.2.10:60249
I0813 19:55:32.514333 26129 slave.cpp:3058] Received ping from slave-observer(2)@172.17.2.10:60249
I0813 19:55:32.996134 26128 slave.cpp:3058] Received ping from slave-observer(3)@172.17.2.10:60249
I0813 19:55:33.213248 26128 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:33.213326 26128 hierarchical.hpp:908] Performed allocation for 3 slaves in 827511ns
I0813 19:55:34.214326 26125 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:34.214391 26125 hierarchical.hpp:908] Performed allocation for 3 slaves in 546422ns
I0813 19:55:35.215909 26123 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:35.215973 26123 hierarchical.hpp:908] Performed allocation for 3 slaves in 627190ns
I0813 19:55:36.217156 26134 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:36.217339 26134 hierarchical.hpp:908] Performed allocation for 3 slaves in 906249ns
I0813 19:55:37.218739 26132 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:37.219169 26132 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.102465ms
I0813 19:55:38.220641 26133 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:38.220711 26133 hierarchical.hpp:908] Performed allocation for 3 slaves in 643146ns
I0813 19:55:39.221976 26133 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:39.222118 26133 hierarchical.hpp:908] Performed allocation for 3 slaves in 845334ns
I0813 19:55:40.223338 26129 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:40.223546 26129 hierarchical.hpp:908] Performed allocation for 3 slaves in 849995ns
I0813 19:55:41.225558 26138 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:41.225752 26138 hierarchical.hpp:908] Performed allocation for 3 slaves in 958480ns
I0813 19:55:42.227176 26131 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:42.227378 26131 hierarchical.hpp:908] Performed allocation for 3 slaves in 927048ns
I0813 19:55:43.228813 26137 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:43.229441 26137 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.310118ms
I0813 19:55:44.230828 26135 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:44.231142 26135 hierarchical.hpp:908] Performed allocation for 3 slaves in 896369ns
I0813 19:55:45.232656 26135 hierarchical.hpp:1008] No resources available to allocate!
I0813 19:55:45.232903 26135 hierarchical.hpp:908] Performed allocation for 3 slaves in 1.357693ms
I0813 19:55:46.234973 26137 hierarchical.hpp:1008
{code}","https://builds.apache.org/job/Mesos/705/COMPILER=clang,CONFIGURATION=--verbose,OS=ubuntu:14.04,label_exp=docker%7C%7CHadoop/consoleFull",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-4040,MESOS-4614,,,,,,,"01/Dec/15 14:12;bbannier;asan.log;https://issues.apache.org/jira/secure/attachment/12775036/asan.log",,,,,1.0,,,,,,,,,,,,,,,,,,,,2015-08-17 03:07:15.015,,,false,MESOS-3302,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 08 23:34:47 UTC 2016,,,,,,,"0|hzzzqe:zzr",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 27,Mesosphere Sprint 28,,,,,,,,,,5.0,,,,,,,,,,,"16/Aug/15 19:24;vinodkone;cc [~anandmazumdar] [~bmahler]","17/Aug/15 03:07;haosdent@gmail.com;In success test cases log, could see the log like this:
{code}
I0817 10:11:16.072263 19122 process.cpp:3043] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0817 10:11:16.072767 19122 master.cpp:1782] Received subscription request for HTTP framework 'Event Call Scheduler using libprocess (C++)'
I0817 10:11:16.072880 19122 master.cpp:1563] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0817 10:11:16.073186 19122 master.cpp:1874] Subscribing framework 'Event Call Scheduler using libprocess (C++)' with checkpointing disabled and capabilities [  ]
I0817 10:11:16.073827 19124 hierarchical.hpp:391] Added framework 20150817-101116-162297291-55446-19097-0000
I0817 10:11:16.073846 19118 master.hpp:1308] Sending heartbeat to 20150817-101116-162297291-55446-19097-0000
I0817 10:11:16.073945 19124 hierarchical.hpp:1008] No resources available to allocate!
I0817 10:11:16.073982 19124 hierarchical.hpp:908] Performed allocation for 0 slaves in 122200ns

Received a SUBSCRIBED event
Subscribed with ID '20150817-101116-162297291-55446-19097-0000
{code}

But in this error log, could not find log like this.","17/Aug/15 19:44;vinodkone;While trying to repro this, observed another issue where the test hangs during termination.

{code}
I0817 19:34:33.166100 60834 master.cpp:3998] Received update of slave 20150817-193432-1828659978-51071-60794-S2 at slave(1)@10.35.255.108:51071 (smfd-atr-11-sr1.devel.twitter.com) with total oversubscribed resources 
I0817 19:34:33.166316 60834 hierarchical.hpp:600] Slave 20150817-193432-1828659978-51071-60794-S2 (smfd-atr-11-sr1.devel.twitter.com) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):454767; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):10240; disk(*):454767; ports(*):[31000-32000])

Received an UPDATE event
Task 4 is in state TASK_FINISHED
I0817 19:34:33.167793 60816 master.cpp:860] Master terminating
I0817 19:34:33.168092 60836 hierarchical.hpp:571] Removed slave 20150817-193432-1828659978-51071-60794-S2
I0817 19:34:33.168654 60816 master.cpp:5673] Removing executor 'default' with resources  of framework 20150817-193432-1828659978-51071-60794-0000 on slave 20150817-193432-1828659978-51071-60794-S1 at slave(2)@10.35.255.108:51071 (smfd-atr-11-sr1.devel.twitter.com)
I0817 19:34:33.168725 60819 hierarchical.hpp:571] Removed slave 20150817-193432-1828659978-51071-60794-S1
I0817 19:34:33.169075 60816 master.cpp:5644] Removing task 4 with resources cpus(*):1; mem(*):128 of framework 20150817-193432-1828659978-51071-60794-0000 on slave 20150817-193432-1828659978-51071-60794-S0 at slave(3)@10.35.255.108:51071 (smfd-atr-11-sr1.devel.twitter.com)
I0817 19:34:33.169153 60818 hierarchical.hpp:571] Removed slave 20150817-193432-1828659978-51071-60794-S0
I0817 19:34:33.169255 60816 master.cpp:5673] Removing executor 'default' with resources  of framework 20150817-193432-1828659978-51071-60794-0000 on slave 20150817-193432-1828659978-51071-60794-S0 at slave(3)@10.35.255.108:51071 (smfd-atr-11-sr1.devel.twitter.com)
I0817 19:34:33.170186 60818 hierarchical.hpp:428] Removed framework 20150817-193432-1828659978-51071-60794-0000
I0817 19:34:33.170919 60827 slave.cpp:3143] master@10.35.255.108:51071 exited
I0817 19:34:33.170903 60817 slave.cpp:3143] master@10.35.255.108:51071 exited
W0817 19:34:33.170959 60827 slave.cpp:3146] Master disconnected! Waiting for a new master to be elected
W0817 19:34:33.170976 60817 slave.cpp:3146] Master disconnected! Waiting for a new master to be elected
I0817 19:34:33.171083 60821 slave.cpp:3143] master@10.35.255.108:51071 exited
W0817 19:34:33.171169 60821 slave.cpp:3146] Master disconnected! Waiting for a new master to be elected
I0817 19:34:33.172170 60817 slave.cpp:564] Slave terminating
I0817 19:34:33.174253 60794 slave.cpp:564] Slave terminating
I0817 19:34:33.174424 60794 slave.cpp:1959] Asked to shut down framework 20150817-193432-1828659978-51071-60794-0000 by @0.0.0.0:0
I0817 19:34:33.174473 60794 slave.cpp:1984] Shutting down framework 20150817-193432-1828659978-51071-60794-0000
I0817 19:34:33.174665 60794 slave.cpp:3710] Shutting down executor 'default' of framework 20150817-193432-1828659978-51071-60794-0000
I0817 19:34:33.175500 60926 exec.cpp:380] Executor asked to shutdown
I0817 19:34:33.176652 60794 slave.cpp:564] Slave terminating
I0817 19:34:33.176762 60794 slave.cpp:1959] Asked to shut down framework 20150817-193432-1828659978-51071-60794-0000 by @0.0.0.0:0
I0817 19:34:33.176909 60794 slave.cpp:1984] Shutting down framework 20150817-193432-1828659978-51071-60794-0000
I0817 19:34:33.176954 60794 slave.cpp:3710] Shutting down executor 'default' of framework 20150817-193432-1828659978-51071-60794-0000
I0817 19:34:33.177781 60879 exec.cpp:380] Executor asked to shutdown
I0817 19:34:33.178567 60822 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 13.870729ms
I0817 19:34:33.178649 60822 replica.cpp:679] Persisted action at 8
I0817 19:34:33.179919 60815 replica.cpp:658] Replica received learned notice for position 8
I0817 19:34:33.195266 60815 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 15.299248ms
I0817 19:34:33.195405 60815 leveldb.cpp:401] Deleting ~2 keys from leveldb took 29964ns
I0817 19:34:33.195428 60815 replica.cpp:679] Persisted action at 8
I0817 19:34:33.195456 60815 replica.cpp:664] Replica learned TRUNCATE action at position 8
{code} 

gdb stack trace points to what looks like a deadlock
{code}
#0  0x00007fd5df472019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007fd5e332293c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007fd5e248f3cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007fd5e3191d35 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007fd5e3181e95 in wait () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007fd5e3185fc3 in wait () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007fd5e313cea2 in await () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007fd5e279b8d2 in await () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007fd5e28786d5 in ~MesosProcess () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007fd5e28787de in ~MesosProcess () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#10 0x00007fd5e2876dd7 in ~Mesos () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#11 0x000000000043662a in ~EventCallScheduler ()
#12 0x00000000004366b6 in ~EventCallScheduler ()
#13 0x000000000042e466 in main ()
{code}","17/Aug/15 21:07;vinodkone;This a trace from a different test iteration that exhibits the original behavior described in the ticket description (i.e., scheduler doesn't send a SUBSCRIBE).

{code}
[----------] Global test environment set-up.
[----------] 1 test from ExamplesTest
[ RUN      ] ExamplesTest.EventCallFramework
Using temporary directory '/tmp/ExamplesTest_EventCallFramework_gBzJlY'
I0817 20:02:48.125079 47640 scheduler.cpp:157] Version: 0.24.0
I0817 20:02:48.293411 47640 leveldb.cpp:176] Opened db in 161.409796ms
I0817 20:02:48.309515 47640 leveldb.cpp:183] Compacted db in 16.0622ms
I0817 20:02:48.309605 47640 leveldb.cpp:198] Created db iterator in 42741ns
I0817 20:02:48.309634 47640 leveldb.cpp:204] Seeked to beginning of db in 2868ns
I0817 20:02:48.309650 47640 leveldb.cpp:273] Iterated through 0 keys in the db in 260ns
I0817 20:02:48.309815 47640 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0817 20:02:48.311734 47640 local.cpp:241] Using 'local' authorizer
I0817 20:02:48.311909 47663 recover.cpp:449] Starting replica recovery
I0817 20:02:48.312535 47663 recover.cpp:475] Replica is in EMPTY status
I0817 20:02:48.313920 47679 master.cpp:378] Master 20150817-200248-1828659978-42709-47640 (smfd-atr-11-sr1.devel.twitter.com) started on 10.35.255.108:42709
I0817 20:02:48.313949 47679 master.cpp:380] Flags at startup: --acls=""permissive: false
register_frameworks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  roles {
    type: SOME
    values: ""*""
  }
}
run_tasks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  users {
    type: SOME
    values: ""root""
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ExamplesTest_EventCallFramework_gBzJlY/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/home/vinod/mesos/src/webui"" --work_dir=""/tmp/mesos-j47645"" --zk_session_timeout=""10secs""
I0817 20:02:48.314568 47679 master.cpp:427] Master allowing unauthenticated frameworks to register
I0817 20:02:48.314579 47679 master.cpp:432] Master allowing unauthenticated slaves to register
I0817 20:02:48.314589 47679 credentials.hpp:37] Loading credentials for authentication from '/tmp/ExamplesTest_EventCallFramework_gBzJlY/credentials'
W0817 20:02:48.314798 47679 credentials.hpp:52] Permissions on credentials file '/tmp/ExamplesTest_EventCallFramework_gBzJlY/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.
I0817 20:02:48.314983 47679 master.cpp:469] Using default 'crammd5' authenticator
I0817 20:02:48.315184 47679 authenticator.cpp:512] Initializing server SASL
I0817 20:02:48.315634 47675 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0817 20:02:48.316375 47666 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0817 20:02:48.317276 47670 recover.cpp:566] Updating replica status to STARTING
I0817 20:02:48.318356 47640 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0817 20:02:48.318434 47679 master.cpp:506] Authorization enabled
I0817 20:02:48.323303 47676 slave.cpp:190] Slave started on 1)@10.35.255.108:42709
I0817 20:02:48.323328 47676 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/home/vinod/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-j47645/0""
I0817 20:02:48.324440 47640 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0817 20:02:48.324724 47676 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):454767; ports(*):[31000-32000]
I0817 20:02:48.325157 47660 master.cpp:1525] The newly elected leader is master@10.35.255.108:42709 with id 20150817-200248-1828659978-42709-47640
I0817 20:02:48.325199 47660 master.cpp:1538] Elected as the leading master!
I0817 20:02:48.325233 47660 master.cpp:1308] Recovering from registrar
I0817 20:02:48.325237 47676 slave.cpp:384] Slave hostname: smfd-atr-11-sr1.devel.twitter.com
I0817 20:02:48.325253 47676 slave.cpp:389] Slave checkpoint: true
I0817 20:02:48.325675 47672 registrar.cpp:311] Recovering registrar
I0817 20:02:48.327256 47666 state.cpp:54] Recovering state from '/tmp/mesos-j47645/0/meta'
I0817 20:02:48.327527 47678 slave.cpp:190] Slave started on 2)@10.35.255.108:42709
I0817 20:02:48.327947 47675 status_update_manager.cpp:202] Recovering status update manager
I0817 20:02:48.327563 47678 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/home/vinod/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-j47645/1""
I0817 20:02:48.328207 47665 containerizer.cpp:379] Recovering containerizer
I0817 20:02:48.329272 47640 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0817 20:02:48.329290 47678 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):454767; ports(*):[31000-32000]
I0817 20:02:48.330018 47678 slave.cpp:384] Slave hostname: smfd-atr-11-sr1.devel.twitter.com
I0817 20:02:48.330111 47678 slave.cpp:389] Slave checkpoint: true
I0817 20:02:48.330608 47679 slave.cpp:4069] Finished recovery
I0817 20:02:48.330890 47663 state.cpp:54] Recovering state from '/tmp/mesos-j47645/1/meta'
I0817 20:02:48.331096 47679 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:02:48.331662 47681 status_update_manager.cpp:202] Recovering status update manager
I0817 20:02:48.332301 47661 status_update_manager.cpp:176] Pausing sending status updates
I0817 20:02:48.332352 47679 slave.cpp:684] New master detected at master@10.35.255.108:42709
I0817 20:02:48.332490 47661 containerizer.cpp:379] Recovering containerizer
I0817 20:02:48.332494 47679 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0817 20:02:48.332664 47679 slave.cpp:720] Detecting new master
I0817 20:02:48.332753 47679 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:02:48.333678 47667 slave.cpp:4069] Finished recovery
I0817 20:02:48.333927 47675 slave.cpp:190] Slave started on 3)@10.35.255.108:42709
I0817 20:02:48.334120 47667 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:02:48.333952 47675 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/home/vinod/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-j47645/2""
I0817 20:02:48.334230 47674 status_update_manager.cpp:176] Pausing sending status updates
I0817 20:02:48.334245 47667 slave.cpp:684] New master detected at master@10.35.255.108:42709
I0817 20:02:48.334306 47667 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0817 20:02:48.334341 47667 slave.cpp:720] Detecting new master
I0817 20:02:48.334477 47667 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:02:48.335167 47675 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):454767; ports(*):[31000-32000]
I0817 20:02:48.335438 47675 slave.cpp:384] Slave hostname: smfd-atr-11-sr1.devel.twitter.com
I0817 20:02:48.335465 47675 slave.cpp:389] Slave checkpoint: true
I0817 20:02:48.336022 47683 state.cpp:54] Recovering state from '/tmp/mesos-j47645/2/meta'
I0817 20:02:48.336626 47664 status_update_manager.cpp:202] Recovering status update manager
I0817 20:02:48.336841 47675 containerizer.cpp:379] Recovering containerizer
I0817 20:02:48.339213 47683 slave.cpp:4069] Finished recovery
I0817 20:02:48.339499 47683 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:02:48.339637 47683 slave.cpp:684] New master detected at master@10.35.255.108:42709
I0817 20:02:48.339679 47668 status_update_manager.cpp:176] Pausing sending status updates
I0817 20:02:48.339895 47683 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0817 20:02:48.339956 47683 slave.cpp:720] Detecting new master
I0817 20:02:48.340039 47683 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:02:48.360288 47669 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 42.533881ms
I0817 20:02:48.360333 47669 replica.cpp:323] Persisted replica status to STARTING
I0817 20:02:48.360765 47681 recover.cpp:475] Replica is in STARTING status
I0817 20:02:48.361826 47662 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0817 20:02:48.362196 47663 recover.cpp:195] Received a recover response from a replica in STARTING status
I0817 20:02:48.363127 47669 recover.cpp:566] Updating replica status to VOTING
I0817 20:02:48.383821 47671 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 20.318768ms
I0817 20:02:48.383874 47671 replica.cpp:323] Persisted replica status to VOTING
I0817 20:02:48.384012 47678 recover.cpp:580] Successfully joined the Paxos group
I0817 20:02:48.384207 47678 recover.cpp:464] Recover process terminated
I0817 20:02:48.385124 47660 log.cpp:661] Attempting to start the writer
I0817 20:02:48.388417 47680 replica.cpp:477] Replica received implicit promise request with proposal 1
I0817 20:02:48.400475 47680 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 11.851747ms
I0817 20:02:48.400529 47680 replica.cpp:345] Persisted promised to 1
I0817 20:02:48.402153 47679 coordinator.cpp:231] Coordinator attemping to fill missing position
I0817 20:02:48.404059 47682 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0817 20:02:48.417143 47682 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 12.966672ms
I0817 20:02:48.417201 47682 replica.cpp:679] Persisted action at 0
I0817 20:02:48.419968 47660 replica.cpp:511] Replica received write request for position 0
I0817 20:02:48.420074 47660 leveldb.cpp:438] Reading position from leveldb took 45361ns
I0817 20:02:48.433841 47660 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 13.683629ms
I0817 20:02:48.433897 47660 replica.cpp:679] Persisted action at 0
I0817 20:02:48.434636 47673 replica.cpp:658] Replica received learned notice for position 0
I0817 20:02:48.450491 47673 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 15.764663ms
I0817 20:02:48.450546 47673 replica.cpp:679] Persisted action at 0
I0817 20:02:48.450582 47673 replica.cpp:664] Replica learned NOP action at position 0
I0817 20:02:48.451218 47671 log.cpp:677] Writer started with ending position 0
I0817 20:02:48.453227 47673 leveldb.cpp:438] Reading position from leveldb took 24061ns
I0817 20:02:48.456646 47672 registrar.cpp:344] Successfully fetched the registry (0B) in 130.881024ms
I0817 20:02:48.456894 47672 registrar.cpp:443] Applied 1 operations in 63990ns; attempting to update the 'registry'
I0817 20:02:48.459537 47671 log.cpp:685] Attempting to append 222 bytes to the log
I0817 20:02:48.459833 47670 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 1
I0817 20:02:48.460832 47668 replica.cpp:511] Replica received write request for position 1
I0817 20:02:48.467144 47668 leveldb.cpp:343] Persisting action (241 bytes) to leveldb took 6.256469ms
I0817 20:02:48.467197 47668 replica.cpp:679] Persisted action at 1
I0817 20:02:48.467941 47675 replica.cpp:658] Replica received learned notice for position 1
I0817 20:02:48.483801 47675 leveldb.cpp:343] Persisting action (243 bytes) to leveldb took 15.803693ms
I0817 20:02:48.483855 47675 replica.cpp:679] Persisted action at 1
I0817 20:02:48.483889 47675 replica.cpp:664] Replica learned APPEND action at position 1
I0817 20:02:48.485134 47673 registrar.cpp:488] Successfully updated the 'registry' in 28.034816ms
I0817 20:02:48.485250 47673 registrar.cpp:374] Successfully recovered registrar
I0817 20:02:48.485545 47670 log.cpp:704] Attempting to truncate the log to 1
I0817 20:02:48.485623 47672 master.cpp:1335] Recovered 0 slaves from the Registry (183B) ; allowing 10mins for slaves to re-register
I0817 20:02:48.485821 47668 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 2
I0817 20:02:48.487413 47665 replica.cpp:511] Replica received write request for position 2
I0817 20:02:48.500469 47665 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 13.01462ms
I0817 20:02:48.500524 47665 replica.cpp:679] Persisted action at 2
I0817 20:02:48.501330 47671 replica.cpp:658] Replica received learned notice for position 2
I0817 20:02:48.517112 47671 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 15.740951ms
I0817 20:02:48.517194 47671 leveldb.cpp:401] Deleting ~1 keys from leveldb took 30019ns
I0817 20:02:48.517230 47671 replica.cpp:679] Persisted action at 2
I0817 20:02:48.517256 47671 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0817 20:02:49.173807 47678 master.cpp:3636] Registering slave at slave(1)@10.35.255.108:42709 (smfd-atr-11-sr1.devel.twitter.com) with id 20150817-200248-1828659978-42709-47640-S0
I0817 20:02:49.174512 47680 registrar.cpp:443] Applied 1 operations in 125454ns; attempting to update the 'registry'
I0817 20:02:49.176681 47661 log.cpp:685] Attempting to append 414 bytes to the log
I0817 20:02:49.176913 47683 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 3
I0817 20:02:49.177523 47671 replica.cpp:511] Replica received write request for position 3
I0817 20:02:49.191833 47671 leveldb.cpp:343] Persisting action (433 bytes) to leveldb took 14.254809ms
I0817 20:02:49.191963 47671 replica.cpp:679] Persisted action at 3
I0817 20:02:49.192550 47662 replica.cpp:658] Replica received learned notice for position 3
I0817 20:02:49.246731 47667 master.cpp:3636] Registering slave at slave(2)@10.35.255.108:42709 (smfd-atr-11-sr1.devel.twitter.com) with id 20150817-200248-1828659978-42709-47640-S1
I0817 20:02:49.252267 47662 leveldb.cpp:343] Persisting action (435 bytes) to leveldb took 59.678071ms
I0817 20:02:49.252331 47662 replica.cpp:679] Persisted action at 3
I0817 20:02:49.252388 47662 replica.cpp:664] Replica learned APPEND action at position 3
I0817 20:02:49.253367 47673 registrar.cpp:488] Successfully updated the 'registry' in 78.771968ms
I0817 20:02:49.253464 47674 log.cpp:704] Attempting to truncate the log to 3
I0817 20:02:49.253656 47673 registrar.cpp:443] Applied 1 operations in 48078ns; attempting to update the 'registry'
I0817 20:02:49.253751 47663 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 4
I0817 20:02:49.254513 47681 replica.cpp:511] Replica received write request for position 4
I0817 20:02:49.254786 47672 master.cpp:3699] Registered slave 20150817-200248-1828659978-42709-47640-S0 at slave(1)@10.35.255.108:42709 (smfd-atr-11-sr1.devel.twitter.com) with cpus(*):2; mem(*):10240; disk(*):454767; ports(*):[31000-32000]
I0817 20:02:49.254858 47666 slave.cpp:859] Registered with master master@10.35.255.108:42709; given slave ID 20150817-200248-1828659978-42709-47640-S0
I0817 20:02:49.254964 47675 status_update_manager.cpp:183] Resuming sending status updates
I0817 20:02:49.254967 47676 hierarchical.hpp:540] Added slave 20150817-200248-1828659978-42709-47640-S0 (smfd-atr-11-sr1.devel.twitter.com) with cpus(*):2; mem(*):10240; disk(*):454767; ports(*):[31000-32000] (allocated: )
I0817 20:02:49.255440 47666 slave.cpp:918] Forwarding total oversubscribed resources 
I0817 20:02:49.255653 47671 master.cpp:3998] Received update of slave 20150817-200248-1828659978-42709-47640-S0 at slave(1)@10.35.255.108:42709 (smfd-atr-11-sr1.devel.twitter.com) with total oversubscribed resources 
I0817 20:02:49.256055 47679 hierarchical.hpp:600] Slave 20150817-200248-1828659978-42709-47640-S0 (smfd-atr-11-sr1.devel.twitter.com) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):454767; ports(*):[31000-32000], allocated: )
I0817 20:02:49.275104 47681 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 20.550499ms
I0817 20:02:49.275147 47681 replica.cpp:679] Persisted action at 4
I0817 20:02:49.275604 47678 replica.cpp:658] Replica received learned notice for position 4
I0817 20:02:49.283427 47678 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 7.716267ms
I0817 20:02:49.283520 47678 leveldb.cpp:401] Deleting ~2 keys from leveldb took 42739ns
I0817 20:02:49.283556 47678 replica.cpp:679] Persisted action at 4
I0817 20:02:49.283581 47678 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0817 20:02:49.284128 47667 log.cpp:685] Attempting to append 603 bytes to the log
I0817 20:02:49.284245 47672 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 5
I0817 20:02:49.285405 47668 replica.cpp:511] Replica received write request for position 5
I0817 20:02:49.291725 47668 leveldb.cpp:343] Persisting action (622 bytes) to leveldb took 6.209225ms
I0817 20:02:49.291779 47668 replica.cpp:679] Persisted action at 5
I0817 20:02:49.292834 47674 replica.cpp:658] Replica received learned notice for position 5
I0817 20:02:49.300074 47674 leveldb.cpp:343] Persisting action (624 bytes) to leveldb took 6.970544ms
I0817 20:02:49.300128 47674 replica.cpp:679] Persisted action at 5
I0817 20:02:49.300163 47674 replica.cpp:664] Replica learned APPEND action at position 5
I0817 20:02:49.301210 47679 registrar.cpp:488] Successfully updated the 'registry' in 47.348992ms
I0817 20:02:49.301401 47680 log.cpp:704] Attempting to truncate the log to 5
I0817 20:02:49.301522 47662 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 6
I0817 20:02:49.302501 47679 hierarchical.hpp:540] Added slave 20150817-200248-1828659978-42709-47640-S1 (smfd-atr-11-sr1.devel.twitter.com) with cpus(*):2; mem(*):10240; disk(*):454767; ports(*):[31000-32000] (allocated: )
I0817 20:02:49.303279 47680 master.cpp:3699] Registered slave 20150817-200248-1828659978-42709-47640-S1 at slave(2)@10.35.255.108:42709 (smfd-atr-11-sr1.devel.twitter.com) with cpus(*):2; mem(*):10240; disk(*):454767; ports(*):[31000-32000]
I0817 20:02:49.303372 47669 slave.cpp:859] Registered with master master@10.35.255.108:42709; given slave ID 20150817-200248-1828659978-42709-47640-S1
I0817 20:02:49.304107 47663 replica.cpp:511] Replica received write request for position 6
I0817 20:02:49.304772 47670 status_update_manager.cpp:183] Resuming sending status updates
I0817 20:02:49.305186 47669 slave.cpp:918] Forwarding total oversubscribed resources 
I0817 20:02:49.305856 47664 master.cpp:3998] Received update of slave 20150817-200248-1828659978-42709-47640-S1 at slave(2)@10.35.255.108:42709 (smfd-atr-11-sr1.devel.twitter.com) with total oversubscribed resources 
I0817 20:02:49.306066 47664 hierarchical.hpp:600] Slave 20150817-200248-1828659978-42709-47640-S1 (smfd-atr-11-sr1.devel.twitter.com) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):454767; ports(*):[31000-32000], allocated: )
I0817 20:02:49.308388 47663 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 4.206195ms
I0817 20:02:49.308477 47663 replica.cpp:679] Persisted action at 6
I0817 20:02:49.309033 47675 replica.cpp:658] Replica received learned notice for position 6
I0817 20:02:49.316745 47675 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 7.632635ms
I0817 20:02:49.316856 47675 leveldb.cpp:401] Deleting ~2 keys from leveldb took 24749ns
I0817 20:02:49.316881 47675 replica.cpp:679] Persisted action at 6
I0817 20:02:49.316910 47675 replica.cpp:664] Replica learned TRUNCATE action at position 6
I0817 20:02:49.658769 47669 master.cpp:3636] Registering slave at slave(3)@10.35.255.108:42709 (smfd-atr-11-sr1.devel.twitter.com) with id 20150817-200248-1828659978-42709-47640-S2
I0817 20:02:49.659023 47669 registrar.cpp:443] Applied 1 operations in 46214ns; attempting to update the 'registry'
I0817 20:02:49.661167 47668 log.cpp:685] Attempting to append 792 bytes to the log
I0817 20:02:49.661257 47683 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 7
I0817 20:02:49.661996 47675 replica.cpp:511] Replica received write request for position 7
I0817 20:02:49.711314 47675 leveldb.cpp:343] Persisting action (811 bytes) to leveldb took 49.262687ms
I0817 20:02:49.711436 47675 replica.cpp:679] Persisted action at 7
I0817 20:02:49.712038 47668 replica.cpp:658] Replica received learned notice for position 7
I0817 20:02:49.726662 47668 leveldb.cpp:343] Persisting action (813 bytes) to leveldb took 14.585693ms
I0817 20:02:49.726719 47668 replica.cpp:679] Persisted action at 7
I0817 20:02:49.726749 47668 replica.cpp:664] Replica learned APPEND action at position 7
I0817 20:02:49.727843 47662 registrar.cpp:488] Successfully updated the 'registry' in 68.766976ms
I0817 20:02:49.728132 47674 log.cpp:704] Attempting to truncate the log to 7
I0817 20:02:49.728226 47681 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 8
I0817 20:02:49.728842 47683 replica.cpp:511] Replica received write request for position 8
I0817 20:02:49.729096 47668 master.cpp:3699] Registered slave 20150817-200248-1828659978-42709-47640-S2 at slave(3)@10.35.255.108:42709 (smfd-atr-11-sr1.devel.twitter.com) with cpus(*):2; mem(*):10240; disk(*):454767; ports(*):[31000-32000]
I0817 20:02:49.729140 47665 slave.cpp:859] Registered with master master@10.35.255.108:42709; given slave ID 20150817-200248-1828659978-42709-47640-S2
I0817 20:02:49.729167 47677 hierarchical.hpp:540] Added slave 20150817-200248-1828659978-42709-47640-S2 (smfd-atr-11-sr1.devel.twitter.com) with cpus(*):2; mem(*):10240; disk(*):454767; ports(*):[31000-32000] (allocated: )
I0817 20:02:49.729280 47678 status_update_manager.cpp:183] Resuming sending status updates
I0817 20:02:49.729676 47665 slave.cpp:918] Forwarding total oversubscribed resources 
I0817 20:02:49.729795 47665 master.cpp:3998] Received update of slave 20150817-200248-1828659978-42709-47640-S2 at slave(3)@10.35.255.108:42709 (smfd-atr-11-sr1.devel.twitter.com) with total oversubscribed resources 
I0817 20:02:49.729964 47665 hierarchical.hpp:600] Slave 20150817-200248-1828659978-42709-47640-S2 (smfd-atr-11-sr1.devel.twitter.com) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):454767; ports(*):[31000-32000], allocated: )
I0817 20:02:49.735064 47683 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 6.121562ms
I0817 20:02:49.735158 47683 replica.cpp:679] Persisted action at 8
I0817 20:02:49.735734 47674 replica.cpp:658] Replica received learned notice for position 8
I0817 20:02:49.743316 47674 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 7.534185ms
I0817 20:02:49.743456 47674 leveldb.cpp:401] Deleting ~2 keys from leveldb took 33479ns
I0817 20:02:49.743526 47674 replica.cpp:679] Persisted action at 8
I0817 20:02:49.743552 47674 replica.cpp:664] Replica learned TRUNCATE action at position 8
I0817 20:03:03.334023 47661 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:03:03.334218 47661 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:03:03.335172 47661 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:03:03.335335 47671 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:03:03.340662 47680 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:03:03.340916 47682 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:03:18.334976 47661 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:03:18.335180 47662 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:03:18.336093 47678 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:03:18.336227 47678 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:03:18.341423 47674 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:03:18.341577 47678 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:03:33.335932 47672 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:03:33.336107 47672 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:03:33.337129 47672 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:03:33.337298 47672 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:03:33.342428 47677 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:03:33.342593 47679 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:03:48.326757 47668 slave.cpp:3885] Current disk usage 33.82%. Max allowed age: 3.932503913473657days
I0817 20:03:48.330942 47663 slave.cpp:3885] Current disk usage 33.82%. Max allowed age: 3.932503913473657days
I0817 20:03:48.336185 47663 slave.cpp:3885] Current disk usage 33.82%. Max allowed age: 3.932503913473657days
I0817 20:03:48.337045 47674 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:03:48.337184 47660 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:03:48.338268 47664 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:03:48.338408 47664 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:03:48.343598 47674 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:03:48.343809 47663 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:04:03.338038 47662 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:04:03.338868 47676 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:04:03.339164 47670 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:04:03.339326 47671 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:04:03.344446 47683 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:04:03.344578 47680 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:04:18.339581 47666 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:04:18.339673 47681 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:04:18.339892 47681 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:04:18.339929 47666 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:04:18.345873 47664 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:04:18.345998 47676 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:04:33.340752 47667 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:04:33.340807 47676 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:04:33.340912 47669 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:04:33.341440 47667 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0817 20:04:33.346962 47683 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0817 20:04:33.347102 47683 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
{code}

gdb trace. 
{code}
(gdb) bt

#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f88e49 in wait () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5f8cf77 in wait () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00000000004321c3 in wait ()
#7  0x000000000042e448 in main ()



(gdb) thread apply all bt

Thread 27 (Thread 0x7f389d919940 (LWP 47660)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 26 (Thread 0x7f389d118940 (LWP 47661)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 25 (Thread 0x7f389c917940 (LWP 47662)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 24 (Thread 0x7f389c116940 (LWP 47663)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 23 (Thread 0x7f389b915940 (LWP 47664)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 22 (Thread 0x7f389b114940 (LWP 47665)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
---Type <return> to continue, or q <return> to quit---
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 21 (Thread 0x7f389a913940 (LWP 47666)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 20 (Thread 0x7f389a112940 (LWP 47667)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 19 (Thread 0x7f3899911940 (LWP 47668)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 18 (Thread 0x7f3899110940 (LWP 47669)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 17 (Thread 0x7f389890f940 (LWP 47670)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
---Type <return> to continue, or q <return> to quit---
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 16 (Thread 0x7f389810e940 (LWP 47671)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 15 (Thread 0x7f389790d940 (LWP 47672)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 14 (Thread 0x7f389710c940 (LWP 47673)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 13 (Thread 0x7f389690b940 (LWP 47674)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 12 (Thread 0x7f389610a940 (LWP 47675)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
---Type <return> to continue, or q <return> to quit---
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 11 (Thread 0x7f3895909940 (LWP 47676)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 10 (Thread 0x7f3895108940 (LWP 47677)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 9 (Thread 0x7f3894907940 (LWP 47678)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 8 (Thread 0x7f3894106940 (LWP 47679)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 7 (Thread 0x7f3893905940 (LWP 47680)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
---Type <return> to continue, or q <return> to quit---
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 6 (Thread 0x7f3893104940 (LWP 47681)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 5 (Thread 0x7f3892903940 (LWP 47682)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 4 (Thread 0x7f3892102940 (LWP 47683)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 3 (Thread 0x7f3891901940 (LWP 47684)):
#0  0x00007f38a1cea3b8 in epoll_wait () from /lib64/libc.so.6
#1  0x00007f38a6026735 in epoll_poll (loop=0x7f38a758a300, timeout=<value optimized out>) at ev_epoll.c:153
#2  0x00007f38a60288c7 in ev_run (loop=0x7f38a758a300, flags=Unhandled dwarf expression opcode 0xf3
) at ev.c:3360
#3  0x00007f38a5fed7f4 in ev_loop () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5fede08 in run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 2 (Thread 0x7f3890cf0940 (LWP 47685)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a605bf3c in BGThread () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a605bcc4 in BGThreadWrapper () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#4  0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6

Thread 1 (Thread 0x7f38a7785780 (LWP 47640)):
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f88e49 in wait () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5f8cf77 in wait () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
---Type <return> to continue, or q <return> to quit---
#6  0x00000000004321c3 in wait ()
#7  0x000000000042e448 in main ()

{code}","17/Aug/15 23:29;vinodkone;Review for the first problem:

https://reviews.apache.org/r/37559/","18/Aug/15 00:32;vinodkone;commit c532490bfcb4470d0614640031ff854af8876ef6
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Mon Aug 17 15:45:51 2015 -0700

    Fixed mutex deadlock issue in ~scheduler::Mesos().
    
    Review: https://reviews.apache.org/r/37559
","18/Aug/15 05:55;vinodkone;Neither me nor benh were able to repro this tonight after running 1K iterations each. Seems like a very rare deadlock.

I'm removing this as a blocker for 0.24.0 release but will keep the ticket open.","18/Aug/15 06:52;haosdent@gmail.com;I use clang/g++ test in Ubuntu 14.04 (--gtest_repeat=1000), also could not reproduce the error in jenkins. Is it possible fixed after https://reviews.apache.org/r/37089 ? From the log, scheduler has already detected the master. No sure what reason make it not pass next step to send SUBSCRIBE call.
{code}
I0813 19:55:17.208312 26134 scheduler.cpp:272] New master detected at master@172.17.2.10:60249
{code}

Form the gdb thread dump, seems could not get any useful message, 24 threads have same stack.
{code}
#0  0x00007f38a2279019 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f38a61298ec in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#2  0x00007f38a52963cc in synchronized_wait<std::condition_variable, std::mutex> () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#3  0x00007f38a5f98ce9 in arrive () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#4  0x00007f38a5f7c0e0 in schedule () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#5  0x00007f38a5fdbf3b in _M_invoke<>(void) () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#6  0x00007f38a5fdbe8b in operator() () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#7  0x00007f38a5fdbe24 in _M_run () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#8  0x00007f38a612a3c0 in execute_native_thread_routine () from /home/vinod/mesos/build/src/.libs/libmesos-0.24.0.so
#9  0x00007f38a227483d in start_thread () from /lib64/libpthread.so.0
#10 0x00007f38a1ce9fcd in clone () from /lib64/libc.so.6
{code}","18/Aug/15 14:51;haosdent@gmail.com;Check out to the branch before that patch also could not reproduce the problem. LoL","20/Aug/15 18:06;marco-mesos;Thanks for the investigation and vigorous attempt at reproducing.
Agree on keeping it open until we are sure the flakiness has gone away.

I've removed from the {{HTTP API}} Epic, but adding appropriate labels so we don't lose track of this.
Also moved to In Progress state, as you seem to have done a rather sizeable amount of work on it already :)

Adding the {{1.0.0}} target version, so we don't forget to double check before releasing, that this is truly solved.","02/Sep/15 17:35;vinodkone;This is still happening :(

https://builds.apache.org/job/Mesos/COMPILER=clang,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=ubuntu:14.04,label_exp=docker%7C%7CHadoop/762/consoleFull

{code}
[ RUN      ] ExamplesTest.EventCallFramework
II0902 00:30:52.212740 28492 exec.cpp:443] Ignoring exited event because the driver is aborted!
I0902 00:30:52.212781 28534 exec.cpp:443] Ignoring exited event because the driver is aborted!
0902 00:30:52.212716 28513 exec.cpp:443] Ignoring exited event because the driver is aborted!
I0902 00:30:52.212926 28551 exec.cpp:443] Ignoring exited event because the driver is aborted!
Using temporary directory '/tmp/ExamplesTest_EventCallFramework_JN7PJN'
I0902 00:30:52.213004 28576 exec.cpp:443] Ignoring exited event because the driver is aborted!
Shutting down
Sending SIGTERM to process tree at pid 28560
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 28561
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 28562
Shutting down
Sending SIGTERM to process tree at pid 28580
Killing the following process trees:
[ 

]
Killing the following process trees:
[ 

]
Shutting down
Sending SIGTERM to process tree at pid 28581
Killing the following process trees:
[ 

]
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0902 00:30:53.636021 28582 process.cpp:1012] libprocess is initialized on 172.17.3.73:36681 for 16 cpus
I0902 00:30:53.636329 28582 logging.cpp:177] Logging to STDERR
I0902 00:30:53.637977 28582 scheduler.cpp:157] Version: 0.25.0
I0902 00:30:53.653961 28582 leveldb.cpp:176] Opened db in 4.138502ms
I0902 00:30:53.655617 28582 leveldb.cpp:183] Compacted db in 1.629758ms
I0902 00:30:53.655752 28582 leveldb.cpp:198] Created db iterator in 80178ns
I0902 00:30:53.655781 28582 leveldb.cpp:204] Seeked to beginning of db in 10727ns
I0902 00:30:53.655792 28582 leveldb.cpp:273] Iterated through 0 keys in the db in 6510ns
I0902 00:30:53.655972 28582 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0902 00:30:53.658758 28612 recover.cpp:449] Starting replica recovery
I0902 00:30:53.659834 28612 recover.cpp:475] Replica is in EMPTY status
I0902 00:30:53.660012 28582 local.cpp:241] Using 'local' authorizer
I0902 00:30:53.662570 28605 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0902 00:30:53.663496 28611 master.cpp:378] Master 20150902-003053-1224937900-36681-28582 (441f06bc837a) started on 172.17.3.73:36681
I0902 00:30:53.663532 28611 master.cpp:380] Flags at startup: --acls=""permissive: false
register_frameworks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  roles {
    type: SOME
    values: ""*""
  }
}
run_tasks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  users {
    type: SOME
    values: ""mesos""
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ExamplesTest_EventCallFramework_JN7PJN/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.25.0/src/webui"" --work_dir=""/tmp/mesos-s1Q2uY"" --zk_session_timeout=""10secs""
I0902 00:30:53.664083 28611 master.cpp:427] Master allowing unauthenticated frameworks to register
I0902 00:30:53.664096 28611 master.cpp:432] Master allowing unauthenticated slaves to register
I0902 00:30:53.664104 28611 credentials.hpp:37] Loading credentials for authentication from '/tmp/ExamplesTest_EventCallFramework_JN7PJN/credentials'
W0902 00:30:53.664187 28611 credentials.hpp:52] Permissions on credentials file '/tmp/ExamplesTest_EventCallFramework_JN7PJN/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.
I0902 00:30:53.664290 28610 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0902 00:30:53.664329 28611 master.cpp:469] Using default 'crammd5' authenticator
I0902 00:30:53.664505 28611 authenticator.cpp:512] Initializing server SASL
I0902 00:30:53.664973 28612 recover.cpp:566] Updating replica status to STARTING
I0902 00:30:53.665920 28619 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 622741ns
I0902 00:30:53.665946 28619 replica.cpp:323] Persisted replica status to STARTING
I0902 00:30:53.666154 28611 auxprop.cpp:66] Initialized in-memory auxiliary property plugin
I0902 00:30:53.666316 28610 recover.cpp:475] Replica is in STARTING status
I0902 00:30:53.666332 28611 master.cpp:506] Authorization enabled
I0902 00:30:53.666404 28582 containerizer.cpp:160] Using isolation: filesystem/posix,posix/cpu,posix/mem
I0902 00:30:53.666851 28619 whitelist_watcher.cpp:79] No whitelist given
I0902 00:30:53.667111 28610 hierarchical.hpp:346] Initialized hierarchical allocator process
I0902 00:30:53.667134 28609 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0902 00:30:53.667367 28609 recover.cpp:195] Received a recover response from a replica in STARTING status
I0902 00:30:53.667743 28605 recover.cpp:566] Updating replica status to VOTING
I0902 00:30:53.668181 28619 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 300973ns
I0902 00:30:53.668211 28619 replica.cpp:323] Persisted replica status to VOTING
I0902 00:30:53.669222 28615 recover.cpp:580] Successfully joined the Paxos group
I0902 00:30:53.669587 28615 recover.cpp:464] Recover process terminated
I0902 00:30:53.671195 28616 slave.cpp:190] Slave started on 1)@172.17.3.73:36681
I0902 00:30:53.671234 28616 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.25.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-s1Q2uY/0""
I0902 00:30:53.672694 28616 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0902 00:30:53.672760 28582 containerizer.cpp:160] Using isolation: filesystem/posix,posix/cpu,posix/mem
I0902 00:30:53.672828 28616 slave.cpp:384] Slave hostname: 441f06bc837a
I0902 00:30:53.672844 28616 slave.cpp:389] Slave checkpoint: true
I0902 00:30:53.674482 28605 master.cpp:1559] The newly elected leader is master@172.17.3.73:36681 with id 20150902-003053-1224937900-36681-28582
I0902 00:30:53.674528 28605 master.cpp:1572] Elected as the leading master!
I0902 00:30:53.674546 28605 master.cpp:1332] Recovering from registrar
I0902 00:30:53.674584 28612 state.cpp:54] Recovering state from '/tmp/mesos-s1Q2uY/0/meta'
I0902 00:30:53.674712 28615 registrar.cpp:311] Recovering registrar
I0902 00:30:53.675217 28605 status_update_manager.cpp:202] Recovering status update manager
I0902 00:30:53.675644 28618 containerizer.cpp:396] Recovering containerizer
I0902 00:30:53.676468 28614 log.cpp:661] Attempting to start the writer
I0902 00:30:53.676507 28619 slave.cpp:190] Slave started on 2)@172.17.3.73:36681
I0902 00:30:53.676807 28619 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.25.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-s1Q2uY/1""
I0902 00:30:53.677983 28619 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0902 00:30:53.678058 28611 slave.cpp:4069] Finished recovery
I0902 00:30:53.678170 28619 slave.cpp:384] Slave hostname: 441f06bc837a
I0902 00:30:53.678203 28619 slave.cpp:389] Slave checkpoint: true
I0902 00:30:53.678493 28582 containerizer.cpp:160] Using isolation: filesystem/posix,posix/cpu,posix/mem
I0902 00:30:53.678817 28611 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0902 00:30:53.679097 28619 state.cpp:54] Recovering state from '/tmp/mesos-s1Q2uY/1/meta'
I0902 00:30:53.679438 28619 status_update_manager.cpp:202] Recovering status update manager
I0902 00:30:53.679734 28616 status_update_manager.cpp:176] Pausing sending status updates
I0902 00:30:53.679805 28611 slave.cpp:684] New master detected at master@172.17.3.73:36681
I0902 00:30:53.680107 28611 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0902 00:30:53.680172 28606 containerizer.cpp:396] Recovering containerizer
I0902 00:30:53.680227 28611 slave.cpp:720] Detecting new master
I0902 00:30:53.680311 28620 replica.cpp:477] Replica received implicit promise request with proposal 1
I0902 00:30:53.680373 28611 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0902 00:30:53.680917 28620 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 458464ns
I0902 00:30:53.681080 28620 replica.cpp:345] Persisted promised to 1
I0902 00:30:53.682653 28613 slave.cpp:4069] Finished recovery
I0902 00:30:53.682688 28620 coordinator.cpp:231] Coordinator attemping to fill missing position
I0902 00:30:53.683275 28613 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0902 00:30:53.683584 28607 status_update_manager.cpp:176] Pausing sending status updates
I0902 00:30:53.683601 28615 slave.cpp:684] New master detected at master@172.17.3.73:36681
I0902 00:30:53.683730 28615 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0902 00:30:53.683789 28615 slave.cpp:720] Detecting new master
I0902 00:30:53.684020 28615 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0902 00:30:53.684445 28619 slave.cpp:190] Slave started on 3)@172.17.3.73:36681
I0902 00:30:53.684656 28605 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0902 00:30:53.684484 28619 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.25.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/mesos-s1Q2uY/2""
I0902 00:30:53.685191 28605 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 495897ns
I0902 00:30:53.685223 28605 replica.cpp:679] Persisted action at 0
I0902 00:30:53.685535 28619 slave.cpp:354] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0902 00:30:53.685657 28619 slave.cpp:384] Slave hostname: 441f06bc837a
I0902 00:30:53.685678 28619 slave.cpp:389] Slave checkpoint: true
I0902 00:30:53.686754 28605 state.cpp:54] Recovering state from '/tmp/mesos-s1Q2uY/2/meta'
I0902 00:30:53.687188 28616 status_update_manager.cpp:202] Recovering status update manager
I0902 00:30:53.687327 28605 replica.cpp:511] Replica received write request for position 0
I0902 00:30:53.687433 28605 leveldb.cpp:438] Reading position from leveldb took 77420ns
I0902 00:30:53.687559 28620 scheduler.cpp:240] New master detected at master@172.17.3.73:36681
I0902 00:30:53.688076 28605 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 546957ns
I0902 00:30:53.688114 28605 replica.cpp:679] Persisted action at 0
I0902 00:30:53.687638 28619 containerizer.cpp:396] Recovering containerizer
I0902 00:30:53.689191 28607 replica.cpp:658] Replica received learned notice for position 0
I0902 00:30:53.689815 28607 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 607808ns
I0902 00:30:53.689865 28607 replica.cpp:679] Persisted action at 0
I0902 00:30:53.689914 28607 replica.cpp:664] Replica learned NOP action at position 0
I0902 00:30:53.690187 28620 slave.cpp:4069] Finished recovery
I0902 00:30:53.690893 28620 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0902 00:30:53.691164 28611 log.cpp:677] Writer started with ending position 0
I0902 00:30:53.691167 28616 status_update_manager.cpp:176] Pausing sending status updates
I0902 00:30:53.691202 28620 slave.cpp:684] New master detected at master@172.17.3.73:36681
I0902 00:30:53.691305 28620 slave.cpp:709] No credentials provided. Attempting to register without authentication
I0902 00:30:53.691411 28620 slave.cpp:720] Detecting new master
I0902 00:30:53.691535 28620 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0902 00:30:53.693686 28609 leveldb.cpp:438] Reading position from leveldb took 55603ns
I0902 00:30:53.697943 28611 registrar.cpp:344] Successfully fetched the registry (0B) in 22.782976ms
I0902 00:30:53.698328 28611 registrar.cpp:443] Applied 1 operations in 137518ns; attempting to update the 'registry'
I0902 00:30:53.701880 28607 log.cpp:685] Attempting to append 176 bytes to the log
I0902 00:30:53.702030 28606 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 1
I0902 00:30:53.702842 28605 replica.cpp:511] Replica received write request for position 1
I0902 00:30:53.703675 28605 leveldb.cpp:343] Persisting action (195 bytes) to leveldb took 810079ns
I0902 00:30:53.703706 28605 replica.cpp:679] Persisted action at 1
I0902 00:30:53.704337 28608 replica.cpp:658] Replica received learned notice for position 1
I0902 00:30:53.705054 28608 leveldb.cpp:343] Persisting action (197 bytes) to leveldb took 702560ns
I0902 00:30:53.705088 28608 replica.cpp:679] Persisted action at 1
I0902 00:30:53.705116 28608 replica.cpp:664] Replica learned APPEND action at position 1
I0902 00:30:53.706897 28608 registrar.cpp:488] Successfully updated the 'registry' in 8.417024ms
I0902 00:30:53.707123 28608 registrar.cpp:374] Successfully recovered registrar
I0902 00:30:53.707484 28616 log.cpp:704] Attempting to truncate the log to 1
I0902 00:30:53.707660 28608 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 2
I0902 00:30:53.707659 28614 master.cpp:1369] Recovered 0 slaves from the Registry (137B) ; allowing 10mins for slaves to re-register
I0902 00:30:53.708447 28612 replica.cpp:511] Replica received write request for position 2
I0902 00:30:53.709228 28612 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 749671ns
I0902 00:30:53.709259 28612 replica.cpp:679] Persisted action at 2
I0902 00:30:53.709920 28616 replica.cpp:658] Replica received learned notice for position 2
I0902 00:30:53.710574 28616 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 640764ns
I0902 00:30:53.710654 28616 leveldb.cpp:401] Deleting ~1 keys from leveldb took 59488ns
I0902 00:30:53.710682 28616 replica.cpp:679] Persisted action at 2
I0902 00:30:53.710728 28616 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0902 00:30:53.741991 28619 slave.cpp:1209] Will retry registration in 1.091568855secs if necessary
I0902 00:30:53.742467 28609 master.cpp:3670] Registering slave at slave(3)@172.17.3.73:36681 (441f06bc837a) with id 20150902-003053-1224937900-36681-28582-S0
I0902 00:30:53.743149 28614 registrar.cpp:443] Applied 1 operations in 170710ns; attempting to update the 'registry'
I0902 00:30:53.745995 28608 log.cpp:685] Attempting to append 347 bytes to the log
I0902 00:30:53.746286 28620 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 3
I0902 00:30:53.747166 28619 replica.cpp:511] Replica received write request for position 3
I0902 00:30:53.747927 28619 leveldb.cpp:343] Persisting action (366 bytes) to leveldb took 738551ns
I0902 00:30:53.747967 28619 replica.cpp:679] Persisted action at 3
I0902 00:30:53.748620 28605 replica.cpp:658] Replica received learned notice for position 3
I0902 00:30:53.749323 28605 leveldb.cpp:343] Persisting action (368 bytes) to leveldb took 570424ns
I0902 00:30:53.749356 28605 replica.cpp:679] Persisted action at 3
I0902 00:30:53.749390 28605 replica.cpp:664] Replica learned APPEND action at position 3
I0902 00:30:53.750951 28615 registrar.cpp:488] Successfully updated the 'registry' in 7.702016ms
I0902 00:30:53.751273 28607 log.cpp:704] Attempting to truncate the log to 3
I0902 00:30:53.751399 28619 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 4
I0902 00:30:53.752230 28614 replica.cpp:511] Replica received write request for position 4
I0902 00:30:53.752382 28609 slave.cpp:3058] Received ping from slave-observer(1)@172.17.3.73:36681
I0902 00:30:53.752697 28616 master.cpp:3733] Registered slave 20150902-003053-1224937900-36681-28582-S0 at slave(3)@172.17.3.73:36681 (441f06bc837a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0902 00:30:53.752836 28619 slave.cpp:859] Registered with master master@172.17.3.73:36681; given slave ID 20150902-003053-1224937900-36681-28582-S0
I0902 00:30:53.752884 28619 fetcher.cpp:77] Clearing fetcher cache
I0902 00:30:53.752969 28620 hierarchical.hpp:542] Added slave 20150902-003053-1224937900-36681-28582-S0 (441f06bc837a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0902 00:30:53.753084 28613 status_update_manager.cpp:183] Resuming sending status updates
I0902 00:30:53.753156 28614 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 906644ns
I0902 00:30:53.753195 28614 replica.cpp:679] Persisted action at 4
I0902 00:30:53.753587 28620 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:30:53.753640 28620 hierarchical.hpp:928] Performed allocation for slave 20150902-003053-1224937900-36681-28582-S0 in 617892ns
I0902 00:30:53.753787 28619 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/mesos-s1Q2uY/2/meta/slaves/20150902-003053-1224937900-36681-28582-S0/slave.info'
I0902 00:30:53.753886 28605 replica.cpp:658] Replica received learned notice for position 4
I0902 00:30:53.754436 28619 slave.cpp:918] Forwarding total oversubscribed resources 
I0902 00:30:53.754580 28605 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 677038ns
I0902 00:30:53.754675 28605 leveldb.cpp:401] Deleting ~2 keys from leveldb took 72858ns
I0902 00:30:53.754695 28608 master.cpp:4032] Received update of slave 20150902-003053-1224937900-36681-28582-S0 at slave(3)@172.17.3.73:36681 (441f06bc837a) with total oversubscribed resources 
I0902 00:30:53.754710 28605 replica.cpp:679] Persisted action at 4
I0902 00:30:53.754761 28605 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0902 00:30:53.755244 28619 hierarchical.hpp:602] Slave 20150902-003053-1224937900-36681-28582-S0 (441f06bc837a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0902 00:30:53.755473 28619 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:30:53.755507 28619 hierarchical.hpp:928] Performed allocation for slave 20150902-003053-1224937900-36681-28582-S0 in 217181ns
I0902 00:30:54.464862 28617 slave.cpp:1209] Will retry registration in 1.181298785secs if necessary
I0902 00:30:54.464993 28609 master.cpp:3670] Registering slave at slave(1)@172.17.3.73:36681 (441f06bc837a) with id 20150902-003053-1224937900-36681-28582-S1
I0902 00:30:54.465430 28619 registrar.cpp:443] Applied 1 operations in 113891ns; attempting to update the 'registry'
I0902 00:30:54.468351 28613 log.cpp:685] Attempting to append 515 bytes to the log
I0902 00:30:54.468456 28618 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 5
I0902 00:30:54.469163 28605 replica.cpp:511] Replica received write request for position 5
I0902 00:30:54.469934 28605 leveldb.cpp:343] Persisting action (534 bytes) to leveldb took 741179ns
I0902 00:30:54.469966 28605 replica.cpp:679] Persisted action at 5
I0902 00:30:54.470788 28614 replica.cpp:658] Replica received learned notice for position 5
I0902 00:30:54.471417 28614 leveldb.cpp:343] Persisting action (536 bytes) to leveldb took 615160ns
I0902 00:30:54.471451 28614 replica.cpp:679] Persisted action at 5
I0902 00:30:54.471477 28614 replica.cpp:664] Replica learned APPEND action at position 5
I0902 00:30:54.473342 28607 registrar.cpp:488] Successfully updated the 'registry' in 7.836928ms
I0902 00:30:54.473642 28616 log.cpp:704] Attempting to truncate the log to 5
I0902 00:30:54.474309 28617 slave.cpp:3058] Received ping from slave-observer(2)@172.17.3.73:36681
I0902 00:30:54.474455 28608 master.cpp:3733] Registered slave 20150902-003053-1224937900-36681-28582-S1 at slave(1)@172.17.3.73:36681 (441f06bc837a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0902 00:30:54.474581 28617 slave.cpp:859] Registered with master master@172.17.3.73:36681; given slave ID 20150902-003053-1224937900-36681-28582-S1
I0902 00:30:54.474618 28617 fetcher.cpp:77] Clearing fetcher cache
I0902 00:30:54.474635 28610 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 6
I0902 00:30:54.474769 28605 status_update_manager.cpp:183] Resuming sending status updates
I0902 00:30:54.474747 28609 hierarchical.hpp:542] Added slave 20150902-003053-1224937900-36681-28582-S1 (441f06bc837a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0902 00:30:54.475209 28617 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/mesos-s1Q2uY/0/meta/slaves/20150902-003053-1224937900-36681-28582-S1/slave.info'
I0902 00:30:54.475317 28609 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:30:54.475446 28609 hierarchical.hpp:928] Performed allocation for slave 20150902-003053-1224937900-36681-28582-S1 in 463257ns
I0902 00:30:54.475603 28607 replica.cpp:511] Replica received write request for position 6
I0902 00:30:54.475780 28617 slave.cpp:918] Forwarding total oversubscribed resources 
I0902 00:30:54.475937 28617 master.cpp:4032] Received update of slave 20150902-003053-1224937900-36681-28582-S1 at slave(1)@172.17.3.73:36681 (441f06bc837a) with total oversubscribed resources 
I0902 00:30:54.476305 28610 hierarchical.hpp:602] Slave 20150902-003053-1224937900-36681-28582-S1 (441f06bc837a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0902 00:30:54.476480 28607 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 772408ns
I0902 00:30:54.476511 28607 replica.cpp:679] Persisted action at 6
I0902 00:30:54.476541 28610 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:30:54.476567 28610 hierarchical.hpp:928] Performed allocation for slave 20150902-003053-1224937900-36681-28582-S1 in 214977ns
I0902 00:30:54.477264 28620 replica.cpp:658] Replica received learned notice for position 6
I0902 00:30:54.477743 28620 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 458654ns
I0902 00:30:54.477814 28620 leveldb.cpp:401] Deleting ~2 keys from leveldb took 51876ns
I0902 00:30:54.477840 28620 replica.cpp:679] Persisted action at 6
I0902 00:30:54.477867 28620 replica.cpp:664] Replica learned TRUNCATE action at position 6
I0902 00:30:54.596190 28611 slave.cpp:1209] Will retry registration in 217.215177ms if necessary
I0902 00:30:54.596360 28608 master.cpp:3670] Registering slave at slave(2)@172.17.3.73:36681 (441f06bc837a) with id 20150902-003053-1224937900-36681-28582-S2
I0902 00:30:54.596786 28611 registrar.cpp:443] Applied 1 operations in 117272ns; attempting to update the 'registry'
I0902 00:30:54.599691 28613 log.cpp:685] Attempting to append 683 bytes to the log
I0902 00:30:54.599798 28618 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 7
I0902 00:30:54.600631 28616 replica.cpp:511] Replica received write request for position 7
I0902 00:30:54.600955 28616 leveldb.cpp:343] Persisting action (702 bytes) to leveldb took 298371ns
I0902 00:30:54.600988 28616 replica.cpp:679] Persisted action at 7
I0902 00:30:54.601665 28609 replica.cpp:658] Replica received learned notice for position 7
I0902 00:30:54.602424 28609 leveldb.cpp:343] Persisting action (704 bytes) to leveldb took 743909ns
I0902 00:30:54.602457 28609 replica.cpp:679] Persisted action at 7
I0902 00:30:54.602481 28609 replica.cpp:664] Replica learned APPEND action at position 7
I0902 00:30:54.604488 28615 registrar.cpp:488] Successfully updated the 'registry' in 7.63008ms
I0902 00:30:54.604832 28617 log.cpp:704] Attempting to truncate the log to 7
I0902 00:30:54.605005 28613 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 8
I0902 00:30:54.605356 28615 slave.cpp:3058] Received ping from slave-observer(3)@172.17.3.73:36681
I0902 00:30:54.605599 28615 slave.cpp:859] Registered with master master@172.17.3.73:36681; given slave ID 20150902-003053-1224937900-36681-28582-S2
I0902 00:30:54.605530 28606 master.cpp:3733] Registered slave 20150902-003053-1224937900-36681-28582-S2 at slave(2)@172.17.3.73:36681 (441f06bc837a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]
I0902 00:30:54.605690 28615 fetcher.cpp:77] Clearing fetcher cache
I0902 00:30:54.605751 28609 hierarchical.hpp:542] Added slave 20150902-003053-1224937900-36681-28582-S2 (441f06bc837a) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )
I0902 00:30:54.605989 28605 status_update_manager.cpp:183] Resuming sending status updates
I0902 00:30:54.606201 28609 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:30:54.606220 28607 replica.cpp:511] Replica received write request for position 8
I0902 00:30:54.606238 28609 hierarchical.hpp:928] Performed allocation for slave 20150902-003053-1224937900-36681-28582-S2 in 348187ns
I0902 00:30:54.606287 28615 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/mesos-s1Q2uY/1/meta/slaves/20150902-003053-1224937900-36681-28582-S2/slave.info'
I0902 00:30:54.607102 28615 slave.cpp:918] Forwarding total oversubscribed resources 
I0902 00:30:54.607115 28607 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 872303ns
I0902 00:30:54.607148 28607 replica.cpp:679] Persisted action at 8
I0902 00:30:54.607295 28615 master.cpp:4032] Received update of slave 20150902-003053-1224937900-36681-28582-S2 at slave(2)@172.17.3.73:36681 (441f06bc837a) with total oversubscribed resources 
I0902 00:30:54.607785 28620 hierarchical.hpp:602] Slave 20150902-003053-1224937900-36681-28582-S2 (441f06bc837a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )
I0902 00:30:54.607882 28618 replica.cpp:658] Replica received learned notice for position 8
I0902 00:30:54.608000 28620 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:30:54.608031 28620 hierarchical.hpp:928] Performed allocation for slave 20150902-003053-1224937900-36681-28582-S2 in 197321ns
I0902 00:30:54.608292 28618 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 389254ns
I0902 00:30:54.608384 28618 leveldb.cpp:401] Deleting ~2 keys from leveldb took 70973ns
I0902 00:30:54.608415 28618 replica.cpp:679] Persisted action at 8
I0902 00:30:54.608456 28618 replica.cpp:664] Replica learned TRUNCATE action at position 8
I0902 00:30:54.668705 28614 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:30:54.668745 28614 hierarchical.hpp:910] Performed allocation for 3 slaves in 557805ns
I0902 00:30:55.669527 28619 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:30:55.669566 28619 hierarchical.hpp:910] Performed allocation for 3 slaves in 465684ns
I0902 00:30:56.671480 28617 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:30:56.671515 28617 hierarchical.hpp:910] Performed allocation for 3 slaves in 485449ns
I0902 00:30:57.673353 28607 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:30:57.673391 28607 hierarchical.hpp:910] Performed allocation for 3 slaves in 518196ns
I0902 00:30:58.675276 28618 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:30:58.675312 28618 hierarchical.hpp:910] Performed allocation for 3 slaves in 503347ns
I0902 00:30:59.677129 28610 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:30:59.677163 28610 hierarchical.hpp:910] Performed allocation for 3 slaves in 468757ns
I0902 00:31:00.678947 28613 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:31:00.678983 28613 hierarchical.hpp:910] Performed allocation for 3 slaves in 497686ns
I0902 00:31:01.680778 28620 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:31:01.680814 28620 hierarchical.hpp:910] Performed allocation for 3 slaves in 433630ns
I0902 00:31:02.682585 28609 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:31:02.682620 28609 hierarchical.hpp:910] Performed allocation for 3 slaves in 507390ns
I0902 00:31:03.683991 28611 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:31:03.684037 28611 hierarchical.hpp:910] Performed allocation for 3 slaves in 694017ns
I0902 00:31:04.686070 28619 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:31:04.686112 28619 hierarchical.hpp:910] Performed allocation for 3 slaves in 490512ns
I0902 00:31:05.687798 28618 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:31:05.687832 28618 hierarchical.hpp:910] Performed allocation for 3 slaves in 421718ns
I0902 00:31:06.689510 28607 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:31:06.689548 28607 hierarchical.hpp:910] Performed allocation for 3 slaves in 448215ns
I0902 00:31:07.691249 28610 hierarchical.hpp:1010] No resources available to allocate!
I0902 00:31:07.691285 28610 hierarchical.hpp:910] Performed allocation for 3 slaves in 469345ns
I0902 00:31:08.683100 28615 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0902 00:31:08.683322 28614 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0902 00:31:08.685286 28606 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0902 00:31:08.685518 28616 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0902 00:31:08.692482 28608 slave.cpp:4226] Querying resource estimator for oversubscribable resources
{code}

[~ijimenez] Are you up for triaging this?
","03/Sep/15 06:43;anandmazumdar;Was able to reproduce this once on my machine. But the issue is not the same as the one seen on ASF build though as per logs. So we still need to dig around for what the issue on ASF test-run was.

In my test run, there is a race between the master successfully recovering its state from registry and the scheduler sending a call. In this case, we just log the error and leave it upon the framework to retry the call. This happened only because I was running in a debugger in a loop and there would have been state left in Master across various test invocations leading to the time in Registry Recovery. So , I won't worry too much about this occurrence.

{code}
I0902 23:29:28.815498 113774592 leveldb.cpp:438] Reading position from leveldb took 32us
I0902 23:29:28.826355 136355840 registrar.cpp:344] Successfully fetched the registry (0B) in 16.811008ms
I0902 23:29:28.826472 136355840 registrar.cpp:443] Applied 1 operations in 35us; attempting to update the 'registry'
I0902 23:29:28.826869 135819264 http.cpp:333] HTTP POST for /master/api/v1/scheduler from 192.168.29.132:56913
W0902 23:29:28.831881 135282688 scheduler.cpp:381] Received '503 Service Unavailable' () for SUBSCRIBE
{code}

[~ijimenez] [~vinodkone]","06/Sep/15 18:31;haosdent@gmail.com;Your env is Ubuntu or OSX?","15/Sep/15 00:05;jieyu;Another occurrence:
https://builds.apache.org/job/Mesos/COMPILER=clang,CONFIGURATION=--verbose,OS=ubuntu:14.04,label_exp=docker%7C%7CHadoop/800/consoleFull
","15/Sep/15 05:22;klaus1982;After checking the code, it seems the same issue that: scheduler doesn't send a SUBSCRIBE.","15/Sep/15 17:42;vinodkone;[~klaus1982] can you repro this locally when running the test in repetition? if yes, you could add more logging statements to find the root cause.","17/Sep/15 01:52;klaus1982;[~vinod@twitter.com], unfortunately, I did not repro it in Ubuntu14.04. Let me have a try in other OS to see what happen.","17/Sep/15 01:58;haosdent@gmail.com;I guess only 14.04 + clang + docker could reproduce this problem. :(","17/Sep/15 02:01;klaus1982;thanks for your suggestion, let me have try for both :).","30/Nov/15 22:58;tillt;This does still happen - definitely catchable.

System: [Ubuntu14.04 Vagrant Image|https://github.com/tillt/mesos-vagrant-ci/blob/master/ubuntu14/setup.sh]

SSL build (enable-ssl, enable-libevent), root test-run, verbose
{noformat}
[ RUN      ] ExamplesTest.EventCallFramework
Using temporary directory '/tmp/ExamplesTest_EventCallFramework_MUpzLQ'
I1130 22:47:23.059108 15950 scheduler.cpp:156] Version: 0.26.0
I1130 22:47:23.107734 15950 leveldb.cpp:176] Opened db in 41.560489ms
I1130 22:47:23.109232 15950 leveldb.cpp:183] Compacted db in 1.42992ms
I1130 22:47:23.109333 15950 leveldb.cpp:198] Created db iterator in 37301ns
I1130 22:47:23.109355 15950 leveldb.cpp:204] Seeked to beginning of db in 2008ns
I1130 22:47:23.109367 15950 leveldb.cpp:273] Iterated through 0 keys in the db in 170ns
I1130 22:47:23.109491 15950 replica.cpp:780] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1130 22:47:23.111325 15974 recover.cpp:449] Starting replica recovery
I1130 22:47:23.112205 15974 recover.cpp:475] Replica is in EMPTY status
I1130 22:47:23.112306 15950 local.cpp:241] Using 'local' authorizer
I1130 22:47:23.114205 15973 replica.cpp:676] Replica in EMPTY status received a broadcasted recover request from (5)@127.0.1.1:51177
I1130 22:47:23.114904 15970 recover.cpp:195] Received a recover response from a replica in EMPTY status
I1130 22:47:23.118305 15970 recover.cpp:566] Updating replica status to STARTING
I1130 22:47:23.118458 15972 master.cpp:367] Master edbdc635-adf5-40f6-bc27-15b19e69f8cd (ubuntu14) started on 127.0.1.1:51177
I1130 22:47:23.119215 15972 master.cpp:369] Flags at startup: --acls=""permissive: false
register_frameworks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  roles {
    type: SOME
    values: ""*""
  }
}
run_tasks {
  principals {
    type: SOME
    values: ""test-principal""
  }
  users {
    type: SOME
    values: ""vagrant""
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ExamplesTest_EventCallFramework_MUpzLQ/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/home/vagrant/mesos/src/webui"" --work_dir=""/tmp/mesos-cqOgRP"" --zk_session_timeout=""10secs""
I1130 22:47:23.119771 15972 master.cpp:416] Master allowing unauthenticated frameworks to register
I1130 22:47:23.119787 15972 master.cpp:421] Master allowing unauthenticated slaves to register
I1130 22:47:23.119798 15972 credentials.hpp:37] Loading credentials for authentication from '/tmp/ExamplesTest_EventCallFramework_MUpzLQ/credentials'
W1130 22:47:23.119976 15972 credentials.hpp:52] Permissions on credentials file '/tmp/ExamplesTest_EventCallFramework_MUpzLQ/credentials' are too open. It is recommended that your credentials file is NOT accessible by others.
I1130 22:47:23.120127 15972 master.cpp:458] Using default 'crammd5' authenticator
I1130 22:47:23.120285 15972 authenticator.cpp:520] Initializing server SASL
I1130 22:47:23.121831 15950 containerizer.cpp:142] Using isolation: filesystem/posix,posix/cpu,posix/mem
I1130 22:47:23.121846 15972 master.cpp:495] Authorization enabled
W1130 22:47:23.122547 15950 backend.cpp:50] Failed to create 'bind' backend: BindBackend requires root privileges
I1130 22:47:23.126610 15970 slave.cpp:191] Slave started on 1)@127.0.1.1:51177
I1130 22:47:23.126672 15970 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/home/vagrant/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-cqOgRP/0""
I1130 22:47:23.127595 15950 containerizer.cpp:142] Using isolation: filesystem/posix,posix/cpu,posix/mem
I1130 22:47:23.127796 15974 master.cpp:1606] The newly elected leader is master@127.0.1.1:51177 with id edbdc635-adf5-40f6-bc27-15b19e69f8cd
I1130 22:47:23.127905 15974 master.cpp:1619] Elected as the leading master!
I1130 22:47:23.128002 15974 master.cpp:1379] Recovering from registrar
I1130 22:47:23.128363 15970 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):34068; ports(*):[31000-32000]
I1130 22:47:23.128432 15971 registrar.cpp:309] Recovering registrar
I1130 22:47:23.128494 15970 slave.cpp:400] Slave attributes: [  ]
I1130 22:47:23.128540 15970 slave.cpp:405] Slave hostname: ubuntu14
I1130 22:47:23.128551 15970 slave.cpp:410] Slave checkpoint: true
W1130 22:47:23.129720 15950 backend.cpp:50] Failed to create 'bind' backend: BindBackend requires root privileges
I1130 22:47:23.130424 15972 state.cpp:54] Recovering state from '/tmp/mesos-cqOgRP/0/meta'
I1130 22:47:23.130970 15972 status_update_manager.cpp:202] Recovering status update manager
I1130 22:47:23.131469 15976 containerizer.cpp:384] Recovering containerizer
I1130 22:47:23.133054 15975 slave.cpp:4230] Finished recovery
I1130 22:47:23.133693 15976 slave.cpp:191] Slave started on 2)@127.0.1.1:51177
I1130 22:47:23.133766 15975 slave.cpp:729] New master detected at master@127.0.1.1:51177
I1130 22:47:23.133836 15975 slave.cpp:754] No credentials provided. Attempting to register without authentication
I1130 22:47:23.133941 15975 slave.cpp:765] Detecting new master
I1130 22:47:23.133754 15976 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/home/vagrant/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-cqOgRP/1""
I1130 22:47:23.133738 15972 status_update_manager.cpp:176] Pausing sending status updates
I1130 22:47:23.134950 15976 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):34068; ports(*):[31000-32000]
I1130 22:47:23.135027 15976 slave.cpp:400] Slave attributes: [  ]
I1130 22:47:23.135051 15976 slave.cpp:405] Slave hostname: ubuntu14
I1130 22:47:23.135063 15976 slave.cpp:410] Slave checkpoint: true
I1130 22:47:23.135736 15950 containerizer.cpp:142] Using isolation: filesystem/posix,posix/cpu,posix/mem
I1130 22:47:23.135828 15975 state.cpp:54] Recovering state from '/tmp/mesos-cqOgRP/1/meta'
I1130 22:47:23.136504 15975 status_update_manager.cpp:202] Recovering status update manager
W1130 22:47:23.136520 15950 backend.cpp:50] Failed to create 'bind' backend: BindBackend requires root privileges
I1130 22:47:23.136754 15974 containerizer.cpp:384] Recovering containerizer
I1130 22:47:23.137874 15972 slave.cpp:4230] Finished recovery
I1130 22:47:23.138635 15974 status_update_manager.cpp:176] Pausing sending status updates
I1130 22:47:23.138628 15970 slave.cpp:729] New master detected at master@127.0.1.1:51177
I1130 22:47:23.138841 15970 slave.cpp:754] No credentials provided. Attempting to register without authentication
I1130 22:47:23.138906 15970 slave.cpp:765] Detecting new master
I1130 22:47:23.138696 15973 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 20.024076ms
I1130 22:47:23.139665 15973 replica.cpp:323] Persisted replica status to STARTING
I1130 22:47:23.139986 15973 recover.cpp:475] Replica is in STARTING status
I1130 22:47:23.141146 15973 replica.cpp:676] Replica in STARTING status received a broadcasted recover request from (36)@127.0.1.1:51177
I1130 22:47:23.141532 15975 recover.cpp:195] Received a recover response from a replica in STARTING status
I1130 22:47:23.141597 15970 slave.cpp:191] Slave started on 3)@127.0.1.1:51177
I1130 22:47:23.141633 15970 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/home/vagrant/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-cqOgRP/2""
I1130 22:47:23.142415 15972 recover.cpp:566] Updating replica status to VOTING
I1130 22:47:23.142632 15970 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):34068; ports(*):[31000-32000]
I1130 22:47:23.142706 15970 slave.cpp:400] Slave attributes: [  ]
I1130 22:47:23.142757 15970 slave.cpp:405] Slave hostname: ubuntu14
I1130 22:47:23.142771 15970 slave.cpp:410] Slave checkpoint: true
2015-11-30 22:47:23,143:15722(0x7fac4de51700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:59080] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1130 22:47:23.145202 15972 state.cpp:54] Recovering state from '/tmp/mesos-cqOgRP/2/meta'
I1130 22:47:23.145670 15971 status_update_manager.cpp:202] Recovering status update manager
I1130 22:47:23.145854 15971 containerizer.cpp:384] Recovering containerizer
I1130 22:47:23.146724 15973 slave.cpp:4230] Finished recovery
I1130 22:47:23.147203 15975 slave.cpp:729] New master detected at master@127.0.1.1:51177
I1130 22:47:23.147258 15973 status_update_manager.cpp:176] Pausing sending status updates
I1130 22:47:23.147320 15975 slave.cpp:754] No credentials provided. Attempting to register without authentication
I1130 22:47:23.147452 15975 slave.cpp:765] Detecting new master
I1130 22:47:23.159584 15974 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 16.845187ms
I1130 22:47:23.159698 15974 replica.cpp:323] Persisted replica status to VOTING
I1130 22:47:23.159997 15974 recover.cpp:580] Successfully joined the Paxos group
I1130 22:47:23.160423 15974 recover.cpp:464] Recover process terminated
I1130 22:47:23.160991 15974 log.cpp:661] Attempting to start the writer
I1130 22:47:23.163434 15973 replica.cpp:496] Replica received implicit promise request from (40)@127.0.1.1:51177 with proposal 1
I1130 22:47:23.174804 15973 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 11.290243ms
I1130 22:47:23.174859 15973 replica.cpp:345] Persisted promised to 1
I1130 22:47:23.176043 15973 coordinator.cpp:240] Coordinator attempting to fill missing positions
I1130 22:47:23.177855 15972 replica.cpp:391] Replica received explicit promise request from (41)@127.0.1.1:51177 for position 0 with proposal 2
I1130 22:47:23.180476 15972 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 2.537426ms
I1130 22:47:23.180531 15972 replica.cpp:715] Persisted action at 0
I1130 22:47:23.182282 15971 replica.cpp:540] Replica received write request for position 0 from (42)@127.0.1.1:51177
I1130 22:47:23.182375 15971 leveldb.cpp:438] Reading position from leveldb took 36138ns
I1130 22:47:23.204586 15971 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 22.150405ms
I1130 22:47:23.204653 15971 replica.cpp:715] Persisted action at 0
I1130 22:47:23.205488 15976 replica.cpp:694] Replica received learned notice for position 0 from @0.0.0.0:0
I1130 22:47:23.215785 15976 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 9.991989ms
I1130 22:47:23.215850 15976 replica.cpp:715] Persisted action at 0
I1130 22:47:23.215875 15976 replica.cpp:700] Replica learned NOP action at position 0
I1130 22:47:23.216578 15972 log.cpp:677] Writer started with ending position 0
I1130 22:47:23.218617 15969 leveldb.cpp:438] Reading position from leveldb took 34179ns
I1130 22:47:23.221410 15973 registrar.cpp:342] Successfully fetched the registry (0B) in 92.845824ms
I1130 22:47:23.221622 15973 registrar.cpp:441] Applied 1 operations in 54823ns; attempting to update the 'registry'
I1130 22:47:23.223924 15976 log.cpp:685] Attempting to append 158 bytes to the log
I1130 22:47:23.224053 15975 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 1
I1130 22:47:23.225028 15971 replica.cpp:540] Replica received write request for position 1 from (43)@127.0.1.1:51177
I1130 22:47:23.240402 15971 leveldb.cpp:343] Persisting action (177 bytes) to leveldb took 15.317491ms
I1130 22:47:23.240471 15971 replica.cpp:715] Persisted action at 1
I1130 22:47:23.241024 15974 replica.cpp:694] Replica received learned notice for position 1 from @0.0.0.0:0
I1130 22:47:23.257127 15974 leveldb.cpp:343] Persisting action (179 bytes) to leveldb took 15.979931ms
I1130 22:47:23.257261 15974 replica.cpp:715] Persisted action at 1
I1130 22:47:23.257338 15974 replica.cpp:700] Replica learned APPEND action at position 1
I1130 22:47:23.258764 15974 registrar.cpp:486] Successfully updated the 'registry' in 37.036032ms
I1130 22:47:23.259004 15974 registrar.cpp:372] Successfully recovered registrar
I1130 22:47:23.259340 15971 log.cpp:704] Attempting to truncate the log to 1
I1130 22:47:23.259637 15976 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 2
I1130 22:47:23.260030 15972 master.cpp:1416] Recovered 0 slaves from the Registry (120B) ; allowing 10mins for slaves to re-register
I1130 22:47:23.260810 15975 replica.cpp:540] Replica received write request for position 2 from (44)@127.0.1.1:51177
I1130 22:47:23.273692 15975 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 12.840856ms
I1130 22:47:23.273748 15975 replica.cpp:715] Persisted action at 2
I1130 22:47:23.274708 15972 replica.cpp:694] Replica received learned notice for position 2 from @0.0.0.0:0
I1130 22:47:23.291226 15972 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 16.42524ms
I1130 22:47:23.291388 15972 leveldb.cpp:401] Deleting ~1 keys from leveldb took 31496ns
I1130 22:47:23.291468 15972 replica.cpp:715] Persisted action at 2
I1130 22:47:23.291543 15972 replica.cpp:700] Replica learned TRUNCATE action at position 2
Shutting down
Sending SIGTERM to process tree at pid 15945
Killing the following process trees:
[

]
Shutting down
Sending SIGTERM to process tree at pid 15946
Shutting down
Sending SIGTERM to process tree at pid 15948
Shutting down
Sending SIGTERM to process tree at pid 15947
Killing the following process trees:
[

]
Shutting down
Sending SIGTERM to process tree at pid 15949
Killing the following process trees:
[

]
Killing the following process trees:
[

]
Killing the following process trees:
[

]
I1130 22:47:23.868094 15975 master.cpp:3859] Registering slave at slave(2)@127.0.1.1:51177 (ubuntu14) with id edbdc635-adf5-40f6-bc27-15b19e69f8cd-S0
I1130 22:47:23.868902 15975 registrar.cpp:441] Applied 1 operations in 83801ns; attempting to update the 'registry'
I1130 22:47:23.871341 15975 log.cpp:685] Attempting to append 324 bytes to the log
I1130 22:47:23.871618 15976 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 3
I1130 22:47:23.872496 15970 replica.cpp:540] Replica received write request for position 3 from (45)@127.0.1.1:51177
I1130 22:47:23.890398 15970 leveldb.cpp:343] Persisting action (343 bytes) to leveldb took 17.833464ms
I1130 22:47:23.890465 15970 replica.cpp:715] Persisted action at 3
I1130 22:47:23.891302 15969 replica.cpp:694] Replica received learned notice for position 3 from @0.0.0.0:0
I1130 22:47:23.916492 15969 leveldb.cpp:343] Persisting action (345 bytes) to leveldb took 25.076422ms
I1130 22:47:23.916555 15969 replica.cpp:715] Persisted action at 3
I1130 22:47:23.916581 15969 replica.cpp:700] Replica learned APPEND action at position 3
I1130 22:47:23.917907 15969 registrar.cpp:486] Successfully updated the 'registry' in 48.825088ms
I1130 22:47:23.918125 15971 log.cpp:704] Attempting to truncate the log to 3
I1130 22:47:23.918313 15969 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 4
I1130 22:47:23.919076 15971 replica.cpp:540] Replica received write request for position 4 from (46)@127.0.1.1:51177
I1130 22:47:23.919075 15972 master.cpp:3927] Registered slave edbdc635-adf5-40f6-bc27-15b19e69f8cd-S0 at slave(2)@127.0.1.1:51177 (ubuntu14) with cpus(*):2; mem(*):10240; disk(*):34068; ports(*):[31000-32000]
I1130 22:47:23.919356 15969 hierarchical.cpp:344] Added slave edbdc635-adf5-40f6-bc27-15b19e69f8cd-S0 (ubuntu14) with cpus(*):2; mem(*):10240; disk(*):34068; ports(*):[31000-32000] (allocated: )
I1130 22:47:23.919670 15974 slave.cpp:904] Registered with master master@127.0.1.1:51177; given slave ID edbdc635-adf5-40f6-bc27-15b19e69f8cd-S0
I1130 22:47:23.919860 15975 status_update_manager.cpp:183] Resuming sending status updates
I1130 22:47:23.920243 15974 slave.cpp:963] Forwarding total oversubscribed resources
I1130 22:47:23.920426 15974 master.cpp:4269] Received update of slave edbdc635-adf5-40f6-bc27-15b19e69f8cd-S0 at slave(2)@127.0.1.1:51177 (ubuntu14) with total oversubscribed resources
I1130 22:47:23.920732 15974 hierarchical.cpp:400] Slave edbdc635-adf5-40f6-bc27-15b19e69f8cd-S0 (ubuntu14) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):34068; ports(*):[31000-32000], allocated: )
I1130 22:47:23.930485 15971 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 11.225812ms
I1130 22:47:23.930588 15971 replica.cpp:715] Persisted action at 4
I1130 22:47:23.931483 15971 replica.cpp:694] Replica received learned notice for position 4 from @0.0.0.0:0
I1130 22:47:23.946909 15971 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 15.365585ms
I1130 22:47:23.947005 15971 leveldb.cpp:401] Deleting ~2 keys from leveldb took 33233ns
I1130 22:47:23.947026 15971 replica.cpp:715] Persisted action at 4
I1130 22:47:23.947062 15971 replica.cpp:700] Replica learned TRUNCATE action at position 4
2015-11-30 22:47:23,976:15722(0x7fac4e652700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:49113] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-30 22:47:24,140:15722(0x7fac84bc4700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:44051] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1130 22:47:24.173096 15970 master.cpp:3859] Registering slave at slave(1)@127.0.1.1:51177 (ubuntu14) with id edbdc635-adf5-40f6-bc27-15b19e69f8cd-S1
I1130 22:47:24.173475 15970 registrar.cpp:441] Applied 1 operations in 60030ns; attempting to update the 'registry'
I1130 22:47:24.174414 15970 log.cpp:685] Attempting to append 486 bytes to the log
I1130 22:47:24.174617 15970 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 5
I1130 22:47:24.175302 15970 replica.cpp:540] Replica received write request for position 5 from (47)@127.0.1.1:51177
I1130 22:47:24.194605 15970 leveldb.cpp:343] Persisting action (505 bytes) to leveldb took 19.220068ms
I1130 22:47:24.194659 15970 replica.cpp:715] Persisted action at 5
I1130 22:47:24.195379 15976 replica.cpp:694] Replica received learned notice for position 5 from @0.0.0.0:0
I1130 22:47:24.224751 15976 leveldb.cpp:343] Persisting action (507 bytes) to leveldb took 29.20734ms
I1130 22:47:24.224807 15976 replica.cpp:715] Persisted action at 5
I1130 22:47:24.224833 15976 replica.cpp:700] Replica learned APPEND action at position 5
I1130 22:47:24.226043 15974 registrar.cpp:486] Successfully updated the 'registry' in 52.468992ms
I1130 22:47:24.226249 15974 log.cpp:704] Attempting to truncate the log to 5
I1130 22:47:24.226369 15974 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 6
I1130 22:47:24.226654 15976 master.cpp:3927] Registered slave edbdc635-adf5-40f6-bc27-15b19e69f8cd-S1 at slave(1)@127.0.1.1:51177 (ubuntu14) with cpus(*):2; mem(*):10240; disk(*):34068; ports(*):[31000-32000]
I1130 22:47:24.226811 15976 slave.cpp:904] Registered with master master@127.0.1.1:51177; given slave ID edbdc635-adf5-40f6-bc27-15b19e69f8cd-S1
I1130 22:47:24.226981 15972 status_update_manager.cpp:183] Resuming sending status updates
I1130 22:47:24.226809 15974 hierarchical.cpp:344] Added slave edbdc635-adf5-40f6-bc27-15b19e69f8cd-S1 (ubuntu14) with cpus(*):2; mem(*):10240; disk(*):34068; ports(*):[31000-32000] (allocated: )
I1130 22:47:24.227459 15971 replica.cpp:540] Replica received write request for position 6 from (48)@127.0.1.1:51177
I1130 22:47:24.227530 15976 slave.cpp:963] Forwarding total oversubscribed resources
I1130 22:47:24.228521 15973 master.cpp:4269] Received update of slave edbdc635-adf5-40f6-bc27-15b19e69f8cd-S1 at slave(1)@127.0.1.1:51177 (ubuntu14) with total oversubscribed resources
I1130 22:47:24.228770 15973 hierarchical.cpp:400] Slave edbdc635-adf5-40f6-bc27-15b19e69f8cd-S1 (ubuntu14) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):34068; ports(*):[31000-32000], allocated: )
I1130 22:47:24.241744 15971 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 13.475397ms
I1130 22:47:24.241809 15971 replica.cpp:715] Persisted action at 6
I1130 22:47:24.242728 15973 replica.cpp:694] Replica received learned notice for position 6 from @0.0.0.0:0
I1130 22:47:24.257985 15973 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 15.20455ms
I1130 22:47:24.258081 15973 leveldb.cpp:401] Deleting ~2 keys from leveldb took 31813ns
I1130 22:47:24.258116 15973 replica.cpp:715] Persisted action at 6
I1130 22:47:24.258139 15973 replica.cpp:700] Replica learned TRUNCATE action at position 6
2015-11-30 22:47:24,387:15722(0x7fac0e7fc700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50280] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-30 22:47:24,666:15722(0x7fac4d650700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:58836] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1130 22:47:25.172317 15969 master.cpp:3859] Registering slave at slave(3)@127.0.1.1:51177 (ubuntu14) with id edbdc635-adf5-40f6-bc27-15b19e69f8cd-S2
I1130 22:47:25.172703 15969 registrar.cpp:441] Applied 1 operations in 59601ns; attempting to update the 'registry'
I1130 22:47:25.174685 15969 log.cpp:685] Attempting to append 648 bytes to the log
I1130 22:47:25.174819 15976 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 7
I1130 22:47:25.175757 15972 replica.cpp:540] Replica received write request for position 7 from (49)@127.0.1.1:51177
I1130 22:47:25.202685 15972 leveldb.cpp:343] Persisting action (667 bytes) to leveldb took 26.833493ms
I1130 22:47:25.202808 15972 replica.cpp:715] Persisted action at 7
I1130 22:47:25.203387 15972 replica.cpp:694] Replica received learned notice for position 7 from @0.0.0.0:0
I1130 22:47:25.221983 15972 leveldb.cpp:343] Persisting action (669 bytes) to leveldb took 18.510109ms
I1130 22:47:25.222126 15972 replica.cpp:715] Persisted action at 7
I1130 22:47:25.222199 15972 replica.cpp:700] Replica learned APPEND action at position 7
I1130 22:47:25.223793 15969 registrar.cpp:486] Successfully updated the 'registry' in 50.982144ms
I1130 22:47:25.224144 15971 log.cpp:704] Attempting to truncate the log to 7
I1130 22:47:25.224392 15971 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 8
I1130 22:47:25.224444 15969 master.cpp:3927] Registered slave edbdc635-adf5-40f6-bc27-15b19e69f8cd-S2 at slave(3)@127.0.1.1:51177 (ubuntu14) with cpus(*):2; mem(*):10240; disk(*):34068; ports(*):[31000-32000]
I1130 22:47:25.224606 15974 hierarchical.cpp:344] Added slave edbdc635-adf5-40f6-bc27-15b19e69f8cd-S2 (ubuntu14) with cpus(*):2; mem(*):10240; disk(*):34068; ports(*):[31000-32000] (allocated: )
I1130 22:47:25.224686 15972 slave.cpp:904] Registered with master master@127.0.1.1:51177; given slave ID edbdc635-adf5-40f6-bc27-15b19e69f8cd-S2
I1130 22:47:25.225249 15976 status_update_manager.cpp:183] Resuming sending status updates
I1130 22:47:25.225592 15972 slave.cpp:963] Forwarding total oversubscribed resources
I1130 22:47:25.226097 15969 master.cpp:4269] Received update of slave edbdc635-adf5-40f6-bc27-15b19e69f8cd-S2 at slave(3)@127.0.1.1:51177 (ubuntu14) with total oversubscribed resources
I1130 22:47:25.226299 15971 replica.cpp:540] Replica received write request for position 8 from (50)@127.0.1.1:51177
I1130 22:47:25.226444 15974 hierarchical.cpp:400] Slave edbdc635-adf5-40f6-bc27-15b19e69f8cd-S2 (ubuntu14) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):34068; ports(*):[31000-32000], allocated: )
I1130 22:47:25.229393 15971 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 3.030976ms
I1130 22:47:25.229516 15971 replica.cpp:715] Persisted action at 8
I1130 22:47:25.230015 15971 replica.cpp:694] Replica received learned notice for position 8 from @0.0.0.0:0
I1130 22:47:25.245934 15971 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 15.818898ms
I1130 22:47:25.246071 15971 leveldb.cpp:401] Deleting ~2 keys from leveldb took 32062ns
I1130 22:47:25.246158 15971 replica.cpp:715] Persisted action at 8
I1130 22:47:25.246229 15971 replica.cpp:700] Replica learned TRUNCATE action at position 8
2015-11-30 22:47:25,512:15722(0x7fac4f257700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:37362] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-30 22:47:25,546:15722(0x7fac0effd700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 12ms
2015-11-30 22:47:25,547:15722(0x7fac0effd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:38547] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-30 22:47:25,608:15722(0x7fac853c5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51060] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-30 22:47:26,089:15722(0x7fac1ffff700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36764] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-30 22:47:26,480:15722(0x7fac4de51700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:59080] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-30 22:47:27,314:15722(0x7fac4e652700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:49113] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-30 22:47:27,477:15722(0x7fac84bc4700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:44051] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-30 22:47:27,725:15722(0x7fac0e7fc700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:50280] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-30 22:47:28,006:15722(0x7fac4d650700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:58836] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-30 22:47:28,862:15722(0x7fac4f257700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 16ms
{noformat} ","01/Dec/15 14:12;bbannier;Clang address sanitizer reports use-after-free errors for this test which appear to come from the libevent bindings; it attached a log. It might be a good idea to address that issue first.","20/Jan/16 00:36;vinodkone;commit 147895b4bd6c421ac15db043b8d243c07e44fd7c
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Tue Jan 19 16:05:59 2016 -0800

    Temporarily disabled EventCallFramework test due to MESOS-3273.
","06/Feb/16 17:54;anandmazumdar;Patch: https://reviews.apache.org/r/43286/","08/Feb/16 23:34;vinodkone;commit 67017f136b9820c493b9d861ccb7fc9e463dfa92
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Mon Feb 8 15:33:25 2016 -0800

    Re-enabled test ExamplesTest.EventCallFramework.
    
    Review: https://reviews.apache.org/r/43288/

commit 5e0f787fee8ad810ed6580cecdc2dc92644944ed
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Mon Feb 8 15:33:17 2016 -0800

    Replaced naked pointer with Owned.
    
    This trivial change replaces the old C style pointer with `process::Owned`.
    
    Review: https://reviews.apache.org/r/43287/

commit d68d3429fcfd351cd3f0907bc798bb3139ed0903
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Mon Feb 8 15:33:05 2016 -0800

    Fixed flakiness in ExamplesTest.EventCallFramework.
    
    The root cause is similar to r43285.
    
    Review: https://reviews.apache.org/r/43286/
",,,,
HTTP requests with nested path are not properly handled by libprocess,MESOS-3237,12853056,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,vinodkone,vinodkone,vinodkone,07/Aug/15 21:31,11/Aug/15 21:29,29/Oct/20 16:32,11/Aug/15 21:29,,,,,,,,,0.24.0,,,,,,,,,,,0,,,,,,,,,"For example, if master adds a route ""/api/v1/scheduler"",  a handler named ""api/v1/scheduler"" is added to 'master' libprocess.

But when a request is posted to the above path, process::visit() looks for a http handler named ""api"" instead of ""api/v1/scheduler"".

Ideally libprocess should look for handlers in the following preference order:

""api/v1/scheduler""  --> ""api/v1"" --> ""api""

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2288,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 11 21:29:45 UTC 2015,,,,,,,"0|i2ihhr:",9223372036854775807,,,,,bmahler,,,,,,Twitter Mesos Q3 Sprint 3,,,,,,,,,,,2.0,,,,,,,,,,,"07/Aug/15 23:44;vinodkone;https://reviews.apache.org/r/37240/","11/Aug/15 21:29;vinodkone;commit cdcaa4823a34b8daccf229db7fef77f3732259f4
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Fri Aug 7 14:35:05 2015 -0700

    Renamed master's ""/call"" endpoint to ""/api/v1/scheduler"".
    
    Review: https://reviews.apache.org/r/37241

commit 10cf391d3a24ef8a3dc34804e5a26812ab64e02e
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Fri Aug 7 16:28:36 2015 -0700

    Added support to handle queries to nested HTTP paths.
    
    Review: https://reviews.apache.org/r/37240
",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix master metrics for scheduler calls,MESOS-3195,12851210,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,vinodkone,vinodkone,vinodkone,03/Aug/15 21:10,09/Aug/15 02:39,29/Oct/20 16:32,09/Aug/15 02:39,,,,,,,,,0.24.0,,,,,,,,,,,0,,,,,,,,,"Currently the master increments metrics for old style messages from the driver but not when it receives Calls. Since the driver is now sending Calls, master should update metrics correctly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2288,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 09 02:39:56 UTC 2015,,,,,,,"0|i2ia9r:",9223372036854775807,,,,,,,,,,,Twitter Mesos Q3 Sprint 3,,,,,,,,,,,3.0,,,,,,,,,,,"08/Aug/15 00:54;vinodkone;https://reviews.apache.org/r/37244/","09/Aug/15 02:39;vinodkone;commit c9a21892134cb8e5aae96a15bf33070426422475
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Sat Aug 8 08:19:46 2015 -0700

    Fixed scheduler message metrics to work with scheduler Calls.
    
    Review: https://reviews.apache.org/r/37244

commit c5ea91ee1260f5a58abd1e5f40c23dbc7f92403f
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Sat Aug 8 08:19:28 2015 -0700

    Added Master::teardown() to handle TEARDOWN call.
    
    Review: https://reviews.apache.org/r/37254
",,,,,,,,,,,,,,,,,,,,,,,,,,
ContainerInfo::Image::AppC::id should be optional,MESOS-3192,12851191,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,xujyan,xujyan,xujyan,03/Aug/15 19:47,13/Aug/15 22:38,29/Oct/20 16:32,13/Aug/15 22:38,,,,,,,,,0.24.0,,,,,,,,,,,0,twitter,,,,,,,,"As I commented here: https://reviews.apache.org/r/34136/

Currently ContainerInfo::Image::Appc is defined as the following

{noformat:title=}
    message AppC {
      required string name = 1;
      required string id = 2;
      optional Labels labels = 3;
    }
{noformat}

In which the {{id}} is a required field. When users specify the image in tasks they likely will not use an image id (much like when you use docker or rkt to launch containers, you often use {{ubuntu}} or {{ubuntu:latest}} and seldom a SHA512 ID) and we should change it to be optional.

The motivating scenario is that: if the frameworks in the Mesos use something like {{image=ubuntu:14.04""}} to run a task and {{image=ubuntu}} defaults to {{image=ubuntu:latest}}, the operator can swap the latest version for all new tasks requesting {{image=ubuntu}}. If they allow users to specify {{image=ubuntu:live}}, they can swap the live version under the covers as well. This allows the operator to release important image updates (e.g., security patches) and have it picked up by new tasks in the cluster without asking the users to update their job/task configs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-08-13 22:38:35.605,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 13 22:38:35 UTC 2015,,,,,,,"0|i2ia5j:",9223372036854775807,,,,,,,,,,,Twitter Mesos Q3 Sprint 3,,,,,,,,,,,1.0,,,,,,,,,,,"10/Aug/15 18:34;xujyan;https://reviews.apache.org/r/37307/","13/Aug/15 22:38;jieyu;commit e90c4485746c0641c6ecf34d26011c79998840a0
Author: Jiang Yan Xu <yan@jxu.me>
Date:   Mon Aug 10 10:51:10 2015 -0700

    Changed Image::AppC::id from required to optional.
    
    Review: https://reviews.apache.org/r/37307",,,,,,,,,,,,,,,,,,,,,,,,,,
Perform a self bind mount of rootfs itself in fs::chroot::enter.,MESOS-3178,12850432,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,jieyu,jieyu,30/Jul/15 21:59,03/Aug/15 18:01,29/Oct/20 16:32,03/Aug/15 18:01,,,,,,,,,0.24.0,,,,,,,,,,,0,,,,,,,,,"Syscall 'pivot_root' requires that the old and the new root are not in the same filesystem. Otherwise, the user will receive a ""Device or resource busy"" error.

Currently, we rely on the provisioner to prepare the rootfs and do proper bind mount if needed so that pivot_root can succeed. The drawback of this approach is that it potentially pollutes the host mount table which requires cleanup logics.

For instance, in the test, we create a test rootfs by copying the host files. We need to do a self bind mount so that we can pivot_root on it. That pollute the host mount table and it might leak mounts if test crashes before we do the lazy umount:
https://github.com/apache/mesos/blob/master/src/tests/containerizer/launch_tests.cpp#L96-L102

What I propose is that we always perform a recursive self bind mount of rootfs itself in fs::chroot::enter (after enter the new mount namespace). Seems that this is also done in libcontainer:
https://github.com/opencontainers/runc/blob/master/libcontainer/rootfs_linux.go#L402",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2386,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 03 18:01:42 UTC 2015,,,,,,,"0|i2i5jj:",9223372036854775807,,,,,,,,,,,Twitter Mesos Q3 Sprint 2,,,,,,,,,,,2.0,,0.24.0,,,,,,,,,"30/Jul/15 22:11;jieyu;https://reviews.apache.org/r/36954/","03/Aug/15 18:01;jieyu;commit 2c888f1999fe2c3aee8fe03fa2780f4269bfa27c
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Thu Jul 30 15:01:47 2015 -0700

    Performed a self bind mount of rootfs itself in fs::chroot::enter().
    
    Review: https://reviews.apache.org/r/36954",,,,,,,,,,,,,,,,,,,,,,,,,,
Design doc for docker image registry client,MESOS-3166,12849684,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jojy,jojy,jojy,28/Jul/15 17:46,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,,,,,,,containerization,,,,,0,mesosphere,,,,,,,,Create design document for the docker registry Authenticator component so that we have a baseline for the implementation. ,linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-3213,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2840,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 17 17:53:05 UTC 2015,,,,,,,"0|i0016f:",9223372036854775807,,,,,tnachen,,,,,,Mesosphere Sprint 15,Mesosphere Sprint 16,,,,,,,,,,3.0,,,,,,,,,,,"17/Aug/15 17:53;jojy;https://docs.google.com/document/d/1kE-HXPQl4lQgamPIiaD4Ytdr-N4HeQc4fnE93WHR4X4/edit#",,,,,,,,,,,,,,,,,,,,,,,,,,,
Port bootstrap to CMake,MESOS-3134,12848456,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hausdorff,hausdorff,hausdorff,23/Jul/15 05:33,29/Apr/19 09:26,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,0.27.0,,,,,,cmake,,,,,0,build,cmake,mesosphere,,,,,,"Bootstrap does a lot of significant things, like setting up the git commit hooks. We will want something like bootstrap to run also on systems that don't have bash -- ideally this should just run in CMake itself.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-09 23:11:54.694,,,false,MESOS-898,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 09 23:12:40 UTC 2015,,,,,,,"0|i2htlj:",9223372036854775807,,,,,jvanremoortere,,,,,,Mesosphere Sprint 24,,,,,,,,,,,5.0,,0.27.0,,,,,,,,,"09/Dec/15 23:11;jvanremoortere;https://reviews.apache.org/r/40131/
https://reviews.apache.org/r/40195/","09/Dec/15 23:12;jvanremoortere;{code}
commit 46d559fe5047c048d4f25574dcd13c5ba35c3279
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Wed Dec 9 15:02:29 2015 -0800

    Changed commit hook linting to ignore empty diffs.
    
    On Windows, if you attempt to commit an empty changeset, the commit
    hooks will attempt to lint the entire repository. This is because we
    pass blank arguments to the call to `xargs` that kicks off the C++
    linter (e.g., in `support/pre-commit`). In the git bash, the default
    behavior of blank arguments is *not* to ignore them, as it is on certain
    other platforms. Note that the `-r` flag is provided to avoid this
    behavior, but it is only available on some platforms, and hence is
    inadmissable here.
    
    Hence, our solution is to check if the results of `git diff` are empty,
    and only call `mesos-style.py` if not.
    
    Review: https://reviews.apache.org/r/40195

commit 56a814862692360fd0cb8107730750ac592b2f99
Author: Alex Clemmer <clemmer.alexander@gmail.com>
Date:   Wed Dec 9 15:00:31 2015 -0800

    Windows: Added `bootstrap.bat`.
    
    Review: https://reviews.apache.org/r/40131
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,
Updating persistent volumes after slave restart is problematic.,MESOS-3124,12846721,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,jieyu,jieyu,22/Jul/15 00:07,22/Jul/15 23:02,29/Oct/20 16:32,22/Jul/15 23:02,0.23.0,,,,,,,,0.24.0,,,,,,,,,,,0,,,,,,,,,"Just realize that while reviewing https://reviews.apache.org/r/34135

Since we don't checkpoint 'resources' in Mesos containerizer, when slave restarts and recovers, the 'resources' in Container struct will be empty, but there are symlinks exists in the sandbox.

We'll end up with trying to create already exist symlinks (and fail). I think we should ignore the creation if it already exists.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-1554,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 22 23:01:54 UTC 2015,,,,,,,"0|i2hiy7:",9223372036854775807,,,,,,,,,,,Twitter Mesos Q3 Sprint 2,,,,,,,,,,,3.0,,0.24.0,,,,,,,,,"22/Jul/15 07:00;jieyu;https://reviews.apache.org/r/36683/
https://reviews.apache.org/r/36684/","22/Jul/15 23:01;jieyu;commit a0304ca5ca888e4ecd3d83422983aed1ed295739
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Tue Jul 21 23:00:16 2015 -0700

    Added a persistent volume test for slave recovery.
    
    Review: https://reviews.apache.org/r/36683

commit 7f29a72602ad96861f1bb80cce466a82fa0e59e4
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Tue Jul 21 23:42:19 2015 -0700

    Fixed a bug related to persistent volumes during slave recovery.
    
    Review: https://reviews.apache.org/r/36684",,,,,,,,,,,,,,,,,,,,,,,,,,
Always disable SSLV2,MESOS-3121,12846657,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jvanremoortere,jvanremoortere,jvanremoortere,21/Jul/15 19:55,26/Nov/18 12:20,29/Oct/20 16:32,26/Nov/18 12:20,,,,,,,,,0.24.0,,,,,,libprocess,,,,,0,mesosphere,ssl,,,,,,,"The SSL protocol mismatch tests are failing on Centos7 when matching SSLV2 with SSLV2. Since this version of the protocol is highly discouraged anyway, let's disable it completely unless requested otherwise.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-07-27 18:40:56.797,,,false,MESOS-910,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 27 18:40:56 UTC 2015,,,,,,,"0|i2hik7:",9223372036854775807,,,,,benjaminhindman,,,,,,Mesosphere Sprint 15,,,,,,,,,,,2.0,,,,,,,,,,,"21/Jul/15 21:40;jvanremoortere;https://reviews.apache.org/r/36656/
https://reviews.apache.org/r/36658/","27/Jul/15 18:40;benjaminhindman;commit 74342007eef1b695999f77175ba18720a8163e4a
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Mon Jul 27 11:39:06 2015 -0700

    Updated SSL documentation. Added to home page.
    
    Review: https://reviews.apache.org/r/36658

commit 539124abf5fc2f62b5392b6979eb5ed67db5bece
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Mon Jul 27 11:38:56 2015 -0700

    Removed 'SSL_ENABLE_SSL_V2' configuration flag for SSLv2.
    
    Review: https://reviews.apache.org/r/36656",,,,,,,,,,,,,,,,,,,,,,,,,,
Master doesn't properly handle SUBSCRIBE call,MESOS-3055,12845311,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,vinodkone,vinodkone,vinodkone,15/Jul/15 17:20,16/Jul/15 22:34,29/Oct/20 16:32,16/Jul/15 22:34,0.23.0,,,,,,,,0.23.0,,,,,,,,,,,0,,,,,,,,,"Master::subscribe() incorrectly handles re-registration. It handles it as a registration request (not ""re-registration"") because of a bug in the if loop (should have been !frameworkInfo.has_id()).

{code}
void Master::subscribe(
    const UPID& from,
    const scheduler::Call::Subscribe& subscribe)
{
  const FrameworkInfo& frameworkInfo = subscribe.framework_info();

  // TODO(vinod): Instead of calling '(re-)registerFramework()' from
  // here refactor those methods to call 'subscribe()'.
  if (frameworkInfo.has_id() || frameworkInfo.id() == """") {
    registerFramework(from, frameworkInfo);
  } else {
    reregisterFramework(from, frameworkInfo, subscribe.force());
  }
}

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-2288,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 16 22:34:28 UTC 2015,,,,,,,"0|i2haiv:",9223372036854775807,,,,,bmahler,,,,,,Twitter Mesos Q3 Sprint 1,,,,,,,,,,,2.0,,0.23.0,,,,,,,,,"15/Jul/15 18:40;vinodkone;https://reviews.apache.org/r/36518/","16/Jul/15 22:34;vinodkone;commit bfe6c07b79550bb3d1f2ab6f5344d740e6eb6f60
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Wed Jul 15 11:38:20 2015 -0700

    Fixed a bug in master to properly handle resubscription.
    
    Review: https://reviews.apache.org/r/36518

commit fc85cc512b7767fc2e3921b15cf6602c0c68593e
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Thu Jul 16 14:21:48 2015 -0700

    Made Subscribe.force optional in the Call protobuf.
    
    Review: https://reviews.apache.org/r/36560
",,,,,,,,,,,,,,,,,,,,,,,,,,
SSL tests can fail depending on hostname configuration,MESOS-3005,12843284,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,jvanremoortere,jvanremoortere,jvanremoortere,07/Jul/15 19:38,07/Jul/15 22:58,29/Oct/20 16:32,07/Jul/15 22:09,,,,,,,,,0.23.0,,,,,,libprocess,,,,,0,libevent,mesosphere,ssl,tests,,,,,"Depending on how /etc/hosts is configured, the SSL tests can fail with a bad hostname match for the certificate.
We can avoid this by explicitly matching the hostname for the certificate to the IP that will be used during the test.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-07-07 22:09:46.667,,,false,MESOS-910,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 07 22:58:31 UTC 2015,,,,,,,"0|i2gydj:",9223372036854775807,,,,,benjaminhindman,,,,,,Mesosphere Sprint 14,,,,,,,,,,,3.0,,0.23.0,,,,,,,,,"07/Jul/15 21:02;jvanremoortere;https://reviews.apache.org/r/36246/
https://reviews.apache.org/r/36275/","07/Jul/15 22:09;benjaminhindman;commit a16be70efd36a910481112434e1c9184aa3e3014
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Tue Jul 7 15:05:47 2015 -0700

    MESOS-3005: Fix SSL test hostname dependency.
    
    We generate the certificate using the hostname associated with
    INADDR_LOOPBACK and explicitly bind the test server on
    INADDR_LOOPBACK. This way there is no inconsistency with the hostname
    of the certificate versus the test.
    
    Review: https://reviews.apache.org/r/36275","07/Jul/15 22:58;adam-mesos;commit 13a4e81dfeb9ed5515a80c2071c7fcbb696d3450
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Tue Jul 7 15:53:40 2015 -0700

    SSL: Fix connection issue on OSX.
    
    Using the protocol based size for the `connect()` argument.
    
    Review: https://reviews.apache.org/r/36246",,,,,,,,,,,,,,,,,,,,,,,,,
"Create a ""demo"" HTTP API client",MESOS-3001,12843260,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,marco-mesos,marco-mesos,07/Jul/15 18:20,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.24.0,,,,,,framework,,,,,0,mesosphere,,,,,,,,"We want to create a simple ""demo"" HTTP API Client (in Java, Python or Go) that can serve as an ""example framework"" for people who will want to use the new API for their Frameworks.

The scope should be fairly limited (eg, launching a simple Container task?) but sufficient to exercise most of the new API endpoint messages/capabilities.

Scope: TBD

Non-Goals: 

- create a ""best-of-breed"" Framework to deliver any specific functionality;
- create an Integration Test for the HTTP API.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-08-14 23:34:17.085,,,false,MESOS-2288,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 14 23:34:17 UTC 2015,,,,,,,"0|i0031j:",9223372036854775807,,,,,benjaminhindman,,,,,,,,,,,,,,,,,8.0,,0.24.0,,,,,,,,,"14/Aug/15 23:34;vinodkone;Reference C++ library has landed in the code base.

We've decided to keep other language implementations outside the code base for now.",,,,,,,,,,,,,,,,,,,,,,,,,,,
SSL connection failure causes failed CHECK.,MESOS-2997,12843077,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,jvanremoortere,jvanremoortere,jvanremoortere,07/Jul/15 02:43,08/Jul/15 01:31,29/Oct/20 16:32,07/Jul/15 20:30,,,,,,,,,0.23.0,,,,,,libprocess,,,,,0,libprocess,mesosphere,ssl,,,,,,"{code}
[ RUN      ] SSLTest.BasicSameProcess
F0706 18:32:28.465451 238583808 libevent_ssl_socket.cpp:507] Check failed: 'self->bev' Must be non NULL
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-910,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 07 20:30:36 UTC 2015,,,,,,,"0|i2gx33:",9223372036854775807,,,,,benjaminhindman,,,,,,Mesosphere Sprint 14,,,,,,,,,,,3.0,,0.23.0,,,,,,,,,"07/Jul/15 02:48;jvanremoortere;https://reviews.apache.org/r/36237/","07/Jul/15 20:30;jvanremoortere;commit 4697d8c5cc5b0ff19273a4e7b50462dd7935b037
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Mon Jul 6 20:24:25 2015 -0700

    SSL: defer connect callbacks.
    
    This avoids executing cleanup logic twice.
    
    Review: https://reviews.apache.org/r/36237",,,,,,,,,,,,,,,,,,,,,,,,,,
SSL tests don't work with --gtest_shuffle,MESOS-2975,12841871,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jvanremoortere,jvanremoortere,jvanremoortere,01/Jul/15 02:22,07/May/18 19:06,29/Oct/20 16:32,02/Jul/15 22:12,,,,,,,,,0.23.0,,,,,,libprocess,,,,,0,ssl,testing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-910,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 02 22:12:35 UTC 2015,,,,,,,"0|i2gpu7:",9223372036854775807,,,,,benjaminhindman,,,,,,Mesosphere Sprint 13,,,,,,,,,,,3.0,,0.23.0,,,,,,,,,"02/Jul/15 22:12;jvanremoortere;commit a42f871e6172d69adc2193850d1f3fac883e2fc6
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Thu Jul 2 14:57:57 2015 -0700

    Fix race in SSL 'Socket::connect()'.
    
    Assign callback and run 'bufferevent_socket_connect' from in event
    loop.
    
    Review: https://reviews.apache.org/r/36148

commit b9da875292270482d804056d0f4b64872e25911c
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Thu Jul 2 14:55:17 2015 -0700

    Disable 'epoll' in libevent implementation.
    
    There are currently issues when using 'epoll' with ssl support. We can
    re-enable this when these are resolved.
    
    Review: https://reviews.apache.org/r/36147",,,,,,,,,,,,,,,,,,,,,,,,,,,
SSL tests don't work with --gtest_repeat,MESOS-2973,12841868,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jvanremoortere,jvanremoortere,jvanremoortere,01/Jul/15 01:51,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.23.0,,,,,,libprocess,,,,,0,mesosphere,ssl,testing,,,,,,"commit bfa89f22e9d6a3f365113b32ee1cac5208a0456f
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Wed Jul 1 16:16:52 2015 -0700

    MESOS-2973: Allow SSL tests to run using gtest_repeat.
    
    The SSL ctx object carried some settings between reinitialize()
    calls. Re-construct the object to avoid this state transition.
    
    Review: https://reviews.apache.org/r/36074",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-07-24 01:10:13.667,,,false,MESOS-910,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 24 01:10:13 UTC 2015,,,,,,,"0|i2gptj:",9223372036854775807,,,,,benjaminhindman,,,,,,Mesosphere Sprint 13,,,,,,,,,,,3.0,,0.23.0,,,,,,,,,"01/Jul/15 01:56;jvanremoortere;https://reviews.apache.org/r/36074/","24/Jul/15 01:10;adam-mesos;Reopened to set Resolution to ""Fixed"".
Resolved by commit listed in Description.",,,,,,,,,,,,,,,,,,,,,,,,,,
mesos fails to compile under mac when libssl and libevent are enabled,MESOS-2943,12840730,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Blocker,Fixed,jvanremoortere,hartem,hartem,26/Jun/15 03:24,07/Jul/15 21:56,29/Oct/20 16:32,07/Jul/15 13:34,0.23.0,,,,,,,,0.23.0,,,,,,libprocess,,,,,1,mesosphere,,,,,,,,"../configure --enable-debug --enable-libevent --enable-ssl && make

produces the following error:

poll.cpp' || echo '../../../3rdparty/libprocess/'`src/libevent_poll.cpp
libtool: compile:  g++ -DPACKAGE_NAME=\""libprocess\"" -DPACKAGE_TARNAME=\""libprocess\"" -DPACKAGE_VERSION=\""0.0.1\"" ""-DPACKAGE_STRING=\""libprocess 0.0.1\"""" -DPACKAGE_BUGREPORT=\""\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""libprocess\"" -DVERSION=\""0.0.1\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_APR_POOLS_H=1 -DHAVE_LIBAPR_1=1 -DHAVE_SVN_VERSION_H=1 -DHAVE_LIBSVN_SUBR_1=1 -DHAVE_SVN_DELTA_H=1 -DHAVE_LIBSVN_DELTA_1=1 -DHAVE_LIBCURL=1 -DHAVE_EVENT2_EVENT_H=1 -DHAVE_LIBEVENT=1 -DHAVE_EVENT2_THREAD_H=1 -DHAVE_LIBEVENT_PTHREADS=1 -DHAVE_OPENSSL_SSL_H=1 -DHAVE_LIBSSL=1 -DHAVE_LIBCRYPTO=1 -DHAVE_EVENT2_BUFFEREVENT_SSL_H=1 -DHAVE_LIBEVENT_OPENSSL=1 -DUSE_SSL_SOCKET=1 -DHAVE_PTHREAD_PRIO_INHERIT=1 -DHAVE_PTHREAD=1 -DHAVE_LIBZ=1 -DHAVE_LIBDL=1 -I. -I../../../3rdparty/libprocess -I../../../3rdparty/libprocess/include -I../../../3rdparty/libprocess/3rdparty/stout/include -I3rdparty/boost-1.53.0 -I3rdparty/libev-4.15 -I3rdparty/picojson-4f93734 -I3rdparty/glog-0.3.3/src -I3rdparty/ry-http-parser-1c3624a -I/usr/local/opt/openssl/include -I/usr/local/opt/libevent/include -I/usr/local/opt/subversion/include/subversion-1 -I/usr/include/apr-1 -I/usr/include/apr-1.0 -g1 -O0 -std=c++11 -stdlib=libc++ -DGTEST_USE_OWN_TR1_TUPLE=1 -MT libprocess_la-libevent_poll.lo -MD -MP -MF .deps/libprocess_la-libevent_poll.Tpo -c ../../../3rdparty/libprocess/src/libevent_poll.cpp  -fno-common -DPIC -o libprocess_la-libevent_poll.o
mv -f .deps/libprocess_la-socket.Tpo .deps/libprocess_la-socket.Plo
mv -f .deps/libprocess_la-subprocess.Tpo .deps/libprocess_la-subprocess.Plo
mv -f .deps/libprocess_la-libevent.Tpo .deps/libprocess_la-libevent.Plo
mv -f .deps/libprocess_la-metrics.Tpo .deps/libprocess_la-metrics.Plo
In file included from ../../../3rdparty/libprocess/src/libevent_ssl_socket.cpp:11:
In file included from ../../../3rdparty/libprocess/include/process/queue.hpp:9:
../../../3rdparty/libprocess/include/process/future.hpp:849:7: error: no viable conversion from 'const process::Future<const process::Future<process::network::Socket> >' to 'const process::network::Socket'
 set(u);
     ^
../../../3rdparty/libprocess/src/libevent_ssl_socket.cpp:769:10: note: in instantiation of function template specialization 'process::Future<process::network::Socket>::Future<process::Future<const process::Future<process::network::Socket> > >' requested here
 return accept_queue.get()
        ^
../../../3rdparty/libprocess/include/process/socket.hpp:21:7: note: candidate constructor (the implicit move constructor) not viable: no known conversion from 'const process::Future<const process::Future<process::network::Socket> >' to
     'process::network::Socket &&' for 1st argument
class Socket
     ^
../../../3rdparty/libprocess/include/process/socket.hpp:21:7: note: candidate constructor (the implicit copy constructor) not viable: no known conversion from 'const process::Future<const process::Future<process::network::Socket> >' to
     'const process::network::Socket &' for 1st argument
class Socket
     ^
../../../3rdparty/libprocess/include/process/future.hpp:411:21: note: passing argument to parameter '_t' here
 bool set(const T& _t);
                   ^
1 error generated.
make[4]: *** [libprocess_la-libevent_ssl_socket.lo] Error 1
make[4]: *** Waiting for unfinished jobs....
mv -f .deps/libprocess_la-libevent_poll.Tpo .deps/libprocess_la-libevent_poll.Plo
mv -f .deps/libprocess_la-openssl.Tpo .deps/libprocess_la-openssl.Plo
mv -f .deps/libprocess_la-process.Tpo .deps/libprocess_la-process.Plo
make[3]: *** [all-recursive] Error 1
make[2]: *** [all-recursive] Error 1
make[1]: *** [all] Error 2
make: *** [all-recursive] Error 1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-2991,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-06-28 13:14:25.344,,,false,MESOS-910,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 07 21:56:24 UTC 2015,,,,,,,"0|i2giuv:",9223372036854775807,,,,,benjaminhindman,,,,,,Mesosphere Sprint 14,,,,,,,,,,,2.0,,0.23.0,,,,,,,,,"28/Jun/15 13:14;haosdent@gmail.com;I build it successfully in my OSX
{code}
../configure --enable-debug --enable-libevent --enable-ssl --disable-python && make -j4
{code}

Seems you not disable-python? [~hartem]","01/Jul/15 07:56;adam-mesos;Did you install libevent and openssl (with recommended versions) as described in https://github.com/apache/mesos/blob/master/docs/mesos-ssl.md#Dependencies ?","02/Jul/15 02:51;xds2000;I came across same error like report.


xiaods at XiaoTommydeMacBook-Pro in ~/Documents/code/mesos/build on master
$ brew install libevent
Warning: libevent-2.0.22 already installed
xiaods at XiaoTommydeMacBook-Pro in ~/Documents/code/mesos/build on master
$ brew install openssl
Warning: openssl-1.0.2 already installed
","06/Jul/15 22:41;adam-mesos;Ping [~jvanremoortere]","07/Jul/15 05:16;jvanremoortere;https://reviews.apache.org/r/36245/","07/Jul/15 11:19;haosdent@gmail.com;Hm, I use g++ to compile it.
{code}
g++ (Homebrew gcc 4.9.1) 4.9.1
Copyright (C) 2014 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
{code}","07/Jul/15 13:34;benjaminhindman;commit 8d4b1d50412993ff63970c493154ae13ebfb6fda
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Tue Jul 7 06:29:09 2015 -0700

    Fix compilation error for clang-3.5 type deduction error.
    
    While we figure out how to avoid this bug in clang-3.5, we can allow
    people to compile by explicitly specifying the return type of the
    lambda.
    
    Review: https://reviews.apache.org/r/36245","07/Jul/15 21:56;benjaminhindman;commit 971583522b3ada19f91d58fd89c0d3d17f5fef34
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Tue Jul 7 14:54:56 2015 -0700

    MESOS-2943: Add comment for explicit return type.
    
    Review: https://reviews.apache.org/r/36267",,,,,,,,,,,,,,,,,,,,
Framework can overcommit oversubscribable resources during master failover.,MESOS-2919,12839993,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,jieyu,jieyu,jieyu,24/Jun/15 00:01,25/Jun/15 17:37,29/Oct/20 16:32,25/Jun/15 17:37,,,,,,,,,0.23.0,,,,,,,,,,,0,twitter,,,,,,,,"This is due to a bug in the hierarchical allocator. Here is the sequence of events:

1) slave uses a fixed resource estimator which advertise 4 revocable cpus
2) a framework A launches a task that uses all the 4 revocable cpus
3) master fails over
4) slave re-registers with the new master, and sends UpdateSlaveMessage with 4 revocable cpus as oversubscribed resources
5) framework A hasn't registered yet, therefore, the slave's available resources will be 4 revocable cpus
6) framework A registered and will receive an additional 4 revocable cpus. So it can launch another task with 4 revocable cpus (that means 8 total!)

The problem is due to the way we calculate 'allocated' resource in allocator when 'updateSlave'. If the framework is not registered, the 'allocation' below is not accurate (check that if block in 'addSlave').

{code}
template <class RoleSorter, class FrameworkSorter>
void
HierarchicalAllocatorProcess<RoleSorter, FrameworkSorter>::updateSlave(
    const SlaveID& slaveId,
    const Resources& oversubscribed)
{
  CHECK(initialized);
  CHECK(slaves.contains(slaveId));

  // Check that all the oversubscribed resources are revocable.
  CHECK_EQ(oversubscribed, oversubscribed.revocable());

  // Update the total resources.

  // First remove the old oversubscribed resources from the total.
  slaves[slaveId].total -= slaves[slaveId].total.revocable();

  // Now add the new estimate of oversubscribed resources.
  slaves[slaveId].total += oversubscribed;

  // Now, update the total resources in the role sorter.
  roleSorter->update(
      slaveId,
      slaves[slaveId].total.unreserved());

  // Calculate the current allocation of oversubscribed resources.
  Resources allocation;
  foreachkey (const std::string& role, roles) {
    allocation += roleSorter->allocation(role, slaveId).revocable();
  }

  // Update the available resources.

  // First remove the old oversubscribed resources from available.
  slaves[slaveId].available -= slaves[slaveId].available.revocable();

  // Now add the new estimate of available oversubscribed resources.
  slaves[slaveId].available += oversubscribed - allocation;

  LOG(INFO) << ""Slave "" << slaveId << "" ("" << slaves[slaveId].hostname
            << "") updated with oversubscribed resources "" << oversubscribed
            << "" (total: "" << slaves[slaveId].total
            << "", available: "" << slaves[slaveId].available << "")"";

  allocate(slaveId);
}

template <class RoleSorter, class FrameworkSorter>
void
HierarchicalAllocatorProcess<RoleSorter, FrameworkSorter>::addSlave(
    const SlaveID& slaveId,
    const SlaveInfo& slaveInfo,
    const Resources& total,
    const hashmap<FrameworkID, Resources>& used)
{
  CHECK(initialized);
  CHECK(!slaves.contains(slaveId));

  roleSorter->add(slaveId, total.unreserved());

  foreachpair (const FrameworkID& frameworkId,
               const Resources& allocated,
               used) {
    if (frameworks.contains(frameworkId)) {
      const std::string& role = frameworks[frameworkId].role;

      // TODO(bmahler): Validate that the reserved resources have the
      // framework's role.

      roleSorter->allocated(role, slaveId, allocated.unreserved());
      frameworkSorters[role]->add(slaveId, allocated);
      frameworkSorters[role]->allocated(
          frameworkId.value(), slaveId, allocated);
    }
  }
  ...
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-354,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 25 17:37:54 UTC 2015,,,,,,,"0|i2ges7:",9223372036854775807,,,,,,,,,,,Twitter Mesos Q2 Sprint 6,,,,,,,,,,,3.0,,0.23.0,,,,,,,,,"24/Jun/15 18:24;jieyu;https://reviews.apache.org/r/35836/","25/Jun/15 17:37;jieyu;commit 1fcb1e447ac52b8baf58dc9c88f186e3dfcaab50
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Wed Jun 24 11:12:21 2015 -0700

    Replaced slave's 'available' with 'allocated' in hierarchical allocator.
    
    Review: https://reviews.apache.org/r/35836",,,,,,,,,,,,,,,,,,,,,,,,,,
Sandbox URL doesn't work in web-ui when using SSL,MESOS-2890,12838918,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,jvanremoortere,jvanremoortere,jvanremoortere,18/Jun/15 20:03,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.23.0,,,,,,webui,,,,,0,mesosphere,sandbox,ssl,webui,,,,,"The links to the sandbox in the web ui don't work when ssl is enabled. 
This can happen if the certificate for the master and the slave do not match. This is a consequence of the redirection that happens when serving files.
The resolution to this is currently to set up your certificates to serve the hostnames of the master and slaves.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-07-06 18:56:50.441,,,false,MESOS-910,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 06 18:56:50 UTC 2015,,,,,,,"0|i2g8an:",9223372036854775807,,,,,benjaminhindman,,,,,,Mesosphere Sprint 13,,,,,,,,,,,3.0,,0.23.0,,,,,,,,,"29/Jun/15 20:45;jvanremoortere;This can happen if the certificate for the master and the slave do not match. This is a consequence of the redirection that happens when serving files.
The resolution to this is currently to set up your certificates to serve the hostnames of the master and slaves.","06/Jul/15 18:56;marco-mesos;Re-opening so we can add the {{Resolution}} field (due to bug caused to workflow change).",,,,,,,,,,,,,,,,,,,,,,,,,,
Add SSL switch to python configuration,MESOS-2889,12838917,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,hartem,jvanremoortere,jvanremoortere,18/Jun/15 20:01,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,0.23.0,,,,,,,,,,,0,mesosphere,,,,,,,,The python egg requires explicit dependencies for SSL. Add these to the python configuration if ssl is enabled.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-06-27 08:59:12.105,,,false,MESOS-910,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 06 18:56:21 UTC 2015,,,,,,,"0|i2g8af:",9223372036854775807,,,,,benjaminhindman,,,,,,Mesosphere Sprint 13,,,,,,,,,,,3.0,,0.23.0,,,,,,,,,"27/Jun/15 08:59;hartem;https://reviews.apache.org/r/35967/","28/Jun/15 23:05;benjaminhindman;commit 14cf8d1ca17bd5a7a2062bb7b610d4749e793f8b
Author: Artem Harutyunyan <artem@mesosphere.io>
Date:   Sun Jun 28 15:57:26 2015 -0700

    Enabled compiling python egg with SSL and libevent.
    
    Review: https://reviews.apache.org/r/35967","06/Jul/15 18:56;marco-mesos;Re-opening so we can add the {{Resolution}} field (due to bug caused to workflow change).",,,,,,,,,,,,,,,,,,,,,,,,,
Convert PortMappingStatistics to use automatic JSON encoding/decoding,MESOS-2874,12838204,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,pbrett,pbrett,pbrett,16/Jun/15 16:00,26/Apr/17 16:54,29/Oct/20 16:32,17/Jun/15 20:50,,,,,,,,,0.23.0,,,,,,containerization,test,,,,0,twitter,,,,,,,,"Simplify PortMappingStatistics by using JSON::Protocol and protobuf::parse to convert ResourceStatistics to/from line format.

This change will simplify the implementation of MESOS-2332.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-06-17 20:50:38.75,,,false,MESOS-1585,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 20:50:38 UTC 2015,,,,,,,"0|i2g3yv:",9223372036854775807,,,,,,,,,,,Twitter Mesos Q2 Sprint 5,,,,,,,,,,,2.0,,,,,,,,,,,"16/Jun/15 20:24;pbrett;https://reviews.apache.org/r/35536/
","17/Jun/15 20:50;jieyu;commit 1fe54856278feafb3caa631895b63c5403b98983
Author: Paul Brett <paul_b@twopensource.com>
Date:   Wed Jun 17 11:28:34 2015 -0700

    Replaced adhoc JSON conversion functions for ResourceStatistics in port
    mapping isolator with the general protocol buffer to JSON converter.
    
    Review: https://reviews.apache.org/r/35536",,,,,,,,,,,,,,,,,,,,,,,,,,
OversubscriptionTest.FixedResourceEstimator is flaky,MESOS-2869,12837598,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,mcypark,mcypark,13/Jun/15 14:17,15/Jun/15 19:34,29/Oct/20 16:32,15/Jun/15 19:34,,,,,,,,,0.23.0,,,,,,,,,,,0,,,,,,,,,"Came up in https://reviews.apache.org/r/35395/

{code}
[ RUN      ] OversubscriptionTest.FixedResourceEstimator
I0613 13:41:02.604904 19367 exec.cpp:132] Version: 0.23.0
I0613 13:41:02.610995 19398 exec.cpp:206] Executor registered on slave 20150613-134102-3142697795-48295-13678-S0
Registered executor on pomona.apache.org
Starting task 7d78a3ef-2de9-46c9-811c-b2c0e2d50578
Forked command at 19410
sh -c 'sleep 1000'
../../src/tests/oversubscription_tests.cpp:579: Failure
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7ffffbc0c4e0, @0x2ade2bffa910 96-byte object <50-3E D7-22 DE-2A 00-00 00-00 00-00 00-00 00-00 D0-C4 00-48 DE-2A 00-00 50-71 AC-01 00-00 00-00 01-00 00-00 02-00 00-00 50-71 AC-01 00-00 00-00 B0-66 00-48 DE-2A 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-2A 00-00 E7-17 A8-BB 0C-5F D5-41 10-31 01-48 DE-2A 00-00 00-00 00-00 4B-03 00-00>)
         Expected: to be called once
           Actual: called twice - over-saturated and active
[  FAILED  ] OversubscriptionTest.FixedResourceEstimator (714 ms)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-06-15 18:48:51.871,,,false,MESOS-354,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 19:34:17 UTC 2015,,,,,,,"0|i2g09r:",9223372036854775807,,,,,,,,,,,Twitter Mesos Q2 Sprint 5,,,,,,,,,,,1.0,,0.23.0,,,,,,,,,"15/Jun/15 18:48;nnielsen;@jieyu: Want to take this on, or can I help?","15/Jun/15 18:49;jieyu;I will try to reproduce. Do you guys have verbose logging for the failure?","15/Jun/15 19:07;jieyu;https://reviews.apache.org/r/35470/","15/Jun/15 19:34;jieyu;commit c397fe2a3b24b5860eb859e5812b115945124cb8
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Mon Jun 15 11:59:38 2015 -0700

    Fixed the flaky oversbuscription tests.
    
    Review: https://reviews.apache.org/r/35470",,,,,,,,,,,,,,,,,,,,,,,,
Slave should send oversubscribed resource information after master failover.,MESOS-2866,12837533,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,bmahler,bmahler,bmahler,12/Jun/15 22:48,15/Jun/15 21:50,29/Oct/20 16:32,15/Jun/15 21:50,,,,,,,,,0.23.0,,,,,,agent,,,,,0,twitter,,,,,,,,"After a master failover, if the total amount of oversubscribed resources does not change, then the slave will not send the UpdateSlave message to the new master. The slave needs to send the information to the new master regardless of this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-354,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 21:50:06 UTC 2015,,,,,,,"0|i2fzvj:",9223372036854775807,,,,,,,,,,,Twitter Mesos Q2 Sprint 5,,,,,,,,,,,3.0,,0.23.0,,,,,,,,,"13/Jun/15 02:05;bmahler;https://reviews.apache.org/r/35410/
https://reviews.apache.org/r/35411/","15/Jun/15 21:50;bmahler;Fix committed:

{noformat}
commit 979a2c5e03a4da69d158391a734b71e9264ebad7
Author: Benjamin Mahler <benjamin.mahler@gmail.com>
Date:   Fri Jun 12 16:44:50 2015 -0700

    Send oversubscribable resources during (re-)registration.

    Review: https://reviews.apache.org/r/35411
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Master crashes when framework changes principal on re-registration,MESOS-2842,12836900,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Critical,Fixed,asekretenko,vinodkone,vinodkone,10/Jun/15 17:48,23/Apr/19 22:11,29/Oct/20 16:32,23/Apr/19 22:10,,,,,,,,,1.9.0,,,,,,,,,,,1,foundations,security,,,,,,,The master should be updated to avoid crashing when a framework re-registers with a different principal.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-02-13 01:45:09.873,,,false,MESOS-703,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 23 22:10:46 UTC 2019,,,,,,,"0|yi0zr1:k",9223372036854775807,,,,,gkleiman,,,,,,Mesos Foundations RI12 Sp 43,Mesos Foundations: RI-13 Sp 44,,,,,,,,,,5.0,,,,,,,,,,,"13/Feb/17 01:45;tillt;This is what this looks like when coming across this issue;

{noformat}
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: I0213 01:38:28.419044  2809 master.cpp:2783] Subscribing framework integration_test with checkpointing enabled and capabilities [ PARTITION_AWARE ]
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: I0213 01:38:28.419072  2809 master.cpp:2861] Updating info for framework 6aec32bf-cd60-4fa1-9992-f35af104f423-0009
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: W0213 01:38:28.419083  2809 master.hpp:2486] Cannot update FrameworkInfo.role to '*' for framework 6aec32bf-cd60-4fa1-9992-f35af104f423-0009. Check MESOS-703
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: W0213 01:38:28.419091  2809 master.hpp:2497] Cannot update FrameworkInfo.principal to 'alice' for framework 6aec32bf-cd60-4fa1-9992-f35af104f423-0009. Check MESOS-703
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: I0213 01:38:28.419111  2809 master.cpp:2874] Framework 6aec32bf-cd60-4fa1-9992-f35af104f423-0009 (integration_test) at scheduler-188c0a58-9b44-4e2b-b133-a7c15b37fc55@127.0.0.1:41805 failed over
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: I0213 01:38:28.419245  2809 hierarchical.cpp:358] Activated framework 6aec32bf-cd60-4fa1-9992-f35af104f423-0009
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: I0213 01:38:28.419543  2809 master.cpp:6664] Sending 1 offers to framework 6aec32bf-cd60-4fa1-9992-f35af104f423-0009 (integration_test) at scheduler-7fff5d25-a121-48bf-8849-1948b161d729@127.0.0.1:46530
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: F0213 01:38:28.426944  2809 master.cpp:1446] Check failed: metrics->frameworks.contains(principal.get())
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: *** Check failure stack trace: ***
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: @     0x7fb678b831ad  google::LogMessage::Fail()
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: @     0x7fb678b84fdd  google::LogMessage::SendToLog()
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: @     0x7fb678b82d9c  google::LogMessage::Flush()
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: @     0x7fb678b858d9  google::LogMessageFatal::~LogMessageFatal()
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: @     0x7fb6780453dd  mesos::internal::master::Master::visit()
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: @     0x7fb678af7ca1  process::ProcessManager::resume()
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: @     0x7fb678b00ba7  _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUt_vEEE6_M_runEv
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: @     0x7fb676f90230  (unknown)
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: @     0x7fb6767aedc5  start_thread
Feb 13 01:38:28 test-277bcd0b-fe0e-468a-a9b5-ee624538ac4b mesos-master: @     0x7fb6764dd73d  __clone
{noformat}

So the framework re-registered using its former framework id but a new principal and role. The result is the above crash on the master.","14/Feb/17 01:25;adam-mesos;Even if we don't support changing a role, it should fail framework registration, not crash the master.
I'm escalating this to Critical, since a misbehaving framework should not be able to crash the master. (or maybe that's a separate issue?)","05/May/17 08:56;BrickXu;We meet the same problem this week, and the logs show us ""Check failed: metrics->frameworks.contains(principal.get())"", I think this issue may be the root cause of our problem.

Our mesos version is 0.28.2","20/Mar/19 10:38;bbannier;We observed the failure reported by [~tillt] in a 1.4.3-based test cluster. While 1.4.3 is EOL, the code doesn't seem to have changed a lot on {{master}}.

What's the plan on fixing this check failure [~tillt]? Can we make a fix independent of whether we implement the feature this ticket is about?

 ","28/Mar/19 20:12;asekretenko;Discussed this with [~greggomann] - we decided that:
  - to have both the ability to update principal and properly working authorization is quite a nontrivial task
  - it is not clear why would such an ability be needed at all

Therefore, I will stick to simply disallowing the framework to change its principal.
 At this point I have a local test that reproduces this issue, debugging that.","28/Mar/19 20:19;vinodkone;Yes, this ticket should just focus on disallowing or ignoring principal changes on re-registration.","29/Mar/19 10:40;asekretenko;The principal is being updated here:
 [https://github.com/apache/mesos/blob/d24b3ad6496cafec8f8ea4b02aa106a30e5c1d75/src/master/master.cpp#L4996]

And there is a comment above which I do not like at all:

// Update the principal mapping for this framework, which is
 // needed to keep the per-principal framework metrics accurate.

I'm wondering if there was a consensus about updating/not updating the principal at the time when that code was written...

Edit -  finally I got it: the purpose of that code is handling the UPID change, not the principal update... and when the framework tries to change principal, this code breaks everything.","11/Apr/19 10:12;asekretenko;Reviews:
https://reviews.apache.org/r/70408/
https://reviews.apache.org/r/70377/
 https://reviews.apache.org/r/70379","23/Apr/19 22:10;gkleiman;{noformat}
commit 89daa08529e85f97acfe02c10b51d8c553a0c225
Author: Andrei Sekretenko <asekretenko@mesosphere.io>
Date:   Tue Apr 23 11:32:48 2019 -0700

    Added validation that the principal stays the same on resubscription.

    Review: https://reviews.apache.org/r/70379/

commit 64d00cdfefc3ea5939efe60eaf6a2df8e7e5f4eb
Author: Andrei Sekretenko <asekretenko@mesosphere.io>
Date:   Tue Apr 23 11:32:42 2019 -0700

    Added tests to check that framework cannot change its principal.

    Review: https://reviews.apache.org/r/70377/

commit 2707eda4fa0bba58db63a9ec59574ac4de970fdc
Author: Andrei Sekretenko <asekretenko@mesosphere.io>
Date:   Tue Apr 23 11:32:31 2019 -0700

    Deduplicated common validation code in Master::subscribe()'s.

    Review: https://reviews.apache.org/r/70408/
{noformat}",,,,,,,,,,,,,,,,,,,
Flaky test: FetcherCacheHttpTest.HttpCachedSerialized,MESOS-2815,12835412,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,Fixed,bernd-mesos,bernd-mesos,bernd-mesos,04/Jun/15 14:11,26/Nov/18 12:23,29/Oct/20 16:32,26/Nov/18 12:23,0.23.0,,,,,,,,0.23.0,,,,,,fetcher,,,,,0,mesosphere,,,,,,,,"FetcherCacheHttpTest.HttpCachedSerialized has been observed to fail (once  so far), but normally works fine. Here is the failure output:

[ RUN      ] FetcherCacheHttpTest.HttpCachedSerialized

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: resourceOffers(0x3cca8e0, @0x2b1053422b20 { 128-byte object <D0-E1 59-49 10-2B 00-00 00-00 00-00 00-00 00-00 10-A1 00-68 10-2B 00-00 A0-DE 00-68 10-2B 00-00 20-DF 00-68 10-2B 00-00 40-9C 00-68 10-2B 00-00 90-2D 00-68 10-2B 00-00 04-00 00-00 04-00 00-00 04-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 10-2B 00-00 00-00 00-00 0F-00 00-00> })
Stack trace:
F0604 13:08:16.377907  6813 fetcher_cache_tests.cpp:354] CHECK_READY(offers): is PENDING Failed to wait for resource offers
*** Check failure stack trace: ***
    @     0x2b10488ff6c0  google::LogMessage::Fail()
    @     0x2b10488ff60c  google::LogMessage::SendToLog()
    @     0x2b10488ff00e  google::LogMessage::Flush()
    @     0x2b1048901f22  google::LogMessageFatal::~LogMessageFatal()
    @           0x9721e4  _CheckFatal::~_CheckFatal()
    @           0xb4da86  mesos::internal::tests::FetcherCacheTest::launchTask()
    @           0xb53f8d  mesos::internal::tests::FetcherCacheHttpTest_HttpCachedSerialized_Test::TestBody()
    @          0x116ac21  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1165e1e  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x114e1df  testing::Test::Run()
    @          0x114e902  testing::TestInfo::Run()
    @          0x114ee8a  testing::TestCase::Run()
    @          0x1153b54  testing::internal::UnitTestImpl::RunAllTests()
    @          0x116ba93  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1166b0f  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1152a60  testing::UnitTest::Run()
    @           0xcbc50f  main
    @     0x2b104af78ec5  (unknown)
    @           0x867559  (unknown)
make[4]: *** [check-local] Aborted
make[4]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src'
make[3]: *** [check-am] Error 2
make[3]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src'
make[2]: *** [check] Error 2
make[2]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src'
make[1]: *** [check-recursive] Error 1
make[1]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build'
make: *** [distcheck] Error 1
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-2829,MESOS-2831,MESOS-2857,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-06-21 06:10:57.845,,,false,MESOS-336,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 21 06:10:57 UTC 2015,,,,,,,"0|i2fmwf:",9223372036854775807,,,,,benjaminhindman,,,,,,,,,,,,,,,,,2.0,,0.23.0,,,,,,,,,"04/Jun/15 14:17;bernd-mesos;Investigating: there may be a race between starting the scheduler driver and awaiting offers. Some refactoring of the fetcher cache test code should prevent this. 

Since all other fetcher cache tests reuse this code, they should also be affected. But this does not seem to be a non-test-code bug.","09/Jun/15 09:31;bernd-mesos;https://reviews.apache.org/r/35247/","21/Jun/15 06:10;adam-mesos;The linked review has been committed.
[~bernd-mesos], is there anything left to do before we can close this/these JIRAs?

commit d2985542a741cfa56663d326c55cf882ac47b683
Author: Bernd Mathiske <bernd@mesosphere.io>
Date:   Thu Jun 11 13:01:46 2015 -0700

    Fixed the race between EXPECT_CALL(resourceOffers, _) and
    driver.start() in fetcher_cache_tests.cpp.
    
    Review: https://reviews.apache.org/r/35247",,,,,,,,,,,,,,,,,,,,,,,,,
Slave should call into resource estimator whenever it wants to forward oversubscribed resources,MESOS-2808,12835201,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,vinodkone,vinodkone,vinodkone,03/Jun/15 20:59,04/Jun/15 00:45,29/Oct/20 16:32,04/Jun/15 00:45,,,,,,,,,0.23.0,,,,,,,,,,,0,,,,,,,,,"Currently, the polling of resource estimator is decoupled from the loop in the slave that forwards oversubscribed resources.

Now that the slave only sends updates when there is a change from the previous estimate, it can just poll the resource estimator whenever it wants to send an estimate. One advantage with this is that if the estimator is slow to respond, the slave doesn't keep forwarding estimates with the stale 'oversubscribable' value causing more revocable tasks to be unintentionally launched.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-354,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 00:45:23 UTC 2015,,,,,,,"0|i2fllz:",9223372036854775807,,,,,,,,,,,Twitter Q2 Sprint 3,,,,,,,,,,,3.0,,,,,,,,,,,"03/Jun/15 23:21;vinodkone;https://reviews.apache.org/r/35038/","04/Jun/15 00:45;vinodkone;commit 100009853959c5b348491c32a7fb0d1913e9e084
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Wed Jun 3 16:18:45 2015 -0700

    Updated slave to query resource estimator whenever it wants to forward an update.
    
    Review: https://reviews.apache.org/r/35038
",,,,,,,,,,,,,,,,,,,,,,,,,,
Allow Resource Estimator to get Resource Usage information.,MESOS-2764,12832281,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,Bartek Plotka,jieyu,jieyu,22/May/15 21:37,05/Jun/15 17:23,29/Oct/20 16:32,05/Jun/15 17:23,,,,,,,,,0.23.0,,,,,,,,,,,0,intel,,,,,,,,"This includes two things:
1) We need to expose ResourceMonitor::Usage so that module writers can access it. We could define a protobuf message for that.
2) We need to allow ResourceEstimator to call 'ResourceMonitor::usages()'. We could either expose the ResourceMonitor, or pass in a lambda to the resources estimator.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-2791,,,,MESOS-2772,MESOS-2773,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-354,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-22 21:37:02.0,,,,,,,"0|i2f4kf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extend queueing discipline wrappers to expose network isolator statistics,MESOS-2750,12831130,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,pbrett,pbrett,pbrett,19/May/15 18:47,26/Apr/17 16:54,29/Oct/20 16:32,04/Jun/15 00:39,,,,,,,,,,,,,,,containerization,,,,,0,twitter,,,,,,,,Export Traffic Control statistics in queueing library to enable reporting out impact of network bandwidth statistics.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-1585,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 20 21:25:54 UTC 2015,,,,,,,"0|i003nz:",9223372036854775807,,,,,,,,,,,Twitter Q2 Sprint 3,,,,,,,,,,,3.0,,,,,,,,,,,"20/May/15 21:25;pbrett;Review: https://reviews.apache.org/r/34428",,,,,,,,,,,,,,,,,,,,,,,,,,,
Exposing Resources along with ResourceStatistics from resource monitor,MESOS-2741,12830279,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Duplicate,haosdent@gmail.com,jieyu,jieyu,15/May/15 19:38,26/Nov/18 12:55,29/Oct/20 16:32,26/Nov/18 12:55,,,,,,,,,,,,,,,,,,,,0,mesosphere,twitter,,,,,,,"Right now, the resource monitor returns a Usage which contains ContainerId, ExecutorInfo and ResourceStatistics. In order for resource estimator/qos controller to calculate usage slack, or tell if a container is using revokable resources or not, we need to expose the Resources that are currently assigned to the container.

This requires us the change the containerizer interface to get the Resources as well while calling 'usage()'.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-2818,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-05-19 16:25:23.437,,,false,MESOS-354,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 22:35:40 UTC 2015,,,,,,,"0|i2esw7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,"19/May/15 16:25;haosdent@gmail.com;`calculate usage slack` or ` calculate usage stack` ","19/May/15 17:28;haosdent@gmail.com;And form implement this issue, how about change the interface in Container from 
{code}
virtual process::Future<ResourceStatistics> usage(
      const ContainerID& containerId) = 0;
{code}

to

{code}
virtual process::Future<ResourceMonitor::Usage> usage(
      const ContainerID& containerId) = 0;
{code}","28/May/15 23:21;nnielsen;Think this ended being duplicated with https://issues.apache.org/jira/browse/MESOS-2772 and https://reviews.apache.org/r/34748/

Thought this was going to track, how to keep track of total resources (executor + task), for the QoS controller to pick up what's BE and what's PR.","07/Jun/15 16:42;haosdent@gmail.com;The patch: https://reviews.apache.org/r/35194/

I add Resource as a new field to ResourceStatistics in protobuf definition. And fill this field in Containerizer::usage .","15/Jun/15 22:35;jieyu;This is done as part of MESOS-2818.

We refactored 'ResourceUsage' to include both 'allocation' and 'statistics' for each executor.",,,,,,,,,,,,,,,,,,,,,,,
Change the interaction between the slave and the resource estimator from polling to pushing ,MESOS-2735,12829982,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Won't Fix,jieyu,jieyu,jieyu,14/May/15 21:01,01/Jun/15 19:36,29/Oct/20 16:32,21/May/15 23:00,,,,,,,,,,,,,,,,,,,,0,twitter,,,,,,,,"This will make the semantics more clear. The resource estimator can control the speed of sending resources estimation to the slave.

To avoid cyclic dependency, slave will register a callback with the resource estimator and the resource estimator will simply invoke that callback when there's a new estimation ready. The callback will be a defer to the slave's main event queue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-05-15 21:04:33.237,,,false,MESOS-354,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 01 19:36:58 UTC 2015,,,,,,,"0|i2er2v:",9223372036854775807,,,,,,,,,,,Twitter Q2 Sprint 3 - 5/11,,,,,,,,,,,3.0,,,,,,,,,,,"15/May/15 21:04;nnielsen;Can you capture some of the recent discussions here? I wanted to understand how providing a callback to update the most recent value was different from having a callback hanging off the future on the estimator::update()","15/May/15 21:23;jieyu;1) This matches the existing allocator interface.
2) Slave no longer needs to configure the polling interval. It's up to the resource estimator to decide when to send estimations.
3) It avoid the potential issue that ""estimator::update()"" is blocking. Slave will hang if ""estimator::update()"" blocks.","15/May/15 23:59;jieyu;https://reviews.apache.org/r/34299/","16/May/15 01:11;nnielsen;Sorry that I am keep banging on this issue, but still not completely convinced this makes a cleaner API.
Hope you can help me understand.

{quote}
1) This matches the existing allocator interface.
{quote}
But doesn't match many other APIs internally - don't feel this is a strong argument.

{quote}
2) Slave no longer needs to configure the polling interval. It's up to the resource estimator to decide when to send estimations.
{quote}
Didn't you agree that it could be done with updates().then(lambda(&Slave::updateOverestimated, lambda::_1))?

{quote}
3) It avoid the potential issue that ""estimator::update()"" is blocking. Slave will hang if ""estimator::update()"" blocks.
{quote}
If the estimator never updates the ""last estimate"" in the slave, is the same effect - no?

Is the problem, that the current design doesn't support the ""multiple firing"" problem, where the estimator updates while the callback is being executed?

[~benjaminhindman] Do you have an opinion on this?
","18/May/15 19:34;jieyu;Sorry that I just notice this reply here. I committed the patch already, but we can certainly revert it if you have strong opinion against it.

> If the estimator never updates the ""last estimate"" in the slave, is the same effect - no?

Not the same effect. The slave won't be blocked at least in the push model, meaning that the slave will still be able to process all messages (e.g., runTask). In the polling model, a bad resource estimator can block slave's event queue.

> Is the problem, that the current design doesn't support the ""multiple firing"" problem, where the estimator updates while the callback is being executed?

Could you please elaborate on the ""multiple firing"" problem? I am curious what example in your mind that makes you think that a push model is hard to use than the polling model.","19/May/15 15:04;benjaminhindman;I'd also like to understand better why to go push instead of pull (poll). One of the advantages that we had discussed in the past was that the pull model enables us to move as fast as we possibly can, rather than just getting a bunch of messages queued up in the slave that we have to process. Even if we want to collect more fine-grained resource estimations a ResourceEstimator could do this and store this information until future polls.","19/May/15 17:35;jieyu;{quote} One of the advantages that we had discussed in the past was that the pull model enables us to move as fast as we possibly can, rather than just getting a bunch of messages queued up in the slave that we have to process. {quote}

I don't think there is a difference in terms of queueing messages. The pull model also queues messages in the slave (e.g., 'estimator->oversubscribed().then(defer(...))' also queues messages in slave's queue).

{quote} Even if we want to collect more fine-grained resource estimations a ResourceEstimator could do this and store this information until future polls. {quote}

I think there's no fundamental difference between the pull and the push model. The are only two subtle differences between the two: 1) the push model makes less assumptions about the slave behavior. 2) the push model is safer in the face of bad behaved resource estimator. Let me elaborate both of them below:

Regarding (1), let's use an example. Say we want to write a resource estimator which sends constant number of cpus (say 2 cpus) every 10 seconds. If we use a push model, we could just follow the [NoopResourceEstimatorProcess|https://github.com/apache/mesos/blob/master/src/slave/resource_estimator.cpp#L52] implementation in the code. Basically, we fork a libprocess and invoke the registered callback every 10 seconds with 2 cpus.

Now, if we use a pull model, we first need to make an assumption that the slave pull the resource estimator as fast as it can without any delay. If there's a delay say 1 second, the resource estimator needs to adjust its internal delay to be 9 seconds so that the total interval between two estimations is 10 seconds apart. When implementing the `Future<Resources> oversubscribed()` interface, the module writer needs to make another assumption about the slave that the slave will not invoke the interface again if the previous estimation is still pending. This is important because otherwise, the module writer needs to maintain a list of Promises (instead of just one). I just feels that there're so many implicit assumptions that the module writer needs to make in a pull model.

Regarding (2), as I already stated in this ticket, since the slave invoked the interface ('oversubscribed()') in its context, the module writer needs to make sure the implementation of the interface does not block, otherwise the slave will hang. An alternative is to use 'async' while invoking the interface in the slave. I just feel this is rather not necessary if we use a push model.","20/May/15 00:42;benjaminhindman;There are definitely differences in message queue behavior, one of which is significantly safer than the other. There are two safety concerns that I can think of, one of which [~jieyu] has addressed here but I'll repeat to be sure I properly understood.

(1) Someone might write a ResourceEstimator that isn't asynchronous, causing the slave to ""block"" while the resource estimator estimates.

(2) The ResourceEstimator might cause a denial of service attack on the slave.

I understand the concern with (1) but I'm not too anxious about it. Why? It should be trivial to make a wrapper module which forces people to implement the ResourceEstimator to be asynchronous, either using `async` like you suggested or implementing a version of ResourceEstimator which wraps an actor (libprocess process). We'll only need to do this once and then other ResourceEstimator implementations can leverage this stuff.

On the other hand, I don't like the behavior of push because of (2). Fundamentally, if the slave can't keep up with the rate at which the ResourceEstimator is pushing then we could create a denial of service issue with the slave, i.e., it takes a long time to process non-ResourceEstimator messages because it's queue is full of just ResourceEstimator messages. I'm more anxious about (2) than (1) because it's harder to find bugs in (2) than with (1) since once you fix (1) it stays fixed forever but any time you updated the algorithm you impact the potential to cause (2).

Now, I acknowledge that implementing this as a pull versus push will make the implementation in the ResourceEstimator slightly more complicated, but not really. In particular, it should be trivial to always use a `Queue` to achieve the push semantics in any ResourceEstimator implementation, while still providing the pull semantics externally. Make sense?

Finally, one of the advantages of the pull model is that it's easier to reason about because we don't have ""anonymous"" lambdas that cause execution in some other random place in the code (i.e., you can easily see in the slave where the future that gets returned from `ResourceEstimator::estimate()` gets handled). In addition, the ResourceEstimator remains ""functional"" in the sense that it just has to return some value (or a future) from it's functions versus invoking some callback that causes something to get run some other place (and in fact, may also block, so isn't it safer for the ResourceEstimator to invoke the callback in it's own `async`?).

The invocation of the `ResourceEstimator::estimate()` followed by the `.then` is a nice pattern that let's us compose with other things as well, which is harder to do with the lambda style callbacks and why we've avoided it where we've been able (in fact, I'm curious which place in the code are you imitating here?).","20/May/15 04:44;vinodkone;Thanks Ben for the comments.

I think the main motivations for the push model were to 1) make the writing of the slave logic (interfacing with estimator) simple and 2) make the writing of estimator module simple.

Originally, with the pull model, it looked like we need to have 2 intervals within the slave: one for slave sending estimates to the master and one for slave getting estimates from the estimator. But if we assume that the estimators will be well behaved then we don't need an interval for the latter.

The other issue, as you discussed in your comment, was about DoS. It *looked* like both the push and pull model had the same scope for DoS on the slave, so we didn't find a compelling a reason to go for pull because push was easier to implement on both sides of the interface. I said *looked*, because after processing your comments, I realized that the DoS behavior is different in push vs pull. In a push model a misbehaving estimator could do head of line blocking of other messages enqueued on the slave's queue, whereas in the pull model head of line blocking is not possible because the next (deferred) pull will be enqueued behind all the other messages.

So, I'm ok going with pull for safety. Also, the composition argument can't be denied.

Btw, the inspiration for the push model came from the allocator (and to a lesser extent Mesos class) which I think is very close to the estimator in terms of interactions. [~jieyu], ok with this?
","20/May/15 19:59;jieyu;Thanks for all the comments!

tl;dr: I am still not fully convinced by the pull model, but I think we should move on and not spending too much time arguing on this. So I'll do the change as [~benjaminhindman] suggested (use 'Future<Resources> oversubscribable()' in ResourceEstimator). [~nnielsen], does this sound good to you?

The following are the reasons why I am still not convinced. Just write them down for future references:

1) Regarding the DoS concern. I think the pull model is vulnerable to that as well. In some sense, it's more vulnerable to that IMO. Why? Imagine that at Twitter, we want to write a constant resource estimator (i.e., returns a fix amount of cpus to the slave when the slave asks about how many resources can be oversubscribed). Now, one natural implementation will be like the following:
{code}
Future<Resources> oversubscribable()
{
  return Resources::parse(""cpus:2"").get();
}
{code}
The estimator writer's thought process could be like: whenever the slave asks me about how many resources can be oversubscribed, I'll tell it it's 2 cpus. Well, this _natural_ implementation will cause a DoS problem for the slave because its queue will be flooded, and it'll flood the logging file as well. I would argue that a push model makes the programmer less likely to make such mistake. Why? The programmer needs to write a tight loop (either synchronous or asynchronous) to flood the slave, and we usually think twice when writing a tight loop, don't we?

2) I liked the composability and and functional arguments Ben made in the ticket. I think the push model mimics the actor style message passing model we also constantly used in our code base. Just imagine replacing the lambda with PID<Slave>, and invoking the callback becomes calling dispatch on the pid. I guess the message passing model is more suitable for entities across process boundaries (e.g., master <-> slave, framework <-> master). Otherwise, we prefer the funcitonal style in general. [~benjaminhindman], is that what you're trying to convey? I think I agree with you on this. Just wondering if in the future, we ever want to move resources estimator to a separate unix process?","21/May/15 19:07;jieyu;OK guys, I reverted the change here:
https://reviews.apache.org/r/34559/","01/Jun/15 19:36;bmahler;{quote}
Finally, one of the advantages of the pull model is that it's easier to reason about because we don't have ""anonymous"" lambdas that cause execution in some other random place in the code (i.e., you can easily see in the slave where the future that gets returned from `ResourceEstimator::estimate()` gets handled). In addition, the ResourceEstimator remains ""functional"" in the sense that it just has to return some value (or a future) from it's functions versus invoking some callback that causes something to get run some other place (and in fact, may also block, so isn't it safer for the ResourceEstimator to invoke the callback in it's own `async`?).

The invocation of the `ResourceEstimator::estimate()` followed by the `.then` is a nice pattern that let's us compose with other things as well, which is harder to do with the lambda style callbacks and why we've avoided it where we've been able (in fact, I'm curious which place in the code are you imitating here?).
{quote}

The anonymous lambdas are because we do not have an abstraction to represent an asychronous stream of results:

{code}
// Returns a stream of the unused resources that can be oversubscribed.
Stream<Resources> unused();
{code}

This allows the caller to hold the stream and do composition on the next() Future result, so we achieve push but we keep the ability to do composition. {{http::Pipe}} is a string-specialized form of this and {{process::Queue<T>}} is an infinite-only version of {{process::Stream<T>}}. An important additional property of {{process::Stream<T>}} is that it should provide ""asynchronous back-pressure"" (e.g. unix pipes provide synchronous back-pressure by blocking writes, however we need an asynchronous mechanism to back-pressure the writes), to control the DoS concerns you guys have mentioned.

Take the allocator for example, it seems non-intuitive if the master had to continually call a method to obtain the next allocation from {{Future<Allocation> allocation()}}. At least, it requires more understanding than what it expressed in the return type. Whereas, if we have a {{Stream<Allocation> initialize()}} or {{Stream<Allocation> allocations()}}, we're clearly capturing the semantics within the return type. Back-pressure is the key to ensure that the master's speed of consumption limits the rate at which the allocator makes allocations. Today, {{process::Queue<T>}} is an equivalent replacement since allocations occur as an infinite stream and there is no back-pressuring.",,,,,,,,,,,,,,,,
Design doc for the Executor HTTP API,MESOS-2708,12828428,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,arojas,arojas,08/May/15 17:28,26/Nov/18 12:21,29/Oct/20 16:32,26/Nov/18 12:21,,,,,,,,,,,,,,,,,,,,0,mesosphere,,,,,,,,"This tracks the design of the Executor HTTP API.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-06-17 21:32:38.504,,,false,MESOS-4793,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 18 23:29:44 UTC 2015,,,,,,,"0|hzzzy6:r",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 17,Mesosphere Sprint 18,Mesosphere Sprint 19,Mesosphere Sprint 20,Mesosphere Sprint 21,,,,,,,2.0,,0.26.0,,,,,,,,,"08/May/15 17:29;arojas;https://docs.google.com/document/d/1dFmTrSZXCo5zj8H8SkJ4HT-V0z2YYnEZVV8Fd_-AupM/edit","17/Jun/15 21:32;nnielsen;[~arojas] Do you need sign off on the design from Vinod in order to close this ticket?","22/Jun/15 19:16;vinodkone;The document is out of date. It still talks about ""/events"" endpoint. [~arojas] or [~anandmazumdar] can you please update the doc?","22/Jun/15 19:33;anandmazumdar;I would take it up. Waiting on [~arojas] to give me ""editing"" permissions on the document.","07/Jul/15 07:38;arojas;Hey [~anandmazumdar], can you take ownership of this entry?","20/Jul/15 17:26;anandmazumdar;My bad, I had missed this comment completely. I would take this up when we start to focus on the JIRA items for Executor HTTP API again.","16/Sep/15 05:00;anandmazumdar;[~vinodkone], Can you take a look at the updated design doc ? I have marked some of the remaining outstanding issues ( couple of them ) via comments that we would like your feedback on. 

Once they are resolved, I would then go ahead and send it to dev@","18/Sep/15 23:29;anandmazumdar;[~vinodkone] I edited the document further based on your feedback. Modified the Agent Recovery section. Added a Backoff Strategies section as we had discussed. Also, modified the name of a few environment variables so that they can be re-used elsewhere in future. Let me know if you have more comments.",,,,,,,,,,,,,,,,,,,,
C++ Scheduler library should send HTTP Calls to master,MESOS-2552,12785715,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,anandmazumdar,vinodkone,vinodkone,25/Mar/15 22:58,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.24.0,,,,,,,,,,,0,mesosphere,,,,,,,,"Once the scheduler library sends Call messages, we should update it to send Calls as HTTP requests to ""/call"" endpoint on master.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-07-20 20:46:34.483,,,false,MESOS-2288,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 14 06:14:40 UTC 2015,,,,,,,"0|i27dav:",9223372036854775807,,,,,bmahler,,,,,,Mesosphere Sprint 16,,,,,,,,,,,3.0,,0.24.0,,,,,,,,,"20/Jul/15 20:46;marco-mesos;Again, should this be in {{Reviewable}}? can you please update on progress?","20/Jul/15 20:50;anandmazumdar;I am pausing the progress on this one. It would be a better idea to resume work on this one once the master can understand calls/send events on the event stream.","11/Aug/15 17:22;anandmazumdar;Review Chain:
https://reviews.apache.org/r/37298
https://reviews.apache.org/r/37300
https://reviews.apache.org/r/37301

Pending:
https://reviews.apache.org/r/37302
https://reviews.apache.org/r/37303
https://reviews.apache.org/r/37304
https://reviews.apache.org/r/37328/
https://reviews.apache.org/r/37335/

","13/Aug/15 06:43;vinodkone;commit 437e7018e184128c9ec7c4113da09bbf9914721f
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Wed Aug 12 23:31:04 2015 -0700

    Removed namespace ambiguity in http_tests.cpp.
    
    Review: https://reviews.apache.org/r/37328

commit 69704a28f115a23c4a8e1119bdda3d452674e679
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Wed Aug 12 23:06:07 2015 -0700

    Updated scheduler library to HTTP.
    
    Review: https://reviews.apache.org/r/37303

commit 138ca6903a3adfcf21783ae613e120b785245304
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Wed Aug 12 23:05:46 2015 -0700

    Deleted old style message handling from the scheduler library.
    
    Review: https://reviews.apache.org/r/37302
","14/Aug/15 06:14;vinodkone;commit 0bdd72ad7e0c0faeba968783d0cfc4d6ecd7c356
Author: Anand Mazumdar <mazumdar.anand@gmail.com>
Date:   Thu Aug 13 23:12:37 2015 -0700

    Fixed EventCallFramework test to not ignore heartbeat events.
    
    Review: https://reviews.apache.org/r/37471

commit 4599f9fe38a74a3fe16d4ab4b83b00b18c305465
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Thu Aug 13 18:49:57 2015 -0700

    Fixed scheduler library to send calls in order.
    
    Review: https://reviews.apache.org/r/37467

commit de8399aeb4d44e518619fd38b41eb20813a1cd5e
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Thu Aug 13 17:15:37 2015 -0700

    Revert ""Revert ""Updated scheduler library to HTTP.""""
    
    This reverts commit ddbd429708f17caca593b4ba52cb10661066a39e.
    
    Review: https://reviews.apache.org/r/37465

commit cbd8b6e2ec8f4f321e111af481964c6138bf085a
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Thu Aug 13 17:15:10 2015 -0700

    Revert ""Revert ""Deleted old style message handling from the scheduler library.""""
    
    This reverts commit 2926208c4108a8467cba00d6d49b549e7286f5a1.
    
    Review: https://reviews.apache.org/r/37464
",,,,,,,,,,,,,,,,,,,,,,,
Change the default leaf qdisc to fq_codel inside containers,MESOS-2514,12783031,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,wangcong,wangcong,wangcong,18/Mar/15 21:35,18/Jun/15 18:15,29/Oct/20 16:32,23/Mar/15 18:36,,,,,,,,,0.23.0,,,,,,,,,,,0,,,,,,,,,"When we enable bandwidth cap, htb is used on egress side inside containers, however, the default leaf qdisc for a htb class is still pfifo_fast, which is known to have buffer bloat. Change the default leaf qdisc to fq_codel too:

`tc qd add dev eth0 parent 1:1 fq_codel`

I can no longer see packet drops after this change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-03-18 22:09:27.354,,,false,MESOS-1585,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 18:36:53 UTC 2015,,,,,,,"0|i26y13:",9223372036854775807,,,,,,,,,,,Twitter Mesos Q1 Sprint 5,,,,,,,,,,,1.0,,,,,,,,,,,"18/Mar/15 22:09;jieyu;Just to understand the problem. Are you saying that pfifo_fast have buffer bloat problem because it's buffer size is small? Or there are some other reasons?","18/Mar/15 22:32;wangcong;pfifo is simply FIFO, and drops the packets at tail. codel measures the delay of a packet, that is the time it sits in the queue. If the delay doesn't meet the target during this interval, the queue could be potentially very long, interval will be shorten and some head packets (instead of tail drops) will be dropped until the target can be met in the next interval. So as its name says, it controls the delay of the packets.
","18/Mar/15 22:59;jieyu;OK, so it still drop packets, but it has a better flow management and packet scheduling algorithm so that a latency sensitive flow will less likely to be affected by other high bandwidth flows. But I am still curious why you ""no longer see packet drops after this change""?","23/Mar/15 18:36;jieyu;commit d82ec92073b0438589e7aa72e608c3dc334a8dd6
Author: Cong Wang <cwang@twopensource.com>
Date:   Mon Mar 23 11:33:09 2015 -0700

    Changed default htb leaf qdisc to fq_codel in port mapping isolator.
    
    Review: https://reviews.apache.org/r/32219",,,,,,,,,,,,,,,,,,,,,,,,
Create synchronous validations for Calls,MESOS-2497,12781901,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,ijimenez,ijimenez,ijimenez,13/Mar/15 19:03,26/Nov/18 12:22,29/Oct/20 16:32,26/Nov/18 12:22,,,,,,,,,0.24.0,,,,,,,,,,,0,HTTP,mesosphere,,,,,,,/call endpoint will return a 202 accepted code but has to do some basic validations before. In case of invalidation it will return a 4xx code. We have to create a mechanism that will validate the 'request' and send back the appropriate code.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-07-29 20:39:12.379,,,false,MESOS-2288,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 13 19:09:42 UTC 2015,,,,,,,"0|i00133:",9223372036854775807,,,,,vinodkone,,,,,,Mesosphere Sprint 15,Mesosphere Sprint 16,,,,,,,,,,8.0,,0.24.0,,,,,,,,,"29/Jul/15 20:39;bmahler;The following pulls out the call validation logic:

{noformat}
commit d5cc1a606ca65821447daef378ee45e0da02864b
Author: Benjamin Mahler <benjamin.mahler@gmail.com>
Date:   Wed Jul 29 12:22:10 2015 -0700

    Pulled out call validation.

    Review: https://reviews.apache.org/r/36919
{noformat}","07/Aug/15 23:46;vinodkone;What's left here?","07/Aug/15 23:53;ijimenez;It's left to add http standard validations once : https://reviews.apache.org/r/36402/ and the dependent patches get merged.
You can see comments of https://reviews.apache.org/r/36037/.","12/Aug/15 18:12;ijimenez;https://reviews.apache.org/r/37403/
https://reviews.apache.org/r/37405/","13/Aug/15 19:01;bmahler;Proper validation of the 'Accept' header and a bug fix:

{noformat}
commit 61f71f05ad2a7e9205565437b3243aa84072bf84
Author: Isabel Jimenez <contact@isabeljimenez.com>
Date:   Thu Aug 13 10:37:19 2015 -0700

    Updated /scheduler endopint to use Request::acceptsMediaType.

    Review: https://reviews.apache.org/r/37403
{noformat}

{noformat}
commit b3c18d6d6179ac34be89545dc3b8a9333c91ebb7
Author: Benjamin Mahler <benjamin.mahler@gmail.com>
Date:   Thu Aug 13 11:43:06 2015 -0700

    Ensure the Content-Type is set for the streaming scheduler endpoint.
{noformat}","13/Aug/15 19:09;bmahler;Only accepting POST requests:

{noformat}
commit dcdc79aa8962b32bf5907e3e40de6aa7681d441e
Author: Isabel Jimenez <contact@isabeljimenez.com>
Date:   Thu Aug 13 12:02:49 2015 -0700

    Only accept POST requests for /scheduler endpoint.

    Review: https://reviews.apache.org/r/37405
{noformat}",,,,,,,,,,,,,,,,,,,,,,
Refactor validators in Master.,MESOS-2305,12771155,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,jieyu,jieyu,29/Jan/15 22:27,02/Jul/15 18:43,29/Oct/20 16:32,03/Feb/15 18:59,,,,,,,,,0.22.0,,,,,,,,,,,0,,,,,,,,,"There are several motivation for this. We are in the process of adding dynamic reservations and persistent volumes support in master. To do that, master needs to validate relevant operations from the framework (See Offer::Operation in mesos.proto). The existing validator style in master is hard to extend, compose and re-use.

Another motivation for this is for unit testing (MESOS-1064). Right now, we write integration tests for those validators which is unfortunate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-1064,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-1554,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 03 18:59:30 UTC 2015,,,,,,,"0|i009vz:",9223372036854775807,,,,,,,,,,,Twitter Mesos Q1 Sprint 1,Twitter Mesos Q1 Sprint 2,,,,,,,,,,3.0,,,,,,,,,,,"03/Feb/15 18:59;jieyu;commit b22d7addbc03dfe4a5aa63a05e4f805b1c15631d
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Jan 30 11:12:13 2015 -0800

    Refactored task/offer/resource valiation in master.
    
    Review: https://reviews.apache.org/r/30459",,,,,,,,,,,,,,,,,,,,,,,,,,,
HookTest.VerifySlaveLaunchExecutorHook is flaky,MESOS-2226,12767833,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Abandoned,karya,vinodkone,vinodkone,15/Jan/15 20:25,23/Sep/15 22:39,29/Oct/20 16:32,06/Jul/15 16:09,0.22.0,,,,,,,,,,,,,,test,,,,,0,flaky,flaky-test,mesosphere,,,,,,"Observed this on internal CI

{code}
[ RUN      ] HookTest.VerifySlaveLaunchExecutorHook
Using temporary directory '/tmp/HookTest_VerifySlaveLaunchExecutorHook_GjBgME'
I0114 18:51:34.659353  4720 leveldb.cpp:176] Opened db in 1.255951ms
I0114 18:51:34.662112  4720 leveldb.cpp:183] Compacted db in 596090ns
I0114 18:51:34.662364  4720 leveldb.cpp:198] Created db iterator in 177877ns
I0114 18:51:34.662719  4720 leveldb.cpp:204] Seeked to beginning of db in 19709ns
I0114 18:51:34.663010  4720 leveldb.cpp:273] Iterated through 0 keys in the db in 18208ns
I0114 18:51:34.663312  4720 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0114 18:51:34.664266  4735 recover.cpp:449] Starting replica recovery
I0114 18:51:34.664908  4735 recover.cpp:475] Replica is in EMPTY status
I0114 18:51:34.667842  4734 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0114 18:51:34.669117  4735 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0114 18:51:34.677913  4735 recover.cpp:566] Updating replica status to STARTING
I0114 18:51:34.683157  4735 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 137939ns
I0114 18:51:34.683507  4735 replica.cpp:323] Persisted replica status to STARTING
I0114 18:51:34.684013  4735 recover.cpp:475] Replica is in STARTING status
I0114 18:51:34.685554  4738 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0114 18:51:34.696512  4736 recover.cpp:195] Received a recover response from a replica in STARTING status
I0114 18:51:34.700552  4735 recover.cpp:566] Updating replica status to VOTING
I0114 18:51:34.701128  4735 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 115624ns
I0114 18:51:34.701478  4735 replica.cpp:323] Persisted replica status to VOTING
I0114 18:51:34.701817  4735 recover.cpp:580] Successfully joined the Paxos group
I0114 18:51:34.702569  4735 recover.cpp:464] Recover process terminated
I0114 18:51:34.716439  4736 master.cpp:262] Master 20150114-185134-2272962752-57018-4720 (fedora-19) started on 192.168.122.135:57018
I0114 18:51:34.716913  4736 master.cpp:308] Master only allowing authenticated frameworks to register
I0114 18:51:34.717136  4736 master.cpp:313] Master only allowing authenticated slaves to register
I0114 18:51:34.717488  4736 credentials.hpp:36] Loading credentials for authentication from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_GjBgME/credentials'
I0114 18:51:34.718077  4736 master.cpp:357] Authorization enabled
I0114 18:51:34.719238  4738 whitelist_watcher.cpp:65] No whitelist given
I0114 18:51:34.719755  4737 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0114 18:51:34.722584  4736 master.cpp:1219] The newly elected leader is master@192.168.122.135:57018 with id 20150114-185134-2272962752-57018-4720
I0114 18:51:34.722865  4736 master.cpp:1232] Elected as the leading master!
I0114 18:51:34.723310  4736 master.cpp:1050] Recovering from registrar
I0114 18:51:34.723760  4734 registrar.cpp:313] Recovering registrar
I0114 18:51:34.725229  4740 log.cpp:660] Attempting to start the writer
I0114 18:51:34.727893  4739 replica.cpp:477] Replica received implicit promise request with proposal 1
I0114 18:51:34.728425  4739 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 114781ns
I0114 18:51:34.728662  4739 replica.cpp:345] Persisted promised to 1
I0114 18:51:34.731271  4741 coordinator.cpp:230] Coordinator attemping to fill missing position
I0114 18:51:34.733223  4734 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0114 18:51:34.734076  4734 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 87441ns
I0114 18:51:34.734441  4734 replica.cpp:679] Persisted action at 0
I0114 18:51:34.740272  4739 replica.cpp:511] Replica received write request for position 0
I0114 18:51:34.740910  4739 leveldb.cpp:438] Reading position from leveldb took 59846ns
I0114 18:51:34.741672  4739 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 189259ns
I0114 18:51:34.741919  4739 replica.cpp:679] Persisted action at 0
I0114 18:51:34.743000  4739 replica.cpp:658] Replica received learned notice for position 0
I0114 18:51:34.746844  4739 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 328487ns
I0114 18:51:34.747118  4739 replica.cpp:679] Persisted action at 0
I0114 18:51:34.747553  4739 replica.cpp:664] Replica learned NOP action at position 0
I0114 18:51:34.751344  4737 log.cpp:676] Writer started with ending position 0
I0114 18:51:34.753504  4734 leveldb.cpp:438] Reading position from leveldb took 61183ns
I0114 18:51:34.762962  4737 registrar.cpp:346] Successfully fetched the registry (0B) in 38.907904ms
I0114 18:51:34.763610  4737 registrar.cpp:445] Applied 1 operations in 67206ns; attempting to update the 'registry'
I0114 18:51:34.766079  4736 log.cpp:684] Attempting to append 130 bytes to the log
I0114 18:51:34.766769  4736 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0114 18:51:34.768215  4741 replica.cpp:511] Replica received write request for position 1
I0114 18:51:34.768759  4741 leveldb.cpp:343] Persisting action (149 bytes) to leveldb took 87970ns
I0114 18:51:34.768995  4741 replica.cpp:679] Persisted action at 1
I0114 18:51:34.770691  4736 replica.cpp:658] Replica received learned notice for position 1
I0114 18:51:34.771273  4736 leveldb.cpp:343] Persisting action (151 bytes) to leveldb took 83590ns
I0114 18:51:34.771579  4736 replica.cpp:679] Persisted action at 1
I0114 18:51:34.771917  4736 replica.cpp:664] Replica learned APPEND action at position 1
I0114 18:51:34.773252  4738 log.cpp:703] Attempting to truncate the log to 1
I0114 18:51:34.773756  4735 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0114 18:51:34.775552  4736 replica.cpp:511] Replica received write request for position 2
I0114 18:51:34.775846  4736 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 71503ns
I0114 18:51:34.776695  4736 replica.cpp:679] Persisted action at 2
I0114 18:51:34.785259  4739 replica.cpp:658] Replica received learned notice for position 2
I0114 18:51:34.786252  4737 registrar.cpp:490] Successfully updated the 'registry' in 22.340864ms
I0114 18:51:34.787094  4737 registrar.cpp:376] Successfully recovered registrar
I0114 18:51:34.787749  4737 master.cpp:1077] Recovered 0 slaves from the Registry (94B) ; allowing 10mins for slaves to re-register
I0114 18:51:34.787282  4739 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 707150ns
I0114 18:51:34.788692  4739 leveldb.cpp:401] Deleting ~1 keys from leveldb took 60262ns
I0114 18:51:34.789048  4739 replica.cpp:679] Persisted action at 2
I0114 18:51:34.789329  4739 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0114 18:51:34.819548  4738 slave.cpp:173] Slave started on 171)@192.168.122.135:57018
I0114 18:51:34.820530  4738 credentials.hpp:84] Loading credential for authentication from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/credential'
I0114 18:51:34.820952  4738 slave.cpp:282] Slave using credential for: test-principal
I0114 18:51:34.821516  4738 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:51:34.822217  4738 slave.cpp:329] Slave hostname: fedora-19
I0114 18:51:34.822502  4738 slave.cpp:330] Slave checkpoint: false
W0114 18:51:34.822857  4738 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0114 18:51:34.824998  4737 state.cpp:33] Recovering state from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/meta'
I0114 18:51:34.834015  4738 status_update_manager.cpp:197] Recovering status update manager
I0114 18:51:34.834810  4738 slave.cpp:3519] Finished recovery
I0114 18:51:34.835906  4734 status_update_manager.cpp:171] Pausing sending status updates
I0114 18:51:34.836423  4738 slave.cpp:613] New master detected at master@192.168.122.135:57018
I0114 18:51:34.836908  4738 slave.cpp:676] Authenticating with master master@192.168.122.135:57018
I0114 18:51:34.837190  4738 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0114 18:51:34.837820  4737 authenticatee.hpp:138] Creating new client SASL connection
I0114 18:51:34.838784  4738 slave.cpp:649] Detecting new master
I0114 18:51:34.839306  4740 master.cpp:4130] Authenticating slave(171)@192.168.122.135:57018
I0114 18:51:34.839957  4740 master.cpp:4141] Using default CRAM-MD5 authenticator
I0114 18:51:34.841236  4740 authenticator.hpp:170] Creating new server SASL connection
I0114 18:51:34.842681  4741 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0114 18:51:34.843118  4741 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0114 18:51:34.843581  4740 authenticator.hpp:276] Received SASL authentication start
I0114 18:51:34.843962  4740 authenticator.hpp:398] Authentication requires more steps
I0114 18:51:34.844357  4740 authenticatee.hpp:275] Received SASL authentication step
I0114 18:51:34.844780  4740 authenticator.hpp:304] Received SASL authentication step
I0114 18:51:34.845113  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0114 18:51:34.845507  4740 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0114 18:51:34.845835  4740 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0114 18:51:34.846238  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0114 18:51:34.846542  4740 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.846806  4740 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.847110  4740 authenticator.hpp:390] Authentication success
I0114 18:51:34.847808  4734 authenticatee.hpp:315] Authentication success
I0114 18:51:34.851029  4734 slave.cpp:747] Successfully authenticated with master master@192.168.122.135:57018
I0114 18:51:34.851608  4737 master.cpp:4188] Successfully authenticated principal 'test-principal' at slave(171)@192.168.122.135:57018
I0114 18:51:34.854962  4720 sched.cpp:151] Version: 0.22.0
I0114 18:51:34.856674  4734 slave.cpp:1075] Will retry registration in 3.085482ms if necessary
I0114 18:51:34.857434  4739 sched.cpp:248] New master detected at master@192.168.122.135:57018
I0114 18:51:34.861433  4739 sched.cpp:304] Authenticating with master master@192.168.122.135:57018
I0114 18:51:34.861693  4739 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0114 18:51:34.857795  4737 master.cpp:3276] Registering slave at slave(171)@192.168.122.135:57018 (fedora-19) with id 20150114-185134-2272962752-57018-4720-S0
I0114 18:51:34.862951  4737 authenticatee.hpp:138] Creating new client SASL connection
I0114 18:51:34.863919  4735 registrar.cpp:445] Applied 1 operations in 120272ns; attempting to update the 'registry'
I0114 18:51:34.864645  4738 master.cpp:4130] Authenticating scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.865033  4738 master.cpp:4141] Using default CRAM-MD5 authenticator
I0114 18:51:34.866904  4738 authenticator.hpp:170] Creating new server SASL connection
I0114 18:51:34.868840  4737 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0114 18:51:34.869125  4737 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0114 18:51:34.869523  4737 authenticator.hpp:276] Received SASL authentication start
I0114 18:51:34.869835  4737 authenticator.hpp:398] Authentication requires more steps
I0114 18:51:34.870213  4737 authenticatee.hpp:275] Received SASL authentication step
I0114 18:51:34.870622  4737 authenticator.hpp:304] Received SASL authentication step
I0114 18:51:34.870946  4737 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0114 18:51:34.871219  4737 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0114 18:51:34.871554  4737 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0114 18:51:34.871968  4737 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0114 18:51:34.872297  4737 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.872655  4737 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.873024  4737 authenticator.hpp:390] Authentication success
I0114 18:51:34.873428  4737 authenticatee.hpp:315] Authentication success
I0114 18:51:34.873632  4739 master.cpp:4188] Successfully authenticated principal 'test-principal' at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.875006  4740 sched.cpp:392] Successfully authenticated with master master@192.168.122.135:57018
I0114 18:51:34.875319  4740 sched.cpp:515] Sending registration request to master@192.168.122.135:57018
I0114 18:51:34.876200  4740 sched.cpp:548] Will retry registration in 1.952991346secs if necessary
I0114 18:51:34.876729  4738 master.cpp:1417] Received registration request for framework 'default' at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.877040  4738 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0114 18:51:34.878059  4738 master.cpp:1481] Registering framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.878473  4739 log.cpp:684] Attempting to append 300 bytes to the log
I0114 18:51:34.879464  4737 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0114 18:51:34.880116  4734 hierarchical_allocator_process.hpp:319] Added framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.880470  4734 hierarchical_allocator_process.hpp:839] No resources available to allocate!
I0114 18:51:34.882331  4734 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 1.901284ms
I0114 18:51:34.884024  4741 sched.cpp:442] Framework registered with 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.884454  4741 sched.cpp:456] Scheduler::registered took 44320ns
I0114 18:51:34.881965  4737 replica.cpp:511] Replica received write request for position 3
I0114 18:51:34.885218  4737 leveldb.cpp:343] Persisting action (319 bytes) to leveldb took 134480ns
I0114 18:51:34.885716  4737 replica.cpp:679] Persisted action at 3
I0114 18:51:34.886034  4739 slave.cpp:1075] Will retry registration in 22.947772ms if necessary
I0114 18:51:34.886291  4740 master.cpp:3264] Ignoring register slave message from slave(171)@192.168.122.135:57018 (fedora-19) as admission is already in progress
I0114 18:51:34.894690  4736 replica.cpp:658] Replica received learned notice for position 3
I0114 18:51:34.898638  4736 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 215501ns
I0114 18:51:34.899055  4736 replica.cpp:679] Persisted action at 3
I0114 18:51:34.899416  4736 replica.cpp:664] Replica learned APPEND action at position 3
I0114 18:51:34.911782  4736 registrar.cpp:490] Successfully updated the 'registry' in 46.176768ms
I0114 18:51:34.912286  4740 log.cpp:703] Attempting to truncate the log to 3
I0114 18:51:34.913108  4740 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0114 18:51:34.915027  4736 master.cpp:3330] Registered slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:51:34.915642  4735 hierarchical_allocator_process.hpp:453] Added slave 20150114-185134-2272962752-57018-4720-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0114 18:51:34.917809  4735 hierarchical_allocator_process.hpp:764] Performed allocation for slave 20150114-185134-2272962752-57018-4720-S0 in 514027ns
I0114 18:51:34.916689  4738 replica.cpp:511] Replica received write request for position 4
I0114 18:51:34.915784  4741 slave.cpp:781] Registered with master master@192.168.122.135:57018; given slave ID 20150114-185134-2272962752-57018-4720-S0
I0114 18:51:34.919293  4741 slave.cpp:2588] Received ping from slave-observer(156)@192.168.122.135:57018
I0114 18:51:34.919775  4740 status_update_manager.cpp:178] Resuming sending status updates
I0114 18:51:34.920374  4736 master.cpp:4072] Sending 1 offers to framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.920569  4738 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 1.540136ms
I0114 18:51:34.921092  4738 replica.cpp:679] Persisted action at 4
I0114 18:51:34.927111  4735 replica.cpp:658] Replica received learned notice for position 4
I0114 18:51:34.927299  4734 sched.cpp:605] Scheduler::resourceOffers took 1.335524ms
I0114 18:51:34.930418  4735 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 1.596377ms
I0114 18:51:34.930882  4735 leveldb.cpp:401] Deleting ~2 keys from leveldb took 67578ns
I0114 18:51:34.931115  4735 replica.cpp:679] Persisted action at 4
I0114 18:51:34.931529  4735 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0114 18:51:34.930356  4734 master.cpp:2541] Processing reply for offers: [ 20150114-185134-2272962752-57018-4720-O0 ] on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) for framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.932834  4734 master.cpp:2647] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins'
W0114 18:51:34.934442  4736 master.cpp:2124] Executor default for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0114 18:51:34.934960  4736 master.cpp:2136] Executor default for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0114 18:51:34.935878  4736 master.hpp:766] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150114-185134-2272962752-57018-4720-S0 (fedora-19)
I0114 18:51:34.939453  4738 hierarchical_allocator_process.hpp:610] Updated allocation of framework 20150114-185134-2272962752-57018-4720-0000 on slave 20150114-185134-2272962752-57018-4720-S0 from cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] to cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:51:34.939950  4736 master.cpp:2897] Launching task 1 of framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19)
I0114 18:51:34.940467  4736 test_hook_module.cpp:52] Executing 'masterLaunchTaskLabelDecorator' hook
I0114 18:51:34.941490  4740 slave.cpp:1130] Got assigned task 1 for framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.942873  4740 slave.cpp:1245] Launching task 1 for framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.943469  4740 test_hook_module.cpp:71] Executing 'slaveLaunchExecutorEnvironmentDecorator' hook
I0114 18:51:34.946705  4740 slave.cpp:3921] Launching executor default of framework 20150114-185134-2272962752-57018-4720-0000 in work directory '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/slaves/20150114-185134-2272962752-57018-4720-S0/frameworks/20150114-185134-2272962752-57018-4720-0000/executors/default/runs/d73da0e7-3d52-4a0e-91d0-eaef735fd65d'
I0114 18:51:34.956496  4740 exec.cpp:147] Version: 0.22.0
I0114 18:51:34.960752  4737 exec.cpp:197] Executor started at: executor(56)@192.168.122.135:57018 with pid 4720
I0114 18:51:34.964501  4740 slave.cpp:1368] Queuing task '1' for executor default of framework '20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.965133  4740 slave.cpp:566] Successfully attached file '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/slaves/20150114-185134-2272962752-57018-4720-S0/frameworks/20150114-185134-2272962752-57018-4720-0000/executors/default/runs/d73da0e7-3d52-4a0e-91d0-eaef735fd65d'
I0114 18:51:34.965605  4740 slave.cpp:1912] Got registration for executor 'default' of framework 20150114-185134-2272962752-57018-4720-0000 from executor(56)@192.168.122.135:57018
I0114 18:51:34.966933  4734 exec.cpp:221] Executor registered on slave 20150114-185134-2272962752-57018-4720-S0
I0114 18:51:34.968889  4740 slave.cpp:2031] Flushing queued task 1 for executor 'default' of framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.969743  4740 slave.cpp:2890] Monitoring executor 'default' of framework '20150114-185134-2272962752-57018-4720-0000' in container 'd73da0e7-3d52-4a0e-91d0-eaef735fd65d'
I0114 18:51:34.973484  4734 exec.cpp:233] Executor::registered took 4.814445ms
I0114 18:51:34.974081  4734 exec.cpp:308] Executor asked to run task '1'
I0114 18:51:34.974431  4734 exec.cpp:317] Executor::launchTask took 184910ns
I0114 18:51:34.975292  4720 sched.cpp:1471] Asked to stop the driver
I0114 18:51:34.975817  4738 sched.cpp:808] Stopping framework '20150114-185134-2272962752-57018-4720-0000'
I0114 18:51:34.975697  4720 master.cpp:654] Master terminating
W0114 18:51:34.976610  4720 master.cpp:4980] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150114-185134-2272962752-57018-4720-0000 on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) in non-terminal state TASK_STAGING
I0114 18:51:34.977880  4720 master.cpp:5023] Removing executor 'default' with resources  of framework 20150114-185134-2272962752-57018-4720-0000 on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19)
I0114 18:51:34.978196  4741 hierarchical_allocator_process.hpp:653] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150114-185134-2272962752-57018-4720-S0 from framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.982658  4735 slave.cpp:2673] master@192.168.122.135:57018 exited
W0114 18:51:34.983065  4735 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected
I0114 18:51:35.029485  4720 slave.cpp:495] Slave terminating
I0114 18:51:35.034024  4720 slave.cpp:1585] Asked to shut down framework 20150114-185134-2272962752-57018-4720-0000 by @0.0.0.0:0
I0114 18:51:35.034335  4720 slave.cpp:1610] Shutting down framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:35.034857  4720 slave.cpp:3198] Shutting down executor 'default' of framework 20150114-185134-2272962752-57018-4720-0000
tests/hook_tests.cpp:271: Failure
Value of: os::isfile(path.get())
  Actual: true
Expected: false
[  FAILED  ] HookTest.VerifySlaveLaunchExecutorHook (412 ms)

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-01-15 21:12:07.827,,,false,MESOS-1384,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 23 22:39:39 UTC 2015,,,,,,,"0|i0030n:",9223372036854775807,,,,,nnielsen,,,,,,Mesosphere Q1 Sprint 2 - 2/6,Mesosphere Q1 Sprint 3 - 2/20,Mesosphere Q1 Sprint 4 - 3/6,Mesosphere Q1 Sprint 5 - 3/20,Mesosphere Q1 Sprint 6 - 4/3,Mesosphere Q1 Sprint 7 - 4/17,Mesosphere Q2 Sprint 8 - 5/1,Mesosphere Q1 Sprint 9 - 5/15,Mesosphere Sprint 10,Mesosphere Sprint 11,,3.0,,,,,,,,,,,"15/Jan/15 21:12;nnielsen;I think there is a race between shutting down the test and actually running the kill hook.

Kapil, can you try to issue a killTask explicitly, await the status update and then check for the file's disappearance?","15/Jan/15 22:02;karya;It's up for review: https://reviews.apache.org/r/29947/","17/Feb/15 01:58;nnielsen;[~karya] What was the conclusion? Did you end up with the catching the libprocess event in the test body (emitted by the hook code?)","19/Feb/15 22:21;nnielsen;Bumping to 0.23.0 - let me know if you plan to have it done by Sunday and we'll change the target version back.","19/May/15 15:57;nnielsen;Author: Kapil Arya <kapil@mesosphere.io>
Date:   Tue May 19 08:08:50 2015 -0700

    Fixed a race condition in hook tests for remove-executor hook.
    
    There is currently no good way to synchronize between the test body and
    the hook code, so we wire a promise (owned by the test code). The
    technical debt is covered by the following JIRA issue:
    
    https://issues.apache.org/jira/browse/MESOS-2641
    
    Review: https://reviews.apache.org/r/29947
","22/May/15 18:19;karya;The test still fails, as seen in:
https://builds.apache.org/job/Mesos/COMPILER=clang,LABEL=docker%7C%7CHadoop,OS=ubuntu%3A14.10/242/changes

","03/Jun/15 17:50;nnielsen;[~karya] Any ideas to what may be going on here?","15/Jun/15 18:50;nnielsen;[~karya] Want to team up this week and look at this?","15/Jun/15 20:38;karya;[~nnielsen]: Yes, let's do that.","23/Jun/15 00:55;karya;Created a RR to handle the issue: https://reviews.apache.org/r/35756/

The current understanding is that the failure was due the race introduced
by the code not checking for TASK_RUNNING status update message
from the MockExecutor before stopping the scheduler driver. This caused
the Executor to be terminated prematurely (before the tasks were
launched) and thus the remove-executor hook was never called.

The fix was to wait for the TASK_RUNNING status update and then wait
for the shutdown() within MockExecutor. Only then we wait for the future
from remove-executor hook.","24/Jun/15 18:29;nnielsen;commit 6b00c3243324b89fe35f1f6699c6c2e4293a3f94
Author: Kapil Arya <kapil@mesosphere.io>
Date:   Tue Jun 23 15:47:34 2015 -0700

    Fixed a race condition in hook tests for remove-executor hook.

    Previously, the code was not checking for TASK_RUNNING status message
    from the MockExecutor before stopping the scheduler driver. This caused
    the Executor to be terminated prematurely (before the tasks were
    launched) and thus the remove-executor hook was never called. The fix
    was to wait for the TASK_RUNNING status update and then wait for the
    shutdown() within MockExecutor. Only then we wait for the future from
    remove-executor hook.

    Review: https://reviews.apache.org/r/35756","29/Jun/15 21:00;karya;The test is still flaky:

https://builds.apache.org/job/Mesos/COMPILER=gcc,LABEL=docker%7C%7CHadoop,OS=centos%3A7/451/changes","02/Jul/15 04:23;adam-mesos;Retargeting out of Mesos 0.23.0, since it is not critical.","21/Sep/15 22:40;nnielsen;https://reviews.apache.org/r/38574/","23/Sep/15 22:39;nnielsen;commit f6706e8921569ce5cda63585ca85d82a4d2459c7
Author: Niklas Nielsen <nik@qni.dk>
Date:   Wed Sep 23 15:36:52 2015 -0700

    Fixed race in hook self-message loop and reenabled VerifySlaveLaunchExecutorHook test
    
    Coordinating events across the library border is hard as we want to
    avoid exporting additional symbols between the test and the module code.
    To migitate this, the VerifySlaveLaunchExecutorHook used a technique
    where it creates a libprocess actors in-place and sends a message to
    itself. This can be caught by a message filter in the shared libprocess
    instance and the test code can synchronize over this, to make sure
    certain module code was executed.
    
    However, the in-place actor could (potentially) shutdown before the
    message was received (and thus, didn't execute the filter).
    
    This patch installs a message handler in the in-place actor and only
    shuts down the actors when the message has been received.
    
    Review: https://reviews.apache.org/r/38574",,,,,,,,,,,,,
Large number of connections slows statistics.json responses.,MESOS-2147,12756837,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,jieyu,jieyu,20/Nov/14 22:19,06/Dec/14 17:45,29/Oct/20 16:32,06/Dec/14 17:44,0.21.0,,,,,,,,0.21.1,,,,,,,,,,,0,,,,,,,,,"We observed that in our production environment with network monitoring being turned on.

If there are many connections (> 10^4) in a container, getting socket information is expensive. It might take 1min to process all the socket information.

One of the reason is that the library we are using (libnl) is not so optimized. Cong Wang has already submitted a patch:
http://lists.infradead.org/pipermail/libnl/2014-November/001715.html",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-1228,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 20 22:21:25 UTC 2014,,,,,,,"0|i22mn3:",9223372036854775807,,,,,,,,,,,Twitter Mesos Q4 Sprint 4,,,,,,,,,,,2.0,,,,,,,,,,,"20/Nov/14 22:20;jieyu;A short term fix that disables this feature by default:
https://reviews.apache.org/r/28295/","20/Nov/14 22:21;jieyu;commit fe30f3db73437e7fdb7743dbfeec10758b1fe6d4
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Thu Nov 20 11:46:25 2014 -0800

    Allowed turning off network socket statistics collection.
    
    Review: https://reviews.apache.org/r/28295",,,,,,,,,,,,,,,,,,,,,,,,,,
Container network stats reported by the port mapping isolator is the reverse of the actual network stats.,MESOS-1989,12750250,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,chzhcn,jieyu,jieyu,24/Oct/14 00:57,26/Oct/14 04:54,29/Oct/20 16:32,26/Oct/14 04:54,0.20.1,,,,,,,,0.21.0,,,,,,,,,,,0,,,,,,,,,"Looks like the TX/RX network stats reported is the reverse of the actual network stats. The reason is because we simply get TX/RX data from veth on the host.

Since veth pair is a tunnel, the ingress of veth on host is the egress of eth0 in container (and vice versa). Therefore, we need to flip the data we got from veth.

{noformat}
[jyu@... ~]$ sudo ip netns exec 24926 /sbin/ip -s link show dev eth0
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether f0:4d:a2:75:74:05 brd ff:ff:ff:ff:ff:ff
    RX: bytes  packets  errors  dropped overrun mcast   
    46030857691178 12561038581 0       0       0       0      
    TX: bytes  packets  errors  dropped carrier collsns 
    29792886058561 15036798198 0       0       0       0      
[jyu@... ~]$ ip -s link show dev mesos24926
7412: mesos24926: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether f0:4d:a2:75:74:05 brd ff:ff:ff:ff:ff:ff
    RX: bytes  packets  errors  dropped overrun mcast   
    29793066979551 15036894749 0       0       0       0      
    TX: bytes  packets  errors  dropped carrier collsns 
    46031126366116 12561113732 0       0       0       0
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-10-24 21:52:27.273,,,false,MESOS-1228,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 24 21:52:27 UTC 2014,,,,,,,"0|i21izr:",9223372036854775807,,,,,,,,,,,Twitter Mesos Q4 Sprint 2,,,,,,,,,,,1.0,,0.21.0,,,,,,,,,"24/Oct/14 21:52;chzhcn;https://reviews.apache.org/r/27154/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Completed tasks remains in TASK_RUNNING when framework is disconnected,MESOS-1817,12742616,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,vinodkone,nnielsen,nnielsen,19/Sep/14 00:17,21/Oct/14 22:49,29/Oct/20 16:32,21/Oct/14 22:49,,,,,,,,,0.21.0,,,,,,,,,,,0,,,,,,,,,"We have run into a problem that cause tasks which completes, when a framework is disconnected and has a fail-over time, to remain in a running state even though the tasks actually finishes. This hogs the cluster and gives users a inconsistent view of the cluster state. Going to the slave, the task is finished. Going to the master, the task is still in a non-terminal state. When the scheduler reattaches or the failover timeout expires, the tasks finishes correctly. The current workflow of this scheduler has a long fail-over timeout, but may on the other hand never reattach.

Here is a test framework we have been able to reproduce the issue with: https://gist.github.com/nqn/9b9b1de9123a6e836f54
It launches many short-lived tasks (1 second sleep) and when killing the framework instance, the master reports the tasks as running even after several minutes: http://cl.ly/image/2R3719461e0t/Screen%20Shot%202014-09-10%20at%203.19.39%20PM.png

When clicking on one of the slaves where, for example, task 49 runs; the slave knows that it completed: http://cl.ly/image/2P410L3m1O1N/Screen%20Shot%202014-09-10%20at%203.21.29%20PM.png

Here is the log of a mesos-local instance where I reproduced it: https://gist.github.com/nqn/f7ee20601199d70787c0 (Here task 10 to 19 are stuck in running state).
There is a lot of output, so here is a filtered log for task 10: https://gist.github.com/nqn/a53e5ea05c5e41cd5a7d

The problem turn out to be an issue with the ack-cycle of status updates:
If the framework disconnects (with a failover timeout set), the status update manage on the slaves will keep trying to send the front of status update stream to the master (which in turn forwards it to the framework). If the first status update after the disconnect is terminal, things work out fine; the master pick the terminal state up, removes the task and release the resources.
If, on the other hand, one non-terminal status is in the stream. The master will never know that the task finished (or failed) before the framework reconnects.

During a discussion on the dev mailing list (http://mail-archives.apache.org/mod_mbox/mesos-dev/201409.mbox/%3cCADKthhAVR5mrq1s9HXw1BB_XFALXWWxjutp7MV4y3wP-Bh=aWg@mail.gmail.com%3e) we enumerated a couple of options to solve this problem.

First off, having two ack-cycles: one between masters and slaves and one between masters and frameworks, would be ideal. We would be able to replay the statuses in order while keeping the master state current. However, this requires us to persist the master state in a replicated storage.

As a first pass, we can make sure that the tasks caught in a running state doesn't hog the cluster when completed and the framework being disconnected.

Here is a proof-of-concept to work out of: https://github.com/nqn/mesos/tree/niklas/status-update-disconnect/

A new (optional) field have been added to the internal status update message:
https://github.com/nqn/mesos/blob/niklas/status-update-disconnect/src/messages/messages.proto#L68

Which makes it possible for the status update manager to set the field, if the latest status was terminal: https://github.com/nqn/mesos/blob/niklas/status-update-disconnect/src/slave/status_update_manager.cpp#L501

I added a test which should high-light the issue as well:
https://github.com/nqn/mesos/blob/niklas/status-update-disconnect/src/tests/fault_tolerance_tests.cpp#L2478

I would love some input on the approach before moving on.
There are rough edges in the PoC which (of course) should be addressed before bringing it for up review.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-1799,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-10-09 18:18:06.752,,,false,MESOS-1407,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 21 22:49:44 UTC 2014,,,,,,,"0|i00b9r:",9223372036854775807,,,,,bmahler,,,,,,Twitter Q4 Sprint 1,Twitter Mesos Q4 Sprint 2,,,,,,,,,,2.0,,,,,,,,,,,"09/Oct/14 18:18;vinodkone;This will be fixed as part of MESOS-1799.","09/Oct/14 18:22;nnielsen;Great!","14/Oct/14 18:09;vinodkone;https://reviews.apache.org/r/26697/
https://reviews.apache.org/r/26698/
https://reviews.apache.org/r/26699/
https://reviews.apache.org/r/26700/
https://reviews.apache.org/r/26701/
https://reviews.apache.org/r/26702/



","21/Oct/14 22:49;vinodkone;commit e960cdffec20d54b4f57f552d13cd92004f8e437
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Fri Oct 10 19:09:35 2014 -0700

    Updated reconciliation semantics to take the task's unacknowledged state into account.
    
    Review: https://reviews.apache.org/r/26702

commit 3c4e3fdf73fdbb2081e58fe3e9831b15d67bd440
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Fri Oct 10 18:50:07 2014 -0700

    Updated master to update task unacknowledged state properly.
    
    Review: https://reviews.apache.org/r/26701

commit da669702e4c6c4050ef49d9a1e399a837a77c143
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Fri Oct 10 14:48:49 2014 -0700

    Updated slave to include latest task state in update.
    
    Review: https://reviews.apache.org/r/26700

commit ca14f37bf7321a977e297e974e9c4c1f0cc57e0e
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Fri Oct 10 12:16:32 2014 -0700

    Updated slave re-registration to send unacknowledged task states.
    
    Review: https://reviews.apache.org/r/26699

commit 65c3c3639b385d880dbfe10bc4f652655695c8b3
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Fri Oct 17 15:26:52 2014 -0700

    Added pause() and resume() to status update manager.
    
    Review: https://reviews.apache.org/r/26957

commit e64dda411bc83963179c92ae71caefa8d21b54b4
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Wed Oct 15 18:22:46 2014 -0700

    Updated status update manager to forward updates via slave.
    
    Review: https://reviews.apache.org/r/26846
",,,,,,,,,,,,,,,,,,,,,,,,
HealthCheckTest.HealthStatusChange is flaky on jenkins.,MESOS-1802,12742006,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,Fixed,haosdent@gmail.com,bmahler,bmahler,17/Sep/14 00:32,26/Apr/17 17:00,29/Oct/20 16:32,25/Nov/16 15:41,0.26.0,0.27.3,0.28.2,1.0.0,1.0.1,,,,1.2.0,,,,,,test,,,,,0,flaky,health-check,mesosphere,,,,,,"https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/2374/consoleFull

{noformat}
[ RUN      ] HealthCheckTest.HealthStatusChange
Using temporary directory '/tmp/HealthCheckTest_HealthStatusChange_IYnlu2'
I0916 22:56:14.034612 21026 leveldb.cpp:176] Opened db in 2.155713ms
I0916 22:56:14.034965 21026 leveldb.cpp:183] Compacted db in 332489ns
I0916 22:56:14.034984 21026 leveldb.cpp:198] Created db iterator in 3710ns
I0916 22:56:14.034996 21026 leveldb.cpp:204] Seeked to beginning of db in 642ns
I0916 22:56:14.035006 21026 leveldb.cpp:273] Iterated through 0 keys in the db in 343ns
I0916 22:56:14.035023 21026 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0916 22:56:14.035200 21054 recover.cpp:425] Starting replica recovery
I0916 22:56:14.035403 21041 recover.cpp:451] Replica is in EMPTY status
I0916 22:56:14.035888 21045 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0916 22:56:14.035969 21052 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0916 22:56:14.036118 21042 recover.cpp:542] Updating replica status to STARTING
I0916 22:56:14.036603 21046 master.cpp:286] Master 20140916-225614-3125920579-47865-21026 (penates.apache.org) started on 67.195.81.186:47865
I0916 22:56:14.036634 21046 master.cpp:332] Master only allowing authenticated frameworks to register
I0916 22:56:14.036648 21046 master.cpp:337] Master only allowing authenticated slaves to register
I0916 22:56:14.036659 21046 credentials.hpp:36] Loading credentials for authentication from '/tmp/HealthCheckTest_HealthStatusChange_IYnlu2/credentials'
I0916 22:56:14.036686 21045 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 480322ns
I0916 22:56:14.036700 21045 replica.cpp:320] Persisted replica status to STARTING
I0916 22:56:14.036769 21046 master.cpp:366] Authorization enabled
I0916 22:56:14.036826 21045 recover.cpp:451] Replica is in STARTING status
I0916 22:56:14.036944 21052 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0916 22:56:14.036968 21049 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.186:47865
I0916 22:56:14.037284 21054 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0916 22:56:14.037312 21046 master.cpp:1212] The newly elected leader is master@67.195.81.186:47865 with id 20140916-225614-3125920579-47865-21026
I0916 22:56:14.037333 21046 master.cpp:1225] Elected as the leading master!
I0916 22:56:14.037345 21046 master.cpp:1043] Recovering from registrar
I0916 22:56:14.037504 21040 registrar.cpp:313] Recovering registrar
I0916 22:56:14.037505 21053 recover.cpp:188] Received a recover response from a replica in STARTING status
I0916 22:56:14.037681 21047 recover.cpp:542] Updating replica status to VOTING
I0916 22:56:14.038072 21052 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 330251ns
I0916 22:56:14.038087 21052 replica.cpp:320] Persisted replica status to VOTING
I0916 22:56:14.038127 21053 recover.cpp:556] Successfully joined the Paxos group
I0916 22:56:14.038202 21053 recover.cpp:440] Recover process terminated
I0916 22:56:14.038364 21048 log.cpp:656] Attempting to start the writer
I0916 22:56:14.038812 21053 replica.cpp:474] Replica received implicit promise request with proposal 1
I0916 22:56:14.038925 21053 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 92623ns
I0916 22:56:14.038944 21053 replica.cpp:342] Persisted promised to 1
I0916 22:56:14.039201 21052 coordinator.cpp:230] Coordinator attemping to fill missing position
I0916 22:56:14.039676 21047 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0916 22:56:14.039836 21047 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 144215ns
I0916 22:56:14.039850 21047 replica.cpp:676] Persisted action at 0
I0916 22:56:14.040243 21047 replica.cpp:508] Replica received write request for position 0
I0916 22:56:14.040267 21047 leveldb.cpp:438] Reading position from leveldb took 10323ns
I0916 22:56:14.040362 21047 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 79471ns
I0916 22:56:14.040375 21047 replica.cpp:676] Persisted action at 0
I0916 22:56:14.040556 21054 replica.cpp:655] Replica received learned notice for position 0
I0916 22:56:14.040658 21054 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 83975ns
I0916 22:56:14.040676 21054 replica.cpp:676] Persisted action at 0
I0916 22:56:14.040689 21054 replica.cpp:661] Replica learned NOP action at position 0
I0916 22:56:14.041023 21043 log.cpp:672] Writer started with ending position 0
I0916 22:56:14.041342 21052 leveldb.cpp:438] Reading position from leveldb took 10642ns
I0916 22:56:14.042325 21050 registrar.cpp:346] Successfully fetched the registry (0B)
I0916 22:56:14.042346 21050 registrar.cpp:422] Attempting to update the 'registry'
I0916 22:56:14.043306 21054 log.cpp:680] Attempting to append 140 bytes to the log
I0916 22:56:14.043354 21050 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0916 22:56:14.043637 21047 replica.cpp:508] Replica received write request for position 1
I0916 22:56:14.044042 21047 leveldb.cpp:343] Persisting action (159 bytes) to leveldb took 386690ns
I0916 22:56:14.044057 21047 replica.cpp:676] Persisted action at 1
I0916 22:56:14.044271 21040 replica.cpp:655] Replica received learned notice for position 1
I0916 22:56:14.044435 21040 leveldb.cpp:343] Persisting action (161 bytes) to leveldb took 145186ns
I0916 22:56:14.044448 21040 replica.cpp:676] Persisted action at 1
I0916 22:56:14.044456 21040 replica.cpp:661] Replica learned APPEND action at position 1
I0916 22:56:14.044729 21055 registrar.cpp:479] Successfully updated 'registry'
I0916 22:56:14.044776 21047 log.cpp:699] Attempting to truncate the log to 1
I0916 22:56:14.044795 21055 registrar.cpp:372] Successfully recovered registrar
I0916 22:56:14.044831 21051 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0916 22:56:14.044899 21053 master.cpp:1070] Recovered 0 slaves from the Registry (102B) ; allowing 10mins for slaves to re-register
I0916 22:56:14.045133 21055 replica.cpp:508] Replica received write request for position 2
I0916 22:56:14.045450 21055 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 300867ns
I0916 22:56:14.045465 21055 replica.cpp:676] Persisted action at 2
I0916 22:56:14.045725 21052 replica.cpp:655] Replica received learned notice for position 2
I0916 22:56:14.045925 21052 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 182657ns
I0916 22:56:14.045948 21052 leveldb.cpp:401] Deleting ~1 keys from leveldb took 10733ns
I0916 22:56:14.045958 21052 replica.cpp:676] Persisted action at 2
I0916 22:56:14.045964 21052 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0916 22:56:14.055306 21026 containerizer.cpp:89] Using isolation: posix/cpu,posix/mem
I0916 22:56:14.057139 21048 slave.cpp:169] Slave started on 102)@67.195.81.186:47865
I0916 22:56:14.057178 21048 credentials.hpp:84] Loading credential for authentication from '/tmp/HealthCheckTest_HealthStatusChange_cGKTiG/credential'
I0916 22:56:14.057283 21048 slave.cpp:276] Slave using credential for: test-principal
I0916 22:56:14.057354 21048 slave.cpp:289] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0916 22:56:14.057457 21048 slave.cpp:317] Slave hostname: penates.apache.org
I0916 22:56:14.057468 21048 slave.cpp:318] Slave checkpoint: false
I0916 22:56:14.057754 21043 state.cpp:33] Recovering state from '/tmp/HealthCheckTest_HealthStatusChange_cGKTiG/meta'
I0916 22:56:14.057864 21042 status_update_manager.cpp:193] Recovering status update manager
I0916 22:56:14.057958 21042 containerizer.cpp:252] Recovering containerizer
I0916 22:56:14.058226 21042 slave.cpp:3219] Finished recovery
I0916 22:56:14.058452 21047 slave.cpp:600] New master detected at master@67.195.81.186:47865
I0916 22:56:14.058485 21047 slave.cpp:674] Authenticating with master master@67.195.81.186:47865
I0916 22:56:14.058506 21042 status_update_manager.cpp:167] New master detected at master@67.195.81.186:47865
I0916 22:56:14.058539 21047 slave.cpp:647] Detecting new master
I0916 22:56:14.058555 21042 authenticatee.hpp:128] Creating new client SASL connection
I0916 22:56:14.058656 21043 master.cpp:3653] Authenticating slave(102)@67.195.81.186:47865
I0916 22:56:14.058737 21040 authenticator.hpp:156] Creating new server SASL connection
I0916 22:56:14.058830 21047 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0916 22:56:14.058852 21047 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0916 22:56:14.058884 21047 authenticator.hpp:262] Received SASL authentication start
I0916 22:56:14.058936 21047 authenticator.hpp:384] Authentication requires more steps
I0916 22:56:14.058981 21047 authenticatee.hpp:265] Received SASL authentication step
I0916 22:56:14.059052 21040 authenticator.hpp:290] Received SASL authentication step
I0916 22:56:14.059074 21040 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0916 22:56:14.059087 21040 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0916 22:56:14.059101 21040 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0916 22:56:14.059111 21040 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0916 22:56:14.059118 21040 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0916 22:56:14.059123 21040 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0916 22:56:14.059135 21040 authenticator.hpp:376] Authentication success
I0916 22:56:14.059182 21047 authenticatee.hpp:305] Authentication success
I0916 22:56:14.059192 21040 master.cpp:3693] Successfully authenticated principal 'test-principal' at slave(102)@67.195.81.186:47865
I0916 22:56:14.059309 21047 slave.cpp:731] Successfully authenticated with master master@67.195.81.186:47865
I0916 22:56:14.059348 21047 slave.cpp:994] Will retry registration in 12.6149ms if necessary
I0916 22:56:14.059396 21040 master.cpp:2843] Registering slave at slave(102)@67.195.81.186:47865 (penates.apache.org) with id 20140916-225614-3125920579-47865-21026-0
I0916 22:56:14.059495 21054 registrar.cpp:422] Attempting to update the 'registry'
I0916 22:56:14.059558 21026 sched.cpp:137] Version: 0.21.0
I0916 22:56:14.059710 21041 sched.cpp:233] New master detected at master@67.195.81.186:47865
I0916 22:56:14.059730 21041 sched.cpp:283] Authenticating with master master@67.195.81.186:47865
I0916 22:56:14.059788 21052 authenticatee.hpp:128] Creating new client SASL connection
I0916 22:56:14.059890 21043 master.cpp:3653] Authenticating scheduler-59427aee-c9d1-45c7-96fc-12d0d48529a4@67.195.81.186:47865
I0916 22:56:14.059960 21055 authenticator.hpp:156] Creating new server SASL connection
I0916 22:56:14.060039 21040 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0916 22:56:14.060061 21040 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0916 22:56:14.060107 21055 authenticator.hpp:262] Received SASL authentication start
I0916 22:56:14.060158 21055 authenticator.hpp:384] Authentication requires more steps
I0916 22:56:14.060189 21055 authenticatee.hpp:265] Received SASL authentication step
I0916 22:56:14.060220 21055 authenticator.hpp:290] Received SASL authentication step
I0916 22:56:14.060236 21055 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0916 22:56:14.060250 21055 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0916 22:56:14.060277 21055 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0916 22:56:14.060288 21055 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'penates.apache.org' server FQDN: 'penates.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0916 22:56:14.060295 21055 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0916 22:56:14.060300 21055 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0916 22:56:14.060312 21055 authenticator.hpp:376] Authentication success
I0916 22:56:14.060349 21040 authenticatee.hpp:305] Authentication success
I0916 22:56:14.060364 21055 master.cpp:3693] Successfully authenticated principal 'test-principal' at scheduler-59427aee-c9d1-45c7-96fc-12d0d48529a4@67.195.81.186:47865
I0916 22:56:14.060480 21046 sched.cpp:357] Successfully authenticated with master master@67.195.81.186:47865
I0916 22:56:14.060499 21046 sched.cpp:476] Sending registration request to master@67.195.81.186:47865
I0916 22:56:14.060564 21050 master.cpp:1331] Received registration request from scheduler-59427aee-c9d1-45c7-96fc-12d0d48529a4@67.195.81.186:47865
I0916 22:56:14.060593 21050 master.cpp:1291] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0916 22:56:14.060767 21053 master.cpp:1390] Registering framework 20140916-225614-3125920579-47865-21026-0000 at scheduler-59427aee-c9d1-45c7-96fc-12d0d48529a4@67.195.81.186:47865
I0916 22:56:14.060797 21049 log.cpp:680] Attempting to append 337 bytes to the log
I0916 22:56:14.060873 21042 hierarchical_allocator_process.hpp:329] Added framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:14.060873 21040 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0916 22:56:14.060899 21042 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I0916 22:56:14.060909 21042 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 11862ns
I0916 22:56:14.061061 21044 sched.cpp:407] Framework registered with 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:14.061115 21044 sched.cpp:421] Scheduler::registered took 34395ns
I0916 22:56:14.061173 21047 replica.cpp:508] Replica received write request for position 3
I0916 22:56:14.061298 21047 leveldb.cpp:343] Persisting action (356 bytes) to leveldb took 108843ns
I0916 22:56:14.061311 21047 replica.cpp:676] Persisted action at 3
I0916 22:56:14.061553 21049 replica.cpp:655] Replica received learned notice for position 3
I0916 22:56:14.061965 21049 leveldb.cpp:343] Persisting action (358 bytes) to leveldb took 392670ns
I0916 22:56:14.061985 21049 replica.cpp:676] Persisted action at 3
I0916 22:56:14.061996 21049 replica.cpp:661] Replica learned APPEND action at position 3
I0916 22:56:14.062268 21050 registrar.cpp:479] Successfully updated 'registry'
I0916 22:56:14.062331 21051 log.cpp:699] Attempting to truncate the log to 3
I0916 22:56:14.062355 21040 master.cpp:2883] Registered slave 20140916-225614-3125920579-47865-21026-0 at slave(102)@67.195.81.186:47865 (penates.apache.org)
I0916 22:56:14.062386 21043 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0916 22:56:14.062376 21040 master.cpp:4126] Adding slave 20140916-225614-3125920579-47865-21026-0 at slave(102)@67.195.81.186:47865 (penates.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0916 22:56:14.062510 21045 slave.cpp:765] Registered with master master@67.195.81.186:47865; given slave ID 20140916-225614-3125920579-47865-21026-0
I0916 22:56:14.062573 21045 slave.cpp:2346] Received ping from slave-observer(98)@67.195.81.186:47865
I0916 22:56:14.062599 21049 hierarchical_allocator_process.hpp:442] Added slave 20140916-225614-3125920579-47865-21026-0 (penates.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0916 22:56:14.062669 21049 hierarchical_allocator_process.hpp:734] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140916-225614-3125920579-47865-21026-0 to framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:14.062764 21041 replica.cpp:508] Replica received write request for position 4
I0916 22:56:14.062788 21049 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20140916-225614-3125920579-47865-21026-0 in 145691ns
I0916 22:56:14.062839 21050 master.hpp:861] Adding offer 20140916-225614-3125920579-47865-21026-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140916-225614-3125920579-47865-21026-0 (penates.apache.org)
I0916 22:56:14.062891 21041 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 110169ns
I0916 22:56:14.062907 21041 replica.cpp:676] Persisted action at 4
I0916 22:56:14.062911 21050 master.cpp:3600] Sending 1 offers to framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:14.063065 21043 sched.cpp:544] Scheduler::resourceOffers took 39808ns
I0916 22:56:14.063163 21046 replica.cpp:655] Replica received learned notice for position 4
I0916 22:56:14.063272 21046 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 89981ns
I0916 22:56:14.063308 21046 leveldb.cpp:401] Deleting ~2 keys from leveldb took 18542ns
I0916 22:56:14.063323 21046 replica.cpp:676] Persisted action at 4
I0916 22:56:14.063333 21046 replica.cpp:661] Replica learned TRUNCATE action at position 4
I0916 22:56:14.063482 21044 master.hpp:871] Removing offer 20140916-225614-3125920579-47865-21026-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140916-225614-3125920579-47865-21026-0 (penates.apache.org)
I0916 22:56:14.063535 21044 master.cpp:2201] Processing reply for offers: [ 20140916-225614-3125920579-47865-21026-0 ] on slave 20140916-225614-3125920579-47865-21026-0 at slave(102)@67.195.81.186:47865 (penates.apache.org) for framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:14.063561 21044 master.cpp:2284] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins'
I0916 22:56:14.063824 21040 master.hpp:833] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140916-225614-3125920579-47865-21026-0 (penates.apache.org)
I0916 22:56:14.063860 21040 master.cpp:2350] Launching task 1 of framework 20140916-225614-3125920579-47865-21026-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140916-225614-3125920579-47865-21026-0 at slave(102)@67.195.81.186:47865 (penates.apache.org)
I0916 22:56:14.063943 21050 slave.cpp:1025] Got assigned task 1 for framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:14.064158 21050 slave.cpp:1135] Launching task 1 for framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:14.065439 21050 slave.cpp:1248] Queuing task '1' for executor 1 of framework '20140916-225614-3125920579-47865-21026-0000
I0916 22:56:14.065460 21041 containerizer.cpp:394] Starting container 'd383a013-89cf-47c6-ad8e-39e2f3e971fd' for executor '1' of framework '20140916-225614-3125920579-47865-21026-0000'
I0916 22:56:14.065477 21050 slave.cpp:554] Successfully attached file '/tmp/HealthCheckTest_HealthStatusChange_cGKTiG/slaves/20140916-225614-3125920579-47865-21026-0/frameworks/20140916-225614-3125920579-47865-21026-0000/executors/1/runs/d383a013-89cf-47c6-ad8e-39e2f3e971fd'
I0916 22:56:14.066735 21055 launcher.cpp:137] Forked child with pid '21858' for container 'd383a013-89cf-47c6-ad8e-39e2f3e971fd'
I0916 22:56:14.067486 21044 containerizer.cpp:510] Fetching URIs for container 'd383a013-89cf-47c6-ad8e-39e2f3e971fd' using command '/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/build/src/mesos-fetcher'
I0916 22:56:15.037449 21050 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 43708ns
I0916 22:56:15.038743 21054 slave.cpp:2559] Monitoring executor '1' of framework '20140916-225614-3125920579-47865-21026-0000' in container 'd383a013-89cf-47c6-ad8e-39e2f3e971fd'
I0916 22:56:15.078441 21053 slave.cpp:1758] Got registration for executor '1' of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:15.078866 21053 slave.cpp:1876] Flushing queued task 1 for executor '1' of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:15.084800 21043 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:15.084969 21041 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:15.084995 21041 status_update_manager.cpp:499] Creating StatusUpdate stream for task 1 of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:15.085160 21041 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000 to master@67.195.81.186:47865
I0916 22:56:15.085314 21043 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:15.085332 21041 master.cpp:3212] Forwarding status update TASK_RUNNING (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:15.085335 21043 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:15.085435 21041 master.cpp:3178] Status update TASK_RUNNING (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000 from slave 20140916-225614-3125920579-47865-21026-0 at slave(102)@67.195.81.186:47865 (penates.apache.org)
I0916 22:56:15.085675 21044 sched.cpp:635] Scheduler::statusUpdate took 113998ns
I0916 22:56:15.085888 21052 master.cpp:2693] Forwarding status update acknowledgement a16d2819-e9f4-4119-bde6-f00ad33033e5 for task 1 of framework 20140916-225614-3125920579-47865-21026-0000 to slave 20140916-225614-3125920579-47865-21026-0 at slave(102)@67.195.81.186:47865 (penates.apache.org)
I0916 22:56:15.086109 21051 status_update_manager.cpp:398] Received status update acknowledgement (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:15.086205 21051 slave.cpp:1698] Status update manager successfully handled status update acknowledgement (UUID: a16d2819-e9f4-4119-bde6-f00ad33033e5) for task 1 of framework 20140916-225614-3125920579-47865-21026-0000
I../../src/tests/health_check_tests.cpp:330: Failure
Failed to wait 10secs for statusHealth1
0916 22:56:16.038705 21049 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 40061ns
I0916 22:56:16.126260 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: 792b8e42-0d72-451b-978a-7d1f29a15751) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.190274 21045 master.cpp:741] Framework 20140916-225614-3125920579-47865-21026-0000 disconnected
I0916 22:56:28.190304 21045 master.cpp:1687] Deactivating framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:19.037235 21050 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0916 22:56:28.190394 21045 master.cpp:763] Giving framework 20140916-225614-3125920579-47865-21026-0000 0ns to failover
../../src/tests/health_check_tests.cpp:319: Failure
Actual function call count doesn't match EXPECT_CALL(sched, statusUpdate(&driver, _))...
         Expected: to be called 4 times
           Actual: called once - unsatisfied and active
I0916 22:56:28.190624 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: 5783bb6f-112f-4434-a160-a336e890398a) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.190757 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: b4a9f647-3894-47f3-b55e-49d0355b20f9) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.190773 21046 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 792b8e42-0d72-451b-978a-7d1f29a15751) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.190831 21040 hierarchical_allocator_process.hpp:405] Deactivated framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.190856 21054 master.cpp:3471] Framework failover timeout, removing framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.190846 21046 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 792b8e42-0d72-451b-978a-7d1f29a15751) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 to master@67.195.81.186:47865
I0916 22:56:28.190887 21054 master.cpp:3976] Removing framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.190887 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: b5d5d6c7-e92c-4ca0-ab72-656542c14ade) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.190994 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: c0225c5c-b15e-4b5e-a063-07a29703ea12) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.190996 21046 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 5783bb6f-112f-4434-a160-a336e890398a) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.190999 21054 master.hpp:851] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140916-225614-3125920579-47865-21026-0 (penates.apache.org)
I0916 22:56:28.191090 21046 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: b4a9f647-3894-47f3-b55e-49d0355b20f9) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
W0916 22:56:28.191141 21054 master.cpp:4419] Removing task 1 of framework 20140916-225614-3125920579-47865-21026-0000 and slave 20140916-225614-3125920579-47865-21026-0 in non-terminal state TASK_RUNNING
I0916 22:56:28.191093 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: e9e3fdb1-d8e0-4bfc-970b-fcd098cace13) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.191181 21046 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: b5d5d6c7-e92c-4ca0-ab72-656542c14ade) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.191256 21054 master.cpp:650] Master terminating
I0916 22:56:28.191258 21043 hierarchical_allocator_process.hpp:563] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20140916-225614-3125920579-47865-21026-0 from framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369088 21046 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: c0225c5c-b15e-4b5e-a063-07a29703ea12) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.191319 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: 780d211b-6ecc-478d-93e9-6744ed0a2d33) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.369132 21043 hierarchical_allocator_process.hpp:360] Removed framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369225 21046 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: e9e3fdb1-d8e0-4bfc-970b-fcd098cace13) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369283 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: f41475b5-9b45-478e-8cd9-2cf7854627dd) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.369323 21046 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 780d211b-6ecc-478d-93e9-6744ed0a2d33) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369415 21046 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: f41475b5-9b45-478e-8cd9-2cf7854627dd) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369420 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: 5f3a1b44-51f0-4deb-ba4c-e7238f63f856) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.369536 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: 4663a09b-147f-455c-a577-3d967ddf5256) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.369642 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: 89cbabd7-0169-4b58-8df7-d8fd4bc4a287) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.369685 21055 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 5f3a1b44-51f0-4deb-ba4c-e7238f63f856) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369753 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: 3c491f72-95f1-4c52-b7ca-c6470f748eb5) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.369802 21055 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 4663a09b-147f-455c-a577-3d967ddf5256) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369884 21055 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 89cbabd7-0169-4b58-8df7-d8fd4bc4a287) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369889 21052 slave.cpp:2110] Handling status update TASK_RUNNING (UUID: 218be9bd-a229-4808-8fb6-1e507830cdaf) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 from executor(1)@67.195.81.186:35510
I0916 22:56:28.369943 21055 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 3c491f72-95f1-4c52-b7ca-c6470f748eb5) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.369978 21052 slave.cpp:1431] Asked to shut down framework 20140916-225614-3125920579-47865-21026-0000 by master@67.195.81.186:47865
I0916 22:56:28.369998 21052 slave.cpp:1456] Shutting down framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.370009 21055 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 218be9bd-a229-4808-8fb6-1e507830cdaf) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.370018 21052 slave.cpp:2899] Shutting down executor '1' of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.370183 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: 792b8e42-0d72-451b-978a-7d1f29a15751) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.370206 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: 792b8e42-0d72-451b-978a-7d1f29a15751) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.370426 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: 5783bb6f-112f-4434-a160-a336e890398a) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.370447 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: 5783bb6f-112f-4434-a160-a336e890398a) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.370635 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: b4a9f647-3894-47f3-b55e-49d0355b20f9) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.370657 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: b4a9f647-3894-47f3-b55e-49d0355b20f9) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.370815 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: b5d5d6c7-e92c-4ca0-ab72-656542c14ade) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.370837 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: b5d5d6c7-e92c-4ca0-ab72-656542c14ade) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.370972 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: c0225c5c-b15e-4b5e-a063-07a29703ea12) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.371000 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: c0225c5c-b15e-4b5e-a063-07a29703ea12) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.371155 21052 slave.cpp:2378] master@67.195.81.186:47865 exited
W0916 22:56:28.371177 21052 slave.cpp:2381] Master disconnected! Waiting for a new master to be elected
I0916 22:56:28.371202 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: e9e3fdb1-d8e0-4bfc-970b-fcd098cace13) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.540035 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: e9e3fdb1-d8e0-4bfc-970b-fcd098cace13) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.371701 21053 containerizer.cpp:882] Destroying container 'd383a013-89cf-47c6-ad8e-39e2f3e971fd'
I0916 22:56:28.540177 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: 780d211b-6ecc-478d-93e9-6744ed0a2d33) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.540196 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: 780d211b-6ecc-478d-93e9-6744ed0a2d33) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.540324 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: f41475b5-9b45-478e-8cd9-2cf7854627dd) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.540350 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: f41475b5-9b45-478e-8cd9-2cf7854627dd) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.540403 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: 5f3a1b44-51f0-4deb-ba4c-e7238f63f856) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.540421 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: 5f3a1b44-51f0-4deb-ba4c-e7238f63f856) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.540530 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: 4663a09b-147f-455c-a577-3d967ddf5256) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.540556 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: 4663a09b-147f-455c-a577-3d967ddf5256) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.540664 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: 89cbabd7-0169-4b58-8df7-d8fd4bc4a287) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.540681 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: 89cbabd7-0169-4b58-8df7-d8fd4bc4a287) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.540889 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: 3c491f72-95f1-4c52-b7ca-c6470f748eb5) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.540918 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: 3c491f72-95f1-4c52-b7ca-c6470f748eb5) for task 1 in health state unhealthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:28.541082 21052 slave.cpp:2267] Status update manager successfully handled status update TASK_RUNNING (UUID: 218be9bd-a229-4808-8fb6-1e507830cdaf) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:28.541111 21052 slave.cpp:2273] Sending acknowledgement for status update TASK_RUNNING (UUID: 218be9bd-a229-4808-8fb6-1e507830cdaf) for task 1 in health state healthy of framework 20140916-225614-3125920579-47865-21026-0000 to executor(1)@67.195.81.186:35510
I0916 22:56:29.047708 21053 containerizer.cpp:997] Executor for container 'd383a013-89cf-47c6-ad8e-39e2f3e971fd' has exited
I0916 22:56:29.048037 21050 slave.cpp:2617] Executor '1' of framework 20140916-225614-3125920579-47865-21026-0000 terminated with signal Killed
I0916 22:56:29.048197 21050 slave.cpp:2753] Cleaning up executor '1' of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:29.048373 21050 slave.cpp:2828] Cleaning up framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:29.048444 21043 status_update_manager.cpp:282] Closing status update streams for framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:29.048457 21050 slave.cpp:477] Slave terminating
I0916 22:56:29.048476 21043 status_update_manager.cpp:530] Cleaning up status update stream for task 1 of framework 20140916-225614-3125920579-47865-21026-0000
I0916 22:56:29.048462 21041 gc.cpp:56] Scheduling '/tmp/HealthCheckTest_HealthStatusChange_cGKTiG/slaves/20140916-225614-3125920579-47865-21026-0/frameworks/20140916-225614-3125920579-47865-21026-0000/executors/1/runs/d383a013-89cf-47c6-ad8e-39e2f3e971fd' for gc 6.99999944121481days in the future
I0916 22:56:29.048568 21041 gc.cpp:56] Scheduling '/tmp/HealthCheckTest_HealthStatusChange_cGKTiG/slaves/20140916-225614-3125920579-47865-21026-0/frameworks/20140916-225614-3125920579-47865-21026-0000/executors/1' for gc 6.99999944031111days in the future
I0916 22:56:29.048607 21041 gc.cpp:56] Scheduling '/tmp/HealthCheckTest_HealthStatusChange_cGKTiG/slaves/20140916-225614-3125920579-47865-21026-0/frameworks/20140916-225614-3125920579-47865-21026-0000' for gc 6.99999943939852days in the future
[  FAILED  ] HealthCheckTest.HealthStatusChange (15019 ms)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/16 18:29;neilc;health_check_flaky_test_log.txt;https://issues.apache.org/jira/secure/attachment/12787104/health_check_flaky_test_log.txt",,,,,1.0,,,,,,,,,,,,,,,,,,,,2015-04-24 17:40:27.758,,,false,MESOS-5916,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 25 15:41:13 UTC 2016,,,,,,,"0|hzzzqn:",9223372036854775807,,,,,alexr,,,,,,Mesosphere Sprint 48,,,,,,,,,,,5.0,,1.2.0,,,,,,,,,"24/Apr/15 17:40;benjaminhindman;Hey [~tnachen], are you working on this? We're doing some backlog grooming!","09/Nov/15 22:59;vinodkone;This is still happening.

https://builds.apache.org/job/Mesos/COMPILER=gcc,CONFIGURATION=--verbose,OS=ubuntu%3A14.04,label_exp=docker%7C%7CHadoop/1204/changes

{code}

[ RUN      ] HealthCheckTest.HealthStatusChange
I1109 19:00:52.094830 29603 leveldb.cpp:176] Opened db in 282.154657ms
I1109 19:00:52.187214 29603 leveldb.cpp:183] Compacted db in 92.28671ms
I1109 19:00:52.187328 29603 leveldb.cpp:198] Created db iterator in 31318ns
I1109 19:00:52.187345 29603 leveldb.cpp:204] Seeked to beginning of db in 3925ns
I1109 19:00:52.187357 29603 leveldb.cpp:273] Iterated through 0 keys in the db in 386ns
I1109 19:00:52.187417 29603 replica.cpp:780] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1109 19:00:52.188252 29628 recover.cpp:449] Starting replica recovery
I1109 19:00:52.188861 29628 recover.cpp:475] Replica is in EMPTY status
I1109 19:00:52.190914 29626 replica.cpp:676] Replica in EMPTY status received a broadcasted recover request from (1864)@172.17.14.49:60889
I1109 19:00:52.191453 29632 recover.cpp:195] Received a recover response from a replica in EMPTY status
I1109 19:00:52.192078 29632 recover.cpp:566] Updating replica status to STARTING
I1109 19:00:52.199542 29624 master.cpp:367] Master 34506b61-7fc1-4e5e-821c-e5f5650782e0 (61f7c652bdf2) started on 172.17.14.49:60889
I1109 19:00:52.200220 29624 master.cpp:369] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/z7oqX0/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.26.0/_inst/share/mesos/webui"" --work_dir=""/tmp/z7oqX0/master"" --zk_session_timeout=""10secs""
I1109 19:00:52.200584 29624 master.cpp:414] Master only allowing authenticated frameworks to register
I1109 19:00:52.200597 29624 master.cpp:419] Master only allowing authenticated slaves to register
I1109 19:00:52.200606 29624 credentials.hpp:37] Loading credentials for authentication from '/tmp/z7oqX0/credentials'
I1109 19:00:52.200917 29624 master.cpp:458] Using default 'crammd5' authenticator
I1109 19:00:52.201328 29624 master.cpp:495] Authorization enabled
I1109 19:00:52.201648 29622 hierarchical.cpp:140] Initialized hierarchical allocator process
I1109 19:00:52.201715 29622 whitelist_watcher.cpp:79] No whitelist given
I1109 19:00:52.204258 29624 master.cpp:1606] The newly elected leader is master@172.17.14.49:60889 with id 34506b61-7fc1-4e5e-821c-e5f5650782e0
I1109 19:00:52.204290 29624 master.cpp:1619] Elected as the leading master!
I1109 19:00:52.204308 29624 master.cpp:1379] Recovering from registrar
I1109 19:00:52.204457 29637 registrar.cpp:309] Recovering registrar
I1109 19:00:52.257841 29632 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 65.359249ms
I1109 19:00:52.257933 29632 replica.cpp:323] Persisted replica status to STARTING
I1109 19:00:52.258283 29631 recover.cpp:475] Replica is in STARTING status
I1109 19:00:52.259827 29630 replica.cpp:676] Replica in STARTING status received a broadcasted recover request from (1865)@172.17.14.49:60889
I1109 19:00:52.260082 29628 recover.cpp:195] Received a recover response from a replica in STARTING status
I1109 19:00:52.260671 29630 recover.cpp:566] Updating replica status to VOTING
I1109 19:00:52.369948 29628 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 109.102187ms
I1109 19:00:52.370046 29628 replica.cpp:323] Persisted replica status to VOTING
I1109 19:00:52.370317 29628 recover.cpp:580] Successfully joined the Paxos group
I1109 19:00:52.370551 29628 recover.cpp:464] Recover process terminated
I1109 19:00:52.371186 29629 log.cpp:661] Attempting to start the writer
I1109 19:00:52.372530 29629 replica.cpp:496] Replica received implicit promise request from (1866)@172.17.14.49:60889 with proposal 1
I1109 19:00:52.505920 29629 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 133.338727ms
I1109 19:00:52.505997 29629 replica.cpp:345] Persisted promised to 1
I1109 19:00:52.506805 29634 coordinator.cpp:240] Coordinator attempting to fill missing positions
I1109 19:00:52.508029 29622 replica.cpp:391] Replica received explicit promise request from (1867)@172.17.14.49:60889 for position 0 with proposal 2
I1109 19:00:52.587518 29622 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 79.418401ms
I1109 19:00:52.587616 29622 replica.cpp:715] Persisted action at 0
I1109 19:00:52.589311 29622 replica.cpp:540] Replica received write request for position 0 from (1868)@172.17.14.49:60889
I1109 19:00:52.589390 29622 leveldb.cpp:438] Reading position from leveldb took 41754ns
I1109 19:00:52.660668 29622 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 71.221465ms
I1109 19:00:52.660756 29622 replica.cpp:715] Persisted action at 0
I1109 19:00:52.661685 29625 replica.cpp:694] Replica received learned notice for position 0 from @0.0.0.0:0
I1109 19:00:52.706508 29625 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 44.76317ms
I1109 19:00:52.706595 29625 replica.cpp:715] Persisted action at 0
I1109 19:00:52.706634 29625 replica.cpp:700] Replica learned NOP action at position 0
I1109 19:00:52.707479 29625 log.cpp:677] Writer started with ending position 0
I1109 19:00:52.708673 29631 leveldb.cpp:438] Reading position from leveldb took 62378ns
I1109 19:00:52.709760 29628 registrar.cpp:342] Successfully fetched the registry (0B) in 505.250304ms
I1109 19:00:52.709918 29628 registrar.cpp:441] Applied 1 operations in 46159ns; attempting to update the 'registry'
I1109 19:00:52.710719 29626 log.cpp:685] Attempting to append 176 bytes to the log
I1109 19:00:52.710878 29622 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 1
I1109 19:00:52.711644 29634 replica.cpp:540] Replica received write request for position 1 from (1869)@172.17.14.49:60889
I1109 19:00:52.757084 29634 leveldb.cpp:343] Persisting action (195 bytes) to leveldb took 45.376201ms
I1109 19:00:52.757189 29634 replica.cpp:715] Persisted action at 1
I1109 19:00:52.758299 29636 replica.cpp:694] Replica received learned notice for position 1 from @0.0.0.0:0
I1109 19:00:52.799000 29636 leveldb.cpp:343] Persisting action (197 bytes) to leveldb took 40.641232ms
I1109 19:00:52.799090 29636 replica.cpp:715] Persisted action at 1
I1109 19:00:52.799129 29636 replica.cpp:700] Replica learned APPEND action at position 1
I1109 19:00:52.801550 29632 registrar.cpp:486] Successfully updated the 'registry' in 91.542016ms
I1109 19:00:52.801815 29632 registrar.cpp:372] Successfully recovered registrar
I1109 19:00:52.802433 29634 master.cpp:1416] Recovered 0 slaves from the Registry (137B) ; allowing 10mins for slaves to re-register
I1109 19:00:52.802548 29632 log.cpp:704] Attempting to truncate the log to 1
I1109 19:00:52.802938 29632 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 2
I1109 19:00:52.804299 29632 replica.cpp:540] Replica received write request for position 2 from (1870)@172.17.14.49:60889
I1109 19:00:52.832676 29632 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 28.304266ms
I1109 19:00:52.832762 29632 replica.cpp:715] Persisted action at 2
I1109 19:00:52.833796 29627 replica.cpp:694] Replica received learned notice for position 2 from @0.0.0.0:0
I1109 19:00:52.881970 29627 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 48.117789ms
I1109 19:00:52.882125 29627 leveldb.cpp:401] Deleting ~1 keys from leveldb took 71118ns
I1109 19:00:52.882160 29627 replica.cpp:715] Persisted action at 2
I1109 19:00:52.882472 29627 replica.cpp:700] Replica learned TRUNCATE action at position 2
I1109 19:00:52.886776 29603 containerizer.cpp:142] Using isolation: posix/cpu,posix/mem,filesystem/posix
W1109 19:00:52.887220 29603 backend.cpp:50] Failed to create 'bind' backend: BindBackend requires root privileges
I1109 19:00:52.892428 29636 slave.cpp:191] Slave started on 59)@172.17.14.49:60889
I1109 19:00:52.892465 29636 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/HealthCheckTest_HealthStatusChange_xdqiRg/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/HealthCheckTest_HealthStatusChange_xdqiRg/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.26.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/HealthCheckTest_HealthStatusChange_xdqiRg""
I1109 19:00:52.892906 29636 credentials.hpp:85] Loading credential for authentication from '/tmp/HealthCheckTest_HealthStatusChange_xdqiRg/credential'
I1109 19:00:52.893141 29636 slave.cpp:322] Slave using credential for: test-principal
I1109 19:00:52.893776 29636 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1109 19:00:52.893852 29636 slave.cpp:400] Slave attributes: [  ]
I1109 19:00:52.893865 29636 slave.cpp:405] Slave hostname: 61f7c652bdf2
I1109 19:00:52.893874 29636 slave.cpp:410] Slave checkpoint: true
I1109 19:00:52.894933 29603 sched.cpp:166] Version: 0.26.0
I1109 19:00:52.895546 29629 state.cpp:54] Recovering state from '/tmp/HealthCheckTest_HealthStatusChange_xdqiRg/meta'
I1109 19:00:52.895649 29630 sched.cpp:264] New master detected at master@172.17.14.49:60889
I1109 19:00:52.895730 29630 sched.cpp:320] Authenticating with master master@172.17.14.49:60889
I1109 19:00:52.895752 29630 sched.cpp:327] Using default CRAM-MD5 authenticatee
I1109 19:00:52.896075 29630 authenticatee.cpp:123] Creating new client SASL connection
I1109 19:00:52.896335 29626 status_update_manager.cpp:202] Recovering status update manager
I1109 19:00:52.896419 29636 master.cpp:5150] Authenticating scheduler-3176fdc7-ed93-4501-810e-6a9ac59907b0@172.17.14.49:60889
I1109 19:00:52.896545 29623 authenticator.cpp:415] Starting authentication session for crammd5_authenticatee(164)@172.17.14.49:60889
I1109 19:00:52.896879 29623 authenticator.cpp:100] Creating new server SASL connection
I1109 19:00:52.897099 29623 authenticatee.cpp:214] Received SASL authentication mechanisms: CRAM-MD5
I1109 19:00:52.897128 29623 authenticatee.cpp:240] Attempting to authenticate with mechanism 'CRAM-MD5'
I1109 19:00:52.897286 29637 authenticator.cpp:205] Received SASL authentication start
I1109 19:00:52.897351 29637 authenticator.cpp:327] Authentication requires more steps
I1109 19:00:52.897400 29630 containerizer.cpp:384] Recovering containerizer
I1109 19:00:52.897434 29637 authenticatee.cpp:260] Received SASL authentication step
I1109 19:00:52.897555 29637 authenticator.cpp:233] Received SASL authentication step
I1109 19:00:52.897585 29637 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '61f7c652bdf2' server FQDN: '61f7c652bdf2' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1109 19:00:52.897600 29637 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I1109 19:00:52.897655 29637 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1109 19:00:52.897689 29637 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '61f7c652bdf2' server FQDN: '61f7c652bdf2' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1109 19:00:52.897703 29637 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1109 19:00:52.897713 29637 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1109 19:00:52.897733 29637 authenticator.cpp:319] Authentication success
I1109 19:00:52.897814 29626 authenticatee.cpp:300] Authentication success
I1109 19:00:52.898047 29625 sched.cpp:409] Successfully authenticated with master master@172.17.14.49:60889
I1109 19:00:52.898072 29625 sched.cpp:716] Sending SUBSCRIBE call to master@172.17.14.49:60889
I1109 19:00:52.898152 29625 sched.cpp:749] Will retry registration in 663.938562ms if necessary
I1109 19:00:52.898216 29637 master.cpp:5180] Successfully authenticated principal 'test-principal' at scheduler-3176fdc7-ed93-4501-810e-6a9ac59907b0@172.17.14.49:60889
I1109 19:00:52.898265 29625 authenticator.cpp:433] Authentication session cleanup for crammd5_authenticatee(164)@172.17.14.49:60889
I1109 19:00:52.898396 29637 master.cpp:2176] Received SUBSCRIBE call for framework 'default' at scheduler-3176fdc7-ed93-4501-810e-6a9ac59907b0@172.17.14.49:60889
I1109 19:00:52.898465 29637 master.cpp:1645] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1109 19:00:52.898475 29625 slave.cpp:4230] Finished recovery
I1109 19:00:52.898919 29637 master.cpp:2247] Subscribing framework default with checkpointing disabled and capabilities [  ]
I1109 19:00:52.899102 29625 slave.cpp:4387] Querying resource estimator for oversubscribable resources
I1109 19:00:52.899441 29625 hierarchical.cpp:185] Added framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
I1109 19:00:52.899560 29625 hierarchical.cpp:971] No resources available to allocate!
I1109 19:00:52.899654 29632 status_update_manager.cpp:176] Pausing sending status updates
I1109 19:00:52.899659 29637 sched.cpp:643] Framework registered with 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
I1109 19:00:52.899585 29633 slave.cpp:729] New master detected at master@172.17.14.49:60889
I1109 19:00:52.899716 29637 sched.cpp:657] Scheduler::registered took 22789ns
I1109 19:00:52.899736 29633 slave.cpp:792] Authenticating with master master@172.17.14.49:60889
I1109 19:00:52.899755 29633 slave.cpp:797] Using default CRAM-MD5 authenticatee
I1109 19:00:52.899827 29625 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:00:52.899948 29625 hierarchical.cpp:870] Performed allocation for 0 slaves in 424469ns
I1109 19:00:52.899956 29634 authenticatee.cpp:123] Creating new client SASL connection
I1109 19:00:52.899889 29633 slave.cpp:765] Detecting new master
I1109 19:00:52.900290 29623 master.cpp:5150] Authenticating slave(59)@172.17.14.49:60889
I1109 19:00:52.900306 29633 slave.cpp:4401] Received oversubscribable resources  from the resource estimator
I1109 19:00:52.900372 29634 authenticator.cpp:415] Starting authentication session for crammd5_authenticatee(165)@172.17.14.49:60889
I1109 19:00:52.900554 29622 authenticator.cpp:100] Creating new server SASL connection
I1109 19:00:52.900802 29622 authenticatee.cpp:214] Received SASL authentication mechanisms: CRAM-MD5
I1109 19:00:52.900884 29622 authenticatee.cpp:240] Attempting to authenticate with mechanism 'CRAM-MD5'
I1109 19:00:52.901053 29622 authenticator.cpp:205] Received SASL authentication start
I1109 19:00:52.901101 29622 authenticator.cpp:327] Authentication requires more steps
I1109 19:00:52.901276 29622 authenticatee.cpp:260] Received SASL authentication step
I1109 19:00:52.901412 29622 authenticator.cpp:233] Received SASL authentication step
I1109 19:00:52.901515 29622 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '61f7c652bdf2' server FQDN: '61f7c652bdf2' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1109 19:00:52.901583 29622 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I1109 19:00:52.901626 29622 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1109 19:00:52.901651 29622 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '61f7c652bdf2' server FQDN: '61f7c652bdf2' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1109 19:00:52.901662 29622 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1109 19:00:52.901670 29622 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1109 19:00:52.901686 29622 authenticator.cpp:319] Authentication success
I1109 19:00:52.901823 29622 authenticatee.cpp:300] Authentication success
I1109 19:00:52.902007 29622 master.cpp:5180] Successfully authenticated principal 'test-principal' at slave(59)@172.17.14.49:60889
I1109 19:00:52.902092 29622 authenticator.cpp:433] Authentication session cleanup for crammd5_authenticatee(165)@172.17.14.49:60889
I1109 19:00:52.902946 29633 slave.cpp:860] Successfully authenticated with master master@172.17.14.49:60889
I1109 19:00:52.903100 29633 slave.cpp:1254] Will retry registration in 492605ns if necessary
I1109 19:00:52.903314 29627 master.cpp:3859] Registering slave at slave(59)@172.17.14.49:60889 (61f7c652bdf2) with id 34506b61-7fc1-4e5e-821c-e5f5650782e0-S0
I1109 19:00:52.903866 29629 registrar.cpp:441] Applied 1 operations in 68516ns; attempting to update the 'registry'
I1109 19:00:52.904614 29633 slave.cpp:1254] Will retry registration in 839715ns if necessary
I1109 19:00:52.904727 29636 log.cpp:685] Attempting to append 345 bytes to the log
I1109 19:00:52.904736 29623 master.cpp:3847] Ignoring register slave message from slave(59)@172.17.14.49:60889 (61f7c652bdf2) as admission is already in progress
I1109 19:00:52.904885 29634 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 3
I1109 19:00:52.905652 29633 replica.cpp:540] Replica received write request for position 3 from (1882)@172.17.14.49:60889
I1109 19:00:52.906004 29624 slave.cpp:1254] Will retry registration in 50.877167ms if necessary
I1109 19:00:52.906114 29629 master.cpp:3847] Ignoring register slave message from slave(59)@172.17.14.49:60889 (61f7c652bdf2) as admission is already in progress
I1109 19:00:52.957790 29623 slave.cpp:1254] Will retry registration in 148.906602ms if necessary
I1109 19:00:52.958108 29623 master.cpp:3847] Ignoring register slave message from slave(59)@172.17.14.49:60889 (61f7c652bdf2) as admission is already in progress
I1109 19:00:52.963148 29633 leveldb.cpp:343] Persisting action (364 bytes) to leveldb took 57.434275ms
I1109 19:00:52.963251 29633 replica.cpp:715] Persisted action at 3
I1109 19:00:52.964395 29623 replica.cpp:694] Replica received learned notice for position 3 from @0.0.0.0:0
I1109 19:00:53.025077 29623 leveldb.cpp:343] Persisting action (366 bytes) to leveldb took 60.616045ms
I1109 19:00:53.025174 29623 replica.cpp:715] Persisted action at 3
I1109 19:00:53.025214 29623 replica.cpp:700] Replica learned APPEND action at position 3
I1109 19:00:53.027107 29624 registrar.cpp:486] Successfully updated the 'registry' in 123.156224ms
I1109 19:00:53.027403 29636 log.cpp:704] Attempting to truncate the log to 3
I1109 19:00:53.027645 29636 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 4
I1109 19:00:53.028303 29625 slave.cpp:3169] Received ping from slave-observer(56)@172.17.14.49:60889
I1109 19:00:53.028708 29627 replica.cpp:540] Replica received write request for position 4 from (1883)@172.17.14.49:60889
I1109 19:00:53.028769 29631 master.cpp:3927] Registered slave 34506b61-7fc1-4e5e-821c-e5f5650782e0-S0 at slave(59)@172.17.14.49:60889 (61f7c652bdf2) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1109 19:00:53.028837 29623 slave.cpp:904] Registered with master master@172.17.14.49:60889; given slave ID 34506b61-7fc1-4e5e-821c-e5f5650782e0-S0
I1109 19:00:53.028861 29623 fetcher.cpp:79] Clearing fetcher cache
I1109 19:00:53.028920 29625 hierarchical.cpp:335] Added slave 34506b61-7fc1-4e5e-821c-e5f5650782e0-S0 (61f7c652bdf2) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I1109 19:00:53.029285 29623 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/HealthCheckTest_HealthStatusChange_xdqiRg/meta/slaves/34506b61-7fc1-4e5e-821c-e5f5650782e0-S0/slave.info'
I1109 19:00:53.029700 29623 slave.cpp:963] Forwarding total oversubscribed resources 
I1109 19:00:53.029793 29623 status_update_manager.cpp:183] Resuming sending status updates
I1109 19:00:53.029832 29625 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:00:53.029875 29625 hierarchical.cpp:886] Performed allocation for slave 34506b61-7fc1-4e5e-821c-e5f5650782e0-S0 in 916734ns
I1109 19:00:53.029930 29623 master.cpp:4269] Received update of slave 34506b61-7fc1-4e5e-821c-e5f5650782e0-S0 at slave(59)@172.17.14.49:60889 (61f7c652bdf2) with total oversubscribed resources 
I1109 19:00:53.030433 29625 hierarchical.cpp:391] Slave 34506b61-7fc1-4e5e-821c-e5f5650782e0-S0 (61f7c652bdf2) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I1109 19:00:53.030494 29623 master.cpp:4979] Sending 1 offers to framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 (default) at scheduler-3176fdc7-ed93-4501-810e-6a9ac59907b0@172.17.14.49:60889
I1109 19:00:53.030859 29625 hierarchical.cpp:971] No resources available to allocate!
I1109 19:00:53.030895 29625 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:00:53.030910 29625 hierarchical.cpp:886] Performed allocation for slave 34506b61-7fc1-4e5e-821c-e5f5650782e0-S0 in 427184ns
I1109 19:00:53.031009 29634 sched.cpp:813] Scheduler::resourceOffers took 155158ns
I1109 19:00:53.032750 29623 master.cpp:2915] Processing ACCEPT call for offers: [ 34506b61-7fc1-4e5e-821c-e5f5650782e0-O0 ] on slave 34506b61-7fc1-4e5e-821c-e5f5650782e0-S0 at slave(59)@172.17.14.49:60889 (61f7c652bdf2) for framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 (default) at scheduler-3176fdc7-ed93-4501-810e-6a9ac59907b0@172.17.14.49:60889
I1109 19:00:53.032804 29623 master.cpp:2711] Authorizing framework principal 'test-principal' to launch task 1 as user 'mesos'
I1109 19:00:53.034595 29635 master.hpp:176] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 34506b61-7fc1-4e5e-821c-e5f5650782e0-S0 (61f7c652bdf2)
I1109 19:00:53.034788 29635 master.cpp:3245] Launching task 1 of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 (default) at scheduler-3176fdc7-ed93-4501-810e-6a9ac59907b0@172.17.14.49:60889 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 34506b61-7fc1-4e5e-821c-e5f5650782e0-S0 at slave(59)@172.17.14.49:60889 (61f7c652bdf2)
I1109 19:00:53.035286 29622 slave.cpp:1294] Got assigned task 1 for framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
I1109 19:00:53.036186 29622 slave.cpp:1410] Launching task 1 for framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
I1109 19:00:53.036998 29622 paths.cpp:417] Trying to chown '/tmp/HealthCheckTest_HealthStatusChange_xdqiRg/slaves/34506b61-7fc1-4e5e-821c-e5f5650782e0-S0/frameworks/34506b61-7fc1-4e5e-821c-e5f5650782e0-0000/executors/1/runs/fa569209-1433-470c-ba09-34756185229a' to user 'mesos'
I1109 19:00:53.040652 29622 slave.cpp:4999] Launching executor 1 of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/HealthCheckTest_HealthStatusChange_xdqiRg/slaves/34506b61-7fc1-4e5e-821c-e5f5650782e0-S0/frameworks/34506b61-7fc1-4e5e-821c-e5f5650782e0-0000/executors/1/runs/fa569209-1433-470c-ba09-34756185229a'
I1109 19:00:53.041373 29628 containerizer.cpp:618] Starting container 'fa569209-1433-470c-ba09-34756185229a' for executor '1' of framework '34506b61-7fc1-4e5e-821c-e5f5650782e0-0000'
I1109 19:00:53.041410 29622 slave.cpp:1628] Queuing task '1' for executor ''1' of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000'
I1109 19:00:53.041533 29622 slave.cpp:682] Successfully attached file '/tmp/HealthCheckTest_HealthStatusChange_xdqiRg/slaves/34506b61-7fc1-4e5e-821c-e5f5650782e0-S0/frameworks/34506b61-7fc1-4e5e-821c-e5f5650782e0-0000/executors/1/runs/fa569209-1433-470c-ba09-34756185229a'
I1109 19:00:53.053426 29630 launcher.cpp:134] Forked child with pid '2245' for container 'fa569209-1433-470c-ba09-34756185229a'
I1109 19:00:53.059900 29627 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 31.125935ms
I1109 19:00:53.059988 29627 replica.cpp:715] Persisted action at 4
I1109 19:00:53.061278 29635 replica.cpp:694] Replica received learned notice for position 4 from @0.0.0.0:0
I1109 19:00:53.110086 29635 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 48.75579ms
I1109 19:00:53.110260 29635 leveldb.cpp:401] Deleting ~2 keys from leveldb took 89854ns
I1109 19:00:53.110287 29635 replica.cpp:715] Persisted action at 4
I1109 19:00:53.110322 29635 replica.cpp:700] Replica learned TRUNCATE action at position 4
I1109 19:00:53.202918 29631 hierarchical.cpp:971] No resources available to allocate!
I1109 19:00:53.202997 29631 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:00:53.203024 29631 hierarchical.cpp:870] Performed allocation for 1 slaves in 598550ns
I1109 19:00:53.227322 29629 slave.cpp:2405] Got registration for executor '1' of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 from executor(1)@172.17.14.49:60202
I1109 19:00:53.230358 29633 slave.cpp:1793] Sending queued task '1' to executor ''1' of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 at executor(1)@172.17.14.49:60202'
I1109 19:00:53.392653 29636 containerizer.cpp:1257] Executor for container 'fa569209-1433-470c-ba09-34756185229a' has exited
I1109 19:00:53.392715 29636 containerizer.cpp:1074] Destroying container 'fa569209-1433-470c-ba09-34756185229a'
I1109 19:00:53.396560 29635 slave.cpp:3553] Executor '1' of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 exited with status 134
I1109 19:00:53.396731 29635 slave.cpp:2762] Handling status update TASK_FAILED (UUID: ac5509fc-0172-4dca-9d66-1a76810277ae) for task 1 of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 from @0.0.0.0:0
I1109 19:00:53.396927 29635 slave.cpp:5298] Terminating task 1
W1109 19:00:53.397447 29628 containerizer.cpp:966] Ignoring update for unknown container: fa569209-1433-470c-ba09-34756185229a
I1109 19:00:53.397917 29627 status_update_manager.cpp:322] Received status update TASK_FAILED (UUID: ac5509fc-0172-4dca-9d66-1a76810277ae) for task 1 of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
I1109 19:00:53.397963 29627 status_update_manager.cpp:499] Creating StatusUpdate stream for task 1 of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
I1109 19:00:53.398464 29627 status_update_manager.cpp:376] Forwarding update TASK_FAILED (UUID: ac5509fc-0172-4dca-9d66-1a76810277ae) for task 1 of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 to the slave
I1109 19:00:53.398849 29628 slave.cpp:3087] Forwarding the update TASK_FAILED (UUID: ac5509fc-0172-4dca-9d66-1a76810277ae) for task 1 of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 to master@172.17.14.49:60889
I1109 19:00:53.399086 29628 slave.cpp:3005] Status update manager successfully handled status update TASK_FAILED (UUID: ac5509fc-0172-4dca-9d66-1a76810277ae) for task 1 of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
I1109 19:00:53.399317 29623 master.cpp:4414] Status update TASK_FAILED (UUID: ac5509fc-0172-4dca-9d66-1a76810277ae) for task 1 of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 from slave 34506b61-7fc1-4e5e-821c-e5f5650782e0-S0 at slave(59)@172.17.14.49:60889 (61f7c652bdf2)
I1109 19:00:53.399370 29623 master.cpp:4462] Forwarding status update TASK_FAILED (UUID: ac5509fc-0172-4dca-9d66-1a76810277ae) for task 1 of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
I1109 19:00:53.399559 29623 master.cpp:6066] Updating the state of task 1 of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
I1109 19:00:53.399818 29628 sched.cpp:921] Scheduler::statusUpdate took 144121ns
../../src/tests/health_check_tests.cpp:480: Failure
Value of: statusRunning.get().state()
  Actual: TASK_FAILED
Expected: TASK_RUNNING
I1109 19:00:53.400254 29626 hierarchical.cpp:739] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 34506b61-7fc1-4e5e-821c-e5f5650782e0-S0 from framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
I1109 19:00:53.400329 29623 master.cpp:3571] Processing ACKNOWLEDGE call ac5509fc-0172-4dca-9d66-1a76810277ae for task 1 of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 (default) at scheduler-3176fdc7-ed93-4501-810e-6a9ac59907b0@172.17.14.49:60889 on slave 34506b61-7fc1-4e5e-821c-e5f5650782e0-S0
I1109 19:00:53.400390 29623 master.cpp:6132] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 on slave 34506b61-7fc1-4e5e-821c-e5f5650782e0-S0 at slave(59)@172.17.14.49:60889 (61f7c652bdf2)
I1109 19:00:53.400840 29626 status_update_manager.cpp:394] Received status update acknowledgement (UUID: ac5509fc-0172-4dca-9d66-1a76810277ae) for task 1 of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
I1109 19:00:53.401018 29626 status_update_manager.cpp:530] Cleaning up status update stream for task 1 of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
I1109 19:00:53.401381 29633 slave.cpp:2345] Status update manager successfully handled status update acknowledgement (UUID: ac5509fc-0172-4dca-9d66-1a76810277ae) for task 1 of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
I1109 19:00:53.401428 29633 slave.cpp:5339] Completing task 1
I1109 19:00:53.401460 29633 slave.cpp:3657] Cleaning up executor '1' of framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 at executor(1)@172.17.14.49:60202
I1109 19:00:53.401671 29631 gc.cpp:56] Scheduling '/tmp/HealthCheckTest_HealthStatusChange_xdqiRg/slaves/34506b61-7fc1-4e5e-821c-e5f5650782e0-S0/frameworks/34506b61-7fc1-4e5e-821c-e5f5650782e0-0000/executors/1/runs/fa569209-1433-470c-ba09-34756185229a' for gc 6.99999535184days in the future
I1109 19:00:53.401804 29633 slave.cpp:3745] Cleaning up framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
I1109 19:00:53.401814 29631 gc.cpp:56] Scheduling '/tmp/HealthCheckTest_HealthStatusChange_xdqiRg/slaves/34506b61-7fc1-4e5e-821c-e5f5650782e0-S0/frameworks/34506b61-7fc1-4e5e-821c-e5f5650782e0-0000/executors/1' for gc 6.99999535011556days in the future
I1109 19:00:53.401914 29627 status_update_manager.cpp:284] Closing status update streams for framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
I1109 19:00:53.402007 29630 gc.cpp:56] Scheduling '/tmp/HealthCheckTest_HealthStatusChange_xdqiRg/slaves/34506b61-7fc1-4e5e-821c-e5f5650782e0-S0/frameworks/34506b61-7fc1-4e5e-821c-e5f5650782e0-0000' for gc 6.99999534828741days in the future
I1109 19:00:54.205024 29629 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:00:54.205111 29629 hierarchical.cpp:870] Performed allocation for 1 slaves in 1.036657ms
I1109 19:00:54.205626 29636 master.cpp:4979] Sending 1 offers to framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 (default) at scheduler-3176fdc7-ed93-4501-810e-6a9ac59907b0@172.17.14.49:60889
I1109 19:00:54.206218 29628 sched.cpp:813] Scheduler::resourceOffers took 27139ns
I1109 19:00:55.206257 29636 hierarchical.cpp:971] No resources available to allocate!
I1109 19:00:55.206348 29636 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:00:55.206382 29636 hierarchical.cpp:870] Performed allocation for 1 slaves in 709301ns
I1109 19:00:56.207715 29625 hierarchical.cpp:971] No resources available to allocate!
I1109 19:00:56.207803 29625 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:00:56.207834 29625 hierarchical.cpp:870] Performed allocation for 1 slaves in 707048ns
I1109 19:00:57.209532 29626 hierarchical.cpp:971] No resources available to allocate!
I1109 19:00:57.209610 29626 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:00:57.209638 29626 hierarchical.cpp:870] Performed allocation for 1 slaves in 601219ns
I1109 19:00:58.211696 29628 hierarchical.cpp:971] No resources available to allocate!
I1109 19:00:58.211796 29628 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:00:58.211830 29628 hierarchical.cpp:870] Performed allocation for 1 slaves in 784177ns
I1109 19:00:59.212745 29622 hierarchical.cpp:971] No resources available to allocate!
I1109 19:00:59.212826 29622 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:00:59.212854 29622 hierarchical.cpp:870] Performed allocation for 1 slaves in 669442ns
I1109 19:01:00.214867 29637 hierarchical.cpp:971] No resources available to allocate!
I1109 19:01:00.214956 29637 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:01:00.214992 29637 hierarchical.cpp:870] Performed allocation for 1 slaves in 736461ns
I1109 19:01:01.216673 29634 hierarchical.cpp:971] No resources available to allocate!
I1109 19:01:01.216763 29634 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:01:01.216800 29634 hierarchical.cpp:870] Performed allocation for 1 slaves in 794520ns
I1109 19:01:02.218780 29634 hierarchical.cpp:971] No resources available to allocate!
I1109 19:01:02.218878 29634 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:01:02.218919 29634 hierarchical.cpp:870] Performed allocation for 1 slaves in 800091ns
I1109 19:01:03.220408 29630 hierarchical.cpp:971] No resources available to allocate!
I1109 19:01:03.220509 29630 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:01:03.220541 29630 hierarchical.cpp:870] Performed allocation for 1 slaves in 793742ns
I1109 19:01:04.222898 29637 hierarchical.cpp:971] No resources available to allocate!
I1109 19:01:04.223003 29637 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:01:04.223045 29637 hierarchical.cpp:870] Performed allocation for 1 slaves in 885294ns
I1109 19:01:05.224395 29631 hierarchical.cpp:971] No resources available to allocate!
I1109 19:01:05.224472 29631 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:01:05.224503 29631 hierarchical.cpp:870] Performed allocation for 1 slaves in 685482ns
I1109 19:01:06.225836 29626 hierarchical.cpp:971] No resources available to allocate!
I1109 19:01:06.225935 29626 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:01:06.225970 29626 hierarchical.cpp:870] Performed allocation for 1 slaves in 855878ns
I1109 19:01:07.227568 29624 hierarchical.cpp:971] No resources available to allocate!
I1109 19:01:07.227658 29624 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:01:07.227699 29624 hierarchical.cpp:870] Performed allocation for 1 slaves in 667674ns
I1109 19:01:07.901265 29625 slave.cpp:4387] Querying resource estimator for oversubscribable resources
I1109 19:01:07.901639 29635 slave.cpp:4401] Received oversubscribable resources  from the resource estimator
I1109 19:01:08.029749 29637 slave.cpp:3169] Received ping from slave-observer(56)@172.17.14.49:60889
I1109 19:01:08.229215 29624 hierarchical.cpp:971] No resources available to allocate!
I1109 19:01:08.229305 29624 hierarchical.cpp:1064] No inverse offers to send out!
I1109 19:01:08.229336 29624 hierarchical.cpp:870] Performed allocation for 1 slaves in 730966ns
../../src/tests/health_check_tests.cpp:482: Failure
Failed to wait 15secs for statusHealth1
../../src/tests/health_check_tests.cpp:471: Failure
Actual function call count doesn't match EXPECT_CALL(sched, statusUpdate(&driver, _))...
         Expected: to be called 4 times
           Actual: called once - unsatisfied and active
I1109 19:01:08.404537 29636 master.cpp:1122] Framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 (default) at scheduler-3176fdc7-ed93-4501-810e-6a9ac59907b0@172.17.14.49:60889 disconnected
I1109 19:01:08.405421 29636 master.cpp:2472] Disconnecting framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 (default) at scheduler-3176fdc7-ed93-4501-810e-6a9ac59907b0@172.17.14.49:60889
I1109 19:01:08.405673 29636 master.cpp:2496] Deactivating framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 (default) at scheduler-3176fdc7-ed93-4501-810e-6a9ac59907b0@172.17.14.49:60889
I1109 19:01:08.406396 29627 hierarchical.cpp:263] Deactivated framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
W1109 19:01:08.407588 29636 master.hpp:1540] Master attempted to send message to disconnected framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 (default) at scheduler-3176fdc7-ed93-4501-810e-6a9ac59907b0@172.17.14.49:60889
I1109 19:01:08.407769 29636 master.cpp:1146] Giving framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 (default) at scheduler-3176fdc7-ed93-4501-810e-6a9ac59907b0@172.17.14.49:60889 0ns to failover
I1109 19:01:08.407088 29627 hierarchical.cpp:739] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 34506b61-7fc1-4e5e-821c-e5f5650782e0-S0 from framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
I1109 19:01:08.410311 29633 master.cpp:4827] Framework failover timeout, removing framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 (default) at scheduler-3176fdc7-ed93-4501-810e-6a9ac59907b0@172.17.14.49:60889
I1109 19:01:08.410362 29633 master.cpp:5559] Removing framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 (default) at scheduler-3176fdc7-ed93-4501-810e-6a9ac59907b0@172.17.14.49:60889
I1109 19:01:08.410769 29633 slave.cpp:2009] Asked to shut down framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000 by master@172.17.14.49:60889
W1109 19:01:08.410820 29633 slave.cpp:2024] Cannot shut down unknown framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
I1109 19:01:08.411039 29633 hierarchical.cpp:220] Removed framework 34506b61-7fc1-4e5e-821c-e5f5650782e0-0000
I1109 19:01:08.411375 29603 master.cpp:922] Master terminating
I1109 19:01:08.412467 29633 hierarchical.cpp:364] Removed slave 34506b61-7fc1-4e5e-821c-e5f5650782e0-S0
I1109 19:01:08.412521 29633 slave.cpp:3215] master@172.17.14.49:60889 exited
W1109 19:01:08.412542 29633 slave.cpp:3218] Master disconnected! Waiting for a new master to be elected
I1109 19:01:08.430963 29637 slave.cpp:601] Slave terminating
[  FAILED  ] HealthCheckTest.HealthStatusChange (16622 ms)
{code}","25/Jan/16 16:53;greggomann;Looks like this is still an issue. Just saw this on the ASF CI, Ubuntu 14.04 with gcc:

{code}
[ RUN      ] HealthCheckTest.HealthStatusChange
I0125 15:22:26.282974 31962 leveldb.cpp:174] Opened db in 83.974532ms
I0125 15:22:26.329022 31962 leveldb.cpp:181] Compacted db in 45.96014ms
I0125 15:22:26.329654 31962 leveldb.cpp:196] Created db iterator in 28398ns
I0125 15:22:26.330154 31962 leveldb.cpp:202] Seeked to beginning of db in 4095ns
I0125 15:22:26.330663 31962 leveldb.cpp:271] Iterated through 0 keys in the db in 468ns
I0125 15:22:26.331181 31962 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0125 15:22:26.332582 31988 recover.cpp:447] Starting replica recovery
I0125 15:22:26.333237 31991 recover.cpp:473] Replica is in EMPTY status
I0125 15:22:26.336228 31993 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (2239)@172.17.0.2:52355
I0125 15:22:26.336789 31992 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0125 15:22:26.338047 31992 recover.cpp:564] Updating replica status to STARTING
I0125 15:22:26.350210 31982 master.cpp:374] Master 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be (6a3fa6c4588a) started on 172.17.0.2:52355
I0125 15:22:26.350651 31982 master.cpp:376] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/inJ0ZL/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/_inst/share/mesos/webui"" --work_dir=""/tmp/inJ0ZL/master"" --zk_session_timeout=""10secs""
I0125 15:22:26.352558 31982 master.cpp:421] Master only allowing authenticated frameworks to register
I0125 15:22:26.352833 31982 master.cpp:426] Master only allowing authenticated slaves to register
I0125 15:22:26.353160 31982 credentials.hpp:35] Loading credentials for authentication from '/tmp/inJ0ZL/credentials'
I0125 15:22:26.356725 31982 master.cpp:466] Using default 'crammd5' authenticator
I0125 15:22:26.357357 31982 master.cpp:535] Using default 'basic' HTTP authenticator
I0125 15:22:26.357851 31982 master.cpp:569] Authorization enabled
I0125 15:22:26.359246 31986 hierarchical.cpp:144] Initialized hierarchical allocator process
I0125 15:22:26.359474 31992 whitelist_watcher.cpp:77] No whitelist given
I0125 15:22:26.364750 31991 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 25.971673ms
I0125 15:22:26.365209 31991 replica.cpp:320] Persisted replica status to STARTING
I0125 15:22:26.365898 31991 recover.cpp:473] Replica is in STARTING status
I0125 15:22:26.369220 31993 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (2241)@172.17.0.2:52355
I0125 15:22:26.370393 31991 recover.cpp:193] Received a recover response from a replica in STARTING status
I0125 15:22:26.371255 31993 recover.cpp:564] Updating replica status to VOTING
I0125 15:22:26.372786 31982 master.cpp:1710] The newly elected leader is master@172.17.0.2:52355 with id 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be
I0125 15:22:26.373251 31982 master.cpp:1723] Elected as the leading master!
I0125 15:22:26.373553 31982 master.cpp:1468] Recovering from registrar
I0125 15:22:26.374184 31991 registrar.cpp:307] Recovering registrar
I0125 15:22:26.398006 31993 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 26.052513ms
I0125 15:22:26.398576 31993 replica.cpp:320] Persisted replica status to VOTING
I0125 15:22:26.399235 31982 recover.cpp:578] Successfully joined the Paxos group
I0125 15:22:26.399518 31982 recover.cpp:462] Recover process terminated
I0125 15:22:26.400599 31993 log.cpp:659] Attempting to start the writer
I0125 15:22:26.402815 31992 replica.cpp:493] Replica received implicit promise request from (2242)@172.17.0.2:52355 with proposal 1
I0125 15:22:26.423198 31992 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 19.954945ms
I0125 15:22:26.423651 31992 replica.cpp:342] Persisted promised to 1
I0125 15:22:26.425547 31992 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0125 15:22:26.427817 31993 replica.cpp:388] Replica received explicit promise request from (2243)@172.17.0.2:52355 for position 0 with proposal 2
I0125 15:22:26.448343 31993 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 20.029847ms
I0125 15:22:26.448870 31993 replica.cpp:712] Persisted action at 0
I0125 15:22:26.450934 31995 replica.cpp:537] Replica received write request for position 0 from (2244)@172.17.0.2:52355
I0125 15:22:26.451378 31995 leveldb.cpp:436] Reading position from leveldb took 118656ns
I0125 15:22:26.473486 31995 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 21.791896ms
I0125 15:22:26.473932 31995 replica.cpp:712] Persisted action at 0
I0125 15:22:26.475330 31995 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0125 15:22:26.498622 31995 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 22.966725ms
I0125 15:22:26.499091 31995 replica.cpp:712] Persisted action at 0
I0125 15:22:26.499519 31995 replica.cpp:697] Replica learned NOP action at position 0
I0125 15:22:26.501288 31994 log.cpp:675] Writer started with ending position 0
I0125 15:22:26.503401 31994 leveldb.cpp:436] Reading position from leveldb took 64755ns
I0125 15:22:26.505323 31986 registrar.cpp:340] Successfully fetched the registry (0B) in 130.733056ms
I0125 15:22:26.505962 31986 registrar.cpp:439] Applied 1 operations in 57111ns; attempting to update the 'registry'
I0125 15:22:26.507951 31994 log.cpp:683] Attempting to append 170 bytes to the log
I0125 15:22:26.508689 31981 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0125 15:22:26.510378 31994 replica.cpp:537] Replica received write request for position 1 from (2245)@172.17.0.2:52355
I0125 15:22:26.532110 31994 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 21.385759ms
I0125 15:22:26.532683 31994 replica.cpp:712] Persisted action at 1
I0125 15:22:26.534441 31987 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0125 15:22:26.557276 31987 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 22.49124ms
I0125 15:22:26.557835 31987 replica.cpp:712] Persisted action at 1
I0125 15:22:26.558156 31987 replica.cpp:697] Replica learned APPEND action at position 1
I0125 15:22:26.560673 31987 registrar.cpp:484] Successfully updated the 'registry' in 54.236928ms
I0125 15:22:26.561043 31984 log.cpp:702] Attempting to truncate the log to 1
I0125 15:22:26.561795 31984 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0125 15:22:26.563083 31984 replica.cpp:537] Replica received write request for position 2 from (2246)@172.17.0.2:52355
I0125 15:22:26.563714 31987 registrar.cpp:370] Successfully recovered registrar
I0125 15:22:26.564749 31987 master.cpp:1520] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0125 15:22:26.565105 31990 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0125 15:22:26.588583 31984 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 25.162931ms
I0125 15:22:26.589154 31984 replica.cpp:712] Persisted action at 2
I0125 15:22:26.590899 31984 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0125 15:22:26.615885 31984 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 24.588268ms
I0125 15:22:26.616564 31984 leveldb.cpp:399] Deleting ~1 keys from leveldb took 197510ns
I0125 15:22:26.616847 31984 replica.cpp:712] Persisted action at 2
I0125 15:22:26.617198 31984 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0125 15:22:26.623775 31962 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0125 15:22:26.625557 31962 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I0125 15:22:26.646574 31985 slave.cpp:192] Slave started on 62)@172.17.0.2:52355
I0125 15:22:26.647231 31985 slave.cpp:193] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/HealthCheckTest_HealthStatusChange_uxXuR2/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/HealthCheckTest_HealthStatusChange_uxXuR2/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/HealthCheckTest_HealthStatusChange_uxXuR2""
I0125 15:22:26.648396 31985 credentials.hpp:83] Loading credential for authentication from '/tmp/HealthCheckTest_HealthStatusChange_uxXuR2/credential'
I0125 15:22:26.649049 31985 slave.cpp:323] Slave using credential for: test-principal
I0125 15:22:26.649721 31985 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0125 15:22:26.650990 31985 slave.cpp:463] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0125 15:22:26.651430 31985 slave.cpp:471] Slave attributes: [  ]
I0125 15:22:26.651746 31985 slave.cpp:476] Slave hostname: 6a3fa6c4588a
I0125 15:22:26.654419 31996 state.cpp:58] Recovering state from '/tmp/HealthCheckTest_HealthStatusChange_uxXuR2/meta'
I0125 15:22:26.655592 31996 status_update_manager.cpp:200] Recovering status update manager
I0125 15:22:26.656435 31988 containerizer.cpp:390] Recovering containerizer
I0125 15:22:26.659112 31988 provisioner.cpp:245] Provisioner recovery complete
I0125 15:22:26.660279 31986 slave.cpp:4495] Finished recovery
I0125 15:22:26.661286 31986 slave.cpp:4667] Querying resource estimator for oversubscribable resources
I0125 15:22:26.662472 31986 slave.cpp:795] New master detected at master@172.17.0.2:52355
I0125 15:22:26.662765 31993 status_update_manager.cpp:174] Pausing sending status updates
I0125 15:22:26.663326 31986 slave.cpp:858] Authenticating with master master@172.17.0.2:52355
I0125 15:22:26.663650 31986 slave.cpp:863] Using default CRAM-MD5 authenticatee
I0125 15:22:26.664311 31992 authenticatee.cpp:121] Creating new client SASL connection
I0125 15:22:26.665274 31987 master.cpp:5521] Authenticating slave(62)@172.17.0.2:52355
I0125 15:22:26.665814 31987 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(171)@172.17.0.2:52355
I0125 15:22:26.666512 31987 authenticator.cpp:98] Creating new server SASL connection
I0125 15:22:26.668195 31986 slave.cpp:831] Detecting new master
I0125 15:22:26.668895 31992 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0125 15:22:26.669280 31992 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0125 15:22:26.669106 31986 slave.cpp:4681] Received oversubscribable resources  from the resource estimator
I0125 15:22:26.670027 31992 authenticator.cpp:203] Received SASL authentication start
I0125 15:22:26.670344 31992 authenticator.cpp:325] Authentication requires more steps
I0125 15:22:26.670738 31992 authenticatee.cpp:258] Received SASL authentication step
I0125 15:22:26.671170 31992 authenticator.cpp:231] Received SASL authentication step
I0125 15:22:26.671408 31992 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6a3fa6c4588a' server FQDN: '6a3fa6c4588a' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0125 15:22:26.671649 31992 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0125 15:22:26.671952 31992 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0125 15:22:26.672349 31992 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6a3fa6c4588a' server FQDN: '6a3fa6c4588a' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0125 15:22:26.672575 31992 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0125 15:22:26.672735 31992 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0125 15:22:26.673010 31992 authenticator.cpp:317] Authentication success
I0125 15:22:26.673619 31992 authenticatee.cpp:298] Authentication success
I0125 15:22:26.673836 31991 master.cpp:5551] Successfully authenticated principal 'test-principal' at slave(62)@172.17.0.2:52355
I0125 15:22:26.673940 31982 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(171)@172.17.0.2:52355
I0125 15:22:26.675019 31992 slave.cpp:926] Successfully authenticated with master master@172.17.0.2:52355
I0125 15:22:26.675532 31992 slave.cpp:1320] Will retry registration in 17.342244ms if necessary
I0125 15:22:26.675969 31984 master.cpp:4235] Registering slave at slave(62)@172.17.0.2:52355 (6a3fa6c4588a) with id 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0
I0125 15:22:26.676969 31984 registrar.cpp:439] Applied 1 operations in 85189ns; attempting to update the 'registry'
I0125 15:22:26.678900 31984 log.cpp:683] Attempting to append 339 bytes to the log
I0125 15:22:26.679414 31990 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0125 15:22:26.680824 31990 replica.cpp:537] Replica received write request for position 3 from (2260)@172.17.0.2:52355
I0125 15:22:26.685106 31962 sched.cpp:164] Version: 0.27.0
I0125 15:22:26.686100 31982 sched.cpp:268] New master detected at master@172.17.0.2:52355
I0125 15:22:26.686529 31982 sched.cpp:324] Authenticating with master master@172.17.0.2:52355
I0125 15:22:26.686744 31982 sched.cpp:331] Using default CRAM-MD5 authenticatee
I0125 15:22:26.687468 31982 authenticatee.cpp:121] Creating new client SASL connection
I0125 15:22:26.688256 31987 master.cpp:5521] Authenticating scheduler-10a860ea-cc32-46dd-922b-0f44bf7b1cb1@172.17.0.2:52355
I0125 15:22:26.688662 31984 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(172)@172.17.0.2:52355
I0125 15:22:26.689288 31988 authenticator.cpp:98] Creating new server SASL connection
I0125 15:22:26.690136 31982 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0125 15:22:26.690374 31982 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0125 15:22:26.690583 31982 authenticator.cpp:203] Received SASL authentication start
I0125 15:22:26.690655 31982 authenticator.cpp:325] Authentication requires more steps
I0125 15:22:26.690773 31996 authenticatee.cpp:258] Received SASL authentication step
I0125 15:22:26.690978 31982 authenticator.cpp:231] Received SASL authentication step
I0125 15:22:26.691009 31982 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6a3fa6c4588a' server FQDN: '6a3fa6c4588a' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0125 15:22:26.691040 31982 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0125 15:22:26.691094 31982 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0125 15:22:26.691155 31982 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6a3fa6c4588a' server FQDN: '6a3fa6c4588a' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0125 15:22:26.691174 31982 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0125 15:22:26.691201 31982 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0125 15:22:26.691220 31982 authenticator.cpp:317] Authentication success
I0125 15:22:26.691395 31991 authenticatee.cpp:298] Authentication success
I0125 15:22:26.691468 31986 master.cpp:5551] Successfully authenticated principal 'test-principal' at scheduler-10a860ea-cc32-46dd-922b-0f44bf7b1cb1@172.17.0.2:52355
I0125 15:22:26.691907 31981 sched.cpp:413] Successfully authenticated with master master@172.17.0.2:52355
I0125 15:22:26.691933 31981 sched.cpp:722] Sending SUBSCRIBE call to master@172.17.0.2:52355
I0125 15:22:26.692033 31981 sched.cpp:755] Will retry registration in 49.26057ms if necessary
I0125 15:22:26.692239 31981 master.cpp:2278] Received SUBSCRIBE call for framework 'default' at scheduler-10a860ea-cc32-46dd-922b-0f44bf7b1cb1@172.17.0.2:52355
I0125 15:22:26.692302 31981 master.cpp:1749] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0125 15:22:26.692577 31981 master.cpp:2349] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0125 15:22:26.693334 31986 sched.cpp:649] Framework registered with 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:26.693387 31986 sched.cpp:663] Scheduler::registered took 26089ns
I0125 15:22:26.693548 31981 hierarchical.cpp:265] Added framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:26.693622 31981 hierarchical.cpp:1355] No resources available to allocate!
I0125 15:22:26.693662 31981 hierarchical.cpp:1450] No inverse offers to send out!
I0125 15:22:26.693691 31981 hierarchical.cpp:1090] Performed allocation for 0 slaves in 123384ns
I0125 15:22:26.693852 31982 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(172)@172.17.0.2:52355
I0125 15:22:26.700639 31989 slave.cpp:1320] Will retry registration in 839715ns if necessary
I0125 15:22:26.701162 31989 master.cpp:4223] Ignoring register slave message from slave(62)@172.17.0.2:52355 (6a3fa6c4588a) as admission is already in progress
I0125 15:22:26.703377 31993 slave.cpp:1320] Will retry registration in 50.877167ms if necessary
I0125 15:22:26.703589 31993 master.cpp:4223] Ignoring register slave message from slave(62)@172.17.0.2:52355 (6a3fa6c4588a) as admission is already in progress
I0125 15:22:26.709228 31990 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 28.107677ms
I0125 15:22:26.709779 31990 replica.cpp:712] Persisted action at 3
I0125 15:22:26.711303 31994 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0125 15:22:26.741838 31994 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 30.474075ms
I0125 15:22:26.742066 31994 replica.cpp:712] Persisted action at 3
I0125 15:22:26.742287 31994 replica.cpp:697] Replica learned APPEND action at position 3
I0125 15:22:26.746264 31983 registrar.cpp:484] Successfully updated the 'registry' in 68.914944ms
I0125 15:22:26.746861 31983 log.cpp:702] Attempting to truncate the log to 3
I0125 15:22:26.748200 31981 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0125 15:22:26.748806 31986 hierarchical.cpp:471] Added slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0 (6a3fa6c4588a) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0125 15:22:26.748986 31984 slave.cpp:970] Registered with master master@172.17.0.2:52355; given slave ID 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0
I0125 15:22:26.750226 31984 fetcher.cpp:81] Clearing fetcher cache
I0125 15:22:26.750797 31984 slave.cpp:993] Checkpointing SlaveInfo to '/tmp/HealthCheckTest_HealthStatusChange_uxXuR2/meta/slaves/8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0/slave.info'
I0125 15:22:26.750919 31995 status_update_manager.cpp:181] Resuming sending status updates
I0125 15:22:26.751628 31984 slave.cpp:1029] Forwarding total oversubscribed resources 
I0125 15:22:26.751771 31984 slave.cpp:3435] Received ping from slave-observer(59)@172.17.0.2:52355
I0125 15:22:26.753825 31986 hierarchical.cpp:1450] No inverse offers to send out!
I0125 15:22:26.753902 31986 hierarchical.cpp:1110] Performed allocation for slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0 in 4.623741ms
I0125 15:22:26.748070 31983 master.cpp:4303] Registered slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0 at slave(62)@172.17.0.2:52355 (6a3fa6c4588a) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0125 15:22:26.754299 31984 replica.cpp:537] Replica received write request for position 4 from (2261)@172.17.0.2:52355
I0125 15:22:26.754783 31983 master.cpp:4644] Received update of slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0 at slave(62)@172.17.0.2:52355 (6a3fa6c4588a) with total oversubscribed resources 
I0125 15:22:26.755764 31996 hierarchical.cpp:527] Slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0 (6a3fa6c4588a) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0125 15:22:26.756073 31996 hierarchical.cpp:1355] No resources available to allocate!
I0125 15:22:26.756146 31996 hierarchical.cpp:1450] No inverse offers to send out!
I0125 15:22:26.756183 31996 hierarchical.cpp:1110] Performed allocation for slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0 in 353455ns
I0125 15:22:26.756825 31983 master.cpp:5350] Sending 1 offers to framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (default) at scheduler-10a860ea-cc32-46dd-922b-0f44bf7b1cb1@172.17.0.2:52355
I0125 15:22:26.757510 31983 sched.cpp:819] Scheduler::resourceOffers took 172909ns
I0125 15:22:26.760304 31993 master.cpp:3136] Processing ACCEPT call for offers: [ 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-O0 ] on slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0 at slave(62)@172.17.0.2:52355 (6a3fa6c4588a) for framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (default) at scheduler-10a860ea-cc32-46dd-922b-0f44bf7b1cb1@172.17.0.2:52355
I0125 15:22:26.760391 31993 master.cpp:2823] Authorizing framework principal 'test-principal' to launch task 1 as user 'mesos'
I0125 15:22:26.763656 31993 master.hpp:176] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0 (6a3fa6c4588a)
I0125 15:22:26.764000 31993 master.cpp:3621] Launching task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (default) at scheduler-10a860ea-cc32-46dd-922b-0f44bf7b1cb1@172.17.0.2:52355 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0 at slave(62)@172.17.0.2:52355 (6a3fa6c4588a)
I0125 15:22:26.764678 31993 slave.cpp:1360] Got assigned task 1 for framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:26.765007 31993 resources.cpp:564] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0125 15:22:26.766017 31993 slave.cpp:1479] Launching task 1 for framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:26.766172 31993 resources.cpp:564] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0125 15:22:26.776538 31993 paths.cpp:472] Trying to chown '/tmp/HealthCheckTest_HealthStatusChange_uxXuR2/slaves/8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0/frameworks/8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000/executors/1/runs/0e4ce8de-c0bb-4498-810e-b154badfc094' to user 'mesos'
I0125 15:22:26.784256 31984 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 29.869861ms
I0125 15:22:26.784363 31984 replica.cpp:712] Persisted action at 4
I0125 15:22:26.786635 31989 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0125 15:22:26.789561 31993 slave.cpp:5281] Launching executor 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/HealthCheckTest_HealthStatusChange_uxXuR2/slaves/8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0/frameworks/8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000/executors/1/runs/0e4ce8de-c0bb-4498-810e-b154badfc094'
I0125 15:22:26.790385 31985 containerizer.cpp:649] Starting container '0e4ce8de-c0bb-4498-810e-b154badfc094' for executor '1' of framework '8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000'
I0125 15:22:26.790431 31993 slave.cpp:1697] Queuing task '1' for executor '1' of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:26.791306 31993 slave.cpp:748] Successfully attached file '/tmp/HealthCheckTest_HealthStatusChange_uxXuR2/slaves/8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0/frameworks/8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000/executors/1/runs/0e4ce8de-c0bb-4498-810e-b154badfc094'
I0125 15:22:26.797827 31985 launcher.cpp:132] Forked child with pid '4766' for container '0e4ce8de-c0bb-4498-810e-b154badfc094'
I0125 15:22:26.818343 31989 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 31.617937ms
I0125 15:22:26.819110 31989 leveldb.cpp:399] Deleting ~2 keys from leveldb took 169424ns
I0125 15:22:26.819483 31989 replica.cpp:712] Persisted action at 4
I0125 15:22:26.819918 31989 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0125 15:22:27.212316 31988 slave.cpp:2642] Got registration for executor '1' of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 from executor(1)@172.17.0.2:56551
I0125 15:22:27.217492 31996 slave.cpp:1862] Sending queued task '1' to executor '1' of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 at executor(1)@172.17.0.2:56551
I0125 15:22:27.254505 31982 slave.cpp:3001] Handling status update TASK_RUNNING (UUID: c14708bc-7cdb-47be-ae75-0d1bd8bd1561) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 from executor(1)@172.17.0.2:56551
I0125 15:22:27.255272 31982 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: c14708bc-7cdb-47be-ae75-0d1bd8bd1561) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.255687 31982 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.256330 31982 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: c14708bc-7cdb-47be-ae75-0d1bd8bd1561) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to the slave
I0125 15:22:27.257153 31982 slave.cpp:3353] Forwarding the update TASK_RUNNING (UUID: c14708bc-7cdb-47be-ae75-0d1bd8bd1561) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to master@172.17.0.2:52355
I0125 15:22:27.258007 31994 master.cpp:4789] Status update TASK_RUNNING (UUID: c14708bc-7cdb-47be-ae75-0d1bd8bd1561) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 from slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0 at slave(62)@172.17.0.2:52355 (6a3fa6c4588a)
I0125 15:22:27.258328 31994 master.cpp:4837] Forwarding status update TASK_RUNNING (UUID: c14708bc-7cdb-47be-ae75-0d1bd8bd1561) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.258908 31990 sched.cpp:927] Scheduler::statusUpdate took 131399ns
I0125 15:22:27.259495 31994 master.cpp:6445] Updating the state of task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0125 15:22:27.260234 31994 master.cpp:3947] Processing ACKNOWLEDGE call c14708bc-7cdb-47be-ae75-0d1bd8bd1561 for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (default) at scheduler-10a860ea-cc32-46dd-922b-0f44bf7b1cb1@172.17.0.2:52355 on slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0
I0125 15:22:27.260895 31982 slave.cpp:3247] Status update manager successfully handled status update TASK_RUNNING (UUID: c14708bc-7cdb-47be-ae75-0d1bd8bd1561) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.261179 31982 slave.cpp:3263] Sending acknowledgement for status update TASK_RUNNING (UUID: c14708bc-7cdb-47be-ae75-0d1bd8bd1561) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to executor(1)@172.17.0.2:56551
I0125 15:22:27.261868 31982 status_update_manager.cpp:392] Received status update acknowledgement (UUID: c14708bc-7cdb-47be-ae75-0d1bd8bd1561) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.262595 31982 slave.cpp:2411] Status update manager successfully handled status update acknowledgement (UUID: c14708bc-7cdb-47be-ae75-0d1bd8bd1561) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.361070 31986 hierarchical.cpp:1355] No resources available to allocate!
I0125 15:22:27.361371 31986 hierarchical.cpp:1450] No inverse offers to send out!
I0125 15:22:27.361552 31986 hierarchical.cpp:1090] Performed allocation for 1 slaves in 826625ns
I0125 15:22:27.604053 31986 slave.cpp:3001] Handling status update TASK_RUNNING (UUID: 0d1cba72-f04b-4623-bd93-49fd10d02e18) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 from executor(1)@172.17.0.2:56551
I0125 15:22:27.605211 31981 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 0d1cba72-f04b-4623-bd93-49fd10d02e18) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.605792 31981 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 0d1cba72-f04b-4623-bd93-49fd10d02e18) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to the slave
I0125 15:22:27.606716 31981 slave.cpp:3353] Forwarding the update TASK_RUNNING (UUID: 0d1cba72-f04b-4623-bd93-49fd10d02e18) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to master@172.17.0.2:52355
I0125 15:22:27.607311 31981 slave.cpp:3247] Status update manager successfully handled status update TASK_RUNNING (UUID: 0d1cba72-f04b-4623-bd93-49fd10d02e18) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.608062 31981 slave.cpp:3263] Sending acknowledgement for status update TASK_RUNNING (UUID: 0d1cba72-f04b-4623-bd93-49fd10d02e18) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to executor(1)@172.17.0.2:56551
I0125 15:22:27.607839 31983 master.cpp:4789] Status update TASK_RUNNING (UUID: 0d1cba72-f04b-4623-bd93-49fd10d02e18) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 from slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0 at slave(62)@172.17.0.2:52355 (6a3fa6c4588a)
I0125 15:22:27.608814 31983 master.cpp:4837] Forwarding status update TASK_RUNNING (UUID: 0d1cba72-f04b-4623-bd93-49fd10d02e18) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.609424 31983 master.cpp:6445] Updating the state of task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0125 15:22:27.610005 31990 sched.cpp:927] Scheduler::statusUpdate took 136834ns
I0125 15:22:27.611498 31990 master.cpp:3947] Processing ACKNOWLEDGE call 0d1cba72-f04b-4623-bd93-49fd10d02e18 for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (default) at scheduler-10a860ea-cc32-46dd-922b-0f44bf7b1cb1@172.17.0.2:52355 on slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0
I0125 15:22:27.612169 31990 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 0d1cba72-f04b-4623-bd93-49fd10d02e18) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.612839 31990 slave.cpp:2411] Status update manager successfully handled status update acknowledgement (UUID: 0d1cba72-f04b-4623-bd93-49fd10d02e18) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.621296 31987 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/state'
I0125 15:22:27.622369 31987 http.cpp:503] HTTP GET for /master/state from 172.17.0.2:35405
I0125 15:22:27.635979 31993 process.cpp:3141] Handling HTTP event for process 'slave(62)' with path: '/slave(62)/state'
I0125 15:22:27.636605 31993 http.cpp:190] HTTP GET for /slave(62)/state from 172.17.0.2:35406
I0125 15:22:27.723979 31985 slave.cpp:3001] Handling status update TASK_RUNNING (UUID: 652cd5e4-98fb-4050-8778-b8fb47bee8c1) for task 1 in health state unhealthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 from executor(1)@172.17.0.2:56551
I0125 15:22:27.725005 31982 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 652cd5e4-98fb-4050-8778-b8fb47bee8c1) for task 1 in health state unhealthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.725406 31982 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 652cd5e4-98fb-4050-8778-b8fb47bee8c1) for task 1 in health state unhealthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to the slave
I0125 15:22:27.726119 31985 slave.cpp:3353] Forwarding the update TASK_RUNNING (UUID: 652cd5e4-98fb-4050-8778-b8fb47bee8c1) for task 1 in health state unhealthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to master@172.17.0.2:52355
I0125 15:22:27.726804 31991 master.cpp:4789] Status update TASK_RUNNING (UUID: 652cd5e4-98fb-4050-8778-b8fb47bee8c1) for task 1 in health state unhealthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 from slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0 at slave(62)@172.17.0.2:52355 (6a3fa6c4588a)
I0125 15:22:27.726954 31991 master.cpp:4837] Forwarding status update TASK_RUNNING (UUID: 652cd5e4-98fb-4050-8778-b8fb47bee8c1) for task 1 in health state unhealthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.727339 31991 master.cpp:6445] Updating the state of task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0125 15:22:27.727805 31986 sched.cpp:927] Scheduler::statusUpdate took 133105ns
I0125 15:22:27.728729 31991 master.cpp:3947] Processing ACKNOWLEDGE call 652cd5e4-98fb-4050-8778-b8fb47bee8c1 for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (default) at scheduler-10a860ea-cc32-46dd-922b-0f44bf7b1cb1@172.17.0.2:52355 on slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0
I0125 15:22:27.729161 31985 slave.cpp:3247] Status update manager successfully handled status update TASK_RUNNING (UUID: 652cd5e4-98fb-4050-8778-b8fb47bee8c1) for task 1 in health state unhealthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.729310 31985 slave.cpp:3263] Sending acknowledgement for status update TASK_RUNNING (UUID: 652cd5e4-98fb-4050-8778-b8fb47bee8c1) for task 1 in health state unhealthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to executor(1)@172.17.0.2:56551
I0125 15:22:27.730073 31986 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 652cd5e4-98fb-4050-8778-b8fb47bee8c1) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.730661 31985 slave.cpp:2411] Status update manager successfully handled status update acknowledgement (UUID: 652cd5e4-98fb-4050-8778-b8fb47bee8c1) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.736881 31991 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/state'
I0125 15:22:27.737740 31991 http.cpp:503] HTTP GET for /master/state from 172.17.0.2:35410
I0125 15:22:27.798907 31987 process.cpp:3141] Handling HTTP event for process 'slave(62)' with path: '/slave(62)/state'
I0125 15:22:27.800882 31987 http.cpp:190] HTTP GET for /slave(62)/state from 172.17.0.2:35411
I0125 15:22:27.860637 31995 slave.cpp:3001] Handling status update TASK_RUNNING (UUID: 43e96c29-fc0f-4eb0-9e49-5c8c36c146d4) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 from executor(1)@172.17.0.2:56551
I0125 15:22:27.861495 31995 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 43e96c29-fc0f-4eb0-9e49-5c8c36c146d4) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.861948 31995 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 43e96c29-fc0f-4eb0-9e49-5c8c36c146d4) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to the slave
I0125 15:22:27.862629 31995 slave.cpp:3353] Forwarding the update TASK_RUNNING (UUID: 43e96c29-fc0f-4eb0-9e49-5c8c36c146d4) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to master@172.17.0.2:52355
I0125 15:22:27.863328 31995 slave.cpp:3247] Status update manager successfully handled status update TASK_RUNNING (UUID: 43e96c29-fc0f-4eb0-9e49-5c8c36c146d4) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.863790 31989 master.cpp:4789] Status update TASK_RUNNING (UUID: 43e96c29-fc0f-4eb0-9e49-5c8c36c146d4) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 from slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0 at slave(62)@172.17.0.2:52355 (6a3fa6c4588a)
I0125 15:22:27.864204 31989 master.cpp:4837] Forwarding status update TASK_RUNNING (UUID: 43e96c29-fc0f-4eb0-9e49-5c8c36c146d4) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.864610 31989 master.cpp:6445] Updating the state of task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0125 15:22:27.866212 31989 sched.cpp:927] Scheduler::statusUpdate took 1.126092ms
I0125 15:22:27.866664 31989 master.cpp:3947] Processing ACKNOWLEDGE call 43e96c29-fc0f-4eb0-9e49-5c8c36c146d4 for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (default) at scheduler-10a860ea-cc32-46dd-922b-0f44bf7b1cb1@172.17.0.2:52355 on slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0
I0125 15:22:27.864078 31995 slave.cpp:3263] Sending acknowledgement for status update TASK_RUNNING (UUID: 43e96c29-fc0f-4eb0-9e49-5c8c36c146d4) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to executor(1)@172.17.0.2:56551
I0125 15:22:27.867688 31995 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 43e96c29-fc0f-4eb0-9e49-5c8c36c146d4) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.868309 31995 slave.cpp:2411] Status update manager successfully handled status update acknowledgement (UUID: 43e96c29-fc0f-4eb0-9e49-5c8c36c146d4) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.900404 31987 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/state'
I0125 15:22:27.900969 31987 http.cpp:503] HTTP GET for /master/state from 172.17.0.2:35413
I0125 15:22:27.979339 31989 slave.cpp:3001] Handling status update TASK_RUNNING (UUID: 80d4fae5-4d58-4441-9bab-7fc118d66c6b) for task 1 in health state unhealthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 from executor(1)@172.17.0.2:56551
I0125 15:22:27.979832 31989 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 80d4fae5-4d58-4441-9bab-7fc118d66c6b) for task 1 in health state unhealthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.980032 31989 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 80d4fae5-4d58-4441-9bab-7fc118d66c6b) for task 1 in health state unhealthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to the slave
I0125 15:22:27.982230 31996 slave.cpp:3353] Forwarding the update TASK_RUNNING (UUID: 80d4fae5-4d58-4441-9bab-7fc118d66c6b) for task 1 in health state unhealthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to master@172.17.0.2:52355
I0125 15:22:27.982642 31996 slave.cpp:3247] Status update manager successfully handled status update TASK_RUNNING (UUID: 80d4fae5-4d58-4441-9bab-7fc118d66c6b) for task 1 in health state unhealthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.982692 31996 slave.cpp:3263] Sending acknowledgement for status update TASK_RUNNING (UUID: 80d4fae5-4d58-4441-9bab-7fc118d66c6b) for task 1 in health state unhealthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to executor(1)@172.17.0.2:56551
I0125 15:22:27.983098 31996 master.cpp:4789] Status update TASK_RUNNING (UUID: 80d4fae5-4d58-4441-9bab-7fc118d66c6b) for task 1 in health state unhealthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 from slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0 at slave(62)@172.17.0.2:52355 (6a3fa6c4588a)
I0125 15:22:27.983160 31996 master.cpp:4837] Forwarding status update TASK_RUNNING (UUID: 80d4fae5-4d58-4441-9bab-7fc118d66c6b) for task 1 in health state unhealthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.983333 31996 master.cpp:6445] Updating the state of task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
../../src/tests/health_check_tests.cpp:529: Failure
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7fffa28454b0, @0x2abda2aab650 112-byte object <70-5E E8-97 BD-2A 00-00 00-00 00-00 00-00 00-00 80-A5 00-E4 BD-2A 00-00 70-5E 0D-02 00-00 00-00 01-00 00-00 02-00 00-00 70-5E 0D-02 00-00 00-00 50-A5 00-E4 BD-2A 00-00 00-37 00-E4 BD-2A 00-00 00-00 00-00 00-2A 00-00 C4-0A FA-6C 8F-A9 D5-41 30-37 00-E4 BD-2A 00-00 00-00 00-00 00-00 00-00 C0-3C 00-E4 BD-2A 00-00 00-00 00-00 CB-17 00-00>)
         Expected: to be called 4 times
           Actual: called 5 times - over-saturated and active
I0125 15:22:27.983691 31996 sched.cpp:927] Scheduler::statusUpdate took 187029ns
I0125 15:22:27.983927 31996 master.cpp:3947] Processing ACKNOWLEDGE call 80d4fae5-4d58-4441-9bab-7fc118d66c6b for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (default) at scheduler-10a860ea-cc32-46dd-922b-0f44bf7b1cb1@172.17.0.2:52355 on slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0
I0125 15:22:27.984235 31996 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 80d4fae5-4d58-4441-9bab-7fc118d66c6b) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.984541 31996 slave.cpp:2411] Status update manager successfully handled status update acknowledgement (UUID: 80d4fae5-4d58-4441-9bab-7fc118d66c6b) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:27.985502 31993 process.cpp:3141] Handling HTTP event for process 'slave(62)' with path: '/slave(62)/state'
I0125 15:22:27.986088 31993 http.cpp:190] HTTP GET for /slave(62)/state from 172.17.0.2:35416
I0125 15:22:28.028769 31985 slave.cpp:3001] Handling status update TASK_RUNNING (UUID: dce2e149-f1fd-40db-8def-f9d8627d71ee) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 from executor(1)@172.17.0.2:56551
I0125 15:22:28.031385 31985 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: dce2e149-f1fd-40db-8def-f9d8627d71ee) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:28.031640 31985 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: dce2e149-f1fd-40db-8def-f9d8627d71ee) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to the slave
I0125 15:22:28.033169 31985 slave.cpp:3353] Forwarding the update TASK_RUNNING (UUID: dce2e149-f1fd-40db-8def-f9d8627d71ee) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to master@172.17.0.2:52355
I0125 15:22:28.033386 31985 slave.cpp:3247] Status update manager successfully handled status update TASK_RUNNING (UUID: dce2e149-f1fd-40db-8def-f9d8627d71ee) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:28.033438 31985 slave.cpp:3263] Sending acknowledgement for status update TASK_RUNNING (UUID: dce2e149-f1fd-40db-8def-f9d8627d71ee) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 to executor(1)@172.17.0.2:56551
I0125 15:22:28.033969 31985 master.cpp:4789] Status update TASK_RUNNING (UUID: dce2e149-f1fd-40db-8def-f9d8627d71ee) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 from slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0 at slave(62)@172.17.0.2:52355 (6a3fa6c4588a)
I0125 15:22:28.034020 31985 master.cpp:4837] Forwarding status update TASK_RUNNING (UUID: dce2e149-f1fd-40db-8def-f9d8627d71ee) for task 1 in health state healthy of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:28.035220 31985 master.cpp:6445] Updating the state of task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
../../src/tests/health_check_tests.cpp:529: Failure
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7fffa28454b0, @0x2abda14a0650 112-byte object <70-5E E8-97 BD-2A 00-00 00-00 00-00 00-00 00-00 C0-30 00-C8 BD-2A 00-00 70-5E 0D-02 00-00 00-00 01-00 00-00 02-00 00-00 70-5E 0D-02 00-00 00-00 B0-36 02-C8 BD-2A 00-00 20-31 00-C8 BD-2A 00-00 00-00 00-00 01-2A 00-00 87-A2 00-6D 8F-A9 D5-41 70-53 00-C8 BD-2A 00-00 00-00 00-00 00-00 00-00 D0-FE 01-C8 BD-2A 00-00 00-00 00-00 CB-17 00-00>)
         Expected: to be called 4 times
           Actual: called 6 times - over-saturated and active
I0125 15:22:28.035569 31985 sched.cpp:927] Scheduler::statusUpdate took 158172ns
I0125 15:22:28.035837 31985 master.cpp:3947] Processing ACKNOWLEDGE call dce2e149-f1fd-40db-8def-f9d8627d71ee for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (default) at scheduler-10a860ea-cc32-46dd-922b-0f44bf7b1cb1@172.17.0.2:52355 on slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0
I0125 15:22:28.037189 31985 status_update_manager.cpp:392] Received status update acknowledgement (UUID: dce2e149-f1fd-40db-8def-f9d8627d71ee) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:28.037554 31985 slave.cpp:2411] Status update manager successfully handled status update acknowledgement (UUID: dce2e149-f1fd-40db-8def-f9d8627d71ee) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
../../src/tests/health_check_tests.cpp:631: Failure
Value of: (find).get()
  Actual: 16-byte object <05-00 00-00 01-7F 00-00 90-05 82-03 00-00 00-00>
Expected: true
Which is: true
I0125 15:22:28.052209 31962 sched.cpp:1851] Asked to stop the driver
I0125 15:22:28.061291 31993 sched.cpp:1089] Stopping framework '8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000'
I0125 15:22:28.061614 31986 master.cpp:5921] Processing TEARDOWN call for framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (default) at scheduler-10a860ea-cc32-46dd-922b-0f44bf7b1cb1@172.17.0.2:52355
I0125 15:22:28.061651 31986 master.cpp:5933] Removing framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (default) at scheduler-10a860ea-cc32-46dd-922b-0f44bf7b1cb1@172.17.0.2:52355
I0125 15:22:28.061858 31986 master.cpp:6445] Updating the state of task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I0125 15:22:28.062573 31986 master.cpp:6511] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 on slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0 at slave(62)@172.17.0.2:52355 (6a3fa6c4588a)
I0125 15:22:28.063097 31986 hierarchical.cpp:375] Deactivated framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:28.063694 31986 hierarchical.cpp:886] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: ) on slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0 from framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:28.063850 31986 hierarchical.cpp:326] Removed framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:28.063923 31986 slave.cpp:2078] Asked to shut down framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 by master@172.17.0.2:52355
I0125 15:22:28.063966 31986 slave.cpp:2103] Shutting down framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:28.064034 31986 slave.cpp:4128] Shutting down executor '1' of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 at executor(1)@172.17.0.2:56551
I0125 15:22:28.065841 31984 master.cpp:1025] Master terminating
I0125 15:22:28.066727 31985 hierarchical.cpp:502] Removed slave 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0
I0125 15:22:28.074441 31984 slave.cpp:3481] master@172.17.0.2:52355 exited
W0125 15:22:28.074491 31984 slave.cpp:3484] Master disconnected! Waiting for a new master to be elected
I0125 15:22:28.087342 31989 containerizer.cpp:1227] Destroying container '0e4ce8de-c0bb-4498-810e-b154badfc094'
I0125 15:22:28.089411 31987 slave.cpp:3001] Handling status update TASK_KILLED (UUID: 2b7bb0b4-b336-4a4b-8d86-aafb7a564442) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 from executor(1)@172.17.0.2:56551
W0125 15:22:28.089488 31987 slave.cpp:3054] Ignoring status update TASK_KILLED (UUID: 2b7bb0b4-b336-4a4b-8d86-aafb7a564442) for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 for terminating framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:28.120442 31991 slave.cpp:3481] executor(1)@172.17.0.2:56551 exited
I0125 15:22:28.184353 31996 containerizer.cpp:1443] Executor for container '0e4ce8de-c0bb-4498-810e-b154badfc094' has exited
I0125 15:22:28.188390 31996 provisioner.cpp:306] Ignoring destroy request for unknown container 0e4ce8de-c0bb-4498-810e-b154badfc094
I0125 15:22:28.188851 31996 slave.cpp:3816] Executor '1' of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 terminated with signal Killed
I0125 15:22:28.188913 31996 slave.cpp:3920] Cleaning up executor '1' of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000 at executor(1)@172.17.0.2:56551
I0125 15:22:28.189599 31996 slave.cpp:4008] Cleaning up framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:28.189793 31996 gc.cpp:54] Scheduling '/tmp/HealthCheckTest_HealthStatusChange_uxXuR2/slaves/8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0/frameworks/8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000/executors/1/runs/0e4ce8de-c0bb-4498-810e-b154badfc094' for gc 6.99999781114667days in the future
I0125 15:22:28.189967 31996 gc.cpp:54] Scheduling '/tmp/HealthCheckTest_HealthStatusChange_uxXuR2/slaves/8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0/frameworks/8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000/executors/1' for gc 6.99999780626074days in the future
I0125 15:22:28.190261 31982 status_update_manager.cpp:282] Closing status update streams for framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:28.190332 31982 status_update_manager.cpp:528] Cleaning up status update stream for task 1 of framework 8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000
I0125 15:22:28.190115 31996 gc.cpp:54] Scheduling '/tmp/HealthCheckTest_HealthStatusChange_uxXuR2/slaves/8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-S0/frameworks/8bd61c8a-63c9-4f67-99a4-d6ce2e3f77be-0000' for gc 6.99999780428444days in the future
I0125 15:22:28.191551 31962 slave.cpp:667] Slave terminating
[  FAILED  ] HealthCheckTest.HealthStatusChange (2002 ms)
{code}","30/Jan/16 00:21;neilc;Just saw this on an Arch Linux VM (Virtualbox):

{noformat}
[ RUN      ] HealthCheckTest.HealthStatusChange
/mesos-2/src/tests/health_check_tests.cpp:529: Failure
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7ffd37ae9bd0, @0x7f2afcdf20a0 112-byte object <70-9D F0-08 2B-7F 00-00 00-00 00-00 00-00 00-00 A0-6D 01-CC 2A-7F 00-00 20-EC 1E-02 00-00 00-00 01-00 00-00 02-00 00-00 20-EC 1E-02 00-00 00-00 60-C4 01-CC 2A-7F 00-00 C0-C3 01-CC 2A-7F 00-00 00-00 00-00 00-00 00-00 0C-74 DD-41 00-AB D5-41 20-C5 01-CC 2A-7F 00-00 00-00 00-00 00-00 00-00 B0-20 00-CC 2A-7F 00-00 00-00 00-00 CB-17 00-00>)
         Expected: to be called 4 times
           Actual: called 5 times - over-saturated and active
/mesos-2/src/tests/health_check_tests.cpp:529: Failure
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7ffd37ae9bd0, @0x7f2afb5ef0a0 112-byte object <70-9D F0-08 2B-7F 00-00 00-00 00-00 00-00 00-00 C0-6E 02-D4 2A-7F 00-00 20-EC 1E-02 00-00 00-00 01-00 00-00 02-00 00-00 20-EC 1E-02 00-00 00-00 10-08 02-D4 2A-7F 00-00 10-09 02-D4 2A-7F 00-00 00-00 00-00 01-00 00-00 2D-D0 E3-41 00-AB D5-41 10-07 02-D4 2A-7F 00-00 00-00 00-00 00-00 00-00 D0-07 02-D4 2A-7F 00-00 00-00 00-00 CB-17 00-00>)
         Expected: to be called 4 times
           Actual: called 6 times - over-saturated and active
/mesos-2/src/tests/health_check_tests.cpp:529: Failure
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7ffd37ae9bd0, @0x7f2afcdf20a0 112-byte object <70-9D F0-08 2B-7F 00-00 00-00 00-00 00-00 00-00 90-C5 01-CC 2A-7F 00-00 20-EC 1E-02 00-00 00-00 01-00 00-00 02-00 00-00 20-EC 1E-02 00-00 00-00 00-83 02-CC 2A-7F 00-00 A0-6D 01-CC 2A-7F 00-00 00-00 00-00 00-00 00-00 F5-F7 EA-41 00-AB D5-41 20-4D 00-CC 2A-7F 00-00 00-00 00-00 00-00 00-00 F0-C5 01-CC 2A-7F 00-00 00-00 00-00 CB-17 00-00>)
         Expected: to be called 4 times
           Actual: called 7 times - over-saturated and active
/mesos-2/src/tests/health_check_tests.cpp:529: Failure
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7ffd37ae9bd0, @0x7f2afa5ed0a0 112-byte object <70-9D F0-08 2B-7F 00-00 00-00 00-00 00-00 00-00 A0-89 01-E8 2A-7F 00-00 20-EC 1E-02 00-00 00-00 01-00 00-00 02-00 00-00 20-EC 1E-02 00-00 00-00 60-34 00-E8 2A-7F 00-00 B0-98 01-E8 2A-7F 00-00 00-00 00-00 01-00 00-00 02-0E F2-41 00-AB D5-41 90-BF 00-E8 2A-7F 00-00 00-00 00-00 00-00 00-00 80-B1 00-E8 2A-7F 00-00 00-00 00-00 CB-17 00-00>)
         Expected: to be called 4 times
           Actual: called 8 times - over-saturated and active
/mesos-2/src/tests/health_check_tests.cpp:529: Failure
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7ffd37ae9bd0, @0x7f2afc5f10a0 112-byte object <70-9D F0-08 2B-7F 00-00 00-00 00-00 00-00 00-00 50-58 02-DC 2A-7F 00-00 20-EC 1E-02 00-00 00-00 01-00 00-00 02-00 00-00 20-EC 1E-02 00-00 00-00 B0-1E 01-DC 2A-7F 00-00 40-1F 01-DC 2A-7F 00-00 00-00 00-00 00-00 00-00 4B-76 F8-41 00-AB D5-41 A0-95 02-DC 2A-7F 00-00 00-00 00-00 00-00 00-00 30-3F 01-DC 2A-7F 00-00 00-00 00-00 CB-17 00-00>)
         Expected: to be called 4 times
           Actual: called 9 times - over-saturated and active
/mesos-2/src/tests/health_check_tests.cpp:529: Failure
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7ffd37ae9bd0, @0x7f2afb5ef0a0 112-byte object <70-9D F0-08 2B-7F 00-00 00-00 00-00 00-00 00-00 50-65 03-D4 2A-7F 00-00 20-EC 1E-02 00-00 00-00 01-00 00-00 02-00 00-00 20-EC 1E-02 00-00 00-00 80-06 02-D4 2A-7F 00-00 20-06 02-D4 2A-7F 00-00 00-00 00-00 01-00 00-00 FC-17 FF-41 00-AB D5-41 10-09 02-D4 2A-7F 00-00 00-00 00-00 00-00 00-00 E0-63 03-D4 2A-7F 00-00 00-00 00-00 CB-17 00-00>)
         Expected: to be called 4 times
           Actual: called 10 times - over-saturated and active
/mesos-2/src/tests/health_check_tests.cpp:529: Failure
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7ffd37ae9bd0, @0x7f2afadee0a0 112-byte object <70-9D F0-08 2B-7F 00-00 00-00 00-00 00-00 00-00 10-A5 05-D8 2A-7F 00-00 20-EC 1E-02 00-00 00-00 01-00 00-00 02-00 00-00 20-EC 1E-02 00-00 00-00 60-1C 06-D8 2A-7F 00-00 C0-FF 0A-D8 2A-7F 00-00 00-00 00-00 00-00 00-00 C3-82 05-42 00-AB D5-41 F0-EB 06-D8 2A-7F 00-00 00-00 00-00 00-00 00-00 E0-FB 06-D8 2A-7F 00-00 00-00 00-00 CB-17 00-00>)
         Expected: to be called 4 times
           Actual: called 11 times - over-saturated and active
/mesos-2/src/tests/health_check_tests.cpp:529: Failure
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7ffd37ae9bd0, @0x7f2af9dec0a0 112-byte object <70-9D F0-08 2B-7F 00-00 00-00 00-00 00-00 00-00 B0-44 06-EC 2A-7F 00-00 20-EC 1E-02 00-00 00-00 01-00 00-00 02-00 00-00 20-EC 1E-02 00-00 00-00 C0-3A 00-EC 2A-7F 00-00 90-73 05-EC 2A-7F 00-00 00-00 00-00 01-00 00-00 E4-10 0C-42 00-AB D5-41 20-43 06-EC 2A-7F 00-00 00-00 00-00 00-00 00-00 A0-11 01-EC 2A-7F 00-00 00-00 00-00 CB-17 00-00>)
         Expected: to be called 4 times
           Actual: called 12 times - over-saturated and active
/mesos-2/src/tests/health_check_tests.cpp:529: Failure
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7ffd37ae9bd0, @0x7f2afbdf00a0 112-byte object <70-9D F0-08 2B-7F 00-00 00-00 00-00 00-00 00-00 B0-59 06-E0 2A-7F 00-00 20-EC 1E-02 00-00 00-00 01-00 00-00 02-00 00-00 20-EC 1E-02 00-00 00-00 10-5A 06-E0 2A-7F 00-00 40-C8 01-E0 2A-7F 00-00 00-00 00-00 00-00 00-00 E7-6F 12-42 00-AB D5-41 B0-C7 01-E0 2A-7F 00-00 00-00 00-00 00-00 00-00 30-79 01-E0 2A-7F 00-00 00-00 00-00 CB-17 00-00>)
         Expected: to be called 4 times
           Actual: called 13 times - over-saturated and active
/mesos-2/src/tests/health_check_tests.cpp:529: Failure
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7ffd37ae9bd0, @0x7f2afc5f10a0 112-byte object <70-9D F0-08 2B-7F 00-00 00-00 00-00 00-00 00-00 F0-6B 05-DC 2A-7F 00-00 20-EC 1E-02 00-00 00-00 01-00 00-00 02-00 00-00 20-EC 1E-02 00-00 00-00 E0-9A 02-DC 2A-7F 00-00 D0-3E 01-DC 2A-7F 00-00 00-00 00-00 01-00 00-00 42-EC 18-42 00-AB D5-41 10-94 01-DC 2A-7F 00-00 00-00 00-00 00-00 00-00 50-6C 05-DC 2A-7F 00-00 00-00 00-00 CB-17 00-00>)
         Expected: to be called 4 times
           Actual: called 14 times - over-saturated and active
[  FAILED  ] HealthCheckTest.HealthStatusChange (5608 ms)
{noformat}","30/Jan/16 13:49;haosdent@gmail.com;Could it easy reproduce in your env? For example, after xxx cycles the failure happens for sure. Last time I try run a lot cycles in a CentOS 7(physical machine), but it runs well.","05/Feb/16 00:24;neilc;Fails pretty reliably for me: ~3 failures in 35 test runs. If you'd like more information, just let me know.","06/Feb/16 17:57;haosdent@gmail.com;Yes, [~neilc] do you have the stdout and stderr log?","09/Feb/16 18:30;neilc;Hi [~haosdent@gmail.com] -- I attached the output of {{--verbose}} for a failing run as a file associated with this issue.","09/Feb/16 18:33;haosdent@gmail.com;Thank you very much. Let me try to get more information from you log.","17/Apr/16 15:12;haosdent@gmail.com;Patch: https://reviews.apache.org/r/46307/","19/Apr/16 17:20;haosdent@gmail.com;According to another flaky log provided by [~neilc] (Thanks a lot!)
{code}
[ RUN      ] HealthCheckTest.HealthStatusChange
I0420 00:33:12.691145 15381 cluster.cpp:149] Creating default 'local' authorizer
I0420 00:33:12.691897 15381 leveldb.cpp:174] Opened db in 620608ns
I0420 00:33:12.692211 15381 leveldb.cpp:181] Compacted db in 283038ns
I0420 00:33:12.692265 15381 leveldb.cpp:196] Created db iterator in 15184ns
I0420 00:33:12.692273 15381 leveldb.cpp:202] Seeked to beginning of db in 2161ns
I0420 00:33:12.692281 15381 leveldb.cpp:271] Iterated through 0 keys in the db in 1084ns
I0420 00:33:12.692312 15381 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0420 00:33:12.693172 15398 recover.cpp:447] Starting replica recovery
I0420 00:33:12.693435 15400 recover.cpp:473] Replica is in EMPTY status
I0420 00:33:12.695106 15400 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (3755)@10.0.2.15:41408
I0420 00:33:12.695829 15400 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0420 00:33:12.696321 15398 recover.cpp:564] Updating replica status to STARTING
I0420 00:33:12.696807 15402 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 101090ns
I0420 00:33:12.696836 15402 replica.cpp:320] Persisted replica status to STARTING
I0420 00:33:12.696964 15402 recover.cpp:473] Replica is in STARTING status
I0420 00:33:12.697968 15400 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (3756)@10.0.2.15:41408
I0420 00:33:12.698274 15400 recover.cpp:193] Received a recover response from a replica in STARTING status
I0420 00:33:12.698691 15400 recover.cpp:564] Updating replica status to VOTING
I0420 00:33:12.698882 15400 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 62549ns
I0420 00:33:12.698938 15400 replica.cpp:320] Persisted replica status to VOTING
I0420 00:33:12.699002 15400 recover.cpp:578] Successfully joined the Paxos group
I0420 00:33:12.699128 15400 recover.cpp:462] Recover process terminated
I0420 00:33:12.721339 15401 master.cpp:382] Master 7cf5923c-3d03-4ed6-826a-efa97f54e765 (archlinux.vagrant.vm) started on 10.0.2.15:41408
I0420 00:33:12.721387 15401 master.cpp:384] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/9zoT36/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/9zoT36/master"" --zk_session_timeout=""10secs""
I0420 00:33:12.721667 15401 master.cpp:433] Master only allowing authenticated frameworks to register
I0420 00:33:12.721736 15401 master.cpp:439] Master only allowing authenticated agents to register
I0420 00:33:12.721745 15401 master.cpp:445] Master only allowing authenticated HTTP frameworks to register
I0420 00:33:12.721750 15401 credentials.hpp:37] Loading credentials for authentication from '/tmp/9zoT36/credentials'
I0420 00:33:12.722005 15401 master.cpp:489] Using default 'crammd5' authenticator
I0420 00:33:12.723206 15401 master.cpp:560] Using default 'basic' HTTP authenticator
I0420 00:33:12.723458 15401 master.cpp:640] Using default 'basic' HTTP framework authenticator
I0420 00:33:12.723574 15401 master.cpp:687] Authorization enabled
I0420 00:33:12.725332 15399 master.cpp:1932] The newly elected leader is master@10.0.2.15:41408 with id 7cf5923c-3d03-4ed6-826a-efa97f54e765
I0420 00:33:12.725376 15399 master.cpp:1945] Elected as the leading master!
I0420 00:33:12.725389 15399 master.cpp:1632] Recovering from registrar
I0420 00:33:12.725544 15399 registrar.cpp:331] Recovering registrar
I0420 00:33:12.726506 15399 log.cpp:524] Attempting to start the writer
I0420 00:33:12.727424 15399 replica.cpp:493] Replica received implicit promise request from (3759)@10.0.2.15:41408 with proposal 1
I0420 00:33:12.727517 15399 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 50404ns
I0420 00:33:12.727541 15399 replica.cpp:342] Persisted promised to 1
I0420 00:33:12.728299 15395 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0420 00:33:12.735360 15402 replica.cpp:388] Replica received explicit promise request from (3760)@10.0.2.15:41408 for position 0 with proposal 2
I0420 00:33:12.735448 15402 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 51677ns
I0420 00:33:12.735525 15402 replica.cpp:712] Persisted action at 0
I0420 00:33:12.736783 15399 replica.cpp:537] Replica received write request for position 0 from (3761)@10.0.2.15:41408
I0420 00:33:12.736955 15399 leveldb.cpp:436] Reading position from leveldb took 27734ns
I0420 00:33:12.737108 15399 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 40858ns
I0420 00:33:12.737249 15399 replica.cpp:712] Persisted action at 0
I0420 00:33:12.738021 15398 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0420 00:33:12.738092 15398 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 35952ns
I0420 00:33:12.738131 15398 replica.cpp:712] Persisted action at 0
I0420 00:33:12.738145 15398 replica.cpp:697] Replica learned NOP action at position 0
I0420 00:33:12.739038 15395 log.cpp:540] Writer started with ending position 0
I0420 00:33:12.740397 15400 leveldb.cpp:436] Reading position from leveldb took 43374ns
I0420 00:33:12.741741 15397 registrar.cpp:364] Successfully fetched the registry (0B) in 16.1408ms
I0420 00:33:12.741844 15397 registrar.cpp:463] Applied 1 operations in 22449ns; attempting to update the 'registry'
I0420 00:33:12.742624 15397 log.cpp:548] Attempting to append 185 bytes to the log
I0420 00:33:12.742825 15399 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0420 00:33:12.743827 15399 replica.cpp:537] Replica received write request for position 1 from (3762)@10.0.2.15:41408
I0420 00:33:12.744009 15399 leveldb.cpp:341] Persisting action (204 bytes) to leveldb took 59247ns
I0420 00:33:12.744189 15399 replica.cpp:712] Persisted action at 1
I0420 00:33:12.745023 15397 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0420 00:33:12.745139 15397 leveldb.cpp:341] Persisting action (206 bytes) to leveldb took 65024ns
I0420 00:33:12.745167 15397 replica.cpp:712] Persisted action at 1
I0420 00:33:12.745183 15397 replica.cpp:697] Replica learned APPEND action at position 1
I0420 00:33:12.746696 15395 registrar.cpp:508] Successfully updated the 'registry' in 4.788992ms
I0420 00:33:12.746841 15395 registrar.cpp:394] Successfully recovered registrar
I0420 00:33:12.746913 15401 log.cpp:567] Attempting to truncate the log to 1
I0420 00:33:12.747383 15399 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0420 00:33:12.747747 15395 master.cpp:1740] Recovered 0 agents from the Registry (146B) ; allowing 10mins for agents to re-register
I0420 00:33:12.749722 15397 replica.cpp:537] Replica received write request for position 2 from (3763)@10.0.2.15:41408
I0420 00:33:12.749837 15397 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 51623ns
I0420 00:33:12.749862 15397 replica.cpp:712] Persisted action at 2
I0420 00:33:12.750529 15400 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0420 00:33:12.750602 15400 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 34873ns
I0420 00:33:12.750695 15400 leveldb.cpp:399] Deleting ~1 keys from leveldb took 23637ns
I0420 00:33:12.750730 15400 replica.cpp:712] Persisted action at 2
I0420 00:33:12.750746 15400 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0420 00:33:12.761435 15381 containerizer.cpp:174] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni
W0420 00:33:12.762092 15381 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0420 00:33:12.810016 15400 slave.cpp:201] Agent started on 76)@10.0.2.15:41408
I0420 00:33:12.810070 15400 slave.cpp:202] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/HealthCheckTest_HealthStatusChange_T3YdIF/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/HealthCheckTest_HealthStatusChange_T3YdIF/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""true"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/tmp/HealthCheckTest_HealthStatusChange_T3YdIF/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/home/vagrant/build-mesos-default-opts/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/HealthCheckTest_HealthStatusChange_T3YdIF""
I0420 00:33:12.810362 15400 credentials.hpp:86] Loading credential for authentication from '/tmp/HealthCheckTest_HealthStatusChange_T3YdIF/credential'
I0420 00:33:12.810511 15400 slave.cpp:339] Agent using credential for: test-principal
I0420 00:33:12.810564 15400 credentials.hpp:37] Loading credentials for authentication from '/tmp/HealthCheckTest_HealthStatusChange_T3YdIF/http_credentials'
I0420 00:33:12.810765 15400 slave.cpp:391] Using default 'basic' HTTP authenticator
I0420 00:33:12.839030 15400 slave.cpp:590] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0420 00:33:12.839357 15400 slave.cpp:598] Agent attributes: [  ]
I0420 00:33:12.839551 15400 slave.cpp:603] Agent hostname: archlinux.vagrant.vm
I0420 00:33:12.841415 15400 state.cpp:57] Recovering state from '/tmp/HealthCheckTest_HealthStatusChange_T3YdIF/meta'
I0420 00:33:12.841830 15400 status_update_manager.cpp:200] Recovering status update manager
I0420 00:33:12.842087 15397 containerizer.cpp:439] Recovering containerizer
I0420 00:33:12.843364 15395 provisioner.cpp:245] Provisioner recovery complete
I0420 00:33:12.843719 15395 slave.cpp:4800] Finished recovery
I0420 00:33:12.844300 15395 slave.cpp:945] New master detected at master@10.0.2.15:41408
I0420 00:33:12.844360 15401 status_update_manager.cpp:174] Pausing sending status updates
I0420 00:33:12.844458 15395 slave.cpp:1008] Authenticating with master master@10.0.2.15:41408
I0420 00:33:12.844509 15395 slave.cpp:1013] Using default CRAM-MD5 authenticatee
I0420 00:33:12.844620 15395 slave.cpp:981] Detecting new master
I0420 00:33:12.844779 15395 authenticatee.cpp:121] Creating new client SASL connection
I0420 00:33:12.845008 15395 master.cpp:5801] Authenticating slave(76)@10.0.2.15:41408
I0420 00:33:12.845345 15395 authenticator.cpp:98] Creating new server SASL connection
I0420 00:33:12.845471 15395 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0420 00:33:12.845504 15395 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0420 00:33:12.845552 15395 authenticator.cpp:203] Received SASL authentication start
I0420 00:33:12.845609 15395 authenticator.cpp:325] Authentication requires more steps
I0420 00:33:12.845657 15395 authenticatee.cpp:258] Received SASL authentication step
I0420 00:33:12.845787 15395 authenticator.cpp:231] Received SASL authentication step
I0420 00:33:12.845851 15395 authenticator.cpp:317] Authentication success
I0420 00:33:12.846011 15400 authenticatee.cpp:298] Authentication success
I0420 00:33:12.846009 15396 master.cpp:5831] Successfully authenticated principal 'test-principal' at slave(76)@10.0.2.15:41408
I0420 00:33:12.846340 15400 slave.cpp:1078] Successfully authenticated with master master@10.0.2.15:41408
I0420 00:33:12.846705 15396 master.cpp:4512] Registering agent at slave(76)@10.0.2.15:41408 (archlinux.vagrant.vm) with id 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0
I0420 00:33:12.847108 15396 registrar.cpp:463] Applied 1 operations in 45782ns; attempting to update the 'registry'
I0420 00:33:12.847733 15400 log.cpp:548] Attempting to append 362 bytes to the log
I0420 00:33:12.848198 15401 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0420 00:33:12.848323 15400 master.cpp:4500] Ignoring register agent message from slave(76)@10.0.2.15:41408 (archlinux.vagrant.vm) as admission is already in progress
I0420 00:33:12.849373 15402 replica.cpp:537] Replica received write request for position 3 from (3779)@10.0.2.15:41408
I0420 00:33:12.849591 15402 leveldb.cpp:341] Persisting action (381 bytes) to leveldb took 56531ns
I0420 00:33:12.849700 15402 replica.cpp:712] Persisted action at 3
I0420 00:33:12.850530 15402 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0420 00:33:12.850774 15402 leveldb.cpp:341] Persisting action (383 bytes) to leveldb took 37554ns
I0420 00:33:12.850862 15402 replica.cpp:712] Persisted action at 3
I0420 00:33:12.850985 15402 replica.cpp:697] Replica learned APPEND action at position 3
I0420 00:33:12.852195 15402 registrar.cpp:508] Successfully updated the 'registry' in 4.989952ms
I0420 00:33:12.852668 15395 log.cpp:567] Attempting to truncate the log to 3
I0420 00:33:12.852953 15401 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0420 00:33:12.853283 15397 master.cpp:4580] Registered agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0 at slave(76)@10.0.2.15:41408 (archlinux.vagrant.vm) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0420 00:33:12.853458 15397 hierarchical.cpp:473] Added agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0 (archlinux.vagrant.vm) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0420 00:33:12.853458 15396 slave.cpp:1122] Registered with master master@10.0.2.15:41408; given agent ID 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0
I0420 00:33:12.854084 15396 slave.cpp:1182] Forwarding total oversubscribed resources
I0420 00:33:12.854123 15398 status_update_manager.cpp:181] Resuming sending status updates
I0420 00:33:12.854295 15402 master.cpp:4924] Received update of agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0 at slave(76)@10.0.2.15:41408 (archlinux.vagrant.vm) with total oversubscribed resources
I0420 00:33:12.854722 15402 hierarchical.cpp:531] Agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0 (archlinux.vagrant.vm) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0420 00:33:12.855062 15396 replica.cpp:537] Replica received write request for position 4 from (3780)@10.0.2.15:41408
I0420 00:33:12.855365 15396 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 130625ns
I0420 00:33:12.855507 15396 replica.cpp:712] Persisted action at 4
I0420 00:33:12.856474 15398 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0420 00:33:12.856557 15398 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 37939ns
I0420 00:33:12.856595 15398 leveldb.cpp:399] Deleting ~2 keys from leveldb took 15093ns
I0420 00:33:12.856617 15398 replica.cpp:712] Persisted action at 4
I0420 00:33:12.856632 15398 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0420 00:33:12.895501 15381 sched.cpp:224] Version: 0.29.0
I0420 00:33:12.896266 15397 sched.cpp:328] New master detected at master@10.0.2.15:41408
I0420 00:33:12.896371 15397 sched.cpp:384] Authenticating with master master@10.0.2.15:41408
I0420 00:33:12.896396 15397 sched.cpp:391] Using default CRAM-MD5 authenticatee
I0420 00:33:12.896672 15399 authenticatee.cpp:121] Creating new client SASL connection
I0420 00:33:12.896965 15399 master.cpp:5801] Authenticating scheduler-5bd5e446-a017-45d9-8193-be7d23002487@10.0.2.15:41408
I0420 00:33:12.897411 15399 authenticator.cpp:98] Creating new server SASL connection
I0420 00:33:12.897600 15399 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0420 00:33:12.897644 15399 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0420 00:33:12.897754 15399 authenticator.cpp:203] Received SASL authentication start
I0420 00:33:12.897883 15399 authenticator.cpp:325] Authentication requires more steps
I0420 00:33:12.898066 15399 authenticatee.cpp:258] Received SASL authentication step
I0420 00:33:12.898208 15399 authenticator.cpp:231] Received SASL authentication step
I0420 00:33:12.898401 15399 authenticator.cpp:317] Authentication success
I0420 00:33:12.898542 15399 master.cpp:5831] Successfully authenticated principal 'test-principal' at scheduler-5bd5e446-a017-45d9-8193-be7d23002487@10.0.2.15:41408
I0420 00:33:12.898576 15400 authenticatee.cpp:298] Authentication success
I0420 00:33:12.898938 15400 sched.cpp:474] Successfully authenticated with master master@10.0.2.15:41408
I0420 00:33:12.899281 15400 master.cpp:2463] Received SUBSCRIBE call for framework 'default' at scheduler-5bd5e446-a017-45d9-8193-be7d23002487@10.0.2.15:41408
I0420 00:33:12.899384 15400 master.cpp:1971] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0420 00:33:12.899658 15395 master.cpp:2539] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0420 00:33:12.900406 15395 sched.cpp:706] Framework registered with 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
I0420 00:33:12.900682 15400 hierarchical.cpp:264] Added framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
I0420 00:33:12.901666 15399 master.cpp:5630] Sending 1 offers to framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 (default) at scheduler-5bd5e446-a017-45d9-8193-be7d23002487@10.0.2.15:41408
I0420 00:33:12.903337 15401 master.cpp:3410] Processing ACCEPT call for offers: [ 7cf5923c-3d03-4ed6-826a-efa97f54e765-O0 ] on agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0 at slave(76)@10.0.2.15:41408 (archlinux.vagrant.vm) for framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 (default) at scheduler-5bd5e446-a017-45d9-8193-be7d23002487@10.0.2.15:41408
I0420 00:33:12.903411 15401 master.cpp:3013] Authorizing framework principal 'test-principal' to launch task 1 as user 'vagrant'
I0420 00:33:12.904501 15399 master.hpp:177] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0 (archlinux.vagrant.vm)
I0420 00:33:12.904616 15399 master.cpp:3895] Launching task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 (default) at scheduler-5bd5e446-a017-45d9-8193-be7d23002487@10.0.2.15:41408 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0 at slave(76)@10.0.2.15:41408 (archlinux.vagrant.vm)
I0420 00:33:12.904985 15401 slave.cpp:1514] Got assigned task 1 for framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
I0420 00:33:12.905398 15401 slave.cpp:1633] Launching task 1 for framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
I0420 00:33:12.905660 15401 paths.cpp:528] Trying to chown '/tmp/HealthCheckTest_HealthStatusChange_T3YdIF/slaves/7cf5923c-3d03-4ed6-826a-efa97f54e765-S0/frameworks/7cf5923c-3d03-4ed6-826a-efa97f54e765-0000/executors/1/runs/8e9198d4-5a23-40f2-b372-8130ec375783' to user 'vagrant'
I0420 00:33:12.909620 15401 slave.cpp:5620] Launching executor 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/HealthCheckTest_HealthStatusChange_T3YdIF/slaves/7cf5923c-3d03-4ed6-826a-efa97f54e765-S0/frameworks/7cf5923c-3d03-4ed6-826a-efa97f54e765-0000/executors/1/runs/8e9198d4-5a23-40f2-b372-8130ec375783'
I0420 00:33:12.910140 15401 slave.cpp:1851] Queuing task '1' for executor '1' of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
I0420 00:33:12.910404 15395 containerizer.cpp:698] Starting container '8e9198d4-5a23-40f2-b372-8130ec375783' for executor '1' of framework '7cf5923c-3d03-4ed6-826a-efa97f54e765-0000'
I0420 00:33:12.914448 15395 launcher.cpp:123] Forked child with pid '20895' for container '8e9198d4-5a23-40f2-b372-8130ec375783'
I0420 00:33:13.059639 20895 exec.cpp:150] Version: 0.29.0
I0420 00:33:13.065830 15399 slave.cpp:2842] Got registration for executor '1' of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 from executor(1)@10.0.2.15:37107
I0420 00:33:13.067417 15402 slave.cpp:2016] Sending queued task '1' to executor '1' of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 at executor(1)@10.0.2.15:37107
I0420 00:33:13.068193 20923 exec.cpp:225] Executor registered on agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0
Registered executor on archlinux.vagrant.vm
Starting task 1
sh -c 'sleep 120'
Forked command at 20931
Launching health check process: /home/vagrant/build-mesos-default-opts/src/mesos-health-check --executor=(1)@10.0.2.15:37107 --health_check_json={""command"":{""shell"":true,""value"":""rm \/tmp\/1NKfr1 || (touch \/tmp\/1NKfr1 && exit 1)""},""consecutive_failures"":3,""delay_seconds"":0.0,""grace_period_seconds"":0.0,""interval_seconds"":0.0,""timeout_seconds"":20.0} --task_id=1
Health check process launched at pid: 20932
I0420 00:33:13.075927 15396 slave.cpp:3201] Handling status update TASK_RUNNING (UUID: 02c2030b-ec12-485c-83e8-5ae201178bea) for task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 from executor(1)@10.0.2.15:37107
I0420 00:33:13.077280 15396 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 02c2030b-ec12-485c-83e8-5ae201178bea) for task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
I0420 00:33:13.077841 15396 slave.cpp:3599] Forwarding the update TASK_RUNNING (UUID: 02c2030b-ec12-485c-83e8-5ae201178bea) for task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 to master@10.0.2.15:41408
I0420 00:33:13.077960 15396 slave.cpp:3509] Sending acknowledgement for status update TASK_RUNNING (UUID: 02c2030b-ec12-485c-83e8-5ae201178bea) for task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 to executor(1)@10.0.2.15:37107
I0420 00:33:13.078277 15397 master.cpp:5069] Status update TASK_RUNNING (UUID: 02c2030b-ec12-485c-83e8-5ae201178bea) for task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 from agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0 at slave(76)@10.0.2.15:41408 (archlinux.vagrant.vm)
I0420 00:33:13.078330 15397 master.cpp:5117] Forwarding status update TASK_RUNNING (UUID: 02c2030b-ec12-485c-83e8-5ae201178bea) for task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
I0420 00:33:13.078431 15397 master.cpp:6725] Updating the state of task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0420 00:33:13.079056 15397 master.cpp:4224] Processing ACKNOWLEDGE call 02c2030b-ec12-485c-83e8-5ae201178bea for task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 (default) at scheduler-5bd5e446-a017-45d9-8193-be7d23002487@10.0.2.15:41408 on agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0
I0420 00:33:13.079445 15396 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 02c2030b-ec12-485c-83e8-5ae201178bea) for task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
Received task health update, healthy: true
I0420 00:33:13.302896 15396 slave.cpp:3201] Handling status update TASK_RUNNING (UUID: 8506a0b4-03cd-435c-bdfa-b88fffe1d977) for task 1 in health state healthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 from executor(1)@10.0.2.15:37107
I0420 00:33:13.304239 15396 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 8506a0b4-03cd-435c-bdfa-b88fffe1d977) for task 1 in health state healthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
I0420 00:33:13.304553 15396 slave.cpp:3599] Forwarding the update TASK_RUNNING (UUID: 8506a0b4-03cd-435c-bdfa-b88fffe1d977) for task 1 in health state healthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 to master@10.0.2.15:41408
I0420 00:33:13.304700 15396 slave.cpp:3509] Sending acknowledgement for status update TASK_RUNNING (UUID: 8506a0b4-03cd-435c-bdfa-b88fffe1d977) for task 1 in health state healthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 to executor(1)@10.0.2.15:37107
I0420 00:33:13.305068 15396 master.cpp:5069] Status update TASK_RUNNING (UUID: 8506a0b4-03cd-435c-bdfa-b88fffe1d977) for task 1 in health state healthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 from agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0 at slave(76)@10.0.2.15:41408 (archlinux.vagrant.vm)
I0420 00:33:13.305097 15396 master.cpp:5117] Forwarding status update TASK_RUNNING (UUID: 8506a0b4-03cd-435c-bdfa-b88fffe1d977) for task 1 in health state healthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
I0420 00:33:13.305197 15396 master.cpp:6725] Updating the state of task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0420 00:33:13.306653 15396 master.cpp:4224] Processing ACKNOWLEDGE call 8506a0b4-03cd-435c-bdfa-b88fffe1d977 for task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 (default) at scheduler-5bd5e446-a017-45d9-8193-be7d23002487@10.0.2.15:41408 on agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0
I0420 00:33:13.307194 15395 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 8506a0b4-03cd-435c-bdfa-b88fffe1d977) for task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
rm: cannot remove '/tmp/1NKfr1': No such file or directory
I0420 00:33:13.308562 15401 http.cpp:313] HTTP GET for /master/state from 10.0.2.15:44472
I0420 00:33:13.314609 15396 http.cpp:178] HTTP GET for /slave(76)/state from 10.0.2.15:44474
Received task health update, healthy: false
I0420 00:33:13.489945 15396 slave.cpp:3201] Handling status update TASK_RUNNING (UUID: 34c9cab0-3f63-4ee6-b2a0-5db4e406afbd) for task 1 in health state unhealthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 from executor(1)@10.0.2.15:37107
I0420 00:33:13.492075 15399 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 34c9cab0-3f63-4ee6-b2a0-5db4e406afbd) for task 1 in health state unhealthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
I0420 00:33:13.492472 15400 slave.cpp:3599] Forwarding the update TASK_RUNNING (UUID: 34c9cab0-3f63-4ee6-b2a0-5db4e406afbd) for task 1 in health state unhealthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 to master@10.0.2.15:41408
I0420 00:33:13.492743 15400 slave.cpp:3509] Sending acknowledgement for status update TASK_RUNNING (UUID: 34c9cab0-3f63-4ee6-b2a0-5db4e406afbd) for task 1 in health state unhealthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 to executor(1)@10.0.2.15:37107
I0420 00:33:13.492822 15397 master.cpp:5069] Status update TASK_RUNNING (UUID: 34c9cab0-3f63-4ee6-b2a0-5db4e406afbd) for task 1 in health state unhealthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 from agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0 at slave(76)@10.0.2.15:41408 (archlinux.vagrant.vm)
I0420 00:33:13.492871 15397 master.cpp:5117] Forwarding status update TASK_RUNNING (UUID: 34c9cab0-3f63-4ee6-b2a0-5db4e406afbd) for task 1 in health state unhealthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
I0420 00:33:13.493135 15397 master.cpp:6725] Updating the state of task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0420 00:33:13.493801 15398 master.cpp:4224] Processing ACKNOWLEDGE call 34c9cab0-3f63-4ee6-b2a0-5db4e406afbd for task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 (default) at scheduler-5bd5e446-a017-45d9-8193-be7d23002487@10.0.2.15:41408 on agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0
I0420 00:33:13.494067 15399 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 34c9cab0-3f63-4ee6-b2a0-5db4e406afbd) for task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
I0420 00:33:13.497138 15400 http.cpp:313] HTTP GET for /master/state from 10.0.2.15:44478
Received task health update, healthy: true
I0420 00:33:13.502598 15400 slave.cpp:3201] Handling status update TASK_RUNNING (UUID: e19c76cc-096a-4398-b616-afb628b8e5b8) for task 1 in health state healthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 from executor(1)@10.0.2.15:37107
I0420 00:33:13.504456 15400 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: e19c76cc-096a-4398-b616-afb628b8e5b8) for task 1 in health state healthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
I0420 00:33:13.505009 15400 slave.cpp:3599] Forwarding the update TASK_RUNNING (UUID: e19c76cc-096a-4398-b616-afb628b8e5b8) for task 1 in health state healthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 to master@10.0.2.15:41408
I0420 00:33:13.505167 15400 slave.cpp:3509] Sending acknowledgement for status update TASK_RUNNING (UUID: e19c76cc-096a-4398-b616-afb628b8e5b8) for task 1 in health state healthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 to executor(1)@10.0.2.15:37107
I0420 00:33:13.505524 15400 master.cpp:5069] Status update TASK_RUNNING (UUID: e19c76cc-096a-4398-b616-afb628b8e5b8) for task 1 in health state healthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 from agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0 at slave(76)@10.0.2.15:41408 (archlinux.vagrant.vm)
I0420 00:33:13.505602 15400 master.cpp:5117] Forwarding status update TASK_RUNNING (UUID: e19c76cc-096a-4398-b616-afb628b8e5b8) for task 1 in health state healthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
I0420 00:33:13.505738 15400 master.cpp:6725] Updating the state of task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0420 00:33:13.505985 15400 master.cpp:4224] Processing ACKNOWLEDGE call e19c76cc-096a-4398-b616-afb628b8e5b8 for task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 (default) at scheduler-5bd5e446-a017-45d9-8193-be7d23002487@10.0.2.15:41408 on agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0
I0420 00:33:13.506142 15400 status_update_manager.cpp:392] Received status update acknowledgement (UUID: e19c76cc-096a-4398-b616-afb628b8e5b8) for task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
rm: cannot remove '/tmp/1NKfr1': No such file or directory
I0420 00:33:13.508203 15400 http.cpp:178] HTTP GET for /slave(76)/state from 10.0.2.15:44482
../../mesos/src/tests/health_check_tests.cpp:647: Failure
Value of: (find).get()
  Actual: 16-byte object <05-00 00-00 00-00 00-00 90-C4 2D-03 00-00 00-00>
Expected: false
Which is: false
*** Aborted at 1461076393 (unix time) try ""date -d @1461076393"" if you are using GNU date ***
PC: @          0x1899ba0 testing::UnitTest::AddTestPartResult()
*** SIGSEGV (@0x0) received by PID 15381 (TID 0x7f0aa958a7c0) from PID 0; stack trace: ***
    @     0x7f0aa3259e80 (unknown)
    @          0x1899ba0 testing::UnitTest::AddTestPartResult()
    @          0x188e319 testing::internal::AssertHelper::operator=()
    @           0xdfa79a mesos::internal::tests::HealthCheckTest_HealthStatusChange_Test::TestBody()
    @          0x18b7e16 testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x18b2e52 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x189379e testing::Test::Run()
    @          0x1893f56 testing::TestInfo::Run()
    @          0x18945a7 testing::TestCase::Run()
    @          0x189b0e5 testing::internal::UnitTestImpl::RunAllTests()
    @          0x18b8add testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x18b3992 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1899dc1 testing::UnitTest::Run()
Received task health update, healthy: false
    @           0xf9375c RUN_ALL_TESTS()
I0420 00:33:13.603853 15399 slave.cpp:3201] Handling status update TASK_RUNNING (UUID: 1b2154a8-6f83-4b4a-a811-1db971288986) for task 1 in health state unhealthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 from executor(1)@10.0.2.15:37107
    @           0xf93378 main
    @     0x7f0aa27b7710 __libc_start_main
I0420 00:33:13.605831 15399 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 1b2154a8-6f83-4b4a-a811-1db971288986) for task 1 in health state unhealthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
I0420 00:33:13.606142 15399 slave.cpp:3599] Forwarding the update TASK_RUNNING (UUID: 1b2154a8-6f83-4b4a-a811-1db971288986) for task 1 in health state unhealthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 to master@10.0.2.15:41408
I0420 00:33:13.606263 15399 slave.cpp:3509] Sending acknowledgement for status update TASK_RUNNING (UUID: 1b2154a8-6f83-4b4a-a811-1db971288986) for task 1 in health state unhealthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 to executor(1)@10.0.2.15:37107
I0420 00:33:13.606644 15399 master.cpp:5069] Status update TASK_RUNNING (UUID: 1b2154a8-6f83-4b4a-a811-1db971288986) for task 1 in health state unhealthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 from agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0 at slave(76)@10.0.2.15:41408 (archlinux.vagrant.vm)
I0420 00:33:13.606685 15399 master.cpp:5117] Forwarding status update TASK_RUNNING (UUID: 1b2154a8-6f83-4b4a-a811-1db971288986) for task 1 in health state unhealthy of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
I0420 00:33:13.606780 15399 master.cpp:6725] Updating the state of task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0420 00:33:13.607326 15399 master.cpp:4224] Processing ACKNOWLEDGE call 1b2154a8-6f83-4b4a-a811-1db971288986 for task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000 (default) at scheduler-5bd5e446-a017-45d9-8193-be7d23002487@10.0.2.15:41408 on agent 7cf5923c-3d03-4ed6-826a-efa97f54e765-S0
I0420 00:33:13.607579 15399 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 1b2154a8-6f83-4b4a-a811-1db971288986) for task 1 of framework 7cf5923c-3d03-4ed6-826a-efa97f54e765-0000
    @           0xa644d9 _start
    @                0x0 (unknown)
I0420 00:33:13.701517 20926 exec.cpp:481] Agent exited ... shutting down
Segmentation fault (core dumped)
vagrant@archlinux:~/build-mesos-default-opts$ Received task health update, healthy: true
Shutting down
Sending SIGTERM to process tree at pid 20931
E0420 00:33:13.705188 20930 process.cpp:1963] Failed to shutdown socket with fd 11: Transport endpoint is not connected
rm: cannot remove '/tmp/1NKfr1': No such file or directory
Sent SIGTERM to the following process trees:
[
--- 20931 sleep 120
]
Command terminated with signal Terminated (pid: 20931)
E0420 00:33:13.786746 20930 process.cpp:1963] Failed to shutdown socket with fd 8: Transport endpoint is not connected
{code}

This test case have another tricky part. We could not guarantee the http queries order match the statusUpdate order. When we send http queries, master have already received the statusUpdate multiple times.

And because {{mesos-health-check}} is launched by {{subprocess}}, could not controlled by {{Clock::pause()}}. Seems not way to ensure http queries are match expected behaviour in this test case. ","26/Apr/16 16:51;haosdent@gmail.com;I prefer to drop the http status check in the test cases, because I still don't have any clues to guarantee the http queries order match the statusUpdate order. [~neilc] [~bmahler] [~tnachen] Do you think this proposal is doable?","07/May/16 09:01;haosdent@gmail.com;Patch:

| Ignored subsequent status update in HealthStatusChange tests. | https://reviews.apache.org/r/46307/ |
| Dropped http status check in HealthCheckTest.HealthStatusChange. | https://reviews.apache.org/r/47088/ |","24/Nov/16 22:33;alexr;I can reproduce it relatively easy by running _parallel_ {{make check}}. Here is a fresh log:
{noformat}
[ RUN      ] HealthCheckTest.HealthStatusChange
I1124 23:20:48.351884 4284416 exec.cpp:162] Version: 1.2.0
I1124 23:20:48.375592 3747840 exec.cpp:237] Executor registered on agent 6db7ef4d-7211-47be-98ba-ad590b528c69-S0
Received SUBSCRIBED event
Subscribed executor on alexr.speedportneo09012801000249
Received LAUNCH event
Starting task 1
/Users/alex/Projects/mesos/build/parallel/src/mesos-containerizer launch --command=""{""shell"":true,""value"":""sleep 120""}"" --help=""false""
Forked command at 73286
Received task health update, healthy: true
rm: /private/tmp/z1PbfH/rG7Gha: No such file or directory
W1124 23:20:48.544631 3211264 health_checker.cpp:245] Health check failed 1 times consecutively: COMMAND health check failed: Command returned exited with status 1
Received task health update, healthy: false
Received task health update, healthy: true
rm: /private/tmp/z1PbfH/rG7Gha: No such file or directory
../../../src/tests/health_check_tests.cpp:790: Failure
Value of: (find).get()
  Actual: 16-byte object <05-00 00-00 00-00 00-00 60-A9 62-1B B2-7F 00-00>
Expected: false
Which is: false
I1124 23:20:48.732457 4284416 exec.cpp:414] Executor asked to shutdown
Received SHUTDOWN event
Shutting down
Sending SIGTERM to process tree at pid 73286
W1124 23:20:48.747885 1064960 health_checker.cpp:245] Health check failed 1 times consecutively: COMMAND health check failed: Command returned exited with status 1
rm: /private/tmp/z1PbfH/rG7Gha: No such file or directory
W1124 23:20:48.948562 3747840 health_checker.cpp:245] Health check failed 1 times consecutively: COMMAND health check failed: Command returned exited with status 1
Sent SIGTERM to the following process trees:
[ 
--- 73286 sleep 120
]
Scheduling escalation to SIGKILL in 3secs from now
[  FAILED  ] HealthCheckTest.HealthStatusChange (1639 ms)
{noformat}

These lines
{noformat}
Received task health update, healthy: true
rm: /private/tmp/z1PbfH/rG7Gha: No such file or directory
../../../src/tests/health_check_tests.cpp:790: Failure
{noformat}
obviously hint that we've queried the HTTP endpoint _after_ the next health status change.","25/Nov/16 15:41;alexr;{noformat}
Commit: a7a1ced113ee6a6633bbb0d78f401d0c01e5205b [a7a1ced]
Author: haosdent huang <haosdent@gmail.com>
Date: 25 November 2016 at 16:32:48 GMT+1
Committer: Alexander Rukletsov <alexr@apache.org>

Ignored subsequent status update in HealthStatusChange tests.

In HealthStatusChange test cases, we launch a task that toggles between
healthy and unhealthy, and will never be killed because no consecutive
health failures occur. We need to ignore subsequent status updates: it
is possible to continue to receive status updates before we stop the
driver.

Review: https://reviews.apache.org/r/46307/
{noformat}
{noformat}
Commit: 83863c7d56ca91f5aa3f5df2dd4887d589462eaf [83863c7]
Author: haosdent huang <haosdent@gmail.com>
Date: 25 November 2016 at 16:33:07 GMT+1
Committer: Alexander Rukletsov <alexr@apache.org>
Commit Date: 25 November 2016 at 16:38:49 GMT+1

Dropped http status check in HealthCheckTest.HealthStatusChange.

Drop http status check because so far we could not guarantee the
http queries order match the `statusUpdate` order or arrive in
sync, i.e., before the next status update has been processed.

Review: https://reviews.apache.org/r/47088/
{noformat}",,,,,,,,,,,,,
Reconciliation can send out-of-order updates.,MESOS-1799,12741911,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,vinodkone,bmahler,bmahler,16/Sep/14 20:04,21/Oct/14 22:49,29/Oct/20 16:32,21/Oct/14 22:49,,,,,,,,,0.21.0,,,,,,agent,master,,,,0,,,,,,,,,"When a slave re-registers with the master, it currently sends the latest task state for all tasks that are not both terminal and acknowledged.

However, reconciliation assumes that we always have the latest unacknowledged state of the task represented in the master.

As a result, out-of-order updates are possible, e.g.

(1) Slave has task T in TASK_FINISHED, with unacknowledged updates: [TASK_RUNNING, TASK_FINISHED].
(2) Master fails over.
(3) New master re-registers the slave with T in TASK_FINISHED.
(4) Reconciliation request arrives, master sends TASK_FINISHED.
(5) Slave sends TASK_RUNNING to master, master sends TASK_RUNNING.

I think the fix here is to preserve the task state invariants in the master, namely, that the master has the latest unacknowledged state of the task. This means when the slave re-registers, it should instead send the latest acknowledged state of each task.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-10-09 18:17:46.104,,,false,MESOS-1407,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 21 22:49:27 UTC 2014,,,,,,,"0|i00b8v:",9223372036854775807,,,,,bmahler,,,,,,Mesos Q3 Sprint 6,Twitter Q4 Sprint 1,Twitter Mesos Q4 Sprint 2,,,,,,,,,3.0,,,,,,,,,,,"09/Oct/14 18:17;vinodkone;Here is my proposal on how to do this (h/t [~bmahler] for participating in discussions).

Problem:
-------------

The ""state of the task"" could be different according to different components of Mesos (framework, master, slave).

Framework: It is the latest unacknowledged state sent by the SUM (via master)

Master: 
 1) In steady state, it is the latest unacked state sent by the SUM
 2) On slave re-registration it is the latest state known to the slave (not necessarily the latest unacked state)

Slave: It is the latest state reported by the executor (not necessarily the latest unacked state).

Due to these discrepancies we have issues like MESOS-1799 (this ticket) and MESOS-1817 (linked ticket).

Solution
-----------
The high level idea is that we want the master (and its UI) and slave (and its UI) to reflect the latest state of the task, *irrespective* of the unacknowledged state, i.e., what framework knows. As far as framework is concerned, it will always know the latest unacked state, because we want to maintain in-order delivery of updates for a task.

Details
----------

--> Master keeps 2 bits of information about a Task. Latest state and Latest unacked state.

--> SUM relays this information (latest and unacked state) when sending an update. It already knows this from its queues.

--> When a slave re-registers, the slave first asks the SUM about the latest unacked update for every task that has pending updates. We will add a new API method to SUM. It then includes both the latest state (already known to the slave) and the unacked state (got via SUM) in its Reregister message.

--> This guarantees that during steady state or master failover or slave re-registration, master always correctly knows the latest state and unacked state of the task.

--> Whenever a reconciliation request comes to the master, master informs the framework with the latest unacked state *irrespective* of the latest state. This guarantees there are no out of order updates.

--> Master and slave will take actions based on the latest state of the task, e.g., free up resources when the latest state of the task is terminal.

I believe this should should solve both the issues raised in this ticket and the linked ticket.

Thoughts?","14/Oct/14 18:09;vinodkone;https://reviews.apache.org/r/26697/
https://reviews.apache.org/r/26698/
https://reviews.apache.org/r/26699/
https://reviews.apache.org/r/26700/
https://reviews.apache.org/r/26701/
https://reviews.apache.org/r/26702/




","21/Oct/14 22:49;vinodkone;commit e960cdffec20d54b4f57f552d13cd92004f8e437
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Fri Oct 10 19:09:35 2014 -0700

    Updated reconciliation semantics to take the task's unacknowledged state into account.
    
    Review: https://reviews.apache.org/r/26702

commit 3c4e3fdf73fdbb2081e58fe3e9831b15d67bd440
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Fri Oct 10 18:50:07 2014 -0700

    Updated master to update task unacknowledged state properly.
    
    Review: https://reviews.apache.org/r/26701

commit da669702e4c6c4050ef49d9a1e399a837a77c143
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Fri Oct 10 14:48:49 2014 -0700

    Updated slave to include latest task state in update.
    
    Review: https://reviews.apache.org/r/26700

commit ca14f37bf7321a977e297e974e9c4c1f0cc57e0e
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Fri Oct 10 12:16:32 2014 -0700

    Updated slave re-registration to send unacknowledged task states.
    
    Review: https://reviews.apache.org/r/26699

commit 65c3c3639b385d880dbfe10bc4f652655695c8b3
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Fri Oct 17 15:26:52 2014 -0700

    Added pause() and resume() to status update manager.
    
    Review: https://reviews.apache.org/r/26957

commit e64dda411bc83963179c92ae71caefa8d21b54b4
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Wed Oct 15 18:22:46 2014 -0700

    Updated status update manager to forward updates via slave.
    
    Review: https://reviews.apache.org/r/26846
",,,,,,,,,,,,,,,,,,,,,,,,,
Slave should send exited executor message when the executor is never launched.,MESOS-1720,12734922,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,mzhu,bmahler,bmahler,19/Aug/14 01:44,26/Mar/18 18:04,29/Oct/20 16:32,14/Feb/18 12:15,,,,,,,,,1.5.1,1.6.0,,,,,agent,master,,,,0,mesosphere,,,,,,,,"When the slave sends TASK_LOST before launching an executor for a task, the slave does not send an exited executor message to the master.

Since the master receives no exited executor message, it still thinks the executor's resources are consumed on the slave.

One possible fix for this would be to send the exited executor message to the master in these cases.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-8624,MESOS-1466,MESOS-1800,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-02-01 02:06:35.353,,,false,MESOS-1961,,,,,,,,,,,,,,,,,412862,,,Wed Feb 14 12:15:13 UTC 2018,,,,,,,"0|i00c5j:",412848,,,,,vinodkone,,,,,,Mesosphere Sprint 73,Mesosphere Sprint 74,,,,,,,,,,8.0,,,,,,,,,,,"01/Feb/18 02:06;mzhu;https://reviews.apache.org/r/65449/","14/Feb/18 11:54;greggomann;Patches on master:
{code}
commit 3e3c582f10e8154e4a76c2b481cc33c8d4d0310c
Author: Meng Zhu <mzhu@mesosphere.io>
Date:   Tue Feb 13 22:45:23 2018 -0800

    Added tests to check that executors which fail to launch are removed.

    Theses tests ensure that the agent sends `ExitedExecutorMessage` when
    a task group fails to launch due to unschedule GC failure, or when a
    task fails to launch due to task authorization failure.

    Review: https://reviews.apache.org/r/65593/
{code}
{code}
commit a8e723b6ca5a268cc97e39919f7a6b4aedfc3222
Author: Meng Zhu <mzhu@mesosphere.io>
Date:   Tue Feb 13 22:45:21 2018 -0800

    Added a mock method for `__run()` to the mock slave.

    Review: https://reviews.apache.org/r/65626/
{code}
{code}
commit a6c065060d94dc04dcdc81021035d846ad7040a0
Author: Meng Zhu <mzhu@mesosphere.io>
Date:   Tue Feb 13 22:45:16 2018 -0800

    Added a test to ensure master removes executors that never launched.

    This test ensures that the agent sends `ExitedExecutorMessage` when
    the executor is never launched so that the master's executor
    bookkeeping entry is removed. See MESOS-1720.

    Review: https://reviews.apache.org/r/65448/
{code}
{code}
commit b5350fecc8604bdddb45303d9363aff4ca60cfcc
Author: Meng Zhu <mzhu@mesosphere.io>
Date:   Tue Feb 13 22:45:07 2018 -0800

    Fixed a bug where executor info lingers on master if failed to launch.

    Master relies on `ExitedExecutorMessage` from the agent to remove
    executor entries. However, this message won't be sent if an executor
    never actually launched (due to transient error), leaving executor
    info on the master and the executor's resources claimed.
    See MESOS-1720.

    This patch fixes this issue by sending the `ExitedExecutorMessage`
    from the agent if the executor is never launched.

    Review: https://reviews.apache.org/r/65449/
{code}
{code}
commit 0321b85ce66f21e9cb6990a3032cb7f8f709c6e6
Author: Meng Zhu <mzhu@mesosphere.io>
Date:   Tue Feb 13 22:45:03 2018 -0800

    Added helper function for the agent to send `ExitedExecutorMessage`.

    Review: https://reviews.apache.org/r/65446/
{code}
{code}
commit ce7f1f6a0807b96b92cb4c755c52f36e1a8e2853
Author: Meng Zhu <mzhu@mesosphere.io>
Date:   Tue Feb 13 22:44:58 2018 -0800

    Made master set `launch_executor` in the RunTask(Group)Message.

    By setting a new field `launch_executor` in the RunTask(Group)Message,
    the master is able to control executor creation on the agent.

    Also refactored the `addTask()` logic. Added two new functions:
    `isTaskLaunchExecutor()` checks if a task needs to launch an executor;
    `addExecutor()` adds an executor to the framework and slave.

    Review: https://reviews.apache.org/r/65504/
{code}
{code}
commit 7c29031bf35232a9e8b0c8bbbb8c826d0185673a
Author: Meng Zhu <mzhu@mesosphere.io>
Date:   Tue Feb 13 22:44:48 2018 -0800

    Added new protobuf field `launch_executor` in RunTask(Group)Message.

    This boolean flag is used for the master to specify whether a
    new executor should be launched for the task or task group (with
    the exception of the command executor). This allows the master
    to control executor creation on the agent.

    Also updated the relevant message handlers and mock functions.

    Review: https://reviews.apache.org/r/65445/
{code}","14/Feb/18 12:15;greggomann;Patches on 1.5.x:
{code}
commit 2bdf4935b7929d0dce614d76461cddb991df89da
Author: Meng Zhu <mzhu@mesosphere.io>
Date:   Tue Feb 13 22:45:07 2018 -0800

    Fixed a bug where executor info lingers on master if failed to launch.

    Master relies on `ExitedExecutorMessage` from the agent to remove
    executor entries. However, this message won't be sent if an executor
    never actually launched (due to transient error), leaving executor
    info on the master and the executor's resources claimed.
    See MESOS-1720.

    This patch fixes this issue by sending the `ExitedExecutorMessage`
    from the agent if the executor is never launched.

    Review: https://reviews.apache.org/r/65449/
{code}
{code}
commit fb0e2f1f81b2256a76cae83893e2a69fdd91fcd7
Author: Meng Zhu <mzhu@mesosphere.io>
Date:   Tue Feb 13 22:45:03 2018 -0800

    Added helper function for the agent to send `ExitedExecutorMessage`.

    Review: https://reviews.apache.org/r/65446/
{code}
{code}
commit 10aa875df8947f8cbfb318820101984d99259070
Author: Meng Zhu <mzhu@mesosphere.io>
Date:   Tue Feb 13 22:44:58 2018 -0800

    Made master set `launch_executor` in the RunTask(Group)Message.

    By setting a new field `launch_executor` in the RunTask(Group)Message,
    the master is able to control executor creation on the agent.

    Also refactored the `addTask()` logic. Added two new functions:
    `isTaskLaunchExecutor()` checks if a task needs to launch an executor;
    `addExecutor()` adds an executor to the framework and slave.

    Review: https://reviews.apache.org/r/65504/
{code}
{code}
commit 08e0ceb84e4bf353e1f938482bb6766bf73310c7
Author: Meng Zhu <mzhu@mesosphere.io>
Date:   Tue Feb 13 22:44:48 2018 -0800

    Added new protobuf field `launch_executor` in RunTask(Group)Message.

    This boolean flag is used for the master to specify whether a
    new executor should be launched for the task or task group (with
    the exception of the command executor). This allows the master
    to control executor creation on the agent.

    Also updated the relevant message handlers and mock functions.

    Review: https://reviews.apache.org/r/65445/
{code}",,,,,,,,,,,,,,,,,,,,,,,,,
The slave does not send pending tasks during re-registration.,MESOS-1715,12734882,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bmahler,bmahler,bmahler,18/Aug/14 23:13,09/Apr/15 23:58,29/Oct/20 16:32,10/Sep/14 18:27,,,,,,,,,0.21.0,,,,,,agent,,,,,0,,,,,,,,,"In what looks like an oversight, the pending tasks and executors in the slave (Framework::pending) are not sent in the re-registration message.

For tasks, this can lead to spurious TASK_LOST notifications being generated by the master when it falsely thinks the tasks are not present on the slave.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-1407,,,,,,,,,,,,,,,,,412822,,,Tue Sep 16 22:36:00 UTC 2014,,,,,,,"0|i00c3j:",412808,,,,,vinodkone,,,,,,Q3 Sprint 4,Mesos Q3 Sprint 5,,,,,,,,,,3.0,,,,,,,,,,,"05/Sep/14 03:12;bmahler;For now I've fixed it to send the pending tasks, since that is important for reconciliation:

https://reviews.apache.org/r/25371/
https://reviews.apache.org/r/25372/
https://reviews.apache.org/r/25373/

I'll pull out a ticket for the executors.","10/Sep/14 18:27;bmahler;{noformat}
commit 00024c3a95d771b6e03f06de2e5e76b1f8754b02
Author: Benjamin Mahler <bmahler@twitter.com>
Date:   Tue Aug 19 17:08:40 2014 -0700

    Send pending tasks during re-registration.

    Review: https://reviews.apache.org/r/25371
{noformat}

{noformat}
commit 1071d3e39ce558aaf541475f8786e4a7003752e0
Author: Benjamin Mahler <bmahler@twitter.com>
Date:   Thu Sep 4 10:23:51 2014 -0700

    Made the GarbageCollector injectable into the Slave.

    Review: https://reviews.apache.org/r/25372
{noformat}

{noformat}
commit d5974c0a7ea79cf5f4812272dd8dd58dcc0014f1
Author: Benjamin Mahler <bmahler@twitter.com>
Date:   Thu Sep 4 10:24:17 2014 -0700

    Added a test for sending pending tasks during re-registration.

    Review: https://reviews.apache.org/r/25373
{noformat}","16/Sep/14 22:36;bmahler;Pulled out MESOS-1800 for the executor side of this.",,,,,,,,,,,,,,,,,,,,,,,,,
Improve reconciliation between master and slave.,MESOS-1696,12733567,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,bmahler,bmahler,bmahler,12/Aug/14 19:01,09/Oct/14 01:32,29/Oct/20 16:32,09/Oct/14 01:32,,,,,,,,,0.21.0,,,,,,agent,master,,,,0,,,,,,,,,"As we update the Master to keep tasks in memory until they are both terminal and acknowledged (MESOS-1410), the lifetime of tasks in Mesos will look as follows:

{code}
Master           Slave
 {}               {}
{Tn}              {}  // Master receives Task T, non-terminal. Forwards to slave.
{Tn}             {Tn} // Slave receives Task T, non-terminal.
{Tn}             {Tt} // Task becomes terminal on slave. Update forwarded.
{Tt}             {Tt} // Master receives update, forwards to framework.
 {}              {Tt} // Master receives ack, forwards to slave.
 {}               {}  // Slave receives ack.
{code}

In the current form of reconciliation, the slave sends to the master all tasks that are not both terminal and acknowledged. At any point in the above lifecycle, the slave's re-registration message can reach the master.

Note the following properties:

*(1)* The master may have a non-terminal task, not present in the slave's re-registration message.
*(2)* The master may have a non-terminal task, present in the slave's re-registration message but in a different state.
*(3)* The slave's re-registration message may contain a terminal unacknowledged task unknown to the master.

In the current master / slave [reconciliation|https://github.com/apache/mesos/blob/0.19.1/src/master/master.cpp#L3146] code, the master assumes that case (1) is because a launch task message was dropped, and it sends TASK_LOST. We've seen above that (1) can happen even when the task reaches the slave correctly, so this can lead to inconsistency!

After chatting with [~vinodkone], we're considering updating the reconciliation to occur as follows:


→ Slave sends all tasks that are not both terminal and acknowledged, during re-registration. This is the same as before.

→ If the master sees tasks that are missing in the slave, the master sends the tasks that need to be reconciled to the slave for the tasks. This can be piggy-backed on the re-registration message.

→ The slave will send TASK_LOST if the task is not known to it. Preferably in a retried manner, unless we update socket closure on the slave to force a re-registration.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-1799,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-1407,,,,,,,,,,,,,,,,,411595,,,Thu Oct 09 01:32:47 UTC 2014,,,,,,,"0|i00bjb:",411586,,,,,,,,,,,Mesos Q3 Sprint 5,Mesos Q3 Sprint 6,Twitter Q4 Sprint 1,,,,,,,,,3.0,,,,,,,,,,,"30/Sep/14 23:32;bmahler;https://reviews.apache.org/r/26202/
https://reviews.apache.org/r/26206/
https://reviews.apache.org/r/26207/
https://reviews.apache.org/r/26208/","09/Oct/14 01:32;bmahler;{noformat}
commit cc4444eb47705649a4e93a4045b4d130dd4b3354
Author: Benjamin Mahler <bmahler@twitter.com>
Date:   Tue Sep 30 11:31:53 2014 -0700

    Introduced Master <-> Slave reconciliation.

    Review: https://reviews.apache.org/r/26206
{noformat}

{noformat}
commit cd03dfa6e4022bc17fa9dd0599a17f391ce7978d
Author: Benjamin Mahler <bmahler@twitter.com>
Date:   Tue Sep 30 12:18:28 2014 -0700

    Split out partition and reconciliation tests from FaultTolerace tests.

    Review: https://reviews.apache.org/r/26207
{noformat}

{noformat}
commit c96ba8f6035329acebb25ca0f52215284bbf8f8f
Author: Benjamin Mahler <bmahler@twitter.com>
Date:   Tue Sep 30 15:52:00 2014 -0700

    Added a test for the Master <-> Slave reconciliation race.

    Review: https://reviews.apache.org/r/26208
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
Handle a temporary one-way master --> slave socket closure.,MESOS-1668,12731876,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,Fixed,vinodkone,bmahler,bmahler,04/Aug/14 22:04,08/Oct/14 21:45,29/Oct/20 16:32,25/Sep/14 20:56,,,,,,,,,0.21.0,,,,,,agent,master,,,,0,reliability,,,,,,,,"In MESOS-1529, we realized that it's possible for a slave to remain disconnected in the master if the following occurs:

→ Master and Slave connected operating normally.
→ Temporary one-way network failure, master→slave link breaks.
→ Master marks slave as disconnected.
→ Network restored and health checking continues normally, slave is not removed as a result. Slave does not attempt to re-register since it is receiving pings once again.
→ Slave remains disconnected according to the master, and the slave does not try to re-register. Bad!

We were originally thinking of using a failover timeout in the master to remove these slaves that don't re-register. However, it can be dangerous when ZooKeeper issues are preventing the slave from re-registering with the master; we do not want to remove a ton of slaves in this situation.

Rather, when the slave is health checking correctly but does not re-register within a timeout, we could send a registration request from the master to the slave, telling the slave that it must re-register. This message could also be used when receiving status updates (or other messages) from slaves that are disconnected in the master.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-1529,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-09-12 00:55:56.725,,,false,MESOS-1407,,,,,,,,,,,,,,,,,409905,,,Thu Sep 25 20:56:28 UTC 2014,,,,,,,"0|i00c13:",409899,,,,,bmahler,,,,,,Mesos Q3 Sprint 5,Mesos Q3 Sprint 6,,,,,,,,,,2.0,,,,,,,,,,,"06/Aug/14 22:46;bmahler;Placing this under reconciliation because, although extremely rare, it can lead to some inconsistent state between the master and slave for an arbitrary amount of time. For example, if the launchTask message is dropped as a result of the socket closure between Master → Slave in the scenario above.","12/Sep/14 00:55;vinodkone;The plan is to handle this by piggybacking the current slave state (e.g., bool registered) on the ping/pong messages.

When the slave receives a ping message which says that the master thinks the slave is disconnected but slave doesn't know it yet (socket only broke on the master side), slave will attempt a re-registration.","20/Sep/14 00:34;vinodkone;https://reviews.apache.org/r/25867/","25/Sep/14 20:56;vinodkone;commit bef49064a830baea87891bfa61a68729a7c06029
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Tue Sep 16 18:09:59 2014 -0700

    Updated ping message to embed the slave connected status.
    
    Review: https://reviews.apache.org/r/25867
",,,,,,,,,,,,,,,,,,,,,,,,
Network isolator should tolerate slave crashes while doing isolate/cleanup.,MESOS-1649,12730407,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,jieyu,jieyu,jieyu,29/Jul/14 05:43,06/Aug/14 16:36,29/Oct/20 16:32,01/Aug/14 17:54,,,,,,,,,0.20.0,,,,,,,,,,,0,,,,,,,,,"A slave may crash while we are installing/removing filters. The slave recovery for the network isolator should tolerate those partially installed filters. Also, we want to avoid leaking a filter on host eth0 and host lo.

The current code cannot tolerate that, thus may cause the following error:

{noformat}
Failed to perform recovery: Collect failed: Failed to recover container d409a100-2afb-497c-864f-fe3002cf65d9 with pid 50405: No ephemeral ports found
To remedy this do as follows:
Step 1: rm -f /var/lib/mesos/meta/slaves/latest
       This ensures slave doesn't recover old live executors.
Step 2: Restart the slave.
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,MESOS-1228,,,,,,,,,,,,,,,,,408480,,,Tue Jul 29 05:43:42 UTC 2014,,,,,,,"0|i1yaa7:",408478,,,,,,,,,,,Q3 Sprint 2,,,,,,,,,,,3.0,,,,,,,,,,,"29/Jul/14 05:43;jieyu;https://reviews.apache.org/r/24020/",,,,,,,,,,,,,,,,,,,,,,,,,,,
SlaveRecoveryTest/0.ReconcileKillTask is flaky,MESOS-1594,12727217,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,greggomann,vinodkone,vinodkone,15/Jul/14 01:01,21/Jan/16 19:17,29/Oct/20 16:32,21/Jan/16 19:17,0.20.0,,,,,,,,0.22.0,,,,,,test,,,,,0,flaky,mesosphere,,,,,,,"Observed this on Jenkins.

{code}
[ RUN      ] SlaveRecoveryTest/0.ReconcileKillTask
Using temporary directory '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_3zJ6DG'
I0714 15:08:43.915114 27216 leveldb.cpp:176] Opened db in 474.695188ms
I0714 15:08:43.933645 27216 leveldb.cpp:183] Compacted db in 18.068942ms
I0714 15:08:43.934129 27216 leveldb.cpp:198] Created db iterator in 7860ns
I0714 15:08:43.934439 27216 leveldb.cpp:204] Seeked to beginning of db in 2560ns
I0714 15:08:43.934779 27216 leveldb.cpp:273] Iterated through 0 keys in the db in 1400ns
I0714 15:08:43.935098 27216 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0714 15:08:43.936027 27238 recover.cpp:425] Starting replica recovery
I0714 15:08:43.936225 27238 recover.cpp:451] Replica is in EMPTY status
I0714 15:08:43.936867 27238 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0714 15:08:43.937049 27238 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0714 15:08:43.937232 27238 recover.cpp:542] Updating replica status to STARTING
I0714 15:08:43.945600 27235 master.cpp:288] Master 20140714-150843-16842879-55850-27216 (quantal) started on 127.0.1.1:55850
I0714 15:08:43.945643 27235 master.cpp:325] Master only allowing authenticated frameworks to register
I0714 15:08:43.945651 27235 master.cpp:330] Master only allowing authenticated slaves to register
I0714 15:08:43.945658 27235 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_3zJ6DG/credentials'
I0714 15:08:43.945808 27235 master.cpp:359] Authorization enabled
I0714 15:08:43.946369 27235 hierarchical_allocator_process.hpp:301] Initializing hierarchical allocator process with master : master@127.0.1.1:55850
I0714 15:08:43.946419 27235 master.cpp:122] No whitelist given. Advertising offers for all slaves
I0714 15:08:43.946614 27235 master.cpp:1128] The newly elected leader is master@127.0.1.1:55850 with id 20140714-150843-16842879-55850-27216
I0714 15:08:43.946630 27235 master.cpp:1141] Elected as the leading master!
I0714 15:08:43.946637 27235 master.cpp:959] Recovering from registrar
I0714 15:08:43.946707 27235 registrar.cpp:313] Recovering registrar
I0714 15:08:43.957895 27238 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 20.529301ms
I0714 15:08:43.957978 27238 replica.cpp:320] Persisted replica status to STARTING
I0714 15:08:43.958142 27238 recover.cpp:451] Replica is in STARTING status
I0714 15:08:43.958664 27238 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0714 15:08:43.958762 27238 recover.cpp:188] Received a recover response from a replica in STARTING status
I0714 15:08:43.958945 27238 recover.cpp:542] Updating replica status to VOTING
I0714 15:08:43.975685 27238 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 16.646136ms
I0714 15:08:43.976367 27238 replica.cpp:320] Persisted replica status to VOTING
I0714 15:08:43.976824 27241 recover.cpp:556] Successfully joined the Paxos group
I0714 15:08:43.977072 27242 recover.cpp:440] Recover process terminated
I0714 15:08:43.980590 27236 log.cpp:656] Attempting to start the writer
I0714 15:08:43.981385 27236 replica.cpp:474] Replica received implicit promise request with proposal 1
I0714 15:08:43.999141 27236 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 17.705787ms
I0714 15:08:43.999222 27236 replica.cpp:342] Persisted promised to 1
I0714 15:08:44.004451 27240 coordinator.cpp:230] Coordinator attemping to fill missing position
I0714 15:08:44.004914 27240 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0714 15:08:44.021456 27240 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 16.499775ms
I0714 15:08:44.021533 27240 replica.cpp:676] Persisted action at 0
I0714 15:08:44.022006 27240 replica.cpp:508] Replica received write request for position 0
I0714 15:08:44.022043 27240 leveldb.cpp:438] Reading position from leveldb took 21376ns
I0714 15:08:44.035969 27240 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 13.885907ms
I0714 15:08:44.036365 27240 replica.cpp:676] Persisted action at 0
I0714 15:08:44.040156 27238 replica.cpp:655] Replica received learned notice for position 0
I0714 15:08:44.058082 27238 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 17.860707ms
I0714 15:08:44.058161 27238 replica.cpp:676] Persisted action at 0
I0714 15:08:44.058176 27238 replica.cpp:661] Replica learned NOP action at position 0
I0714 15:08:44.058526 27238 log.cpp:672] Writer started with ending position 0
I0714 15:08:44.058872 27238 leveldb.cpp:438] Reading position from leveldb took 25660ns
I0714 15:08:44.060556 27238 registrar.cpp:346] Successfully fetched the registry (0B)
I0714 15:08:44.060845 27238 registrar.cpp:422] Attempting to update the 'registry'
I0714 15:08:44.062304 27238 log.cpp:680] Attempting to append 120 bytes to the log
I0714 15:08:44.062866 27236 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0714 15:08:44.063154 27236 replica.cpp:508] Replica received write request for position 1
I0714 15:08:44.082813 27236 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 19.61683ms
I0714 15:08:44.082890 27236 replica.cpp:676] Persisted action at 1
I0714 15:08:44.083256 27236 replica.cpp:655] Replica received learned notice for position 1
I0714 15:08:44.097398 27236 leveldb.cpp:343] Persisting action (139 bytes) to leveldb took 14.104796ms
I0714 15:08:44.097475 27236 replica.cpp:676] Persisted action at 1
I0714 15:08:44.097488 27236 replica.cpp:661] Replica learned APPEND action at position 1
I0714 15:08:44.098569 27236 registrar.cpp:479] Successfully updated 'registry'
I0714 15:08:44.098906 27240 log.cpp:699] Attempting to truncate the log to 1
I0714 15:08:44.099608 27240 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0714 15:08:44.100005 27240 replica.cpp:508] Replica received write request for position 2
I0714 15:08:44.100566 27236 registrar.cpp:372] Successfully recovered registrar
I0714 15:08:44.101227 27239 master.cpp:986] Recovered 0 slaves from the Registry (84B) ; allowing 10mins for slaves to re-register
I0714 15:08:44.118376 27240 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 18.329495ms
I0714 15:08:44.118455 27240 replica.cpp:676] Persisted action at 2
I0714 15:08:44.122258 27242 replica.cpp:655] Replica received learned notice for position 2
I0714 15:08:44.137336 27242 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 15.023553ms
I0714 15:08:44.137460 27242 leveldb.cpp:401] Deleting ~1 keys from leveldb took 55049ns
I0714 15:08:44.137480 27242 replica.cpp:676] Persisted action at 2
I0714 15:08:44.137492 27242 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0714 15:08:44.143729 27216 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem
I0714 15:08:44.145934 27242 slave.cpp:168] Slave started on 43)@127.0.1.1:55850
I0714 15:08:44.145953 27242 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/credential'
I0714 15:08:44.146040 27242 slave.cpp:266] Slave using credential for: test-principal
I0714 15:08:44.146136 27242 slave.cpp:279] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0714 15:08:44.146198 27242 slave.cpp:324] Slave hostname: quantal
I0714 15:08:44.146209 27242 slave.cpp:325] Slave checkpoint: true
I0714 15:08:44.146708 27242 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta'
I0714 15:08:44.146824 27242 status_update_manager.cpp:193] Recovering status update manager
I0714 15:08:44.146901 27242 containerizer.cpp:287] Recovering containerizer
I0714 15:08:44.147228 27242 slave.cpp:3126] Finished recovery
I0714 15:08:44.147531 27242 slave.cpp:599] New master detected at master@127.0.1.1:55850
I0714 15:08:44.147562 27242 slave.cpp:675] Authenticating with master master@127.0.1.1:55850
I0714 15:08:44.147614 27242 slave.cpp:648] Detecting new master
I0714 15:08:44.147652 27242 status_update_manager.cpp:167] New master detected at master@127.0.1.1:55850
I0714 15:08:44.147691 27242 authenticatee.hpp:128] Creating new client SASL connection
I0714 15:08:44.148533 27235 master.cpp:3507] Authenticating slave(43)@127.0.1.1:55850
I0714 15:08:44.148666 27235 authenticator.hpp:156] Creating new server SASL connection
I0714 15:08:44.149054 27242 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0714 15:08:44.149447 27242 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0714 15:08:44.149917 27236 authenticator.hpp:262] Received SASL authentication start
I0714 15:08:44.149974 27236 authenticator.hpp:384] Authentication requires more steps
I0714 15:08:44.150208 27242 authenticatee.hpp:265] Received SASL authentication step
I0714 15:08:44.150720 27239 authenticator.hpp:290] Received SASL authentication step
I0714 15:08:44.150749 27239 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0714 15:08:44.150758 27239 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0714 15:08:44.150771 27239 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0714 15:08:44.150781 27239 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0714 15:08:44.150787 27239 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.150792 27239 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.150804 27239 authenticator.hpp:376] Authentication success
I0714 15:08:44.150848 27239 master.cpp:3547] Successfully authenticated principal 'test-principal' at slave(43)@127.0.1.1:55850
I0714 15:08:44.157696 27242 authenticatee.hpp:305] Authentication success
I0714 15:08:44.158855 27242 slave.cpp:732] Successfully authenticated with master master@127.0.1.1:55850
I0714 15:08:44.158936 27242 slave.cpp:970] Will retry registration in 10.352612ms if necessary
I0714 15:08:44.161813 27216 sched.cpp:139] Version: 0.20.0
I0714 15:08:44.162608 27236 sched.cpp:235] New master detected at master@127.0.1.1:55850
I0714 15:08:44.162637 27236 sched.cpp:285] Authenticating with master master@127.0.1.1:55850
I0714 15:08:44.162747 27236 authenticatee.hpp:128] Creating new client SASL connection
I0714 15:08:44.163506 27239 master.cpp:2789] Registering slave at slave(43)@127.0.1.1:55850 (quantal) with id 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.164086 27238 registrar.cpp:422] Attempting to update the 'registry'
I0714 15:08:44.165694 27238 log.cpp:680] Attempting to append 295 bytes to the log
I0714 15:08:44.166231 27240 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0714 15:08:44.166517 27240 replica.cpp:508] Replica received write request for position 3
I0714 15:08:44.167199 27239 master.cpp:3507] Authenticating scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.167867 27241 authenticator.hpp:156] Creating new server SASL connection
I0714 15:08:44.168058 27241 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0714 15:08:44.168081 27241 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0714 15:08:44.168107 27241 authenticator.hpp:262] Received SASL authentication start
I0714 15:08:44.168149 27241 authenticator.hpp:384] Authentication requires more steps
I0714 15:08:44.168176 27241 authenticatee.hpp:265] Received SASL authentication step
I0714 15:08:44.168215 27241 authenticator.hpp:290] Received SASL authentication step
I0714 15:08:44.168233 27241 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0714 15:08:44.168793 27241 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0714 15:08:44.168820 27241 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0714 15:08:44.168834 27241 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0714 15:08:44.168840 27241 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.168845 27241 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.168858 27241 authenticator.hpp:376] Authentication success
I0714 15:08:44.168895 27241 authenticatee.hpp:305] Authentication success
I0714 15:08:44.168970 27241 sched.cpp:359] Successfully authenticated with master master@127.0.1.1:55850
I0714 15:08:44.168987 27241 sched.cpp:478] Sending registration request to master@127.0.1.1:55850
I0714 15:08:44.169426 27239 master.cpp:1239] Queuing up registration request from scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850 because authentication is still in progress
I0714 15:08:44.169958 27239 master.cpp:3547] Successfully authenticated principal 'test-principal' at scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.170440 27241 slave.cpp:970] Will retry registration in 8.76707ms if necessary
I0714 15:08:44.175359 27239 master.cpp:2777] Ignoring register slave message from slave(43)@127.0.1.1:55850 (quantal) as admission is already in progress
I0714 15:08:44.175916 27239 master.cpp:1247] Received registration request from scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.176298 27239 master.cpp:1207] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0714 15:08:44.176858 27239 master.cpp:1306] Registering framework 20140714-150843-16842879-55850-27216-0000 at scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.177408 27236 sched.cpp:409] Framework registered with 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.177443 27236 sched.cpp:423] Scheduler::registered took 12527ns
I0714 15:08:44.177727 27241 hierarchical_allocator_process.hpp:331] Added framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.177747 27241 hierarchical_allocator_process.hpp:724] No resources available to allocate!
I0714 15:08:44.177753 27241 hierarchical_allocator_process.hpp:686] Performed allocation for 0 slaves in 8120ns
I0714 15:08:44.179908 27241 slave.cpp:970] Will retry registration in 66.781028ms if necessary
I0714 15:08:44.180007 27241 master.cpp:2777] Ignoring register slave message from slave(43)@127.0.1.1:55850 (quantal) as admission is already in progress
I0714 15:08:44.183082 27240 leveldb.cpp:343] Persisting action (314 bytes) to leveldb took 16.533189ms
I0714 15:08:44.183125 27240 replica.cpp:676] Persisted action at 3
I0714 15:08:44.183465 27240 replica.cpp:655] Replica received learned notice for position 3
I0714 15:08:44.203276 27240 leveldb.cpp:343] Persisting action (316 bytes) to leveldb took 19.768951ms
I0714 15:08:44.203376 27240 replica.cpp:676] Persisted action at 3
I0714 15:08:44.203392 27240 replica.cpp:661] Replica learned APPEND action at position 3
I0714 15:08:44.204033 27240 registrar.cpp:479] Successfully updated 'registry'
I0714 15:08:44.204138 27240 log.cpp:699] Attempting to truncate the log to 3
I0714 15:08:44.204221 27240 master.cpp:2829] Registered slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.204241 27240 master.cpp:3975] Adding slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0714 15:08:44.204387 27240 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0714 15:08:44.204489 27240 slave.cpp:766] Registered with master master@127.0.1.1:55850; given slave ID 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.204745 27240 slave.cpp:779] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/slave.info'
I0714 15:08:44.204954 27240 hierarchical_allocator_process.hpp:444] Added slave 20140714-150843-16842879-55850-27216-0 (quantal) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0714 15:08:44.205023 27240 hierarchical_allocator_process.hpp:750] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 to framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.205122 27240 hierarchical_allocator_process.hpp:706] Performed allocation for slave 20140714-150843-16842879-55850-27216-0 in 131192ns
I0714 15:08:44.205189 27240 slave.cpp:2323] Received ping from slave-observer(32)@127.0.1.1:55850
I0714 15:08:44.205258 27240 master.hpp:801] Adding offer 20140714-150843-16842879-55850-27216-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:44.205303 27240 master.cpp:3454] Sending 1 offers to framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.205469 27240 sched.cpp:546] Scheduler::resourceOffers took 23591ns
I0714 15:08:44.206351 27241 replica.cpp:508] Replica received write request for position 4
I0714 15:08:44.208353 27237 master.hpp:811] Removing offer 20140714-150843-16842879-55850-27216-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:44.208436 27237 master.cpp:2133] Processing reply for offers: [ 20140714-150843-16842879-55850-27216-0 ] on slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) for framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.208472 27237 master.cpp:2219] Authorizing framework principal 'test-principal' to launch task 4a6783aa-8d07-46e3-8399-2a5d047f0021 as user 'jenkins'
I0714 15:08:44.208909 27237 master.hpp:773] Adding task 4a6783aa-8d07-46e3-8399-2a5d047f0021 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:44.208947 27237 master.cpp:2285] Launching task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.209090 27237 slave.cpp:1001] Got assigned task 4a6783aa-8d07-46e3-8399-2a5d047f0021 for framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.209190 27237 slave.cpp:3398] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.info'
I0714 15:08:44.209413 27237 slave.cpp:3405] Checkpointing framework pid 'scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.pid'
I0714 15:08:44.209710 27237 slave.cpp:1111] Launching task 4a6783aa-8d07-46e3-8399-2a5d047f0021 for framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.210978 27237 slave.cpp:3720] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/executor.info'
I0714 15:08:44.211520 27237 slave.cpp:3835] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/tasks/4a6783aa-8d07-46e3-8399-2a5d047f0021/task.info'
I0714 15:08:44.211714 27237 slave.cpp:1221] Queuing task '4a6783aa-8d07-46e3-8399-2a5d047f0021' for executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework '20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.211937 27236 containerizer.cpp:427] Starting container '19c466f8-bb5a-4842-a152-f585ff88762a' for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework '20140714-150843-16842879-55850-27216-0000'
I0714 15:08:44.212242 27236 slave.cpp:560] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a'
I0714 15:08:44.216187 27236 launcher.cpp:137] Forked child with pid '28451' for container '19c466f8-bb5a-4842-a152-f585ff88762a'
I0714 15:08:44.217281 27236 containerizer.cpp:705] Checkpointing executor's forked pid 28451 to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/pids/forked.pid'
I0714 15:08:44.219408 27236 containerizer.cpp:537] Fetching URIs for container '19c466f8-bb5a-4842-a152-f585ff88762a' using command '/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src/mesos-fetcher'
I0714 15:08:44.223963 27241 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 17.554461ms
I0714 15:08:44.224501 27241 replica.cpp:676] Persisted action at 4
I0714 15:08:44.225051 27241 replica.cpp:655] Replica received learned notice for position 4
I0714 15:08:44.242923 27241 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 17.806547ms
I0714 15:08:44.243057 27241 leveldb.cpp:401] Deleting ~2 keys from leveldb took 57154ns
I0714 15:08:44.243078 27241 replica.cpp:676] Persisted action at 4
I0714 15:08:44.243096 27241 replica.cpp:661] Replica learned TRUNCATE action at position 4
I0714 15:08:44.401140 27241 slave.cpp:2468] Monitoring executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework '20140714-150843-16842879-55850-27216-0000' in container '19c466f8-bb5a-4842-a152-f585ff88762a'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0714 15:08:44.434221 28486 process.cpp:1671] libprocess is initialized on 127.0.1.1:34669 for 8 cpus
I0714 15:08:44.436146 28486 exec.cpp:131] Version: 0.20.0
I0714 15:08:44.438555 28500 exec.cpp:181] Executor started at: executor(1)@127.0.1.1:34669 with pid 28486
I0714 15:08:44.440846 27241 slave.cpp:1732] Got registration for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.440917 27241 slave.cpp:1817] Checkpointing executor pid 'executor(1)@127.0.1.1:34669' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/pids/libprocess.pid'
I0714 15:08:44.442373 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.442790 27241 slave.cpp:1851] Flushing queued task 4a6783aa-8d07-46e3-8399-2a5d047f0021 for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.443192 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.443994 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.444144 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.444741 28500 exec.cpp:205] Executor registered on slave 20140714-150843-16842879-55850-27216-0
Registered executor on quantal
I0714 15:08:44.446338 28500 exec.cpp:217] Executor::registered took 534236ns
I0714 15:08:44.446715 28500 exec.cpp:292] Executor asked to run task '4a6783aa-8d07-46e3-8399-2a5d047f0021'
Starting task 4a6783aa-8d07-46e3-8399-2a5d047f0021
I0714 15:08:44.447548 28500 exec.cpp:301] Executor::launchTask took 584306ns
sh -c 'sleep 1000'
Forked command at 28509
I0714 15:08:44.451202 28506 exec.cpp:524] Executor sending status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.452327 27239 slave.cpp:2086] Handling status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from executor(1)@127.0.1.1:34669
I0714 15:08:44.452503 27239 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.452520 27239 status_update_manager.cpp:499] Creating StatusUpdate stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.452775 27239 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.472384 27239 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to master@127.0.1.1:55850
I0714 15:08:44.472764 27237 master.cpp:3115] Status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.472854 27237 sched.cpp:637] Scheduler::statusUpdate took 17656ns
I0714 15:08:44.472920 27237 master.cpp:2639] Forwarding status update acknowledgement 323fc20a-b5b8-475d-8752-b1f853797f55 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.473122 27239 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.473146 27239 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.473244 27237 slave.cpp:2244] Status update manager successfully handled status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.473258 27237 slave.cpp:2250] Sending acknowledgement for status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to executor(1)@127.0.1.1:34669
I0714 15:08:44.473567 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.474095 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.474676 28502 exec.cpp:338] Executor received status update acknowledgement 323fc20a-b5b8-475d-8752-b1f853797f55 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.491111 27239 slave.cpp:1672] Status update manager successfully handled status update acknowledgement (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.491761 27216 slave.cpp:484] Slave terminating
I0714 15:08:44.492559 27216 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem
I0714 15:08:44.494635 27237 master.cpp:766] Slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) disconnected
I0714 15:08:44.494663 27237 master.cpp:1608] Disconnecting slave 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.495120 27237 slave.cpp:168] Slave started on 44)@127.0.1.1:55850
I0714 15:08:44.495133 27237 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/credential'
I0714 15:08:44.495226 27237 slave.cpp:266] Slave using credential for: test-principal
I0714 15:08:44.495322 27237 slave.cpp:279] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0714 15:08:44.495407 27237 slave.cpp:324] Slave hostname: quantal
I0714 15:08:44.495419 27237 slave.cpp:325] Slave checkpoint: true
I0714 15:08:44.495939 27242 master.cpp:2469] Asked to kill task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.496207 27238 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta'
I0714 15:08:44.498291 27240 slave.cpp:3194] Recovering framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.498325 27240 slave.cpp:3570] Recovering executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.498940 27240 status_update_manager.cpp:193] Recovering status update manager
I0714 15:08:44.498956 27240 status_update_manager.cpp:201] Recovering executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.498975 27240 status_update_manager.cpp:499] Creating StatusUpdate stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.499092 27240 status_update_manager.hpp:306] Replaying status update stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021
I0714 15:08:44.499241 27240 slave.cpp:560] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a'
I0714 15:08:44.499433 27240 containerizer.cpp:287] Recovering containerizer
I0714 15:08:44.499457 27240 containerizer.cpp:329] Recovering container '19c466f8-bb5a-4842-a152-f585ff88762a' for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.495811 27237 hierarchical_allocator_process.hpp:483] Slave 20140714-150843-16842879-55850-27216-0 disconnected
I0714 15:08:44.501255 27240 slave.cpp:3067] Sending reconnect request to executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 at executor(1)@127.0.1.1:34669
I0714 15:08:44.502030 28501 exec.cpp:251] Received reconnect request from slave 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.502627 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.502681 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.503211 27240 slave.cpp:1911] Re-registering executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.504238 28501 exec.cpp:228] Executor re-registered on slave 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.505033 28501 exec.cpp:240] Executor::reregistered took 45053ns
Re-registered executor on quantal
I0714 15:08:44.505507 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.505558 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.948043 27241 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 124255ns
I0714 15:08:45.948671 27237 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 61521ns
I0714 15:08:46.503978 27238 slave.cpp:2035] Cleaning up un-reregistered executors
I0714 15:08:46.504050 27238 slave.cpp:3126] Finished recovery
I0714 15:08:46.504590 27238 slave.cpp:599] New master detected at master@127.0.1.1:55850
I0714 15:08:46.504639 27238 slave.cpp:675] Authenticating with master master@127.0.1.1:55850
I0714 15:08:46.504729 27238 slave.cpp:648] Detecting new master
I0714 15:08:46.504772 27238 status_update_manager.cpp:167] New master detected at master@127.0.1.1:55850
I0714 15:08:46.504863 27238 authenticatee.hpp:128] Creating new client SASL connection
I0714 15:08:46.505091 27238 master.cpp:3507] Authenticating slave(44)@127.0.1.1:55850
I0714 15:08:46.505239 27238 authenticator.hpp:156] Creating new server SASL connection
I0714 15:08:46.505363 27238 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0714 15:08:46.505393 27238 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0714 15:08:46.505420 27238 authenticator.hpp:262] Received SASL authentication start
I0714 15:08:46.505476 27238 authenticator.hpp:384] Authentication requires more steps
I0714 15:08:46.505506 27238 authenticatee.hpp:265] Received SASL authentication step
I0714 15:08:46.505542 27238 authenticator.hpp:290] Received SASL authentication step
I0714 15:08:46.505558 27238 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0714 15:08:46.505566 27238 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0714 15:08:46.505584 27238 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0714 15:08:46.505595 27238 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0714 15:08:46.505601 27238 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:46.505606 27238 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:46.505616 27238 authenticator.hpp:376] Authentication success
I0714 15:08:46.505646 27238 authenticatee.hpp:305] Authentication success
I0714 15:08:46.505671 27238 master.cpp:3547] Successfully authenticated principal 'test-principal' at slave(44)@127.0.1.1:55850
I0714 15:08:46.505769 27238 slave.cpp:732] Successfully authenticated with master master@127.0.1.1:55850
I0714 15:08:46.505873 27238 slave.cpp:970] Will retry registration in 17.903094ms if necessary
W0714 15:08:46.505991 27238 master.cpp:2904] Slave at slave(44)@127.0.1.1:55850 (quantal) is being allowed to re-register with an already in use id (20140714-150843-16842879-55850-27216-0)
W0714 15:08:46.506063 27238 master.cpp:3679]  Slave 20140714-150843-16842879-55850-27216-0 at slave(44)@127.0.1.1:55850 (quantal) has non-terminal task 4a6783aa-8d07-46e3-8399-2a5d047f0021 that is supposed to be killed. Killing it now!
I0714 15:08:46.506150 27238 slave.cpp:816] Re-registered with master master@127.0.1.1:55850
I0714 15:08:46.506186 27238 slave.cpp:1277] Asked to kill task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.507275 27241 hierarchical_allocator_process.hpp:497] Slave 20140714-150843-16842879-55850-27216-0 reconnected
I0714 15:08:46.508061 28504 exec.cpp:312] Executor asked to kill task '4a6783aa-8d07-46e3-8399-2a5d047f0021'
I0714 15:08:46.508117 28504 exec.cpp:321] Executor::killTask took 24954ns
Shutting down
Sending SIGTERM to process tree at pid 28509
I0714 15:08:46.512238 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:46.512508 27238 slave.cpp:1582] Updating framework 20140714-150843-16842879-55850-27216-0000 pid to scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:46.512846 27238 slave.cpp:1590] Checkpointing framework pid 'scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.pid'
I0714 15:08:46.513419 28508 process.cpp:1037] Socket closed while receiving
Killing the following process trees:
[ 
-+- 28509 sh -c sleep 1000 
 \--- 28510 sleep 1000 
]
Command terminated with signal Terminated (pid: 28509)
I0714 15:08:46.940232 28506 exec.cpp:524] Executor sending status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.940918 27240 slave.cpp:2086] Handling status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from executor(1)@127.0.1.1:34669
I0714 15:08:46.940979 27240 slave.cpp:3768] Terminating task 4a6783aa-8d07-46e3-8399-2a5d047f0021
I0714 15:08:46.941603 27240 status_update_manager.cpp:320] Received status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.941644 27240 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.949417 27236 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 63926ns
I0714 15:08:46.965200 27240 status_update_manager.cpp:373] Forwarding status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to master@127.0.1.1:55850
I0714 15:08:46.965625 27239 master.cpp:3115] Status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from slave 20140714-150843-16842879-55850-27216-0 at slave(44)@127.0.1.1:55850 (quantal)
I0714 15:08:46.965724 27239 master.hpp:791] Removing task 4a6783aa-8d07-46e3-8399-2a5d047f0021 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:46.965903 27239 sched.cpp:637] Scheduler::statusUpdate took 39326ns
I0714 15:08:46.966022 27239 hierarchical_allocator_process.hpp:635] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20140714-150843-16842879-55850-27216-0 from framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.966120 27239 master.cpp:2639] Forwarding status update acknowledgement e3a5f8fd-eefc-42c6-94a7-086c93c01968 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to slave 20140714-150843-16842879-55850-27216-0 at slave(44)@127.0.1.1:55850 (quantal)
I0714 15:08:46.966501 27241 slave.cpp:2244] Status update manager successfully handled status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.966519 27241 slave.cpp:2250] Sending acknowledgement for status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to executor(1)@127.0.1.1:34669
I0714 15:08:46.966754 27240 status_update_manager.cpp:398] Received status update acknowledgement (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.966785 27240 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_KILLED (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.967386 28500 exec.cpp:338] Executor received status update acknowledgement e3a5f8fd-eefc-42c6-94a7-086c93c01968 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.967562 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:46.968147 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:46.984608 27240 status_update_manager.cpp:530] Cleaning up status update stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.985239 27236 slave.cpp:1672] Status update manager successfully handled status update acknowledgement (UUID: e3a5f8fd-eefc-42c6-94a7-086c93c01968) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:46.985280 27236 slave.cpp:3810] Completing task 4a6783aa-8d07-46e3-8399-2a5d047f0021
I0714 15:08:47.940703 27243 process.cpp:1037] Socket closed while receiving
I0714 15:08:47.940984 27238 containerizer.cpp:1019] Executor for container '19c466f8-bb5a-4842-a152-f585ff88762a' has exited
I0714 15:08:47.941007 27238 containerizer.cpp:903] Destroying container '19c466f8-bb5a-4842-a152-f585ff88762a'
I0714 15:08:47.950192 27241 hierarchical_allocator_process.hpp:750] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 to framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:47.950405 27241 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 320604ns
I0714 15:08:47.950518 27241 master.hpp:801] Adding offer 20140714-150843-16842879-55850-27216-1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:47.950572 27241 master.cpp:3454] Sending 1 offers to framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:47.950774 27241 sched.cpp:546] Scheduler::resourceOffers took 37944ns
I0714 15:08:47.951179 27216 master.cpp:625] Master terminating
I0714 15:08:47.951263 27216 master.hpp:811] Removing offer 20140714-150843-16842879-55850-27216-1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:47.953447 27242 sched.cpp:747] Stopping framework '20140714-150843-16842879-55850-27216-0000'
I0714 15:08:47.953547 27242 slave.cpp:2330] master@127.0.1.1:55850 exited
W0714 15:08:47.953567 27242 slave.cpp:2333] Master disconnected! Waiting for a new master to be elected
I0714 15:08:47.964512 27238 slave.cpp:2526] Executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000 exited with status 0
I0714 15:08:47.968690 27238 slave.cpp:2660] Cleaning up executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:47.969348 27236 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a' for gc 6.99998878298667days in the future
I0714 15:08:47.969751 27241 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021' for gc 6.99998877682963days in the future
I0714 15:08:47.970082 27239 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a' for gc 6.99998877336889days in the future
I0714 15:08:47.970379 27242 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021' for gc 6.99998876968889days in the future
I0714 15:08:47.970587 27238 slave.cpp:2735] Cleaning up framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:47.970960 27237 status_update_manager.cpp:282] Closing status update streams for framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:47.971225 27236 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000' for gc 6.99998875966519days in the future
I0714 15:08:47.971549 27241 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000' for gc 6.99998875612148days in the future
W0714 15:08:47.975971 27235 containerizer.cpp:893] Ignoring destroy of unknown container: 19c466f8-bb5a-4842-a152-f585ff88762a
./tests/cluster.hpp:530: Failure
(wait).failure(): Unknown container: 19c466f8-bb5a-4842-a152-f585ff88762a
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00000000005a0299, pid=27216, tid=47907931709760
#
# JRE version: OpenJDK Runtime Environment (7.0_55-b14) (build 1.7.0_55-b14)
# Java VM: OpenJDK 64-Bit Server VM (24.51-b03 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [lt-mesos-tests+0x1a0299]  mlock@@GLIBC_2.2.5+0x1a0299
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
#
# An error report file with more information is saved as:
# /var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src/hs_err_pid27216.log
#
# If you would like to submit a bug report, please include
# instructions on how to reproduce the bug and visit:
#   http://icedtea.classpath.org/bugzilla
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
make[3]: *** [check-local] Aborted
make[3]: Leaving directory `/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src'
make[2]: *** [check-am] Error 2
make[2]: Leaving directory `/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src'
make[1]: *** [check] Error 2
make[1]: Leaving directory `/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src'
make: *** [check-recursive] Error 1
Build step 'Execute shell' marked build as failure
erreicht: 1854109
Sending e-mails to: kernel-test@twitter.com apache-mesos@twitter.com
Finished: FAILURE
 Help us localize this page Page generated: Jul 14, 2014 5:57:17 PMREST 
{code}",Ubuntu 12.10 with GCC,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-09-09 23:24:53.968,,,false,MESOS-2174,,,,,,,,,,,,,,,,,405324,,,Wed Jan 20 22:59:30 UTC 2016,,,,,,,"0|i1xr4v:",405351,,,,,vinodkone,,,,,,Mesosphere Sprint 27,,,,,,,,,,,1.0,,,,,,,,,,,"09/Sep/14 23:24;xujyan;Another instance on centos-7-gcc Jenkins VM.

{noformat:title=}
[ RUN      ] SlaveRecoveryTest/0.ReconcileKillTask
Using temporary directory '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_IJPbbd'
I0909 09:51:40.897189   544 leveldb.cpp:176] Opened db in 1.795293765secs
I0909 09:51:40.903898   544 leveldb.cpp:183] Compacted db in 6.65008ms
I0909 09:51:40.903934   544 leveldb.cpp:198] Created db iterator in 13655ns
I0909 09:51:40.903949   544 leveldb.cpp:204] Seeked to beginning of db in 937ns
I0909 09:51:40.903955   544 leveldb.cpp:273] Iterated through 0 keys in the db in 511ns
I0909 09:51:40.903985   544 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0909 09:51:40.905460   570 recover.cpp:425] Starting replica recovery
I0909 09:51:40.905555   570 recover.cpp:451] Replica is in EMPTY status
I0909 09:51:40.905923   570 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0909 09:51:40.905977   570 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0909 09:51:40.906060   570 recover.cpp:542] Updating replica status to STARTING
I0909 09:51:40.906870   563 master.cpp:286] Master 20140909-095140-1148889280-43580-544 (centos-7) started on 192.168.122.68:43580
I0909 09:51:40.906908   563 master.cpp:332] Master only allowing authenticated frameworks to register
I0909 09:51:40.906915   563 master.cpp:337] Master only allowing authenticated slaves to register
I0909 09:51:40.906926   563 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_IJPbbd/credentials'
I0909 09:51:40.907069   563 master.cpp:366] Authorization enabled
I0909 09:51:40.907492   563 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@192.168.122.68:43580
I0909 09:51:40.907528   563 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0909 09:51:40.907690   563 master.cpp:1212] The newly elected leader is master@192.168.122.68:43580 with id 20140909-095140-1148889280-43580-544
I0909 09:51:40.907703   563 master.cpp:1225] Elected as the leading master!
I0909 09:51:40.907708   563 master.cpp:1043] Recovering from registrar
I0909 09:51:40.907793   563 registrar.cpp:313] Recovering registrar
I0909 09:51:40.913550   570 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 7.435041ms
I0909 09:51:40.913574   570 replica.cpp:320] Persisted replica status to STARTING
I0909 09:51:40.913655   570 recover.cpp:451] Replica is in STARTING status
I0909 09:51:40.913939   570 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0909 09:51:40.913988   570 recover.cpp:188] Received a recover response from a replica in STARTING status
I0909 09:51:40.914070   570 recover.cpp:542] Updating replica status to VOTING
I0909 09:51:40.919895   570 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 5.778157ms
I0909 09:51:40.919927   570 replica.cpp:320] Persisted replica status to VOTING
I0909 09:51:40.919983   570 recover.cpp:556] Successfully joined the Paxos group
I0909 09:51:40.920027   570 recover.cpp:440] Recover process terminated
I0909 09:51:40.920142   570 log.cpp:656] Attempting to start the writer
I0909 09:51:40.920397   570 replica.cpp:474] Replica received implicit promise request with proposal 1
I0909 09:51:40.925762   570 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 5.343628ms
I0909 09:51:40.925794   570 replica.cpp:342] Persisted promised to 1
I0909 09:51:40.925971   570 coordinator.cpp:230] Coordinator attemping to fill missing position
I0909 09:51:40.926249   570 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0909 09:51:40.931947   570 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 5.676183ms
I0909 09:51:40.931980   570 replica.cpp:676] Persisted action at 0
I0909 09:51:40.932229   570 replica.cpp:508] Replica received write request for position 0
I0909 09:51:40.932255   570 leveldb.cpp:438] Reading position from leveldb took 11652ns
I0909 09:51:40.938361   570 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 6.087894ms
I0909 09:51:40.938395   570 replica.cpp:676] Persisted action at 0
I0909 09:51:40.938524   570 replica.cpp:655] Replica received learned notice for position 0
I0909 09:51:40.943999   570 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.451739ms
I0909 09:51:40.944031   570 replica.cpp:676] Persisted action at 0
I0909 09:51:40.944046   570 replica.cpp:661] Replica learned NOP action at position 0
I0909 09:51:40.944229   570 log.cpp:672] Writer started with ending position 0
I0909 09:51:40.944416   570 leveldb.cpp:438] Reading position from leveldb took 10354ns
I0909 09:51:40.945526   570 registrar.cpp:346] Successfully fetched the registry (0B)
I0909 09:51:40.945554   570 registrar.cpp:422] Attempting to update the 'registry'
I0909 09:51:40.946728   570 log.cpp:680] Attempting to append 127 bytes to the log
I0909 09:51:40.946769   570 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0909 09:51:40.946936   570 replica.cpp:508] Replica received write request for position 1
I0909 09:51:40.952258   570 leveldb.cpp:343] Persisting action (145 bytes) to leveldb took 5.300201ms
I0909 09:51:40.952286   570 replica.cpp:676] Persisted action at 1
I0909 09:51:40.952437   570 replica.cpp:655] Replica received learned notice for position 1
I0909 09:51:40.958236   570 leveldb.cpp:343] Persisting action (147 bytes) to leveldb took 5.781888ms
I0909 09:51:40.958261   570 replica.cpp:676] Persisted action at 1
I0909 09:51:40.958268   570 replica.cpp:661] Replica learned APPEND action at position 1
I0909 09:51:40.958499   570 registrar.cpp:479] Successfully updated 'registry'
I0909 09:51:40.958533   570 registrar.cpp:372] Successfully recovered registrar
I0909 09:51:40.958561   570 log.cpp:699] Attempting to truncate the log to 1
I0909 09:51:40.958611   570 master.cpp:1070] Recovered 0 slaves from the Registry (91B) ; allowing 10mins for slaves to re-register
I0909 09:51:40.958643   570 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0909 09:51:40.958839   570 replica.cpp:508] Replica received write request for position 2
I0909 09:51:40.964298   570 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.439108ms
I0909 09:51:40.964323   570 replica.cpp:676] Persisted action at 2
I0909 09:51:40.964465   570 replica.cpp:655] Replica received learned notice for position 2
I0909 09:51:40.969729   570 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 5.226703ms
I0909 09:51:40.969776   570 leveldb.cpp:401] Deleting ~1 keys from leveldb took 21142ns
I0909 09:51:40.969789   570 replica.cpp:676] Persisted action at 2
I0909 09:51:40.969795   570 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0909 09:51:40.980108   544 containerizer.cpp:89] Using isolation: posix/cpu,posix/mem
I0909 09:51:40.982628   570 slave.cpp:167] Slave started on 8)@192.168.122.68:43580
I0909 09:51:40.982650   570 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/credential'
I0909 09:51:40.982750   570 slave.cpp:274] Slave using credential for: test-principal
I0909 09:51:40.982841   570 slave.cpp:287] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0909 09:51:40.983360   570 slave.cpp:315] Slave hostname: centos-7
I0909 09:51:40.983378   570 slave.cpp:316] Slave checkpoint: true
I0909 09:51:40.983768   570 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/meta'
I0909 09:51:40.983871   570 status_update_manager.cpp:193] Recovering status update manager
I0909 09:51:40.983922   570 containerizer.cpp:252] Recovering containerizer
I0909 09:51:40.984138   570 slave.cpp:3202] Finished recovery
I0909 09:51:40.984550   570 slave.cpp:598] New master detected at master@192.168.122.68:43580
I0909 09:51:40.984580   570 slave.cpp:672] Authenticating with master master@192.168.122.68:43580
I0909 09:51:40.984614   570 slave.cpp:645] Detecting new master
I0909 09:51:40.984643   570 status_update_manager.cpp:167] New master detected at master@192.168.122.68:43580
I0909 09:51:40.984671   570 authenticatee.hpp:128] Creating new client SASL connection
I0909 09:51:40.985360   570 master.cpp:3653] Authenticating slave(8)@192.168.122.68:43580
I0909 09:51:40.985465   570 authenticator.hpp:156] Creating new server SASL connection
I0909 09:51:40.986243   570 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0909 09:51:40.986265   570 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0909 09:51:40.986290   570 authenticator.hpp:262] Received SASL authentication start
I0909 09:51:40.986322   570 authenticator.hpp:384] Authentication requires more steps
I0909 09:51:40.986346   570 authenticatee.hpp:265] Received SASL authentication step
I0909 09:51:40.986388   570 authenticator.hpp:290] Received SASL authentication step
I0909 09:51:40.986410   570 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0909 09:51:40.986418   570 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0909 09:51:40.986428   570 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0909 09:51:40.986435   570 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0909 09:51:40.986441   570 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0909 09:51:40.986446   570 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0909 09:51:40.986456   570 authenticator.hpp:376] Authentication success
I0909 09:51:40.986490   570 authenticatee.hpp:305] Authentication success
I0909 09:51:40.986510   570 master.cpp:3693] Successfully authenticated principal 'test-principal' at slave(8)@192.168.122.68:43580
I0909 09:51:40.986567   570 slave.cpp:729] Successfully authenticated with master master@192.168.122.68:43580
I0909 09:51:40.986611   570 slave.cpp:980] Will retry registration in 10.02298ms if necessary
I0909 09:51:40.986747   570 master.cpp:2843] Registering slave at slave(8)@192.168.122.68:43580 (centos-7) with id 20140909-095140-1148889280-43580-544-0
I0909 09:51:40.986847   570 registrar.cpp:422] Attempting to update the 'registry'
I0909 09:51:40.987818   570 log.cpp:680] Attempting to append 304 bytes to the log
I0909 09:51:40.987864   570 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0909 09:51:40.988065   570 replica.cpp:508] Replica received write request for position 3
I0909 09:51:40.991019   544 sched.cpp:137] Version: 0.21.0
I0909 09:51:40.991173   563 sched.cpp:233] New master detected at master@192.168.122.68:43580
I0909 09:51:40.991199   563 sched.cpp:283] Authenticating with master master@192.168.122.68:43580
I0909 09:51:40.991260   563 authenticatee.hpp:128] Creating new client SASL connection
I0909 09:51:40.992020   563 master.cpp:3653] Authenticating scheduler-4c7a352c-0857-42f7-a3ed-05246db01c5e@192.168.122.68:43580
I0909 09:51:40.992122   563 authenticator.hpp:156] Creating new server SASL connection
I0909 09:51:40.992588   563 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0909 09:51:40.992619   563 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0909 09:51:40.992647   563 authenticator.hpp:262] Received SASL authentication start
I0909 09:51:40.992677   563 authenticator.hpp:384] Authentication requires more steps
I0909 09:51:40.992748   563 authenticatee.hpp:265] Received SASL authentication step
I0909 09:51:40.992854   563 authenticator.hpp:290] Received SASL authentication step
I0909 09:51:40.992873   563 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0909 09:51:40.992880   563 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0909 09:51:40.992887   563 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0909 09:51:40.992895   563 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0909 09:51:40.992902   563 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0909 09:51:40.992905   563 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0909 09:51:40.992915   563 authenticator.hpp:376] Authentication success
I0909 09:51:40.992938   563 authenticatee.hpp:305] Authentication success
I0909 09:51:40.992957   563 master.cpp:3693] Successfully authenticated principal 'test-principal' at scheduler-4c7a352c-0857-42f7-a3ed-05246db01c5e@192.168.122.68:43580
I0909 09:51:40.993015   563 sched.cpp:357] Successfully authenticated with master master@192.168.122.68:43580
I0909 09:51:40.993026   563 sched.cpp:476] Sending registration request to master@192.168.122.68:43580
I0909 09:51:40.993063   563 master.cpp:1331] Received registration request from scheduler-4c7a352c-0857-42f7-a3ed-05246db01c5e@192.168.122.68:43580
I0909 09:51:40.993093   563 master.cpp:1291] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0909 09:51:40.993198   563 master.cpp:1390] Registering framework 20140909-095140-1148889280-43580-544-0000 at scheduler-4c7a352c-0857-42f7-a3ed-05246db01c5e@192.168.122.68:43580
I0909 09:51:40.993289   563 sched.cpp:407] Framework registered with 20140909-095140-1148889280-43580-544-0000
I0909 09:51:40.993310   563 sched.cpp:421] Scheduler::registered took 8514ns
I0909 09:51:40.993348   563 hierarchical_allocator_process.hpp:329] Added framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:40.993358   563 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I0909 09:51:40.993365   563 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 6610ns
I0909 09:51:40.995043   570 leveldb.cpp:343] Persisting action (323 bytes) to leveldb took 6.956739ms
I0909 09:51:40.995069   570 replica.cpp:676] Persisted action at 3
I0909 09:51:40.995234   570 replica.cpp:655] Replica received learned notice for position 3
I0909 09:51:40.997666   563 slave.cpp:980] Will retry registration in 38.204816ms if necessary
I0909 09:51:40.997764   563 master.cpp:2831] Ignoring register slave message from slave(8)@192.168.122.68:43580 (centos-7) as admission is already in progress
I0909 09:51:41.000252   570 leveldb.cpp:343] Persisting action (325 bytes) to leveldb took 4.9979ms
I0909 09:51:41.000277   570 replica.cpp:676] Persisted action at 3
I0909 09:51:41.000285   570 replica.cpp:661] Replica learned APPEND action at position 3
I0909 09:51:41.000547   570 registrar.cpp:479] Successfully updated 'registry'
I0909 09:51:41.000612   570 log.cpp:699] Attempting to truncate the log to 3
I0909 09:51:41.000668   570 master.cpp:2883] Registered slave 20140909-095140-1148889280-43580-544-0 at slave(8)@192.168.122.68:43580 (centos-7)
I0909 09:51:41.000682   570 master.cpp:4126] Adding slave 20140909-095140-1148889280-43580-544-0 at slave(8)@192.168.122.68:43580 (centos-7) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0909 09:51:41.000797   570 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0909 09:51:41.000859   570 slave.cpp:763] Registered with master master@192.168.122.68:43580; given slave ID 20140909-095140-1148889280-43580-544-0
I0909 09:51:41.001266   570 slave.cpp:776] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/meta/slaves/20140909-095140-1148889280-43580-544-0/slave.info'
I0909 09:51:41.001503   570 hierarchical_allocator_process.hpp:442] Added slave 20140909-095140-1148889280-43580-544-0 (centos-7) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0909 09:51:41.001555   570 hierarchical_allocator_process.hpp:734] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140909-095140-1148889280-43580-544-0 to framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.001622   570 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20140909-095140-1148889280-43580-544-0 in 85987ns
I0909 09:51:41.001665   570 slave.cpp:2329] Received ping from slave-observer(7)@192.168.122.68:43580
I0909 09:51:41.001760   570 master.hpp:861] Adding offer 20140909-095140-1148889280-43580-544-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140909-095140-1148889280-43580-544-0 (centos-7)
I0909 09:51:41.001811   570 master.cpp:3600] Sending 1 offers to framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.001915   570 sched.cpp:544] Scheduler::resourceOffers took 16228ns
I0909 09:51:41.002053   570 replica.cpp:508] Replica received write request for position 4
I0909 09:51:41.003602   563 master.hpp:871] Removing offer 20140909-095140-1148889280-43580-544-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140909-095140-1148889280-43580-544-0 (centos-7)
I0909 09:51:41.003660   563 master.cpp:2201] Processing reply for offers: [ 20140909-095140-1148889280-43580-544-0 ] on slave 20140909-095140-1148889280-43580-544-0 at slave(8)@192.168.122.68:43580 (centos-7) for framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.003690   563 master.cpp:2284] Authorizing framework principal 'test-principal' to launch task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 as user 'jenkins'
I0909 09:51:41.003962   563 master.hpp:833] Adding task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140909-095140-1148889280-43580-544-0 (centos-7)
I0909 09:51:41.003998   563 master.cpp:2350] Launching task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140909-095140-1148889280-43580-544-0 at slave(8)@192.168.122.68:43580 (centos-7)
I0909 09:51:41.004109   563 slave.cpp:1011] Got assigned task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 for framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.004178   563 slave.cpp:3548] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/meta/slaves/20140909-095140-1148889280-43580-544-0/frameworks/20140909-095140-1148889280-43580-544-0000/framework.info'
I0909 09:51:41.004513   563 slave.cpp:3555] Checkpointing framework pid 'scheduler-4c7a352c-0857-42f7-a3ed-05246db01c5e@192.168.122.68:43580' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/meta/slaves/20140909-095140-1148889280-43580-544-0/frameworks/20140909-095140-1148889280-43580-544-0000/framework.pid'
I0909 09:51:41.004886   563 slave.cpp:1121] Launching task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 for framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.006407   563 slave.cpp:3866] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/meta/slaves/20140909-095140-1148889280-43580-544-0/frameworks/20140909-095140-1148889280-43580-544-0000/executors/b81d5e8b-cd35-4c14-8a47-a5f5a3777f17/executor.info'
I0909 09:51:41.007109   563 slave.cpp:3977] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/meta/slaves/20140909-095140-1148889280-43580-544-0/frameworks/20140909-095140-1148889280-43580-544-0000/executors/b81d5e8b-cd35-4c14-8a47-a5f5a3777f17/runs/bfb0723d-ad52-4dc8-b95e-4610ddb9a819/tasks/b81d5e8b-cd35-4c14-8a47-a5f5a3777f17/task.info'
I0909 09:51:41.007434   565 containerizer.cpp:394] Starting container 'bfb0723d-ad52-4dc8-b95e-4610ddb9a819' for executor 'b81d5e8b-cd35-4c14-8a47-a5f5a3777f17' of framework '20140909-095140-1148889280-43580-544-0000'
I0909 09:51:41.008872   565 launcher.cpp:137] Forked child with pid '901' for container 'bfb0723d-ad52-4dc8-b95e-4610ddb9a819'
I0909 09:51:41.009019   565 containerizer.cpp:678] Checkpointing executor's forked pid 901 to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/meta/slaves/20140909-095140-1148889280-43580-544-0/frameworks/20140909-095140-1148889280-43580-544-0000/executors/b81d5e8b-cd35-4c14-8a47-a5f5a3777f17/runs/bfb0723d-ad52-4dc8-b95e-4610ddb9a819/pids/forked.pid'
I0909 09:51:41.010238   563 slave.cpp:1231] Queuing task 'b81d5e8b-cd35-4c14-8a47-a5f5a3777f17' for executor b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework '20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.010334   563 slave.cpp:552] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/slaves/20140909-095140-1148889280-43580-544-0/frameworks/20140909-095140-1148889280-43580-544-0000/executors/b81d5e8b-cd35-4c14-8a47-a5f5a3777f17/runs/bfb0723d-ad52-4dc8-b95e-4610ddb9a819'
I0909 09:51:41.011068   565 containerizer.cpp:510] Fetching URIs for container 'bfb0723d-ad52-4dc8-b95e-4610ddb9a819' using command '/var/jenkins/workspace/mesos-centos-7-gcc/src/mesos-fetcher'
I0909 09:51:41.017060   570 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 14.916871ms
I0909 09:51:41.017137   570 replica.cpp:676] Persisted action at 4
I0909 09:51:41.017464   570 replica.cpp:655] Replica received learned notice for position 4
I0909 09:51:41.032018   570 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 14.522155ms
I0909 09:51:41.032094   570 leveldb.cpp:401] Deleting ~2 keys from leveldb took 32909ns
I0909 09:51:41.032109   570 replica.cpp:676] Persisted action at 4
I0909 09:51:41.032119   570 replica.cpp:661] Replica learned TRUNCATE action at position 4
I0909 09:51:41.877382   570 slave.cpp:2542] Monitoring executor 'b81d5e8b-cd35-4c14-8a47-a5f5a3777f17' of framework '20140909-095140-1148889280-43580-544-0000' in container 'bfb0723d-ad52-4dc8-b95e-4610ddb9a819'
I0909 09:51:41.915293   570 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 32237ns
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0909 09:51:41.916180   901 process.cpp:1771] libprocess is initialized on 192.168.122.68:50645 for 8 cpus
I0909 09:51:41.916468   901 logging.cpp:177] Logging to STDERR
I0909 09:51:41.917357   901 exec.cpp:132] Version: 0.21.0
I0909 09:51:41.919569   952 exec.cpp:182] Executor started at: executor(1)@192.168.122.68:50645 with pid 901
I0909 09:51:41.920593   570 slave.cpp:1741] Got registration for executor 'b81d5e8b-cd35-4c14-8a47-a5f5a3777f17' of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.920641   570 slave.cpp:1826] Checkpointing executor pid 'executor(1)@192.168.122.68:50645' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/meta/slaves/20140909-095140-1148889280-43580-544-0/frameworks/20140909-095140-1148889280-43580-544-0000/executors/b81d5e8b-cd35-4c14-8a47-a5f5a3777f17/runs/bfb0723d-ad52-4dc8-b95e-4610ddb9a819/pids/libprocess.pid'
I0909 09:51:41.922448   570 slave.cpp:1859] Flushing queued task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 for executor 'b81d5e8b-cd35-4c14-8a47-a5f5a3777f17' of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.923576   952 exec.cpp:206] Executor registered on slave 20140909-095140-1148889280-43580-544-0
I0909 09:51:41.924360   952 exec.cpp:218] Executor::registered took 80708ns
I0909 09:51:41.924443   952 exec.cpp:293] Executor asked to run task 'b81d5e8b-cd35-4c14-8a47-a5f5a3777f17'
I0909 09:51:41.924475   952 exec.cpp:302] Executor::launchTask took 21066ns
Registered executor on centos-7
Starting task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17
sh -c 'sleep 1000'
Forked command at 956
I0909 09:51:41.929075   952 exec.cpp:525] Executor sending status update TASK_RUNNING (UUID: 22d70b8d-ee74-40e9-b6fb-0224d1c5fa96) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.929788   570 slave.cpp:2093] Handling status update TASK_RUNNING (UUID: 22d70b8d-ee74-40e9-b6fb-0224d1c5fa96) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000 from executor(1)@192.168.122.68:50645
I0909 09:51:41.929898   570 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 22d70b8d-ee74-40e9-b6fb-0224d1c5fa96) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.929911   570 status_update_manager.cpp:499] Creating StatusUpdate stream for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.930209   570 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 22d70b8d-ee74-40e9-b6fb-0224d1c5fa96) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.941706   570 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 22d70b8d-ee74-40e9-b6fb-0224d1c5fa96) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000 to master@192.168.122.68:43580
I0909 09:51:41.941956   570 master.cpp:3212] Forwarding status update TASK_RUNNING (UUID: 22d70b8d-ee74-40e9-b6fb-0224d1c5fa96) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.941999   570 master.cpp:3178] Status update TASK_RUNNING (UUID: 22d70b8d-ee74-40e9-b6fb-0224d1c5fa96) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000 from slave 20140909-095140-1148889280-43580-544-0 at slave(8)@192.168.122.68:43580 (centos-7)
I0909 09:51:41.942039   570 slave.cpp:2250] Status update manager successfully handled status update TASK_RUNNING (UUID: 22d70b8d-ee74-40e9-b6fb-0224d1c5fa96) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.942049   570 slave.cpp:2256] Sending acknowledgement for status update TASK_RUNNING (UUID: 22d70b8d-ee74-40e9-b6fb-0224d1c5fa96) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000 to executor(1)@192.168.122.68:50645
I0909 09:51:41.942381   570 sched.cpp:635] Scheduler::statusUpdate took 15922ns
I0909 09:51:41.942433   570 master.cpp:2693] Forwarding status update acknowledgement 22d70b8d-ee74-40e9-b6fb-0224d1c5fa96 for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000 to slave 20140909-095140-1148889280-43580-544-0 at slave(8)@192.168.122.68:43580 (centos-7)
I0909 09:51:41.942498   570 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 22d70b8d-ee74-40e9-b6fb-0224d1c5fa96) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.942515   570 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_RUNNING (UUID: 22d70b8d-ee74-40e9-b6fb-0224d1c5fa96) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.943263   952 exec.cpp:339] Executor received status update acknowledgement 22d70b8d-ee74-40e9-b6fb-0224d1c5fa96 for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.954205   570 slave.cpp:1681] Status update manager successfully handled status update acknowledgement (UUID: 22d70b8d-ee74-40e9-b6fb-0224d1c5fa96) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.954396   544 slave.cpp:475] Slave terminating
I0909 09:51:41.954929   544 containerizer.cpp:89] Using isolation: posix/cpu,posix/mem
I0909 09:51:41.956308   568 master.cpp:791] Slave 20140909-095140-1148889280-43580-544-0 at slave(8)@192.168.122.68:43580 (centos-7) disconnected
I0909 09:51:41.956329   568 master.cpp:1712] Disconnecting slave 20140909-095140-1148889280-43580-544-0
I0909 09:51:41.956609   568 hierarchical_allocator_process.hpp:481] Slave 20140909-095140-1148889280-43580-544-0 deactivated
I0909 09:51:41.956657   568 master.cpp:2527] Asked to kill task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.957993   568 slave.cpp:167] Slave started on 9)@192.168.122.68:43580
I0909 09:51:41.958017   568 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/credential'
I0909 09:51:41.958104   568 slave.cpp:274] Slave using credential for: test-principal
I0909 09:51:41.958187   568 slave.cpp:287] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0909 09:51:41.958969   568 slave.cpp:315] Slave hostname: centos-7
I0909 09:51:41.958994   568 slave.cpp:316] Slave checkpoint: true
I0909 09:51:41.959348   568 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/meta'
I0909 09:51:41.960574   568 slave.cpp:3273] Recovering framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.960600   568 slave.cpp:3714] Recovering executor 'b81d5e8b-cd35-4c14-8a47-a5f5a3777f17' of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.960989   568 status_update_manager.cpp:193] Recovering status update manager
I0909 09:51:41.961002   568 status_update_manager.cpp:201] Recovering executor 'b81d5e8b-cd35-4c14-8a47-a5f5a3777f17' of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.961019   568 status_update_manager.cpp:499] Creating StatusUpdate stream for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.961112   568 status_update_manager.hpp:306] Replaying status update stream for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17
I0909 09:51:41.961216   568 slave.cpp:552] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/slaves/20140909-095140-1148889280-43580-544-0/frameworks/20140909-095140-1148889280-43580-544-0000/executors/b81d5e8b-cd35-4c14-8a47-a5f5a3777f17/runs/bfb0723d-ad52-4dc8-b95e-4610ddb9a819'
I0909 09:51:41.961318   568 containerizer.cpp:252] Recovering containerizer
I0909 09:51:41.961335   568 containerizer.cpp:294] Recovering container 'bfb0723d-ad52-4dc8-b95e-4610ddb9a819' for executor 'b81d5e8b-cd35-4c14-8a47-a5f5a3777f17' of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.962028   568 slave.cpp:3143] Sending reconnect request to executor b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000 at executor(1)@192.168.122.68:50645
I0909 09:51:41.962991   952 exec.cpp:252] Received reconnect request from slave 20140909-095140-1148889280-43580-544-0
I0909 09:51:41.963770   570 slave.cpp:1919] Re-registering executor b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:41.964565   952 exec.cpp:229] Executor re-registered on slave 20140909-095140-1148889280-43580-544-0
I0909 09:51:41.965406   952 exec.cpp:241] Executor::reregistered took 29855ns
Re-registered executor on centos-7
I0909 09:51:42.949388   570 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 9516ns
I0909 09:51:43.952998   570 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 72052ns
I0909 09:51:43.974743   571 slave.cpp:2042] Cleaning up un-reregistered executors
I0909 09:51:43.974859   571 slave.cpp:3202] Finished recovery
I0909 09:51:43.975504   571 slave.cpp:598] New master detected at master@192.168.122.68:43580
I0909 09:51:43.975543   571 slave.cpp:672] Authenticating with master master@192.168.122.68:43580
I0909 09:51:43.975636   571 slave.cpp:645] Detecting new master
I0909 09:51:43.975673   571 status_update_manager.cpp:167] New master detected at master@192.168.122.68:43580
I0909 09:51:43.975770   571 authenticatee.hpp:128] Creating new client SASL connection
I0909 09:51:43.980680   571 master.cpp:3653] Authenticating slave(9)@192.168.122.68:43580
I0909 09:51:43.980831   571 authenticator.hpp:156] Creating new server SASL connection
I0909 09:51:43.992446   571 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0909 09:51:43.992470   571 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0909 09:51:43.992492   571 authenticator.hpp:262] Received SASL authentication start
I0909 09:51:43.992528   571 authenticator.hpp:384] Authentication requires more steps
I0909 09:51:43.992552   571 authenticatee.hpp:265] Received SASL authentication step
I0909 09:51:43.992581   571 authenticator.hpp:290] Received SASL authentication step
I0909 09:51:43.992605   571 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0909 09:51:43.992614   571 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0909 09:51:43.992624   571 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0909 09:51:43.992638   571 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'centos-7' server FQDN: 'centos-7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0909 09:51:43.992645   571 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0909 09:51:43.992650   571 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0909 09:51:43.992660   571 authenticator.hpp:376] Authentication success
I0909 09:51:43.992686   571 authenticatee.hpp:305] Authentication success
I0909 09:51:43.992704   571 master.cpp:3693] Successfully authenticated principal 'test-principal' at slave(9)@192.168.122.68:43580
I0909 09:51:43.992817   571 slave.cpp:729] Successfully authenticated with master master@192.168.122.68:43580
I0909 09:51:43.992920   571 slave.cpp:980] Will retry registration in 2.002547ms if necessary
W0909 09:51:43.993034   571 master.cpp:2958] Slave at slave(9)@192.168.122.68:43580 (centos-7) is being allowed to re-register with an already in use id (20140909-095140-1148889280-43580-544-0)
W0909 09:51:43.993115   571 master.cpp:3826]  Slave 20140909-095140-1148889280-43580-544-0 at slave(9)@192.168.122.68:43580 (centos-7) has non-terminal task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 that is supposed to be killed. Killing it now!
I0909 09:51:43.993190   571 slave.cpp:825] Re-registered with master master@192.168.122.68:43580
I0909 09:51:43.993214   571 slave.cpp:1286] Asked to kill task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:43.993443   571 slave.cpp:1591] Updating framework 20140909-095140-1148889280-43580-544-0000 pid to scheduler-4c7a352c-0857-42f7-a3ed-05246db01c5e@192.168.122.68:43580
I0909 09:51:43.993474   571 slave.cpp:1599] Checkpointing framework pid 'scheduler-4c7a352c-0857-42f7-a3ed-05246db01c5e@192.168.122.68:43580' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/meta/slaves/20140909-095140-1148889280-43580-544-0/frameworks/20140909-095140-1148889280-43580-544-0000/framework.pid'
I0909 09:51:43.993677   567 hierarchical_allocator_process.hpp:495] Slave 20140909-095140-1148889280-43580-544-0 reactivated
I0909 09:51:44.121048   952 exec.cpp:313] Executor asked to kill task 'b81d5e8b-cd35-4c14-8a47-a5f5a3777f17'
I0909 09:51:44.121096   952 exec.cpp:322] Executor::killTask took 17461ns
Shutting down
Sending SIGTERM to process tree at pid 956
Killing the following process trees:
[ 
--- 956 sleep 1000 
]
Command terminated with signal Terminated (pid: 956)
I0909 09:51:44.976544   949 exec.cpp:525] Executor sending status update TASK_KILLED (UUID: 1fb55fe4-b0e3-4d80-8429-6d6a280583da) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:44.977221   563 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 45292ns
I0909 09:51:44.977294   563 slave.cpp:2093] Handling status update TASK_KILLED (UUID: 1fb55fe4-b0e3-4d80-8429-6d6a280583da) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000 from executor(1)@192.168.122.68:50645
I0909 09:51:44.977352   563 slave.cpp:3913] Terminating task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17
I0909 09:51:44.977848   563 status_update_manager.cpp:320] Received status update TASK_KILLED (UUID: 1fb55fe4-b0e3-4d80-8429-6d6a280583da) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:44.977874   563 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_KILLED (UUID: 1fb55fe4-b0e3-4d80-8429-6d6a280583da) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:44.990294   563 status_update_manager.cpp:373] Forwarding status update TASK_KILLED (UUID: 1fb55fe4-b0e3-4d80-8429-6d6a280583da) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000 to master@192.168.122.68:43580
I0909 09:51:44.990542   563 master.cpp:3212] Forwarding status update TASK_KILLED (UUID: 1fb55fe4-b0e3-4d80-8429-6d6a280583da) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:44.990581   563 master.cpp:3178] Status update TASK_KILLED (UUID: 1fb55fe4-b0e3-4d80-8429-6d6a280583da) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000 from slave 20140909-095140-1148889280-43580-544-0 at slave(9)@192.168.122.68:43580 (centos-7)
I0909 09:51:44.990669   563 master.hpp:851] Removing task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140909-095140-1148889280-43580-544-0 (centos-7)
I0909 09:51:44.990814   563 slave.cpp:2250] Status update manager successfully handled status update TASK_KILLED (UUID: 1fb55fe4-b0e3-4d80-8429-6d6a280583da) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:44.990829   563 slave.cpp:2256] Sending acknowledgement for status update TASK_KILLED (UUID: 1fb55fe4-b0e3-4d80-8429-6d6a280583da) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000 to executor(1)@192.168.122.68:50645
I0909 09:51:44.991266   563 sched.cpp:635] Scheduler::statusUpdate took 71953ns
I0909 09:51:44.991360   563 hierarchical_allocator_process.hpp:563] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20140909-095140-1148889280-43580-544-0 from framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:44.991443   563 master.cpp:2693] Forwarding status update acknowledgement 1fb55fe4-b0e3-4d80-8429-6d6a280583da for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000 to slave 20140909-095140-1148889280-43580-544-0 at slave(9)@192.168.122.68:43580 (centos-7)
I0909 09:51:44.991580   563 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 1fb55fe4-b0e3-4d80-8429-6d6a280583da) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:44.991601   563 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_KILLED (UUID: 1fb55fe4-b0e3-4d80-8429-6d6a280583da) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:44.992183   949 exec.cpp:339] Executor received status update acknowledgement 1fb55fe4-b0e3-4d80-8429-6d6a280583da for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:45.027616   563 status_update_manager.cpp:530] Cleaning up status update stream for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:45.027806   563 slave.cpp:1681] Status update manager successfully handled status update acknowledgement (UUID: 1fb55fe4-b0e3-4d80-8429-6d6a280583da) for task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17 of framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:45.027838   563 slave.cpp:3952] Completing task b81d5e8b-cd35-4c14-8a47-a5f5a3777f17
I0909 09:51:45.911133   565 master.cpp:120] No whitelist given. Advertising offers for all slaves
I0909 09:51:45.980281   571 hierarchical_allocator_process.hpp:734] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140909-095140-1148889280-43580-544-0 to framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:45.980523   571 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 433319ns
I0909 09:51:45.980589   571 containerizer.cpp:997] Executor for container 'bfb0723d-ad52-4dc8-b95e-4610ddb9a819' has exited
I0909 09:51:45.980612   571 containerizer.cpp:882] Destroying container 'bfb0723d-ad52-4dc8-b95e-4610ddb9a819'
I0909 09:51:45.990910   565 master.hpp:861] Adding offer 20140909-095140-1148889280-43580-544-1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140909-095140-1148889280-43580-544-0 (centos-7)
I0909 09:51:45.991014   565 master.cpp:3600] Sending 1 offers to framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:45.991368   565 sched.cpp:544] Scheduler::resourceOffers took 35039ns
I0909 09:51:45.991673   544 master.cpp:650] Master terminating
I0909 09:51:45.991749   544 master.hpp:871] Removing offer 20140909-095140-1148889280-43580-544-1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140909-095140-1148889280-43580-544-0 (centos-7)
I0909 09:51:45.993424   567 sched.cpp:745] Stopping framework '20140909-095140-1148889280-43580-544-0000'
I0909 09:51:45.993504   567 slave.cpp:2361] master@192.168.122.68:43580 exited
W0909 09:51:45.993525   567 slave.cpp:2364] Master disconnected! Waiting for a new master to be elected
W0909 09:51:46.006973   571 containerizer.cpp:872] Ignoring destroy of unknown container: bfb0723d-ad52-4dc8-b95e-4610ddb9a819
I0909 09:51:46.007072   571 slave.cpp:2600] Executor 'b81d5e8b-cd35-4c14-8a47-a5f5a3777f17' of framework 20140909-095140-1148889280-43580-544-0000 exited with status 0
I0909 09:51:46.007222   571 slave.cpp:2736] Cleaning up executor 'b81d5e8b-cd35-4c14-8a47-a5f5a3777f17' of framework 20140909-095140-1148889280-43580-544-0000
./tests/cluster.hpp:530: Failure
(wait).failure(): Unknown container: bfb0723d-ad52-4dc8-b95e-4610ddb9a819
I0909 09:51:46.010267   571 slave.cpp:2811] Cleaning up framework 20140909-095140-1148889280-43580-544-0000
I0909 09:51:46.010457   571 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/slaves/20140909-095140-1148889280-43580-544-0/frameworks/20140909-095140-1148889280-43580-544-0000/executors/b81d5e8b-cd35-4c14-8a47-a5f5a3777f17/runs/bfb0723d-ad52-4dc8-b95e-4610ddb9a819' for gc 6.9999998843763days in the future
I0909 09:51:46.010556   571 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/slaves/20140909-095140-1148889280-43580-544-0/frameworks/20140909-095140-1148889280-43580-544-0000/executors/b81d5e8b-cd35-4c14-8a47-a5f5a3777f17' for gc 6.99999988291556days in the future
I0909 09:51:46.010619   571 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/meta/slaves/20140909-095140-1148889280-43580-544-0/frameworks/20140909-095140-1148889280-43580-544-0000/executors/b81d5e8b-cd35-4c14-8a47-a5f5a3777f17/runs/bfb0723d-ad52-4dc8-b95e-4610ddb9a819' for gc 6.99999988221037days in the future
I0909 09:51:46.010679   571 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/meta/slaves/20140909-095140-1148889280-43580-544-0/frameworks/20140909-095140-1148889280-43580-544-0000/executors/b81d5e8b-cd35-4c14-8a47-a5f5a3777f17' for gc 6.99999988152889days in the future
I0909 09:51:46.010767   571 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/slaves/20140909-095140-1148889280-43580-544-0/frameworks/20140909-095140-1148889280-43580-544-0000' for gc 6.99999988012741days in the future
I0909 09:51:46.010838   571 gc.cpp:56] Scheduling '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_w02LJn/meta/slaves/20140909-095140-1148889280-43580-544-0/frameworks/20140909-095140-1148889280-43580-544-0000' for gc 6.99999987967704days in the future
I0909 09:51:46.010892   571 status_update_manager.cpp:282] Closing status update streams for framework 20140909-095140-1148889280-43580-544-0000
*** Aborted at 1410270706 (unix time) try ""date -d @1410270706"" if you are using GNU date ***
PC: @           0x504973 mesos::internal::tests::Cluster::Slaves::shutdown()
*** SIGSEGV (@0x0) received by PID 544 (TID 0x7f61f2183840) from PID 0; stack trace: ***
    @     0x7f61f0344130 (unknown)
    @           0x504973 mesos::internal::tests::Cluster::Slaves::shutdown()
    @           0x797ff4 mesos::internal::tests::MesosTest::ShutdownSlaves()
    @           0x797a69 mesos::internal::tests::ContainerizerTest<>::TearDown()
    @           0x984253 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0x97b4d0 testing::Test::Run()
    @           0x97b5ae testing::TestInfo::Run()
    @           0x97b6b5 testing::TestCase::Run()
    @           0x97b958 testing::internal::UnitTestImpl::RunAllTests()
    @           0x97bbe7 testing::UnitTest::Run()
    @           0x491beb main
    @     0x7f61eeed6af5 __libc_start_main
    @           0x4a01f9 (unknown)
make[3]: *** [check-local] Segmentation fault (core dumped)
{noformat}","20/Jan/16 22:59;greggomann;This error is not an issue any longer. It was present in 0.20.0: https://github.com/apache/mesos/blob/0.20.0/src/tests/cluster.hpp#L530 but was solved with commit [977b80ae5e9f818648adc7b9f6d959264a84a9fe|https://github.com/apache/mesos/commit/977b80ae5e9f818648adc7b9f6d959264a84a9fe].",,,,,,,,,,,,,,,,,,,,,,,,,,
Improve framework rate limiting by imposing the max number of outstanding messages per framework principal,MESOS-1578,12726862,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,xujyan,xujyan,xujyan,11/Jul/14 19:05,07/Aug/14 07:20,29/Oct/20 16:32,07/Aug/14 07:20,,,,,,,,,0.20.0,,,,,,,,,,,0,,,,,,,,,"* Rate limits config takes a configurable *capacity* for each principal.
* To ensure that Master maintain the message order of a framework it's important that Master sends an FrameworkErrorMessage back to the scheduler to ask it to abort.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-07-31 22:34:19.659,,,false,MESOS-1306,,,,,,,,,,,,,,,,,404969,,,Thu Aug 07 07:20:43 UTC 2014,,,,,,,"0|i00cvz:",76,,,,,,,,,,,Q3 Sprint 1,Q3 Sprint 2,,,,,,,,,,5.0,,0.20.0,,,,,,,,,"31/Jul/14 22:34;dhamon;Asking a framework to abort when it hits capacity without first giving it a chance to back-off seems aggressive and unfriendly. This is especially the case as operators can set a rate limit on a framework that doesn't have any notion that it's being rate-limited.

Consider sending some messages (non-fatal) to the framework when incoming messages are rate-limited so they can track it and respond before we abort them.","06/Aug/14 00:01;xujyan;[~dhamon]'s concern is captured in MESOS-1664 which will be addressed in the next release.","06/Aug/14 00:02;xujyan;https://reviews.apache.org/r/24343/","07/Aug/14 07:20;xujyan;commit 4a6e69e69e04caba58dae8fe656ad2d81a8a6eb5
Author: Jiang Yan Xu <yan@jxu.me>
Date:   Wed Aug 6 16:31:53 2014 -0700

    Improved framework rate limiting by imposing the max number of outstanding messages per framework principal.
    
    Review: https://reviews.apache.org/r/24343",,,,,,,,,,,,,,,,,,,,,,,,
Race between executor exited event and launch task can cause overcommit of resources,MESOS-1466,12720252,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,mzhu,vinodkone,vinodkone,10/Jun/14 00:32,27/Mar/18 13:30,29/Oct/20 16:32,27/Mar/18 13:30,,,,,,,,,,,,,,,allocation,master,,,,0,reliability,twitter,,,,,,,"The following sequence of events can cause an overcommit

--> Launch task is called for a task whose executor is already running

--> Executor's resources are not accounted for on the master

--> Executor exits and the event is enqueued behind launch tasks on the master

--> Master sends the task to the slave which needs to commit for resources for task and the (new) executor.

--> Master processes the executor exited event and re-offers the executor's resources causing an overcommit of resources.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-1654,,,,,,,,,MESOS-1674,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-08-01 23:01:29.792,,,false,MESOS-1961,,,,,,,,,,,,,,,,,398451,,,Tue Mar 27 13:30:03 UTC 2018,,,,,,,"0|i200un:",145,,,,,vinodkone,,,,,,Q3 Sprint 3,Q3 Sprint 4,,,,,,,,,,8.0,,,,,,,,,,,"01/Aug/14 23:01;bmahler;Linking this as a potential blocker for MESOS-1654 as this overcommit bug will be problematic in the presence of ephemeral port allocations. 

A possible fix:

(1) Add an overcommit check in the slave when launching a task, if an overcommit occurs, then the slave will reject the task and effect a re-registration with the master.

(2) When the master reconciles a re-registering slave, add a check for executors present on the slave but missing on the master (we currently don't look for this).","11/Aug/14 18:54;dhamon;Adding story points as 8 to capture uncertainty. If there are specific things to be done to get more clarity around a solution here, please feel free to break this down into other tasks to capture that.","19/Aug/14 02:30;bmahler;We're going to proceed with a mitigation of this by rejecting tasks once the slave is overcommitted:
https://issues.apache.org/jira/browse/MESOS-1721

However, we would also like to ensure that this kind of race is not possible. One solution is to use master acknowledgments for executor exits:

(1) When an executor terminates (or the executor could not be launched: MESOS-1720), we send an exited executor message.
(2) The master acknowledges these message.
(3) The slave will not accept tasks for unacknowledged terminal executors (this must include those executors that could not be launched, per MESOS-1720).

The result of this is that a new executor cannot be launched until the master is aware of the old executor exiting.","27/Mar/18 13:30;bennoe;If I understand the issue correctly, this race seems to have been eliminated as a side-effect of introducing the `launch_executor` flag in Mesos 1.5:

When the master sends the `RunTaskMessage` to the agent, it thinks that the specified executor is still running on the agent, so it will set `launch_executor = false`:
{noformat}
// src/master/master.cpp:3841
bool Master::isLaunchExecutor(
    const ExecutorID& executorId,
    Framework* framework,
    Slave* slave) const
{
  CHECK_NOTNULL(framework);
  CHECK_NOTNULL(slave);

  if (!slave->hasExecutor(framework->id(), executorId)) {
    CHECK(!framework->hasExecutor(slave->id, executorId))
      << ""Executor '"" << executorId
      << ""' known to the framework "" << *framework
      << "" but unknown to the agent "" << *slave;

    return true;
  }

  return false;
}{noformat}
On the slave, when the executor doesn't exist anymore, the task is dropped with reason `REASON_EXECUTOR_TERMINATED`:
{noformat}
// src/slave/slave.cpp:2881

        // Master does not want to launch executor.
        if (executor == nullptr) {
          // Master wants no new executor launched and there is none running on
          // the agent. This could happen if the task expects some previous
          // tasks to launch the executor. However, the earlier task got killed
          // or dropped hence did not launch the executor but the master doesn't
          // know about it yet because the `ExitedExecutorMessage` is still in
          // flight. In this case, we will drop the task.
          //
          // We report TASK_DROPPED to the framework because the task was
          // never launched. For non-partition-aware frameworks, we report
          // TASK_LOST for backward compatibility.
          mesos::TaskState taskState = TASK_DROPPED;
          if (!protobuf::frameworkHasCapability(
              frameworkInfo, FrameworkInfo::Capability::PARTITION_AWARE)) {
            taskState = TASK_LOST;
          }

          foreach (const TaskInfo& _task, tasks) {
            const StatusUpdate update = protobuf::createStatusUpdate(
                frameworkId,
                info.id(),
                _task.task_id(),
                taskState,
                TaskStatus::SOURCE_SLAVE,
                id::UUID::random(),
                ""No executor is expected to launch and there is none running"",
                TaskStatus::REASON_EXECUTOR_TERMINATED,
                executorId);

            statusUpdate(update, UPID());
          }

          // We do not send `ExitedExecutorMessage` here because the expectation
          // is that there is already one on the fly to master. If the message
          // gets dropped, we will hopefully reconcile with the master later.

          return;
        }{noformat}",,,,,,,,,,,,,,,,,,,,,,,,
SlaveRecoveryTest/0.MultipleFrameworks is flaky,MESOS-1365,12714052,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Minor,Cannot Reproduce,greggomann,dhamon,dhamon,13/May/14 22:57,29/Jan/16 22:36,29/Oct/20 16:32,29/Jan/16 22:36,,,,,,,,,,,,,,,test,,,,,0,flaky,flaky-test,mesosphere,,,,,,"--gtest_repeat=-1 --gtest_shuffle --gtest_break_on_failure

{noformat}
[ RUN      ] SlaveRecoveryTest/0.MultipleFrameworks
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0513 15:42:05.931761  4320 exec.cpp:131] Version: 0.19.0
I0513 15:42:05.936698  4340 exec.cpp:205] Executor registered on slave 20140513-154204-16842879-51872-13062-0
Registered executor on artoo
Starting task 51991f97-f5fd-4905-ad0f-02668083af7c
Forked command at 4367
sh -c 'sleep 1000'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0513 15:42:06.915061  4408 exec.cpp:131] Version: 0.19.0
I0513 15:42:06.931149  4435 exec.cpp:205] Executor registered on slave 20140513-154204-16842879-51872-13062-0
Registered executor on artoo
Starting task eaf5d8d6-3a6c-4ee1-84c1-fae20fb1df83
sh -c 'sleep 1000'
Forked command at 4439
I0513 15:42:06.998332  4340 exec.cpp:251] Received reconnect request from slave 20140513-154204-16842879-51872-13062-0
I0513 15:42:06.998414  4436 exec.cpp:251] Received reconnect request from slave 20140513-154204-16842879-51872-13062-0
I0513 15:42:07.006350  4437 exec.cpp:228] Executor re-registered on slave 20140513-154204-16842879-51872-13062-0
Re-registered executor on artoo
I0513 15:42:07.027039  4337 exec.cpp:378] Executor asked to shutdown
Shutting down
Sending SIGTERM to process tree at pid 4367
Killing the following process trees:
[ 
-+- 4367 sh -c sleep 1000 
 \--- 4368 sleep 1000 
]
../../src/tests/slave_recovery_tests.cpp:2807: Failure
Value of: status1.get().state()
  Actual: TASK_FAILED
Expected: TASK_KILLED

Program received signal SIGSEGV, Segmentation fault.
testing::UnitTest::AddTestPartResult (this=0x154dac0 <testing::UnitTest::GetInstance()::instance>, result_type=testing::TestPartResult::kFatalFailure, file_name=0xeb6b6c ""../../src/tests/slave_recovery_tests.cpp"", line_number=2807, message=..., os_stack_trace=...) at gmock-1.6.0/gtest/src/gtest.cc:3795
3795          *static_cast<volatile int*>(NULL) = 1;
(gdb) bt
#0  testing::UnitTest::AddTestPartResult (this=0x154dac0 <testing::UnitTest::GetInstance()::instance>, result_type=testing::TestPartResult::kFatalFailure, file_name=0xeb6b6c ""../../src/tests/slave_recovery_tests.cpp"", line_number=2807, message=..., os_stack_trace=...) at gmock-1.6.0/gtest/src/gtest.cc:3795
#1  0x0000000000df98b9 in testing::internal::AssertHelper::operator= (this=0x7fffffffb860, message=...) at gmock-1.6.0/gtest/src/gtest.cc:356
#2  0x0000000000cdfa57 in SlaveRecoveryTest_MultipleFrameworks_Test<mesos::internal::slave::MesosContainerizer>::TestBody (this=0x1954db0) at ../../src/tests/slave_recovery_tests.cpp:2807
#3  0x0000000000e22583 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void> (object=0x1954db0, method=&virtual testing::Test::TestBody(), location=0xed0af0 ""the test body"") at gmock-1.6.0/gtest/src/gtest.cc:2090
#4  0x0000000000e12467 in testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void> (object=0x1954db0, method=&virtual testing::Test::TestBody(), location=0xed0af0 ""the test body"") at gmock-1.6.0/gtest/src/gtest.cc:2126
#5  0x0000000000e010d5 in testing::Test::Run (this=0x1954db0) at gmock-1.6.0/gtest/src/gtest.cc:2161
#6  0x0000000000e01ceb in testing::TestInfo::Run (this=0x158cf80) at gmock-1.6.0/gtest/src/gtest.cc:2338
#7  0x0000000000e02387 in testing::TestCase::Run (this=0x158a880) at gmock-1.6.0/gtest/src/gtest.cc:2445
#8  0x0000000000e079ed in testing::internal::UnitTestImpl::RunAllTests (this=0x1558b40) at gmock-1.6.0/gtest/src/gtest.cc:4237
#9  0x0000000000e1ec83 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (object=0x1558b40, method=(bool (testing::internal::UnitTestImpl::*)(testing::internal::UnitTestImpl * const)) 0xe07700 <testing::internal::UnitTestImpl::RunAllTests()>, 
    location=0xed1219 ""auxiliary test code (environments or event listeners)"") at gmock-1.6.0/gtest/src/gtest.cc:2090
#10 0x0000000000e14217 in testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (object=0x1558b40, method=(bool (testing::internal::UnitTestImpl::*)(testing::internal::UnitTestImpl * const)) 0xe07700 <testing::internal::UnitTestImpl::RunAllTests()>, 
    location=0xed1219 ""auxiliary test code (environments or event listeners)"") at gmock-1.6.0/gtest/src/gtest.cc:2126
#11 0x0000000000e076d7 in testing::UnitTest::Run (this=0x154dac0 <testing::UnitTest::GetInstance()::instance>) at gmock-1.6.0/gtest/src/gtest.cc:3872
#12 0x0000000000b99887 in main (argc=1, argv=0x7fffffffd9f8) at ../../src/tests/main.cpp:107
(gdb) frame 2
#2  0x0000000000cdfa57 in SlaveRecoveryTest_MultipleFrameworks_Test<mesos::internal::slave::MesosContainerizer>::TestBody (this=0x1954db0) at ../../src/tests/slave_recovery_tests.cpp:2807
2807      ASSERT_EQ(TASK_KILLED, status1.get().state());
(gdb) p status1
$1 = {data = {<std::__shared_ptr<process::Future<mesos::TaskStatus>::Data, 2>> = {_M_ptr = 0x1963140, _M_refcount = {_M_pi = 0x198a620}}, <No data fields>}}
(gdb) p status1.get()
$2 = (const mesos::TaskStatus &) @0x7fffdc5bf5f0: {<google::protobuf::Message> = {<google::protobuf::MessageLite> = {_vptr$MessageLite = 0x7ffff74bc940 <vtable for mesos::TaskStatus+16>}, <No data fields>}, static kTaskIdFieldNumber = 1, static kStateFieldNumber = 2, static kMessageFieldNumber = 4, 
  static kDataFieldNumber = 3, static kSlaveIdFieldNumber = 5, static kTimestampFieldNumber = 6, _unknown_fields_ = {fields_ = 0x0}, task_id_ = 0x7fffdc5ce9a0, message_ = 0x7fffdc5f5880, data_ = 0x154b4b0 <google::protobuf::internal::kEmptyString>, slave_id_ = 0x7fffdc59c4f0, timestamp_ = 1429688582.046252, 
  state_ = 3, _cached_size_ = 0, _has_bits_ = {55}, static default_instance_ = 0x0}
(gdb) p status1.get().state()
$3 = mesos::TASK_FAILED
(gdb) list
2802      // Kill task 1.
2803      driver1.killTask(task1.task_id());
2804
2805      // Wait for TASK_KILLED update.
2806      AWAIT_READY(status1);
2807      ASSERT_EQ(TASK_KILLED, status1.get().state());
2808
2809      // Kill task 2.
2810      driver2.killTask(task2.task_id());
2811
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-05-23 17:56:57.371,,,false,MESOS-2174,,,,,,,,,,,,,,,,,392365,,,Fri Jan 29 22:36:04 UTC 2016,,,,,,,"0|i00eif:",275,,,,,vinodkone,,,,,,Q2'14 Sprint 2,Mesosphere Sprint 27,,,,,,,,,,1.0,,,,,,,,,,,"23/May/14 17:56;jieyu;https://reviews.apache.org/r/21871/","23/May/14 18:09;jieyu;Looks like the second offer, which is expected to be received by sched2, can be sent to sched1. If we don't decline the offer in sched1, sched2 may never receive the offer.","10/Feb/15 18:48;nnielsen;{code}
[ RUN      ] SlaveRecoveryTest/0.MultipleFrameworks
I0210 14:21:28.372095  5984 exec.cpp:145] Version: 0.22.0
I0210 14:21:28.519111  6000 exec.cpp:219] Executor registered on slave 20150210-142127-3176252227-43005-2587-S0
Shutdown timeout is set to 3secsRegistered executor on proserpina.apache.org
Starting task 33c005df-cb5b-4dbb-8087-7eb251158734
sh -c 'sleep 1000'
Forked command at 6015
Failed to chdir into work directory '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_DY1KVn/slaves/20150210-142127-3176252227-43005-2587-S0/frameworks/20150210-142127-3176252227-43005-2587-0001/executors/eda2219a-163a-47cb-a319-934111319425/runs/59373fb8-9025-46da-8171-f4f26d95aec8': No such file or directory
I0210 14:21:29.099647  6003 exec.cpp:265] Received reconnect request from slave 20150210-142127-3176252227-43005-2587-S0
I0210 14:21:29.297351  6005 exec.cpp:242] Executor re-registered on slave 20150210-142127-3176252227-43005-2587-S0
Re-registered executor on proserpina.apache.org
2015-02-10 14:21:30,410:2587(0x2af435211700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40348] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-02-10 14:21:33,746:2587(0x2af435211700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40348] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-02-10 14:21:37,083:2587(0x2af435211700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40348] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-02-10 14:21:40,419:2587(0x2af435211700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40348] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-02-10 14:21:43,756:2587(0x2af435211700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40348] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
../../src/tests/slave_recovery_tests.cpp:2988: Failure
Failed to wait 15secs for reregisterExecutorMessage2
2015-02-10 14:21:47,092:2587(0x2af435211700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40348] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-02-10 14:21:50,429:2587(0x2af435211700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40348] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-02-10 14:21:53,765:2587(0x2af435211700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40348] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-02-10 14:21:57,102:2587(0x2af435211700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40348] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
../../src/tests/cluster.hpp:451: Failure
Failed to wait 15secs for wait
2015-02-10 14:22:00,438:2587(0x2af435211700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40348] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-02-10 14:22:03,775:2587(0x2af435211700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40348] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-02-10 14:22:07,111:2587(0x2af435211700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40348] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-02-10 14:22:10,448:2587(0x2af435211700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40348] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-02-10 14:22:13,784:2587(0x2af435211700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40348] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
../../src/tests/cluster.hpp:451: Failure
Failed to wait 15secs for wait
../../3rdparty/libprocess/include/process/gmock.hpp:356: Failure
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...
    Expected args: message matcher (8-byte object <98-FB 02-CC F0-2A 00-00>, 1-byte object <93>, 1-byte object <82>)
         Expected: to be called once
           Actual: never called - unsatisfied and active
../../3rdparty/libprocess/include/process/gmock.hpp:356: Failure
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...
    Expected args: message matcher (8-byte object <88-BE 02-CC F0-2A 00-00>, 1-byte object <93>, 1-byte object <82>)
         Expected: to be called once
           Actual: never called - unsatisfied and active
[  FAILED  ] SlaveRecoveryTest/0.MultipleFrameworks, where TypeParam = mesos::slave::MesosContainerizer (46509 ms)
[ RUN      ] SlaveRecoveryTest/0.MultipleSlaves
*** Aborted at 1423578134 (unix time) try ""date -d @1423578134"" if you are using GNU date ***
PC: @          0x3557440 (unknown)
*** SIGSEGV (@0x3557440) received by PID 2587 (TID 0x2af0a8a5d700) from PID 55931968; stack trace: ***
    @     0x2af0f45db85a call_chained_handler()
    @     0x2af0f45d8deb os::Linux::chained_handler()
    @     0x2af0f45dc420 JVM_handle_linux_signal
    @     0x2af0f45d8cfe signalHandler()
    @     0x2af0a55e2340 (unknown)
    @          0x3557440 (unknown)
make[4]: *** [check-local] Segmentation fault
make[4]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.22.0/_build/src'
make[3]: *** [check-am] Error 2
make[3]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.22.0/_build/src'
make[2]: *** [check] Error 2
make[2]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.22.0/_build/src'
make[1]: *** [check-recursive] Error 1
make[1]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.22.0/_build'
make: *** [distcheck] Error 1
{code}","10/Feb/15 18:49;nnielsen;Just seen on the ASF CI","26/Jan/16 19:01;greggomann;I've been unable to reproduce this failure thus far. The newer failure reported by [~nnielsen] seems to be due to the executor's failure to {{chdir}} into its work directory. I haven't yet found a viable code path that would lead to the folder being destroyed before the executor uses it. I tried running the test repeatedly in a VM while my machine was under heavy load, and no failures occurred. We have the above failure report, but I don't see any others in the last 6 months on the ASF CI build mailing list. I'll try running the test on a couple different platforms to see if I can reproduce; any advice on reproducing is appreciated!","29/Jan/16 22:36;greggomann;I was unable to reproduce this failure, but did uncover a different issue with this same test on Ubuntu 14.04, tracked in MESOS-4560.",,,,,,,,,,,,,,,,,,,,,,
systemd.slice + cgroup enablement fails in multiple ways. ,MESOS-1195,12707000,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,tstclair,tstclair,tstclair,07/Apr/14 18:24,07/Jul/15 20:12,29/Oct/20 16:32,22/Sep/14 20:39,0.18.0,,,,,,,,0.21.0,,,,,,containerization,,,,,2,,,,,,,,,"When attempting to configure mesos to use systemd slices on a 'rawhide/f21' machine, it fails creating the isolator: 

I0407 12:39:28.035354 14916 containerizer.cpp:180] Using isolation: cgroups/cpu,cgroups/mem
Failed to create a containerizer: Could not create isolator cgroups/cpu: Failed to create isolator: The cpu subsystem is co-mounted at /sys/fs/cgroup/cpu with other subsytems

------ details ------
/sys/fs/cgroup
total 0
drwxr-xr-x. 12 root root 280 Mar 18 08:47 .
drwxr-xr-x.  6 root root   0 Mar 18 08:47 ..
drwxr-xr-x.  2 root root   0 Mar 18 08:47 blkio
lrwxrwxrwx.  1 root root  11 Mar 18 08:47 cpu -> cpu,cpuacct
lrwxrwxrwx.  1 root root  11 Mar 18 08:47 cpuacct -> cpu,cpuacct
drwxr-xr-x.  2 root root   0 Mar 18 08:47 cpu,cpuacct
drwxr-xr-x.  2 root root   0 Mar 18 08:47 cpuset
drwxr-xr-x.  2 root root   0 Mar 18 08:47 devices
drwxr-xr-x.  2 root root   0 Mar 18 08:47 freezer
drwxr-xr-x.  2 root root   0 Mar 18 08:47 hugetlb
drwxr-xr-x.  3 root root   0 Apr  3 11:26 memory
drwxr-xr-x.  2 root root   0 Mar 18 08:47 net_cls
drwxr-xr-x.  2 root root   0 Mar 18 08:47 perf_event
drwxr-xr-x.  4 root root   0 Mar 18 08:47 systemd
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-04-07 18:29:40.457,,,false,MESOS-3007,,,,,,,,,,,,,,,,,385323,,,Mon Sep 22 20:39:56 UTC 2014,,,,,,,"0|i1udqf:",385590,,,,,jieyu,,,,,,Mesos Q3 Sprint 6,,,,,,,,,,,3.0,,0.21.0,,,,,,,,,"07/Apr/14 18:27;tstclair;FWIW the logic fails in cgroups.cpp @: 

if (attached.get().size() > 1) {
      return Error(""The "" + subsystem + "" subsystem is co-mounted at "" +
                   hierarchy + "" with other subsytems"");
}

This logic has no comment, and doesn't really make sense to me, so perhaps a comment is in order why it exists at all 
","07/Apr/14 18:29;nekto0n;Are you trying to make mesos manipulate processes (put/remove) in cgroups, managed by systemd?","07/Apr/14 18:33;tstclair;Correct, but the issue is due to the afore mentioned logic.  ","07/Apr/14 18:37;nekto0n;Is it a good idea? I mean doesn't systemd demand using dbus api to communicate with it?
E.g. docker is using systemd's scopes to manage resources used by containers.","07/Apr/14 18:40;tstclair;It is not required, ""for some time"", to leverage the dbus interface.  
There are tickets to address that separately.  ","07/Apr/14 18:43;nekto0n;I see, thanks!","07/Apr/14 18:47;idownes;The reasoning behind not supporting multiple subsystems mounted together was so different Isolators could take ownership of hierarchies and not stomp on each other. However, cpu and cpuacct are so closely related that they are generally used together (hence systemd's layout) and indeed the CgroupsCpuIsolator uses both. It can readily be updated to support this configuration.","20/May/14 15:47;tstclair;I'll patch shortly to add the single exception. ","22/May/14 02:53;tstclair;There were a couple of issues.
-reviews.apache.org/r/21799/-","29/May/14 19:05;tstclair;[~jieyu] ^ FYI.","25/Jun/14 20:02;tstclair;*DONE* -reviews.apache.org/r/22977/-
*DONE* -reviews.apache.org/r/22979- ","08/Aug/14 17:47;vinodkone;moving this back to ""in progress"" because this is awaiting updates from tim.","15/Aug/14 15:02;tstclair;I'm planning to circle back to this after creation of 0.20 rpms, for the next point release. ","27/Aug/14 18:14;tstclair;Back on *this. ","16/Sep/14 15:03;tstclair;reviews.apache.org/r/25695/","16/Sep/14 15:05;tstclair;[~bhuvan] please eval for 0.20.1 inclusion.  ","16/Sep/14 23:06;bhuvan;[~tstclair] the patch is still not reviewed. i'm going to offload it to 0.21.0.","17/Sep/14 01:13;bhuvan;moving it to 0.21.0, as discussed in reviewboard.
  http://reviews.apache.org/r/25695/","22/Sep/14 20:39;jieyu;commit 31337348cef29719890bffb59fbf8df8b18b39d0
Author: Jie Yu <yujie.jay@gmail.com>
Date:   Fri Sep 19 17:18:38 2014 -0700

    Allowed co-mounted cgroup subsystems to enable Mesos on machines with
    systemd.
    
    Review: https://reviews.apache.org/r/25858",,,,,,,,,
Allocator should make an allocation decision per slave instead of per framework/role.,MESOS-1119,12702210,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,vinodkone,vinodkone,vinodkone,18/Mar/14 18:25,08/Aug/14 06:33,29/Oct/20 16:32,08/Aug/14 06:33,,,,,,,,,0.20.0,,,,,,allocation,,,,,1,,,,,,,,,"Currently the Allocator::allocate() code loops through roles and frameworks (based on DRF sort) and allocates *all* slaves resources to the first framework.

This logic should be a bit inversed. Instead, the slave should go through each slave, allocate it a role/framework and update the DRF shares.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MESOS-1581,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-05-29 18:02:30.888,,,false,MESOS-1596,,,,,,,,,,,,,,,,,380550,,,Fri Aug 08 06:33:33 UTC 2014,,,,,,,"0|i00cvj:",74,,,,,,,,,,,Q3 Sprint 1,Q3 Sprint 2,,,,,,,,,,2.0,,,,,,,,,,,"29/May/14 18:02;bernd-mesos;Can you elaborate on the reasons for this change and the specifics, please? How does your proposal work better and why? Pointers to literature? DRF = Dominant Resource Fairness?","30/May/14 21:05;adam-mesos;Here's the dev@ thread that prompted this JIRA:
https://mail-archives.apache.org/mod_mbox/mesos-dev/201403.mbox/%3cCAAkWvAxRjJHtB=N1FnDNvE3NJhB-B+WJZxXfGK=e2L67Eo+SLg@mail.gmail.com%3e","06/Aug/14 00:50;vinodkone;https://reviews.apache.org/r/24356/","08/Aug/14 06:33;vinodkone;commit d376f05fea7f7432a287a7b87c5d1fe44dae01c4
Author: Vinod Kone <vinodkone@gmail.com>
Date:   Fri Jul 18 14:11:14 2014 -0700

    Fixed allocator to do allocations per slave rather than per framework.
    
    Review: https://reviews.apache.org/r/24356
",,,,,,,,,,,,,,,,,,,,,,,,
Update semantics of when framework registered()/reregistered() get called,MESOS-786,12676707,Bug,Resolved,MESOS,Mesos,software,benjaminhindman,Distributed cluster manager.,http://mesos.apache.org/,Major,Fixed,vinodkone,vinodkone,vinodkone,30/Oct/13 19:39,05/Oct/16 20:58,29/Oct/20 16:32,07/Jul/15 19:35,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,"Current semantics:

1) Framework connects w/ master very first time --> registered()
2) Framework reconnects w/ same master after a zk blip --> reregistered()
3) Framework reconnects w/ failed over master --> registered()
4) Failed over framework connects w/ same master --> registered()
5) Failed over framework connects w/ failed over master --> registered() 

Updated semantics:

Everything same except 
3) Framework reconnects w/ failed over master --> reregistered()",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-10-30 21:33:29.778,,,false,MESOS-2288,,,,,,,,,,,,,,,,,356139,,,Mon Jul 27 20:53:14 UTC 2015,,,,,,,"0|i001kf:",356427,,,,,,,,,,,Twitter Mesos Q2 Sprint 6,Twitter Mesos Q3 Sprint 1,,,,,,,,,,3.0,,,,,,,,,,,"30/Oct/13 21:33;wfarner;Are there use cases you're thinking of that push for (2) and (3) to be different from the framework's perspective?  The 'state machine' i had in mind was something like:

I assumed this was the call flow:
{noformat}
 Following+-->Registered+--->Disconnected+-------+
                                   ^             |
                                   |             |
                                   +             |
                             Re-registered<------+
{noformat}

In this flow, a framework can use registered as a one-time startup signal, and re-registered/disconnected are advisory.





","30/Oct/13 23:35;vinodkone;The *updated* semantics are supposed to guarantee that both 2) and 3) will result in reregistered() call back. And I think that follows your state diagram? Did I understand your question correctly?","30/Oct/13 23:47;wfarner;Ahh, i misunderstood the contents of the description.  Yes, the updated semantics sound more reasonable.","17/Mar/15 18:03;vinodkone;We will likely update these semantics with HTTP API as it provides a clean start for frameworks.","07/Jul/15 19:35;vinodkone;I've updated the design doc https://docs.google.com/document/d/1pnIY_HckimKNvpqhKRhbc9eSItWNFT-priXh_urR-T0/edit# with the new subscription semantics.

In a nutshell, with HTTP API there is only a SUBSCRIBE call (replacing register/reregister) and a SUBSCRIBED event (replaceing registered/re-registered).

Please take a look at the design doc for further details.","27/Jul/15 20:53;xujyan;Fixed by MESOS-3088 and MESOS-2910.",,,,,,,,,,,,,,,,,,,,,,
[Front-end]: Minor issues connected with localization,DATALAB-2087,13333933,Bug,Closed,DATALAB,Apache DataLab,software,Liaskovskyi,"DataLab is a platform for creating self-service, exploratory data science environments in the cloud using best-of-breed data science tools.",https://datalab.incubator.apache.org,Minor,Done,dgnatyshyn,vvitanska,vvitanska,06/Oct/20 13:35,08/Oct/20 10:22,29/Oct/20 16:32,08/Oct/20 10:22,v.2.5,,,,,,,,v.2.5,,,,,,DataLab Main,,,,,0,AWS,AZURE,Debian,Front-end,GCP,pull-request-available,RedHat,,"*1.*

*Preconditions:*
 1. Billing is available on DataLab Web UI

*Steps to reproduce:*
 1. Go to 'List of Resources' page

*Actual result:*
 1. Billing on the 'List of Resources' page is not localized

*Expected result:*
 1. Billing on the 'List of Resources' page is localized
----
*2.* Convert date period according to selected language for 'Billing report' page
----
*3. (-)* Convey language key to back-end for billing export (will do it in branch with back-end fix) - it will be done in task 2089.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,DATALAB-2089,,,,,,,,,,"06/Oct/20 13:27;vvitanska;Billing localization.png;https://issues.apache.org/jira/secure/attachment/13013054/Billing+localization.png","06/Oct/20 13:33;vvitanska;p2-billing report page.png;https://issues.apache.org/jira/secure/attachment/13013053/p2-billing+report+page.png",,,,2.0,,,,,,,,,,,,,,,,,,,,2020-10-07 09:56:55.025,,,false,DATALAB-2042,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 08 10:22:46 UTC 2020,,,,,,,"0|z0jggo:",9223372036854775807,,,,,,,,,,,Apache DataLab release 2.5,,,,,,,,,,,4.0,,,,,,,,,,,"07/Oct/20 09:56;dgnatyshyn;Changed date format in notebook and cluster details popups","08/Oct/20 10:22;vvitanska;*Closed:*

Commit ID 3ff30a6dc33078e514654cc51ee0e19ba1d7f430.",,,,,,,,,,,,,,,,,,,,,,,,,,
[Environment management page]: Drop down list values is broken,DATALAB-2073,13330200,Bug,Closed,DATALAB,Apache DataLab,software,Liaskovskyi,"DataLab is a platform for creating self-service, exploratory data science environments in the cloud using best-of-breed data science tools.",https://datalab.incubator.apache.org,Minor,Done,dgnatyshyn,vvitanska,vvitanska,30/Sep/20 15:56,07/Oct/20 11:39,29/Oct/20 16:32,07/Oct/20 11:39,,,,,,,,,v.2.5,,,,,,DataLab Main,,,,,0,AWS,AZURE,Debian,Front-end,GCP,pull-request-available,RedHat,,"*Preconditions:*

1. User is located on 'Environment management' page

*Steps to reproduce:*
 # Expand the filter header
 # Click 'Project'/'Endpoint' drop down list

*Actual result:*
 # Values of dropdown list is broken
 # A part of dropdown list name is visible in the right side

*Expected:*
 # Values of dropdown list is not broken
 # A part of dropdown list name is not visible in the right side
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/20 15:56;vvitanska;Administration.png;https://issues.apache.org/jira/secure/attachment/13012340/Administration.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,DATALAB-461,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 07 11:39:51 UTC 2020,,,,,,,"0|z0j1xs:",9223372036854775807,,,,,,,,,,,Apache DataLab release 2.5,,,,,,,,,,,8.0,,,,,,,,,,,"07/Oct/20 11:39;vvitanska;*Closed:*

Commit ID 7c484be22a0d93e915e4904613c6118c7a1d878.",,,,,,,,,,,,,,,,,,,,,,,,,,,
Libraries are not installed due to permission absence to the directory,DATALAB-1250,13266820,Bug,Open,DATALAB,Apache DataLab,software,Liaskovskyi,"DataLab is a platform for creating self-service, exploratory data science environments in the cloud using best-of-breed data science tools.",https://datalab.incubator.apache.org,Major,,mykolabodnar,vvitanska,vvitanska,07/Nov/19 12:20,04/Sep/20 07:58,29/Oct/20 16:32,,,,,,,,,,,,,,,,DataLab Main,,,,,0,2.3_old,AWS,AZURE,Debian,DevOps,GCP,RedHat,,"*Preconditions:*
 # DLab is deployed on K8S
 # Notebook is created

*Steps to reproduce:*
 # Go to 'resources_list' page
 # Click action menu for notebook
 # Choose 'manage libraries'
 # Choose available resource in 'Select resource' drop down list
 # Select apt/Yum/Python/Others groups
 # Choose available libraries
 # Click installed

*Actual result:*
 # Docker runs with '0'
 # Libraries installation failed

*Expected result:*
 # Docker runs with '0'
 # Libraries installation is successful

For java and r package groups I could not check because there were issues not from our side.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Nov/19 12:16;vvitanska;Dlab UI.png;https://issues.apache.org/jira/secure/attachment/12985211/Dlab+UI.png","07/Nov/19 12:18;vvitanska;Docker log.png;https://issues.apache.org/jira/secure/attachment/12985210/Docker+log.png",,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,DATALAB-756,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-11-07 12:20:13.0,,,,,,,"0|z08cu8:",9223372036854775807,,,,,,,,,,,Apache DataLab release 2.6,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provisioning service have no access to response files,DATALAB-1240,13266569,Bug,Open,DATALAB,Apache DataLab,software,Liaskovskyi,"DataLab is a platform for creating self-service, exploratory data science environments in the cloud using best-of-breed data science tools.",https://datalab.incubator.apache.org,Major,,mykolabodnar,vvitanska,vvitanska,06/Nov/19 12:10,04/Sep/20 07:58,29/Oct/20 16:32,,,,,,,,,,,,,,,,DataLab Main,,,,,0,2.3_old,AWS,AZURE,Debian,GCP,RedHat,,,"*Preconditions:*
1. Notebook is created 

*Steps to reproduce:*
1. Go to 'resources_list' page
2. Click action menu for notebook
3. Choose 'Manage libraries'
4. Choose available resource in 'Select resource' drop down list

*Actual result:*
1. Available lib list getting is stuck on UI
2. User is not able to install library

*Expected result:*
1. User is able to install library


Could not find file because it finds from dlab-user, but this json is available for root.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/19 12:08;vvitanska;file1.png;https://issues.apache.org/jira/secure/attachment/12985050/file1.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,DATALAB-756,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-11-06 12:10:24.0,,,,,,,"0|z08bag:",9223372036854775807,,,,,,,,,,,Apache DataLab release 2.6,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Scheduler]: 'Default timezone_offset' is rewritten if switching scheduler types for notebook,DATALAB-1101,13256369,Bug,Closed,DATALAB,Apache DataLab,software,Liaskovskyi,"DataLab is a platform for creating self-service, exploratory data science environments in the cloud using best-of-breed data science tools.",https://datalab.incubator.apache.org,Minor,Done,dgnatyshyn,vira_vitanska@epam.com,vira_vitanska@epam.com,12/Sep/19 15:08,05/Oct/20 12:35,29/Oct/20 16:32,05/Oct/20 12:35,,,,,,,,,,,,,,,DataLab Main,,,,,0,2.3_old,AWS,AZURE,Back-end,Debian,GCP,pull-request-available,RedHat,"*Preconditions:*

1. Notebook is created (in 'running' or 'stopped' statuses)

*Steps to reproduce:*
 # Turn on scheduler by inactivity for notebook
 # Click 'Save' button
 # Turn on scheduler by time

*Actual result:*

1. Value 'Select offset' is absent ('Z' in F12)

*Expected result:*

1. Value 'Select offset' is default value of your browser",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/19 15:08;vira_vitanska@epam.com;Select offset.png;https://issues.apache.org/jira/secure/attachment/12980206/Select+offset.png",,,,,1.0,,,,,,,,,,,,,,,,,,,,2019-10-07 09:44:05.146,,,false,DATALAB-460,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 05 12:35:37 UTC 2020,,,,,,,"0|z06l8g:",9223372036854775807,,,,,,,,,,,Apache DataLab release 2.5,,,,,,,,,,,8.0,,,,,,,,,,,"07/Oct/19 09:44;vvitanska;This bug is still reproduced.

And ''Select offset' should be mandatory field

 

 ","30/Sep/20 14:41;vvitanska;Reopened:

Commit ID 7598662be5611d882dadd2b5ee327415c63ade57

This bug is still reproduced.","05/Oct/20 12:35;vvitanska;*Closed:*
Commit ID 7fa39283eefbe43e848eccc936c87b4b952706d7.",,,,,,,,,,,,,,,,,,,,,,,,,
[Terraform]: It is impossible to do any git actions via ungit,DATALAB-1041,13253232,Bug,Open,DATALAB,Apache DataLab,software,Liaskovskyi,"DataLab is a platform for creating self-service, exploratory data science environments in the cloud using best-of-breed data science tools.",https://datalab.incubator.apache.org,Minor,,mykolabodnar,vira_vitanska@epam.com,vira_vitanska@epam.com,27/Aug/19 10:56,04/Sep/20 07:58,29/Oct/20 16:32,,,,,,,,,,,,,,,,DataLab Main,,,,,0,2.3_old,AWS,Debian,DevOps,,,,,"*Preconditions:*
 # User is located on ungit page
 # Git account is created in DLab

*Steps to reproduce:*
 # Put in </home/dlab-user> in ungit search
 # Paste https path to git repository in ungit
 # Click 'clone' button

*Actual result:*
 # For every git actions username and password are required in ungit
 # required credentials are so frequent, that it is impossible to do git actions 

*Expected result:*
 # Git action is done without credential requiring in ungit

Previously git credentials were stored in notebook in <.netrc>, but now this file is absent now",DLAB-terraform branch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,DATALAB-756,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-08-27 10:56:57.0,,,,,,,"0|z062qr:yi",9223372036854775807,,,,,,,,,,,Apache DataLab release 2.6,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PreemptorService failure does not trigger shutdown,AURORA-1511,12901982,Bug,Resolved,AURORA,Aurora,software,jfarrell,"<h3><a name=""Aurora""></a>Welcome to the Aurora project</h3>
<p>Aurora is a service scheduler used to schedule jobs onto Apache Mesos.</p>", http://aurora.apache.org/,Major,Fixed,zmanji,zmanji,zmanji,01/Oct/15 19:49,10/Nov/15 17:59,29/Oct/20 16:32,26/Oct/15 21:04,,,,,,,,,0.10.0,,,,,,,,,,,0,,,,,,,,,"While observing AURORA-1510 in production I noticed the bug caused the {{PreemptorService}} to transition to the FAILED state. The {{/services}} endpoint had:
{noformat}
{
name: ""PreemptorService"",
state: ""FAILED"",
failureCause: ""java.util.ConcurrentModificationException""
},
{noformat}

However the scheduler continued to run. I believe this is a bug.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AURORA-1510,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-10-01 20:01:31.453,,,false,AURORA-720,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 26 21:04:32 UTC 2015,,,,,,,"0|hzzzx3:",9223372036854775807,,,,,,,,,,,Twitter Aurora Q4'15 Sprint 15,,,,,,,,,,,2.0,,,,,,,,,,,"01/Oct/15 20:01;wfarner;I realized this lack of failure handling as well when doing scheduler lifecycle refactoring.  I think we should add a listener to our service managers that will trigger shutdown.","01/Oct/15 20:26;kevints;It looks like we don't register a Listener on our ServiceManagers to shutdown the JVM / execute a shutdown action as described in http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/util/concurrent/ServiceManager.html. I think we should adopt our use of the library to more-or-less copy the pattern presented in ServiceManager's javadoc.
I think this is a regression, as such I would block 0.10.0 until this is released.
","25/Oct/15 06:43;zmanji;Review @ https://reviews.apache.org/r/39631/","26/Oct/15 21:04;zmanji;{noformat}
commit e6f940b245b903c30c1c05cc18769919e5915ba5
Author: Zameer Manji <zmanji@apache.org>
Date:   Mon Oct 26 14:03:52 2015 -0700

    Add listener to trigger scheduler shutdown on service failure.
    
    Bugs closed: AURORA-1511
    
    Reviewed at https://reviews.apache.org/r/39631/

 3 files changed, 37 insertions(+), 5 deletions(-)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,
In-progress instances on Update page continue to pulse after update is aborted,AURORA-1445,12859204,Bug,Resolved,AURORA,Aurora,software,jfarrell,"<h3><a name=""Aurora""></a>Welcome to the Aurora project</h3>
<p>Aurora is a service scheduler used to schedule jobs onto Apache Mesos.</p>", http://aurora.apache.org/,Minor,Fixed,joshua.cohen,joshua.cohen,joshua.cohen,26/Aug/15 16:41,10/Nov/15 17:59,29/Oct/20 16:32,02/Sep/15 03:43,,,,,,,,,0.10.0,,,,,,UI,,,,,0,,,,,,,,,"When an update is aborted, any instances that were in progress still have the css class of {{instance-updating}} and as such, continue to pulse.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,AURORA-1374,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 01 17:17:43 UTC 2015,,,,,,,"0|i2jkjb:",9223372036854775807,,,,,,,,,,,Twitter Aurora Q3'15 Sprint 11,,,,,,,,,,,3.0,,,,,,,,,,,"01/Sep/15 17:17;joshua.cohen;https://reviews.apache.org/r/38010/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove duplicate thermos_observer,AURORA-1381,12842355,Bug,Resolved,AURORA,Aurora,software,jfarrell,"<h3><a name=""Aurora""></a>Welcome to the Aurora project</h3>
<p>Aurora is a service scheduler used to schedule jobs onto Apache Mesos.</p>", http://aurora.apache.org/,Minor,Fixed,,brianbrazil,brianbrazil,02/Jul/15 16:19,17/Dec/15 23:14,29/Oct/20 16:32,11/Dec/15 17:47,,,,,,,,,0.11.0,,,,,,,,,,,0,,,,,,,,,"There are currently two thermos_observer python_binaries in the codebase, one in src/main/python/apache/aurora/tools/BUILD and the other in ./src/main/python/apache/thermos/observer/bin/BUILD. This is confusing.

Let's get rid of one of them, I think the latter is the one that's not meant to be used.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AURORA-1403,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-07-06 19:07:13.345,,,false,AURORA-1424,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 11 17:47:10 UTC 2015,,,,,,,"0|i00107:",9223372036854775807,,,,,,,,,,,Twitter Aurora Q2'15 Sprint 8,Twitter Aurora Q2'15 Sprint 9,,,,,,,,,,2.0,,,,,,,,,,,"06/Jul/15 19:07;zmanji;I think this is a side effect of our thermos abstraction. The latter target only looks for checkpoints in a global directory while the former looks inside the executor sandboxes. [~wfarner], [~wickman] What are your thoughts on keeping or removing the target?","06/Jul/15 19:09;brianbrazil;Both copies existed before that change, so I don't think that's the reason for them existing.","06/Jul/15 19:14;zmanji;I see, unfortunately that reason makes it difficult to figure out what to do here.","06/Jul/15 22:59;wfarner;IMHO unless we have a known good reason to keep the one we don't use, we should nuke it.","07/Jul/15 19:35;wickman;Once we move to running the observer in user sandboxes, we only need to keep the thermos observer.  The specialized aurora observer is a temporary hack to allow running a one-per-machine global observer that can detect checkpoints in user sandboxes.","08/Jul/15 09:31;brianbrazil;Is there a reason we can't consolidate now? The duplicate name breaks builds and causes confusion.","22/Jul/15 18:50;wfarner;https://reviews.apache.org/r/36700/","11/Dec/15 17:47;maximk;This appears to be fixed by the linked RB.",,,,,,,,,,,,,,,,,,,,
Referential integrity violation when replaying storage,AURORA-1379,12841792,Bug,Resolved,AURORA,Aurora,software,jfarrell,"<h3><a name=""Aurora""></a>Welcome to the Aurora project</h3>
<p>Aurora is a service scheduler used to schedule jobs onto Apache Mesos.</p>", http://aurora.apache.org/,Major,Fixed,wfarner,kevints,kevints,30/Jun/15 20:47,20/Jul/15 16:43,29/Oct/20 16:32,06/Jul/15 21:21,,,,,,,,,0.9.0,,,,,,Scheduler,,,,,0,,,,,,,,,"At startup in the Vagrant environment I observed:

{noformat}
E0630 20:45:54.427 THREAD1 org.apache.aurora.scheduler.SchedulerLifecycle$9.execute: Caught unchecked exception: org.apache.aurora.scheduler.storage.Storage$StorageExce
ption: 
### Error updating database.  Cause: org.h2.jdbc.JdbcSQLException: Referential integrity constraint violation: ""CONSTRAINT_4BE: PUBLIC.TASKS FOREIGN KEY(SLAVE_ROW_ID) R
EFERENCES PUBLIC.HOST_ATTRIBUTES(ID) (1)""; SQL statement:
DELETE FROM host_attributes
    WHERE host = ? [23503-187]
### The error may involve defaultParameterMap
### The error occurred while setting parameters
### SQL: DELETE FROM host_attributes     WHERE host = ?
### Cause: org.h2.jdbc.JdbcSQLException: Referential integrity constraint violation: ""CONSTRAINT_4BE: PUBLIC.TASKS FOREIGN KEY(SLAVE_ROW_ID) REFERENCES PUBLIC.HOST_ATTR
IBUTES(ID) (1)""; SQL statement:
DELETE FROM host_attributes
    WHERE host = ? [23503-187]
org.apache.aurora.scheduler.storage.Storage$StorageException: 
### Error updating database.  Cause: org.h2.jdbc.JdbcSQLException: Referential integrity constraint violation: ""CONSTRAINT_4BE: PUBLIC.TASKS FOREIGN KEY(SLAVE_ROW_ID) R
EFERENCES PUBLIC.HOST_ATTRIBUTES(ID) (1)""; SQL statement:
DELETE FROM host_attributes
    WHERE host = ? [23503-187]
### The error may involve defaultParameterMap
### The error occurred while setting parameters
### SQL: DELETE FROM host_attributes     WHERE host = ?
### Cause: org.h2.jdbc.JdbcSQLException: Referential integrity constraint violation: ""CONSTRAINT_4BE: PUBLIC.TASKS FOREIGN KEY(SLAVE_ROW_ID) REFERENCES PUBLIC.HOST_ATTR
IBUTES(ID) (1)""; SQL statement:
DELETE FROM host_attributes
    WHERE host = ? [23503-187]
        at org.apache.aurora.scheduler.storage.db.DbStorage.write(DbStorage.java:144)
        at org.mybatis.guice.transactional.TransactionalMethodInterceptor.invoke(TransactionalMethodInterceptor.java:101)
        at com.twitter.common.inject.TimedInterceptor.invoke(TimedInterceptor.java:87)
        at org.apache.aurora.scheduler.storage.log.LogStorage.write(LogStorage.java:632)
        at org.apache.aurora.scheduler.storage.log.LogStorage$3.execute(LogStorage.java:316)
        at org.apache.aurora.scheduler.storage.log.LogStorage$3.execute(LogStorage.java:313)
        at org.apache.aurora.scheduler.storage.log.LogStorage.replay(LogStorage.java:525)
        at org.apache.aurora.scheduler.storage.log.LogStorage.access$1200(LogStorage.java:116)
        at org.apache.aurora.scheduler.storage.log.LogStorage$21$1.execute(LogStorage.java:503)
        at org.apache.aurora.scheduler.storage.log.LogStorage$21$1.execute(LogStorage.java:500)
        at org.apache.aurora.scheduler.storage.log.StreamManagerImpl.readFromBeginning(StreamManagerImpl.java:127)
        at org.apache.aurora.scheduler.storage.log.LogStorage$21.execute(LogStorage.java:500)
        at org.apache.aurora.scheduler.storage.Storage$MutateWork$NoResult.apply(Storage.java:131)
        at org.apache.aurora.scheduler.storage.db.DbStorage.bulkLoad(DbStorage.java:165)
        at com.twitter.common.inject.TimedInterceptor.invoke(TimedInterceptor.java:87)
        at org.apache.aurora.scheduler.storage.log.LogStorage.recover(LogStorage.java:496)
        at com.twitter.common.inject.TimedInterceptor.invoke(TimedInterceptor.java:87)
        at org.apache.aurora.scheduler.storage.log.LogStorage$20.execute(LogStorage.java:477)
        at org.apache.aurora.scheduler.storage.Storage$MutateWork$NoResult.apply(Storage.java:131)
        at org.apache.aurora.scheduler.storage.Storage$MutateWork$NoResult$Quiet.apply(Storage.java:148)
        at org.apache.aurora.scheduler.storage.db.DbStorage.write(DbStorage.java:142)
        at org.mybatis.guice.transactional.TransactionalMethodInterceptor.invoke(TransactionalMethodInterceptor.java:101)
        at com.twitter.common.inject.TimedInterceptor.invoke(TimedInterceptor.java:87)
        at org.apache.aurora.scheduler.storage.log.LogStorage.write(LogStorage.java:632)
        at org.apache.aurora.scheduler.storage.log.LogStorage.start(LogStorage.java:471)
        at org.apache.aurora.scheduler.storage.CallOrderEnforcingStorage.start(CallOrderEnforcingStorage.java:92)
        at org.apache.aurora.scheduler.SchedulerLifecycle$6.execute(SchedulerLifecycle.java:252)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-07-01 17:14:40.289,,,false,AURORA-334,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 01 17:14:40 UTC 2015,,,,,,,"0|i001hr:",9223372036854775807,,,,,,,,,,,Twitter Aurora Q2'15 Sprint 6,Twitter Aurora Q2'15 Sprint 7,,,,,,,,,,2.0,,,,,,,,,,,"01/Jul/15 17:14;wfarner;https://reviews.apache.org/r/36096/",,,,,,,,,,,,,,,,,,,,,,,,,,,
Update status page for large jobs is killed by Chrome,AURORA-1345,12836598,Bug,Resolved,AURORA,Aurora,software,jfarrell,"<h3><a name=""Aurora""></a>Welcome to the Aurora project</h3>
<p>Aurora is a service scheduler used to schedule jobs onto Apache Mesos.</p>", http://aurora.apache.org/,Major,Fixed,joshua.cohen,joshua.cohen,joshua.cohen,09/Jun/15 17:08,10/Nov/15 17:59,29/Oct/20 16:32,02/Sep/15 18:33,,,,,,,,,0.10.0,,,,,,UI,,,,,0,,,,,,,,,"When I say ""killed by Chrome"" I mean if left open long enough the page turns into the Chrome ""Aw Snap"" page.

This is presumably due to the continued addition of instance events to the page causing it to OOM and Chrome to kill it. One suggestion made to me by those more familiar with Angular was to add {{track by}} to the {{ng-repeat}} used to build the DOM for the instance events.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,AURORA-1374,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 31 18:51:36 UTC 2015,,,,,,,"0|i000kv:",9223372036854775807,,,,,,,,,,,Twitter Aurora Q3'15 Sprint 11,,,,,,,,,,,5.0,,,,,,,,,,,"31/Aug/15 18:51;joshua.cohen;https://reviews.apache.org/r/37956/",,,,,,,,,,,,,,,,,,,,,,,,,,,
auth_module is not installed in child injector,AURORA-1201,12782319,Bug,Resolved,AURORA,Aurora,software,jfarrell,"<h3><a name=""Aurora""></a>Welcome to the Aurora project</h3>
<p>Aurora is a service scheduler used to schedule jobs onto Apache Mesos.</p>", http://aurora.apache.org/,Blocker,Fixed,kevints,kevints,kevints,16/Mar/15 18:28,16/Mar/15 22:27,29/Oct/20 16:32,16/Mar/15 22:27,,,,,,,,,0.8.0,,,,,,Scheduler,Security,,,,0,,,,,,,,,"The recent patch to enable HTTP Basic Authentication moved construction of ThriftAuthModule to the Jetty child injector, but the user-supplied module was still installed in the parent injector. This leads to a stack trace for anyone using their own {{auth_module}}:

{noformat}
INFO: Connecting to master using authentication (principal: TwitterScheduler).
Exception in thread ""main"" com.google.inject.CreationException: Guice creation errors:

1) No implementation for java.util.Map<org.apache.aurora.auth.CapabilityValidator$Capability, java.lang.String> was bound.
  while locating java.util.Map<org.apache.aurora.auth.CapabilityValidator$Capability, java.lang.String>
    for parameter 1 at com.twitter.aurora.internal.auth.TwitterCapabilityValidator.<init>(TwitterCapabilityValidator.java:28)
  at com.twitter.aurora.internal.auth.TwitterAuthModule.configure(TwitterAuthModule.java:97)

2) No implementation for java.util.Map<org.apache.aurora.auth.CapabilityValidator$Capability, java.lang.String> was bound.
  at com.twitter.aurora.internal.auth.TwitterAuthModule.configure(TwitterAuthModule.java:94)

2 errors
        at com.google.inject.internal.Errors.throwCreationExceptionIfErrorsExist(Errors.java:435)
        at com.google.inject.internal.InternalInjectorCreator.initializeStatically(InternalInjectorCreator.java:154)
        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:106)
        at com.google.inject.Guice.createInjector(Guice.java:95)
        at com.google.inject.Guice.createInjector(Guice.java:83)
        at com.twitter.common.application.AppLauncher.configureInjection(AppLauncher.java:120)
        at com.twitter.common.application.AppLauncher.run(AppLauncher.java:87)
        at com.twitter.common.application.AppLauncher.launch(AppLauncher.java:181)
        at com.twitter.common.application.AppLauncher.launch(AppLauncher.java:142)
        at org.apache.aurora.scheduler.app.SchedulerMain.main(SchedulerMain.java:279)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AURORA-904,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,AURORA-720,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 16 18:59:04 UTC 2015,,,,,,,"0|i0053z:",9223372036854775807,,,,,,,,,,,Twitter Aurora Q1'15 Sprint 5,,,,,,,,,,,2.0,,,,,,,,,,,"16/Mar/15 18:59;kevints;https://reviews.apache.org/r/32118/",,,,,,,,,,,,,,,,,,,,,,,,,,,
The scheduler synchronously writes a backup while writing a snapshot to the replicated log,AURORA-1108,12772885,Bug,Resolved,AURORA,Aurora,software,jfarrell,"<h3><a name=""Aurora""></a>Welcome to the Aurora project</h3>
<p>Aurora is a service scheduler used to schedule jobs onto Apache Mesos.</p>", http://aurora.apache.org/,Critical,Fixed,maximk,wfarner,wfarner,06/Feb/15 01:10,04/May/15 14:02,29/Oct/20 16:32,23/Feb/15 21:08,,,,,,,,,0.8.0,,,,,,Reliability,Scheduler,,,,0,,,,,,,,,"In the course of writing a snapshot to the replicated log, the scheduler may block while writing a snapshot to the disk.  There is no need for this activity to be done synchronously, and doing so causes the write lock to be unnecessarily held for an additional period of time.

From StorageBackup.java:
{code}
    @Override
    public Snapshot createSnapshot() {
      Snapshot snapshot = delegate.createSnapshot();
      if (clock.nowMillis() >= (lastBackupMs + backupIntervalMs)) {
        save(snapshot);
      }
      return snapshot;
    }
{code}

{{StorageBackup}} happens to be the unqualified binding to {{SnapshotStore<Snapshot>}} that is used in {{LogStorage}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-02-19 00:27:26.157,,,false,AURORA-999,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 19 00:27:26 UTC 2015,,,,,,,"0|i00bxr:",9223372036854775807,,,,,,,,,,,Twitter Aurora Q1'15 Sprint 3,,,,,,,,,,,3.0,,,,,,,,,,,"16/Feb/15 18:45;wfarner;Relevant TODO in StorageBackup.java:
{noformat}
 * TODO(William Farner): Perform backups asynchronously.  As written, they are performed in a
 * blocking write operation, which is asking for trouble.
{noformat}","19/Feb/15 00:27;maximk;https://reviews.apache.org/r/31171/",,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove the ""enable_legacy_constraints"" flag.",AURORA-1074,12771409,Bug,Resolved,AURORA,Aurora,software,jfarrell,"<h3><a name=""Aurora""></a>Welcome to the Aurora project</h3>
<p>Aurora is a service scheduler used to schedule jobs onto Apache Mesos.</p>", http://aurora.apache.org/,Major,Fixed,zmanji,zmanji,zmanji,30/Jan/15 19:30,20/Jul/15 16:43,29/Oct/20 16:32,26/Jun/15 18:10,,,,,,,,,0.9.0,,,,,,,,,,,0,,,,,,,,,"As a part of AURORA-184 we added a flag called ""enable_legacy_constraints"" which enables behaviour that injects a host and rack limit into every job.

We should deprecate and remove this flag in some future release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AURORA-174,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,AURORA-1079,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 26 18:10:18 UTC 2015,,,,,,,"0|i250s7:",9223372036854775807,,,,,,,,,,,Twitter Aurora Q2'15 Sprint 6,,,,,,,,,,,2.0,,,,,,,,,,,"23/Jun/15 23:58;zmanji;Review: https://reviews.apache.org/r/35812/","26/Jun/15 18:10;zmanji;{noformat}
commit 2ef6a05e2ec31b4a4a66cf1f255733045c3160a6
Author: Zameer Manji <zmanji@apache.org>
Date:   Fri Jun 26 11:09:06 2015 -0700

    Remove ""enable_legacy_constraints"" flag.
    
    Remove the ""enable_legacy_constraints"" flag and associated behaviour.
    
    Testing Done:
    ./gradlew build -Pq
    
    Bugs closed: AURORA-1074
    
    Reviewed at https://reviews.apache.org/r/35812/

 NEWS                                               |  3 +
 .../configuration/ConfigurationManager.java        | 65 +---------------------
 .../local/simulator/ClusterSimulatorModule.java    |  6 +-
 .../configuration/ConfigurationManagerTest.java    | 29 ----------
 .../org/apache/aurora/scheduler/mesos/Offers.java  |  4 +-
 .../thrift/SchedulerThriftInterfaceTest.java       |  4 +-
 6 files changed, 9 insertions(+), 102 deletions(-)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,
